hydra:
  run:
    # dynamic output directory according to running time and override name
    dir: outputs_lorenz_enn
  job:
    name: ${mode} # name of logfile
    chdir: false # keep current working directory unchanged
    config:
      override_dirname:
        exclude_keys:
          - TRAIN.checkpoint_path
          - TRAIN.pretrained_model_path
          - EVAL.pretrained_model_path
          - mode
          - output_dir
          - log_freq
  sweep:
    # output directory for multirun
    dir: ${hydra.run.dir}
    subdir: ./

# general settings
mode: train # running mode: train/eval
seed: 42
output_dir: ${hydra:run.dir}
TRAIN_BLOCK_SIZE: 16
VALID_BLOCK_SIZE: 32
TRAIN_FILE_PATH: ./datasets/lorenz_training_rk.hdf5
VALID_FILE_PATH: ./datasets/lorenz_valid_rk.hdf5

# model settings
MODEL:
  input_keys: ["states"]
  output_keys: ["pred_states", "recover_states"]

# training settings
TRAIN:
  epochs: 300
  batch_size: 512
  lr_scheduler:
    epochs: ${TRAIN.epochs}
    learning_rate: 0.001
    gamma: 0.995
    by_epoch: true
  optimizer:
    weight_decay: 1e-8
  pretrained_model_path: null
  checkpoint_path: null

# evaluation settings
EVAL:
  batch_size: 512
  pretrained_model_path: null
