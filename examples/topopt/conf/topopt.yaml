hydra:
  run:
    # dynamic output directory
    dir: outputs_topopt/${now:%Y-%m-%d}/${now:%H-%M-%S}/${hydra.job.override_dirname}
  job:
    name: ${mode} # name of logfile
    chdir: false # keep current working direcotry unchaned
    config:
      override_dirname:
        exclude_keys:
          - TRAIN.batch_size
          - TRAIN.epochs
          - TRAIN.learning_rate
          - EVAL.pretrained_model_path_dict
          - EVAL.batch_size
          - EVAL.num_val_step
          - mode
          - vol_coeff
  sweep:
    # output directory for multirun
    dir: ${hydra.run.dir}
    subdir: ./

# general settings
mode: train # running mode: train/eval
seed: 42
output_dir: ${hydra:run.dir}

# set default cases parameters
CASE_PARAM: [[Poisson, 5], [Poisson, 10], [Poisson, 30], [Uniform, null]]

# set data path
DATA_PATH: ./datasets/top_dataset.h5

# model settings
MODEL:
  in_channel: 2
  out_channel: 1
  kernel_size: 3
  filters: [16, 32, 64]
  layers: 2

# other parameters
n_samples: 10000
train_test_ratio: 1.0 # use 10000 original data with different channels for training
vol_coeff: 1 # coefficient for volume fraction constraint in the loss - beta in equation (3) in paper
num_params: 192113 # the number of parameters in Unet specified in paper, used to verify the parameter number

# training settings
TRAIN:
  epochs: 30
  learning_rate: 0.001
  epsilon:
    log_loss: 1.0e-7
    optimizer: 1.0e-7
  batch_size: 64
  eval_during_train: false

# evaluation settings
EVAL:
  pretrained_model_path_dict: null # a dict: {casename1:path1, casename2:path2, casename3:path3, casename4:path4}
  num_val_step: 10 # the number of iteration for each evaluation case
  batch_size: 16