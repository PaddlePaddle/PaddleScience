{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u4e3b\u9875","text":""},{"location":"#paddlescience","title":"PaddleScience","text":"<p>Developed with PaddlePaddle</p> <p> </p> <p>\ud83d\udd25 IJCAI 2024: \u4efb\u610f\u4e09\u7ef4\u51e0\u4f55\u5916\u5f62\u8f66\u8f86\u7684\u98ce\u963b\u5feb\u901f\u9884\u6d4b\u7ade\u8d5b\uff0c\u6b22\u8fce\u62a5\u540d\u53c2\u8d5b\u3002</p>"},{"location":"#_1","title":"\ud83d\udc40\u7b80\u4ecb","text":"<p>PaddleScience \u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6 PaddlePaddle \u5f00\u53d1\u7684\u79d1\u5b66\u8ba1\u7b97\u5957\u4ef6\uff0c\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u80fd\u529b\u548c PaddlePaddle \u6846\u67b6\u7684\u81ea\u52a8(\u9ad8\u9636)\u5fae\u5206\u673a\u5236\uff0c\u89e3\u51b3\u7269\u7406\u3001\u5316\u5b66\u3001\u6c14\u8c61\u7b49\u9886\u57df\u7684\u95ee\u9898\u3002\u652f\u6301\u7269\u7406\u673a\u7406\u9a71\u52a8\u3001\u6570\u636e\u9a71\u52a8\u3001\u6570\u7406\u878d\u5408\u4e09\u79cd\u6c42\u89e3\u65b9\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u7840 API \u548c\u8be6\u5c3d\u6587\u6863\u4f9b\u7528\u6237\u4f7f\u7528\u4e0e\u4e8c\u6b21\u5f00\u53d1\u3002</p> <p></p>"},{"location":"#_2","title":"\ud83d\udcdd\u6848\u4f8b\u5217\u8868","text":"<p>\u6570\u5b66(AI for Math)</p> \u95ee\u9898\u7c7b\u578b \u6848\u4f8b\u540d\u79f0 \u4f18\u5316\u7b97\u6cd5 \u6a21\u578b\u7c7b\u578b \u8bad\u7ec3\u65b9\u5f0f \u6570\u636e\u96c6 \u53c2\u8003\u8d44\u6599 \u76f8\u573a\u65b9\u7a0b Allen-Cahn \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 Data Paper \u5fae\u5206\u65b9\u7a0b \u62c9\u666e\u62c9\u65af\u65b9\u7a0b \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - - \u5fae\u5206\u65b9\u7a0b \u4f2f\u683c\u65af\u65b9\u7a0b \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 Data Paper \u5fae\u5206\u65b9\u7a0b \u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b \u673a\u7406\u9a71\u52a8 PIRBN \u65e0\u76d1\u7763\u5b66\u4e60 - Paper \u5fae\u5206\u65b9\u7a0b \u6d1b\u4f26\u5179\u65b9\u7a0b \u6570\u636e\u9a71\u52a8 Transformer-Physx \u76d1\u7763\u5b66\u4e60 Data Paper \u5fae\u5206\u65b9\u7a0b \u82e5\u65af\u53fb\u65b9\u7a0b \u6570\u636e\u9a71\u52a8 Transformer-Physx \u76d1\u7763\u5b66\u4e60 Data Paper \u7b97\u5b50\u5b66\u4e60 DeepONet \u6570\u636e\u9a71\u52a8 MLP \u76d1\u7763\u5b66\u4e60 Data Paper \u5fae\u5206\u65b9\u7a0b \u68af\u5ea6\u589e\u5f3a\u7684\u7269\u7406\u77e5\u8bc6\u878d\u5408 PDE \u6c42\u89e3 \u673a\u7406\u9a71\u52a8 gPINN \u65e0\u76d1\u7763\u5b66\u4e60 - Paper \u79ef\u5206\u65b9\u7a0b \u6c83\u5c14\u6cf0\u62c9\u79ef\u5206\u65b9\u7a0b \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - Project \u5fae\u5206\u65b9\u7a0b \u5206\u6570\u9636\u5fae\u5206\u65b9\u7a0b \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - - \u5149\u5b64\u5b50 Optical soliton \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - Paper \u5149\u7ea4\u602a\u6ce2 Optical rogue wave \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - Paper \u57df\u5206\u89e3 XPINN \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - Paper <p></p> <p>\u6280\u672f\u79d1\u5b66(AI for Technology)</p> \u95ee\u9898\u7c7b\u578b \u6848\u4f8b\u540d\u79f0 \u4f18\u5316\u7b97\u6cd5 \u6a21\u578b\u7c7b\u578b \u8bad\u7ec3\u65b9\u5f0f \u6570\u636e\u96c6 \u53c2\u8003\u8d44\u6599 \u5b9a\u5e38\u4e0d\u53ef\u538b\u6d41\u4f53 2D \u5b9a\u5e38\u65b9\u8154\u6d41 \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - \u5b9a\u5e38\u4e0d\u53ef\u538b\u6d41\u4f53 2D \u8fbe\u897f\u6d41 \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - \u5b9a\u5e38\u4e0d\u53ef\u538b\u6d41\u4f53 2D \u7ba1\u9053\u6d41 \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - Paper \u5b9a\u5e38\u4e0d\u53ef\u538b\u6d41\u4f53 3D \u8840\u7ba1\u7624 \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 Data Project \u5b9a\u5e38\u4e0d\u53ef\u538b\u6d41\u4f53 \u4efb\u610f 2D \u51e0\u4f55\u4f53\u7ed5\u6d41 \u6570\u636e\u9a71\u52a8 DeepCFD \u76d1\u7763\u5b66\u4e60 - Paper \u975e\u5b9a\u5e38\u4e0d\u53ef\u538b\u6d41\u4f53 2D \u975e\u5b9a\u5e38\u65b9\u8154\u6d41 \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - - \u975e\u5b9a\u5e38\u4e0d\u53ef\u538b\u6d41\u4f53 Re100 2D \u5706\u67f1\u7ed5\u6d41 \u673a\u7406\u9a71\u52a8 MLP \u534a\u76d1\u7763\u5b66\u4e60 Data Paper \u975e\u5b9a\u5e38\u4e0d\u53ef\u538b\u6d41\u4f53 Re100~750 2D \u5706\u67f1\u7ed5\u6d41 \u6570\u636e\u9a71\u52a8 Transformer-Physx \u76d1\u7763\u5b66\u4e60 Data Paper \u53ef\u538b\u7f29\u6d41\u4f53 2D \u7a7a\u6c14\u6fc0\u6ce2 \u673a\u7406\u9a71\u52a8 PINN-WE \u65e0\u76d1\u7763\u5b66\u4e60 Data - \u98de\u884c\u5668\u8bbe\u8ba1 MeshGraphNets \u6570\u636e\u9a71\u52a8 GNN \u76d1\u7763\u5b66\u4e60 Data Paper \u98de\u884c\u5668\u8bbe\u8ba1 \u706b\u7bad\u53d1\u52a8\u673a\u771f\u7a7a\u7fbd\u6d41 \u6570\u636e\u9a71\u52a8 CNN \u76d1\u7763\u5b66\u4e60 Data - \u98de\u884c\u5668\u8bbe\u8ba1 Deep-Flow-Prediction \u6570\u636e\u9a71\u52a8 TurbNetG \u76d1\u7763\u5b66\u4e60 Data Paper \u901a\u7528\u6d41\u573a\u6a21\u62df \u6c14\u52a8\u5916\u5f62\u8bbe\u8ba1 \u6570\u636e\u9a71\u52a8 AMGNet \u76d1\u7763\u5b66\u4e60 Data Paper \u6d41\u56fa\u8026\u5408 \u6da1\u6fc0\u632f\u52a8 \u673a\u7406\u9a71\u52a8 MLP \u534a\u76d1\u7763\u5b66\u4e60 Data Paper \u591a\u76f8\u6d41 \u6c14\u6db2\u4e24\u76f8\u6d41 \u673a\u7406\u9a71\u52a8 BubbleNet \u534a\u76d1\u7763\u5b66\u4e60 Data Paper \u591a\u76f8\u6d41 twophasePINN \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - Paper \u591a\u76f8\u6d41 \u975e\u9ad8\u65af\u6e17\u900f\u7387\u573a\u4f30\u8ba1<sup>coming soon</sup> \u673a\u7406\u9a71\u52a8 cINN \u76d1\u7763\u5b66\u4e60 - Paper \u6d41\u573a\u9ad8\u5206\u8fa8\u7387\u91cd\u6784 2D \u6e4d\u6d41\u6d41\u573a\u91cd\u6784 \u6570\u636e\u9a71\u52a8 tempoGAN \u76d1\u7763\u5b66\u4e60 Train DataEval Data Paper \u6d41\u573a\u9ad8\u5206\u8fa8\u7387\u91cd\u6784 2D \u6e4d\u6d41\u6d41\u573a\u91cd\u6784 \u6570\u636e\u9a71\u52a8 cycleGAN \u76d1\u7763\u5b66\u4e60 Train DataEval Data Paper \u6d41\u573a\u9ad8\u5206\u8fa8\u7387\u91cd\u6784 \u57fa\u4e8eVoronoi\u5d4c\u5165\u8f85\u52a9\u6df1\u5ea6\u5b66\u4e60\u7684\u7a00\u758f\u4f20\u611f\u5668\u5168\u5c40\u573a\u91cd\u5efa \u6570\u636e\u9a71\u52a8 CNN \u76d1\u7763\u5b66\u4e60 Data1Data2Data3 Paper \u6d41\u573a\u9ad8\u5206\u8fa8\u7387\u91cd\u6784 \u57fa\u4e8e\u6269\u6563\u7684\u6d41\u4f53\u8d85\u5206\u91cd\u6784<sup>coming soon</sup> \u6570\u7406\u878d\u5408 DDPM \u76d1\u7763\u5b66\u4e60 - Paper \u6c42\u89e3\u5668\u8026\u5408 CFD-GCN \u6570\u636e\u9a71\u52a8 GCN \u76d1\u7763\u5b66\u4e60 DataMesh Paper \u53d7\u529b\u5206\u6790 1D \u6b27\u62c9\u6881\u53d8\u5f62 \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - - \u53d7\u529b\u5206\u6790 2D \u5e73\u677f\u53d8\u5f62 \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - Paper \u53d7\u529b\u5206\u6790 3D \u8fde\u63a5\u4ef6\u53d8\u5f62 \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 Data Tutorial \u53d7\u529b\u5206\u6790 \u7ed3\u6784\u9707\u52a8\u6a21\u62df \u673a\u7406\u9a71\u52a8 PhyLSTM \u76d1\u7763\u5b66\u4e60 Data Paper \u53d7\u529b\u5206\u6790 2D \u5f39\u5851\u6027\u7ed3\u6784 \u673a\u7406\u9a71\u52a8 EPNN \u65e0\u76d1\u7763\u5b66\u4e60 Train DataEval Data Paper \u53d7\u529b\u5206\u6790\u548c\u9006\u95ee\u9898 3D \u6c7d\u8f66\u63a7\u5236\u81c2\u53d8\u5f62 \u673a\u7406\u9a71\u52a8 MLP \u65e0\u76d1\u7763\u5b66\u4e60 - - \u62d3\u6251\u4f18\u5316 2D \u62d3\u6251\u4f18\u5316 \u6570\u636e\u9a71\u52a8 TopOptNN \u76d1\u7763\u5b66\u4e60 Data Paper \u70ed\u4eff\u771f 1D \u6362\u70ed\u5668\u70ed\u4eff\u771f \u673a\u7406\u9a71\u52a8 PI-DeepONet \u65e0\u76d1\u7763\u5b66\u4e60 - - \u70ed\u4eff\u771f 2D \u70ed\u4eff\u771f \u673a\u7406\u9a71\u52a8 PINN \u65e0\u76d1\u7763\u5b66\u4e60 - Paper \u70ed\u4eff\u771f 2D \u82af\u7247\u70ed\u4eff\u771f \u673a\u7406\u9a71\u52a8 PI-DeepONet \u65e0\u76d1\u7763\u5b66\u4e60 - Paper <p></p> <p>\u6750\u6599\u79d1\u5b66(AI for Material)</p> \u95ee\u9898\u7c7b\u578b \u6848\u4f8b\u540d\u79f0 \u4f18\u5316\u7b97\u6cd5 \u6a21\u578b\u7c7b\u578b \u8bad\u7ec3\u65b9\u5f0f \u6570\u636e\u96c6 \u53c2\u8003\u8d44\u6599 \u6750\u6599\u8bbe\u8ba1 \u6563\u5c04\u677f\u8bbe\u8ba1(\u53cd\u95ee\u9898) \u6570\u7406\u878d\u5408 \u6570\u636e\u9a71\u52a8 \u76d1\u7763\u5b66\u4e60 Train DataEval Data Paper \u6750\u6599\u751f\u6210 \u9762\u5411\u5bf9\u79f0\u611f\u77e5\u7684\u5468\u671f\u6027\u6750\u6599\u751f\u6210<sup>coming soon</sup> \u6570\u636e\u9a71\u52a8 SyMat \u76d1\u7763\u5b66\u4e60 - - <p></p> <p>\u5730\u7403\u79d1\u5b66(AI for Earth Science)</p> \u95ee\u9898\u7c7b\u578b \u6848\u4f8b\u540d\u79f0 \u4f18\u5316\u7b97\u6cd5 \u6a21\u578b\u7c7b\u578b \u8bad\u7ec3\u65b9\u5f0f \u6570\u636e\u96c6 \u53c2\u8003\u8d44\u6599 \u5929\u6c14\u9884\u62a5 FourCastNet \u6c14\u8c61\u9884\u62a5 \u6570\u636e\u9a71\u52a8 FourCastNet \u76d1\u7763\u5b66\u4e60 ERA5 Paper \u5929\u6c14\u9884\u62a5 NowCastNet \u6c14\u8c61\u9884\u62a5 \u6570\u636e\u9a71\u52a8 NowCastNet \u76d1\u7763\u5b66\u4e60 MRMS Paper \u5929\u6c14\u9884\u62a5 GraphCast \u6c14\u8c61\u9884\u62a5 \u6570\u636e\u9a71\u52a8 GraphCastNet \u76d1\u7763\u5b66\u4e60 - Paper \u5927\u6c14\u6c61\u67d3\u7269 UNet \u6c61\u67d3\u7269\u6269\u6563 \u6570\u636e\u9a71\u52a8 UNet \u76d1\u7763\u5b66\u4e60 Data - \u5929\u6c14\u9884\u62a5 DGMR \u6c14\u8c61\u9884\u62a5 \u6570\u636e\u9a71\u52a8 DGMR \u76d1\u7763\u5b66\u4e60 UK dataset Paper"},{"location":"#_3","title":"\ud83d\ude80\u5feb\u901f\u5b89\u88c5","text":"\u65b9\u5f0f1: \u6e90\u7801\u5b89\u88c5[\u63a8\u8350]\u65b9\u5f0f2: pip\u5b89\u88c5 <pre><code>git clone -b develop https://github.com/PaddlePaddle/PaddleScience.git\n# \u82e5 github clone \u901f\u5ea6\u6bd4\u8f83\u6162\uff0c\u53ef\u4ee5\u4f7f\u7528 gitee clone\n# git clone -b develop https://gitee.com/paddlepaddle/PaddleScience.git\n\ncd PaddleScience\n\n# install paddlesci with editable mode\npip install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre> <pre><code>pip install paddlesci\n</code></pre> <p>\u5b8c\u6574\u5b89\u88c5\u6d41\u7a0b\uff1a\u5b89\u88c5\u4e0e\u4f7f\u7528</p>"},{"location":"#_4","title":"\ud83d\udd58\u6700\u8fd1\u66f4\u65b0","text":"<ul> <li>\u6dfb\u52a0 PirateNet(\u57fa\u4e8e Allen-cahn \u65b9\u7a0b\u548c N-S \u65b9\u7a0b\u6c42\u89e3) Allen-Cahn\u3001LDC2D(Re3200)\u3002</li> <li>\u57fa\u4e8e PaddleScience \u7684\u5feb\u901f\u70ed\u4eff\u771f\u65b9\u6cd5 A fast general thermal simulation model based on MultiBranch Physics-Informed deep operator neural network \u88ab Physics of Fluids 2024 \u63a5\u53d7\u3002</li> <li>\u6dfb\u52a0\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5 Relobralo \u3002</li> <li>\u6dfb\u52a0\u6c14\u6ce1\u6d41\u6c42\u89e3\u6848\u4f8b(Bubble)\u3001\u673a\u7ffc\u4f18\u5316\u6848\u4f8b(DeepCFD)\u3001\u70ed\u4f20\u5bfc\u4eff\u771f\u6848\u4f8b(HeatPINN)\u3001\u975e\u7ebf\u6027\u77ed\u4e34\u9884\u62a5\u6a21\u578b(Nowcasting(\u4ec5\u63a8\u7406))\u3001\u62d3\u6251\u4f18\u5316\u6848\u4f8b(TopOpt)\u3001\u77e9\u5f62\u5e73\u677f\u7ebf\u5f39\u6027\u65b9\u7a0b\u6c42\u89e3\u6848\u4f8b(Biharmonic2D)\u3002</li> <li>\u6dfb\u52a0\u4e8c\u7ef4\u8840\u7ba1\u6848\u4f8b(LabelFree-DNN-Surrogate)\u3001\u7a7a\u6c14\u6fc0\u6ce2\u6848\u4f8b(ShockWave)\u3001\u53bb\u566a\u7f51\u7edc\u6a21\u578b(DUCNN)\u3001\u98ce\u7535\u9884\u6d4b\u6a21\u578b(Deep Spatial Temporal)\u3001\u57df\u5206\u89e3\u6a21\u578b(XPINNs)\u3001\u79ef\u5206\u65b9\u7a0b\u6c42\u89e3\u6848\u4f8b(Volterra Equation)\u3001\u5206\u6570\u9636\u65b9\u7a0b\u6c42\u89e3\u6848\u4f8b(Fractional Poisson 2D)\u3002</li> <li>\u9488\u5bf9\u4e32\u8054\u65b9\u7a0b\u548c\u590d\u6742\u65b9\u7a0b\u573a\u666f\uff0c<code>Equation</code> \u6a21\u5757\u652f\u6301\u57fa\u4e8e sympy \u7684\u7b26\u53f7\u8ba1\u7b97\uff0c\u5e76\u652f\u6301\u548c python \u51fd\u6570\u6df7\u5408\u4f7f\u7528(#507\u3001#505)\u3002</li> <li><code>Geometry</code> \u6a21\u5757\u548c <code>InteriorConstraint</code>\u3001<code>InitialConstraint</code> \u652f\u6301\u8ba1\u7b97 SDF \u5fae\u5206\u529f\u80fd(#539)\u3002</li> <li>\u6dfb\u52a0 MultiTaskLearning(<code>ppsci.loss.mtl</code>) \u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u5757\uff0c\u9488\u5bf9\u591a\u4efb\u52a1\u4f18\u5316(\u5982 PINN \u65b9\u6cd5)\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u4f7f\u7528\u65b9\u5f0f\uff1a\u591a\u4efb\u52a1\u5b66\u4e60\u6307\u5357(#493\u3001#492)\u3002</li> </ul>"},{"location":"#_5","title":"\u2728\u7279\u6027","text":"<ul> <li>\u652f\u6301\u7b80\u5355\u51e0\u4f55\u548c\u590d\u6742 STL \u51e0\u4f55\u7684\u91c7\u6837\u4e0e\u5e03\u5c14\u8fd0\u7b97\u3002</li> <li>\u652f\u6301\u5305\u62ec Dirichlet\u3001Neumann\u3001Robin \u4ee5\u53ca\u81ea\u5b9a\u4e49\u8fb9\u754c\u6761\u4ef6\u3002</li> <li>\u652f\u6301\u7269\u7406\u673a\u7406\u9a71\u52a8\u3001\u6570\u636e\u9a71\u52a8\u3001\u6570\u7406\u878d\u5408\u4e09\u79cd\u95ee\u9898\u6c42\u89e3\u65b9\u5f0f\u3002\u6db5\u76d6\u6d41\u4f53\u3001\u7ed3\u6784\u3001\u6c14\u8c61\u7b49\u9886\u57df 20+ \u6848\u4f8b\u3002</li> <li>\u652f\u6301\u7ed3\u679c\u53ef\u89c6\u5316\u8f93\u51fa\u4e0e\u65e5\u5fd7\u7ed3\u6784\u5316\u4fdd\u5b58\u3002</li> <li>\u5b8c\u5584\u7684 type hints\uff0c\u7528\u6237\u4f7f\u7528\u548c\u4ee3\u7801\u8d21\u732e\u5168\u6d41\u7a0b\u6587\u6863\uff0c\u7ecf\u5178\u6848\u4f8b AI studio \u5feb\u901f\u4f53\u9a8c\uff0c\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002</li> <li>\u652f\u6301\u57fa\u4e8e sympy \u7b26\u53f7\u8ba1\u7b97\u5e93\u7684\u65b9\u7a0b\u8868\u793a\u4e0e\u8054\u7acb\u65b9\u7a0b\u7ec4\u8ba1\u7b97\u3002</li> <li>\u66f4\u591a\u7279\u6027\u6b63\u5728\u5f00\u53d1\u4e2d...</li> </ul>"},{"location":"#_6","title":"\ud83c\udf88\u5176\u4ed6\u9886\u57df\u652f\u6301","text":"<p>\u9664 PaddleScience \u5957\u4ef6\u5916\uff0cPaddle \u6846\u67b6\u8fd8\u652f\u6301\u4e86 DeepXDE \u7684\u6240\u6709\u6848\u4f8b\uff0c\u5206\u5b50\u52a8\u529b\u5b66\u5957\u4ef6 DeepMD-kit \u90e8\u5206\u6848\u4f8b\u548c\u529f\u80fd\uff0c\u4ee5\u53ca\u6b63\u5728\u9002\u914d\u4e2d\u7684 Modulus \u3002</p>              DeepXDE             \u5168\u91cf\u652f\u6301              DeepMD             \u9002\u914d\u4e2d              Modulus             \u9002\u914d\u4e2d"},{"location":"#_7","title":"\ud83d\udcac\u652f\u6301\u4e0e\u5efa\u8bae","text":"<p>\u5982\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u9047\u5230\u95ee\u9898\u6216\u60f3\u63d0\u51fa\u5f00\u53d1\u5efa\u8bae\uff0c\u6b22\u8fce\u5728 Discussion \u63d0\u51fa\u5efa\u8bae\uff0c\u6216\u8005\u5728 Issue \u9875\u9762\u65b0\u5efa issue\uff0c\u4f1a\u6709\u4e13\u4e1a\u7684\u7814\u53d1\u4eba\u5458\u8fdb\u884c\u89e3\u7b54\u3002</p>"},{"location":"#_8","title":"\ud83d\udc6b\u5f00\u6e90\u5171\u5efa","text":"<p>PaddleScience \u9879\u76ee\u6b22\u8fce\u5e76\u4f9d\u8d56\u5f00\u53d1\u4eba\u5458\u548c\u5f00\u6e90\u793e\u533a\u4e2d\u7684\u7528\u6237\uff0c\u4f1a\u4e0d\u5b9a\u671f\u63a8\u51fa\u5f00\u6e90\u6d3b\u52a8\u3002</p> <p>\u5728\u5f00\u6e90\u6d3b\u52a8\u4e2d\u5982\u9700\u4f7f\u7528 PaddleScience \u8fdb\u884c\u5f00\u53d1\uff0c\u53ef\u53c2\u8003 PaddleScience \u5f00\u53d1\u4e0e\u8d21\u732e\u6307\u5357 \u4ee5\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u8d28\u91cf\u3002</p> <ul> <li> <p>\ud83c\udf81\u5feb\u4e50\u5f00\u6e90</p> <p>\u65e8\u5728\u9f13\u52b1\u66f4\u591a\u7684\u5f00\u53d1\u8005\u53c2\u4e0e\u5230\u98de\u6868\u79d1\u5b66\u8ba1\u7b97\u793e\u533a\u7684\u5f00\u6e90\u5efa\u8bbe\u4e2d\uff0c\u5e2e\u52a9\u793e\u533a\u4fee\u590d bug \u6216\u8d21\u732e feature\uff0c\u52a0\u5165\u5f00\u6e90\u3001\u5171\u5efa\u98de\u6868\u3002\u4e86\u89e3\u7f16\u7a0b\u57fa\u672c\u77e5\u8bc6\u7684\u5165\u95e8\u7528\u6237\u5373\u53ef\u53c2\u4e0e\uff0c\u6d3b\u52a8\u8fdb\u884c\u4e2d\uff1a PaddleScience \u5feb\u4e50\u5f00\u6e90\u6d3b\u52a8\u8868\u5355</p> </li> <li> <p>\ud83d\udd25\u7b2c\u516d\u671f\u9ed1\u5ba2\u677e</p> <p>\u9762\u5411\u5168\u7403\u5f00\u53d1\u8005\u7684\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7f16\u7a0b\u6d3b\u52a8\uff0c\u9f13\u52b1\u5f00\u53d1\u8005\u4e86\u89e3\u4e0e\u53c2\u4e0e\u98de\u6868\u6df1\u5ea6\u5b66\u4e60\u5f00\u6e90\u9879\u76ee\u4e0e\u6587\u5fc3\u5927\u6a21\u578b\u5f00\u53d1\u5b9e\u8df5\u3002\u6d3b\u52a8\u8fdb\u884c\u4e2d\uff1a\u3010PaddlePaddle Hackathon 5th\u3011\u5f00\u6e90\u8d21\u732e\u4e2a\u4eba\u6311\u6218\u8d5b</p> </li> </ul>"},{"location":"#_9","title":"\ud83c\udfaf\u5171\u521b\u8ba1\u5212","text":"<p>PaddleScience \u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u9879\u76ee\uff0c\u6b22\u8fce\u6765\u5404\u884c\u5404\u4e1a\u7684\u4f19\u4f34\u643a\u624b\u5171\u5efa\u57fa\u4e8e\u98de\u6868\u7684 AI for Science \u9886\u57df\u9876\u5c16\u5f00\u6e90\u9879\u76ee, \u6253\u9020\u6d3b\u8dc3\u7684\u524d\u77bb\u6027\u7684 AI for Science \u5f00\u6e90\u793e\u533a\uff0c\u5efa\u7acb\u4ea7\u5b66\u7814\u95ed\u73af\uff0c\u63a8\u52a8\u79d1\u7814\u521b\u65b0\u4e0e\u4ea7\u4e1a\u8d4b\u80fd\u3002\u70b9\u51fb\u4e86\u89e3 \u98de\u6868AI for Science\u5171\u521b\u8ba1\u5212\u3002</p>"},{"location":"#_10","title":"\u2764\ufe0f\u81f4\u8c22","text":"<ul> <li> <p>PaddleScience \u7684\u90e8\u5206\u6a21\u5757\u548c\u6848\u4f8b\u8bbe\u8ba1\u53d7 NVIDIA-Modulus\u3001DeepXDE\u3001PaddleNLP\u3001PaddleClas \u7b49\u4f18\u79c0\u5f00\u6e90\u5957\u4ef6\u7684\u542f\u53d1\u3002</p> </li> <li> <p>PaddleScience \u7684\u90e8\u5206\u4ee3\u7801\u7531\u4ee5\u4e0b\u4f18\u79c0\u5f00\u53d1\u8005\u8d21\u732e\uff08\u6309 Contributors \u6392\u5e8f\uff09\uff1a</p> <p> </p> </li> </ul>"},{"location":"#_11","title":"\ud83e\udd1d\u5408\u4f5c\u5355\u4f4d","text":""},{"location":"#_12","title":"\ud83d\udcdc\u5f00\u6e90\u534f\u8bae","text":"<p>Apache License 2.0</p>"},{"location":"zh/cooperation/","title":"\u5171\u521b\u8ba1\u5212","text":""},{"location":"zh/cooperation/#_1","title":"\u5171\u521b\u8ba1\u5212","text":"<p>PaddleScience \u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u9879\u76ee\uff0c\u6b22\u8fce\u6765\u5404\u884c\u5404\u4e1a\u7684\u4f19\u4f34\u643a\u624b\u5171\u5efa\u57fa\u4e8e\u98de\u6868\u7684 AI for Science \u9886\u57df\u9876\u5c16\u5f00\u6e90\u9879\u76ee\uff0c\u6253\u9020\u6d3b\u8dc3\u7684\u524d\u77bb\u6027\u7684 AI for Science \u5f00\u6e90\u793e\u533a\uff0c\u5efa\u7acb\u4ea7\u5b66\u7814\u95ed\u73af\uff0c\u63a8\u52a8\u79d1\u7814\u521b\u65b0\u4e0e\u4ea7\u4e1a\u8d4b\u80fd\u3002\u70b9\u51fb\u4e86\u89e3 \u98de\u6868 AI for Science \u5171\u521b\u8ba1\u5212</p>"},{"location":"zh/cooperation/#_2","title":"\u9879\u76ee\u7cbe\u9009","text":"<ul> <li>\u4f7f\u7528\u5d4c\u5957\u5085\u7acb\u53f6\u795e\u7ecf\u7b97\u5b50\u8fdb\u884c\u5b9e\u65f6\u9ad8\u5206\u8fa8\u4e8c\u6c27\u5316\u78b3\u5730\u8d28\u5c01\u5b58\u9884\u6d4b: https://aistudio.baidu.com/projectdetail/7390303</li> <li>\u591a\u6e90\u5f02\u6784\u6570\u636e\u4e0e\u673a\u7406\u878d\u5408\u7684\u6781\u7aef\u5929\u6c14\u9884\u62a5\u7b97\u6cd5\u7814\u7a76: https://aistudio.baidu.com/projectdetail/7586532</li> <li>\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u590d\u6742\u7cfb\u7edf\u63a7\u5236 \u2014\u2014 \u4ee5\u75be\u75c5\u4f20\u64ad: https://aistudio.baidu.com/projectdetail/7520457</li> <li>\u57fa\u4e8e Transformer \u67b6\u6784\u7684\u6d41\u4f53\u6d41\u52a8\u964d\u9636\u6a21\u62df: https://aistudio.baidu.com/projectdetail/7509905</li> <li>\u57fa\u4e8e Transformer \u7684\u795e\u7ecf\u7b97\u5b50\u9884\u6d4b\u6a21\u578b: https://aistudio.baidu.com/projectdetail/7309026</li> <li>\u57fa\u4e8e PINN \u65b9\u6cd5\u6c42\u89e3\u53ef\u538b\u7f29\u6d41\u4f53\u6b27\u62c9\u65b9\u7a0b\u7ec4\u7684\u6b63\u95ee\u9898: https://aistudio.baidu.com/projectdetail/7502148</li> <li>\u57fa\u4e8e\u8fde\u7eed\u6f14\u5316\u6570\u636e\u9884\u6d4b\u53cc\u66f2\u65b9\u7a0b\u7b80\u65ad\u89e3: https://aistudio.baidu.com/projectdetail/7620492</li> <li>\u62c9\u683c\u6717\u65e5\u7c92\u5b50\u6d41\u4f53 Benchmark \u5f00\u6e90\u6570\u636e\u96c6: https://aistudio.baidu.com/projectdetail/7507477</li> <li>\u57fa\u4e8e PINN \u65b9\u6cd5\u6c42\u89e3\u53ef\u538b\u7f29\u6d41\u4f53\u6b27\u62c9\u65b9\u7a0b\u7ec4\u7684\u6b63\u95ee\u9898: https://aistudio.baidu.com/projectdetail/7593837</li> <li>\u6570\u636e\u9a71\u52a8 AI \u6a21\u578b\u7684 PDE \u65b9\u7a0b\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30: https://aistudio.baidu.com/projectdetail/7463477</li> <li>\u6570\u636e\u9a71\u52a8 AI \u6a21\u578b\u7684 PDE \u65b9\u7a0b\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30: https://aistudio.baidu.com/projectdetail/7512749</li> </ul>"},{"location":"zh/development/","title":"\u5f00\u53d1\u6307\u5357","text":""},{"location":"zh/development/#_1","title":"\u5f00\u53d1\u6307\u5357","text":"<p>\u672c\u6587\u6863\u4ecb\u7ecd\u5982\u4f55\u57fa\u4e8e PaddleScience \u5957\u4ef6\u8fdb\u884c\u4ee3\u7801\u5f00\u53d1\u5e76\u6700\u7ec8\u8d21\u732e\u5230 PaddleScience \u5957\u4ef6\u4e2d\u3002</p> <p>PaddleScience \u76f8\u5173\u7684\u8bba\u6587\u590d\u73b0\u3001API \u5f00\u53d1\u4efb\u52a1\u5f00\u59cb\u4e4b\u524d\u9700\u63d0\u4ea4 RFC \u6587\u6863\uff0c\u8bf7\u53c2\u8003\uff1aPaddleScience RFC Template</p>"},{"location":"zh/development/#1","title":"1. \u51c6\u5907\u5de5\u4f5c","text":"<ol> <li>\u5728\u7f51\u9875\u4e0a\u5c06 PaddleScience fork \u5230\u81ea\u5df1\u7684\u4ed3\u5e93</li> <li> <p>\u514b\u9686\u81ea\u5df1\u4ed3\u5e93\u91cc\u7684 PaddleScience \u5230\u672c\u5730\uff0c\u5e76\u8fdb\u5165\u8be5\u76ee\u5f55</p> <pre><code>git clone -b develop https://github.com/USER_NAME/PaddleScience.git\ncd PaddleScience\n</code></pre> <p>\u4e0a\u65b9 <code>clone</code> \u547d\u4ee4\u4e2d\u7684 <code>USER_NAME</code> \u5b57\u6bb5\u8bf7\u586b\u5165\u81ea\u5df1\u7684 github \u7528\u6237\u540d\u3002</p> </li> <li> <p>\u5b89\u88c5\u5fc5\u8981\u7684\u4f9d\u8d56\u5305</p> <pre><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre> </li> <li> <p>\u57fa\u4e8e\u5f53\u524d\u6240\u5728\u7684 <code>develop</code> \u5206\u652f\uff0c\u65b0\u5efa\u4e00\u4e2a\u5206\u652f(\u5047\u8bbe\u65b0\u5206\u652f\u540d\u5b57\u4e3a <code>dev_model</code>)</p> <pre><code>git checkout -b \"dev_model\"\n</code></pre> </li> <li> <p>\u6dfb\u52a0 PaddleScience \u76ee\u5f55\u5230\u7cfb\u7edf\u73af\u5883\u53d8\u91cf <code>PYTHONPATH</code> \u4e2d</p> <pre><code>export PYTHONPATH=$PWD:$PYTHONPATH\n</code></pre> </li> <li> <p>\u6267\u884c\u4ee5\u4e0b\u4ee3\u7801\uff0c\u9a8c\u8bc1\u5b89\u88c5\u7684 PaddleScience \u57fa\u7840\u529f\u80fd\u662f\u5426\u6b63\u5e38</p> <pre><code>python -c \"import ppsci; ppsci.run_check()\"\n</code></pre> <p>\u5982\u679c\u51fa\u73b0 PaddleScience is installed successfully.\u2728 \ud83c\udf70 \u2728\uff0c\u5219\u8bf4\u660e\u5b89\u88c5\u9a8c\u8bc1\u6210\u529f\u3002</p> </li> </ol>"},{"location":"zh/development/#2","title":"2. \u7f16\u5199\u4ee3\u7801","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u51c6\u5907\u5de5\u4f5c\u540e\uff0c\u5c31\u53ef\u4ee5\u57fa\u4e8e PaddleScience \u5f00\u59cb\u5f00\u53d1\u81ea\u5df1\u7684\u6848\u4f8b\u6216\u8005\u529f\u80fd\u4e86\u3002</p> <p>\u5047\u8bbe\u65b0\u5efa\u7684\u6848\u4f8b\u4ee3\u7801\u6587\u4ef6\u8def\u5f84\u4e3a\uff1a<code>PaddleScience/examples/demo/demo.py</code>\uff0c\u63a5\u4e0b\u6765\u5f00\u59cb\u8be6\u7ec6\u4ecb\u7ecd\u8fd9\u4e00\u6d41\u7a0b</p>"},{"location":"zh/development/#21","title":"2.1 \u5bfc\u5165\u5fc5\u8981\u7684\u5305","text":"<p>PaddleScience \u6240\u63d0\u4f9b\u7684 API \u5168\u90e8\u5728 <code>ppsci.*</code> \u6a21\u5757\u4e0b\uff0c\u56e0\u6b64\u5728 <code>demo.py</code> \u7684\u5f00\u5934\u9996\u5148\u9700\u8981\u5bfc\u5165 <code>ppsci</code> \u8fd9\u4e2a\u9876\u5c42\u6a21\u5757\uff0c\u63a5\u7740\u5bfc\u5165\u65e5\u5fd7\u6253\u5370\u6a21\u5757 <code>logger</code>\uff0c\u65b9\u4fbf\u6253\u5370\u65e5\u5fd7\u65f6\u81ea\u52a8\u8bb0\u5f55\u65e5\u5fd7\u5230\u672c\u5730\u6587\u4ef6\u4e2d\uff0c\u6700\u540e\u518d\u6839\u636e\u60a8\u81ea\u5df1\u7684\u9700\u8981\uff0c\u5bfc\u5165\u5176\u4ed6\u5fc5\u8981\u7684\u6a21\u5757\u3002</p> examples/demo/demo.py<pre><code>import ppsci\nfrom ppsci.utils import logger\n\n# \u5bfc\u5165\u5176\u4ed6\u5fc5\u8981\u7684\u6a21\u5757\n# import ...\n</code></pre>"},{"location":"zh/development/#22","title":"2.2 \u8bbe\u7f6e\u8fd0\u884c\u73af\u5883","text":"<p>\u5728\u8fd0\u884c <code>demo.py</code> \u4e4b\u524d\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u5fc5\u8981\u7684\u8fd0\u884c\u73af\u5883\u8bbe\u7f6e\uff0c\u5982\u56fa\u5b9a\u968f\u673a\u79cd\u5b50(\u4fdd\u8bc1\u5b9e\u9a8c\u53ef\u590d\u73b0\u6027)\u3001\u8bbe\u7f6e\u8f93\u51fa\u76ee\u5f55\u5e76\u521d\u59cb\u5316\u65e5\u5fd7\u6253\u5370\u6a21\u5757(\u4fdd\u5b58\u91cd\u8981\u5b9e\u9a8c\u6570\u636e)\u3002</p> examples/demo/demo.py<pre><code>if __name__ == \"__main__\":\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(42)\n    # set output directory\n    OUTPUT_DIR = \"./output_example\"\n    # initialize logger\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n</code></pre> <p>\u5b8c\u6210\u4e0a\u8ff0\u6b65\u9aa4\u4e4b\u540e\uff0c<code>demo.py</code> \u5df2\u7ecf\u642d\u597d\u4e86\u5fc5\u8981\u6846\u67b6\u3002\u63a5\u4e0b\u6765\u4ecb\u7ecd\u5982\u4f55\u57fa\u4e8e\u81ea\u5df1\u5177\u4f53\u7684\u9700\u6c42\uff0c\u5bf9 <code>ppsci.*</code> \u4e0b\u7684\u5176\u4ed6\u6a21\u5757\u8fdb\u884c\u5f00\u53d1\u6216\u8005\u590d\u7528\uff0c\u4ee5\u6700\u7ec8\u5728 <code>demo.py</code> \u4e2d\u4f7f\u7528\u3002</p>"},{"location":"zh/development/#23","title":"2.3 \u6784\u5efa\u6a21\u578b","text":""},{"location":"zh/development/#231","title":"2.3.1 \u6784\u5efa\u5df2\u6709\u6a21\u578b","text":"<p>PaddleScience \u5185\u7f6e\u4e86\u4e00\u4e9b\u5e38\u89c1\u7684\u6a21\u578b\uff0c\u5982 <code>MLP</code> \u6a21\u578b\uff0c\u5982\u679c\u60a8\u60f3\u4f7f\u7528\u8fd9\u4e9b\u5185\u7f6e\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u76f4\u63a5\u8c03\u7528 <code>ppsci.arch.*</code> \u4e0b\u7684 API\uff0c\u5e76\u586b\u5165\u6a21\u578b\u5b9e\u4f8b\u5316\u6240\u9700\u7684\u53c2\u6570\uff0c\u5373\u53ef\u5feb\u901f\u6784\u5efa\u6a21\u578b\u3002</p> examples/demo/demo.py<pre><code># create a MLP model\nmodel = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\", \"p\"), 9, 50, \"tanh\")\n</code></pre> <p>\u4e0a\u8ff0\u4ee3\u7801\u5b9e\u4f8b\u5316\u4e86\u4e00\u4e2a <code>MLP</code> \u5168\u8fde\u63a5\u6a21\u578b\uff0c\u5176\u8f93\u5165\u6570\u636e\u4e3a\u4e24\u4e2a\u5b57\u6bb5\uff1a<code>\"x\"</code>\u3001<code>\"y\"</code>\uff0c\u8f93\u51fa\u6570\u636e\u4e3a\u4e09\u4e2a\u5b57\u6bb5\uff1a<code>\"u\"</code>\u3001<code>\"v\"</code>\u3001<code>\"w\"</code>\uff1b\u6a21\u578b\u5177\u6709 \\(9\\) \u5c42\u9690\u85cf\u5c42\uff0c\u6bcf\u5c42\u7684\u795e\u7ecf\u5143\u4e2a\u6570\u4e3a \\(50\\) \u4e2a\uff0c\u6bcf\u5c42\u9690\u85cf\u5c42\u4f7f\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\u5747\u4e3a \\(\\tanh\\) \u53cc\u66f2\u6b63\u5207\u51fd\u6570\u3002</p>"},{"location":"zh/development/#232","title":"2.3.2 \u6784\u5efa\u65b0\u7684\u6a21\u578b","text":"<p>\u5f53 PaddleScience \u5185\u7f6e\u7684\u6a21\u578b\u65e0\u6cd5\u6ee1\u8db3\u60a8\u7684\u9700\u6c42\u65f6\uff0c\u60a8\u5c31\u53ef\u4ee5\u901a\u8fc7\u65b0\u589e\u6a21\u578b\u6587\u4ef6\u5e76\u7f16\u5199\u6a21\u578b\u4ee3\u7801\u7684\u65b9\u5f0f\uff0c\u4f7f\u7528\u60a8\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\uff0c\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <ol> <li>\u5728 <code>ppsci/arch/</code> \u6587\u4ef6\u5939\u4e0b\u65b0\u5efa\u6a21\u578b\u7ed3\u6784\u6587\u4ef6\uff0c\u4ee5 <code>new_model.py</code> \u4e3a\u4f8b\u3002</li> <li> <p>\u5728 <code>new_model.py</code> \u6587\u4ef6\u4e2d\u5bfc\u5165 PaddleScience \u7684\u6a21\u578b\u57fa\u7c7b\u6240\u5728\u7684\u6a21\u5757 <code>base</code>\uff0c\u5e76\u4ece <code>base.Arch</code> \u6d3e\u751f\u51fa\u60a8\u60f3\u521b\u5efa\u7684\u65b0\u6a21\u578b\u7c7b\uff08\u4ee5 <code>Class NewModel</code> \u4e3a\u4f8b\uff09\u3002</p> ppsci/arch/new_model.py<pre><code>from ppsci.arch import base\n\nclass NewModel(base.Arch):\n    def __init__(self, ...):\n        ...\n        # initialization\n\n    def forward(self, ...):\n        ...\n        # forward\n</code></pre> </li> <li> <p>\u7f16\u5199 <code>NewModel.__init__</code> \u65b9\u6cd5\uff0c\u5176\u88ab\u7528\u4e8e\u6a21\u578b\u521b\u5efa\u65f6\u7684\u521d\u59cb\u5316\u64cd\u4f5c\uff0c\u5305\u62ec\u6a21\u578b\u5c42\u3001\u53c2\u6570\u53d8\u91cf\u521d\u59cb\u5316\uff1b\u7136\u540e\u518d\u7f16\u5199 <code>NewModel.forward</code> \u65b9\u6cd5\uff0c\u5176\u5b9a\u4e49\u4e86\u6a21\u578b\u4ece\u63a5\u53d7\u8f93\u5165\u3001\u8ba1\u7b97\u8f93\u51fa\u8fd9\u4e00\u8fc7\u7a0b\u3002\u4ee5 <code>MLP.__init__</code> \u548c <code>MLP.forward</code> \u4e3a\u4f8b\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> MLP.__init__MLP.forward <pre><code>class MLP(base.Arch):\n    \"\"\"Multi layer perceptron network.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"x\", \"y\", \"z\").\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"u\", \"v\", \"w\").\n        num_layers (int): Number of hidden layers.\n        hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size.\n            An integer for all layers, or list of integer specify each layer's size.\n        activation (str, optional): Name of activation function. Defaults to \"tanh\".\n        skip_connection (bool, optional): Whether to use skip connection. Defaults to False.\n        weight_norm (bool, optional): Whether to apply weight norm on parameter(s). Defaults to False.\n        input_dim (Optional[int]): Number of input's dimension. Defaults to None.\n        output_dim (Optional[int]): Number of output's dimension. Defaults to None.\n        periods (Optional[Dict[int, Tuple[float, bool]]]): Period of each input key,\n            input in given channel will be period embeded if specified, each tuple of\n            periods list is [period, trainable]. Defaults to None.\n        fourier (Optional[Dict[str, Union[float, int]]]): Random fourier feature embedding,\n            e.g. {'dim': 256, 'scale': 1.0}. Defaults to None.\n        random_weight (Optional[Dict[str, float]]): Mean and std of random weight\n            factorization layer, e.g. {\"mean\": 0.5, \"std: 0.1\"}. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP(\n        ...     input_keys=(\"x\", \"y\"),\n        ...     output_keys=(\"u\", \"v\"),\n        ...     num_layers=5,\n        ...     hidden_size=128\n        ... )\n        &gt;&gt;&gt; input_dict = {\"x\": paddle.rand([64, 1]),\n        ...               \"y\": paddle.rand([64, 1])}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict[\"u\"].shape)\n        [64, 1]\n        &gt;&gt;&gt; print(output_dict[\"v\"].shape)\n        [64, 1]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        num_layers: int,\n        hidden_size: Union[int, Tuple[int, ...]],\n        activation: str = \"tanh\",\n        skip_connection: bool = False,\n        weight_norm: bool = False,\n        input_dim: Optional[int] = None,\n        output_dim: Optional[int] = None,\n        periods: Optional[Dict[int, Tuple[float, bool]]] = None,\n        fourier: Optional[Dict[str, Union[float, int]]] = None,\n        random_weight: Optional[Dict[str, float]] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.linears = []\n        self.acts = []\n        self.periods = periods\n        self.fourier = fourier\n        if periods:\n            self.period_emb = PeriodEmbedding(periods)\n\n        if isinstance(hidden_size, (tuple, list)):\n            if num_layers is not None:\n                raise ValueError(\n                    \"num_layers should be None when hidden_size is specified\"\n                )\n        elif isinstance(hidden_size, int):\n            if not isinstance(num_layers, int):\n                raise ValueError(\n                    \"num_layers should be an int when hidden_size is an int\"\n                )\n            hidden_size = [hidden_size] * num_layers\n        else:\n            raise ValueError(\n                f\"hidden_size should be list of int or int, but got {type(hidden_size)}\"\n            )\n\n        # initialize FC layer(s)\n        cur_size = len(self.input_keys) if input_dim is None else input_dim\n        if input_dim is None and periods:\n            # period embeded channel(s) will be doubled automatically\n            # if input_dim is not specified\n            cur_size += len(periods)\n\n        if fourier:\n            self.fourier_emb = FourierEmbedding(\n                cur_size, fourier[\"dim\"], fourier[\"scale\"]\n            )\n            cur_size = fourier[\"dim\"]\n\n        for i, _size in enumerate(hidden_size):\n            if weight_norm:\n                self.linears.append(WeightNormLinear(cur_size, _size))\n            elif random_weight:\n                self.linears.append(\n                    RandomWeightFactorization(\n                        cur_size,\n                        _size,\n                        mean=random_weight[\"mean\"],\n                        std=random_weight[\"std\"],\n                    )\n                )\n            else:\n                self.linears.append(nn.Linear(cur_size, _size))\n\n            # initialize activation function\n            self.acts.append(\n                act_mod.get_activation(activation)\n                if activation != \"stan\"\n                else act_mod.get_activation(activation)(_size)\n            )\n            # special initialization for certain activation\n            # TODO: Adapt code below to a more elegant style\n            if activation == \"siren\":\n                if i == 0:\n                    act_mod.Siren.init_for_first_layer(self.linears[-1])\n                else:\n                    act_mod.Siren.init_for_hidden_layer(self.linears[-1])\n\n            cur_size = _size\n\n        self.linears = nn.LayerList(self.linears)\n        self.acts = nn.LayerList(self.acts)\n        if random_weight:\n            self.last_fc = RandomWeightFactorization(\n                cur_size,\n                len(self.output_keys) if output_dim is None else output_dim,\n                mean=random_weight[\"mean\"],\n                std=random_weight[\"std\"],\n            )\n        else:\n            self.last_fc = nn.Linear(\n                cur_size,\n                len(self.output_keys) if output_dim is None else output_dim,\n            )\n\n        self.skip_connection = skip_connection\n</code></pre> <pre><code>def forward(self, x):\n    if self._input_transform is not None:\n        x = self._input_transform(x)\n\n    if self.periods:\n        x = self.period_emb(x)\n\n    y = self.concat_to_tensor(x, self.input_keys, axis=-1)\n\n    if self.fourier:\n        y = self.fourier_emb(y)\n\n    y = self.forward_tensor(y)\n    y = self.split_to_dict(y, self.output_keys, axis=-1)\n\n    if self._output_transform is not None:\n        y = self._output_transform(x, y)\n    return y\n</code></pre> </li> <li> <p>\u5728 <code>ppsci/arch/__init__.py</code> \u4e2d\u5bfc\u5165\u7f16\u5199\u7684\u65b0\u6a21\u578b\u7c7b <code>NewModel</code>\uff0c\u5e76\u6dfb\u52a0\u5230 <code>__all__</code> \u4e2d</p> ppsci/arch/__init__.py<pre><code>...\n...\nfrom ppsci.arch.new_model import NewModel\n\n__all__ = [\n    ...,\n    ...,\n    \"NewModel\",\n]\n</code></pre> </li> </ol> <p>\u5b8c\u6210\u4e0a\u8ff0\u65b0\u6a21\u578b\u4ee3\u7801\u7f16\u5199\u7684\u5de5\u4f5c\u4e4b\u540e\uff0c\u5728 <code>demo.py</code> \u4e2d\uff0c\u5c31\u80fd\u901a\u8fc7\u8c03\u7528 <code>ppsci.arch.NewModel</code>\uff0c\u5b9e\u4f8b\u5316\u521a\u624d\u7f16\u5199\u7684\u6a21\u578b\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> examples/demo/demo.py<pre><code>model = ppsci.arch.NewModel(...)\n</code></pre>"},{"location":"zh/development/#24","title":"2.4 \u6784\u5efa\u65b9\u7a0b","text":"<p>\u5982\u679c\u60a8\u7684\u6848\u4f8b\u95ee\u9898\u4e2d\u6d89\u53ca\u5230\u65b9\u7a0b\u8ba1\u7b97\uff0c\u90a3\u4e48\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u65b9\u7a0b\uff0c\u6216\u8005\u7f16\u5199\u81ea\u5df1\u7684\u65b9\u7a0b\u3002</p>"},{"location":"zh/development/#241","title":"2.4.1 \u6784\u5efa\u5df2\u6709\u65b9\u7a0b","text":"<p>PaddleScience \u5185\u7f6e\u4e86\u4e00\u4e9b\u5e38\u89c1\u7684\u65b9\u7a0b\uff0c\u5982 <code>NavierStokes</code> \u65b9\u7a0b\uff0c\u5982\u679c\u60a8\u60f3\u4f7f\u7528\u8fd9\u4e9b\u5185\u7f6e\u7684\u65b9\u7a0b\uff0c\u53ef\u4ee5\u76f4\u63a5 \u8c03\u7528 <code>ppsci.equation.*</code> \u4e0b\u7684 API\uff0c\u5e76\u586b\u5165\u65b9\u7a0b\u5b9e\u4f8b\u5316\u6240\u9700\u7684\u53c2\u6570\uff0c\u5373\u53ef\u5feb\u901f\u6784\u5efa\u65b9\u7a0b\u3002</p> examples/demo/demo.py<pre><code># create a Vibration equation\nviv_equation = ppsci.equation.Vibration(2, -4, 0)\n</code></pre>"},{"location":"zh/development/#242","title":"2.4.2 \u6784\u5efa\u65b0\u7684\u65b9\u7a0b","text":"<p>\u5f53 PaddleScience \u5185\u7f6e\u7684\u65b9\u7a0b\u65e0\u6cd5\u6ee1\u8db3\u60a8\u7684\u9700\u6c42\u65f6\uff0c\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7\u65b0\u589e\u65b9\u7a0b\u6587\u4ef6\u5e76\u7f16\u5199\u65b9\u7a0b\u4ee3\u7801\u7684\u65b9\u5f0f\uff0c\u4f7f\u7528\u60a8\u81ea\u5b9a\u4e49\u7684\u65b9\u7a0b\u3002</p> <p>\u5047\u8bbe\u9700\u8981\u8ba1\u7b97\u7684\u65b9\u7a0b\u516c\u5f0f\u5982\u4e0b\u6240\u793a\u3002</p> \\[ \\begin{cases}     \\begin{align}         \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial u}{\\partial y} &amp;= u + 1, \\tag{1} \\\\         \\dfrac{\\partial v}{\\partial x} + \\dfrac{\\partial v}{\\partial y} &amp;= v. \\tag{2}     \\end{align} \\end{cases} \\] <p>\u5176\u4e2d \\(x\\), \\(y\\) \u4e3a\u6a21\u578b\u8f93\u5165\uff0c\u8868\u793a\\(x\\)\u3001\\(y\\)\u8f74\u5750\u6807\uff1b\\(u=u(x,y)\\)\u3001\\(v=v(x,y)\\) \u662f\u6a21\u578b\u8f93\u51fa\uff0c\u8868\u793a \\((x,y)\\) \u5904\u7684 \\(x\\)\u3001\\(y\\) \u8f74\u65b9\u5411\u901f\u5ea6\u3002</p> <p>\u9996\u5148\u6211\u4eec\u9700\u8981\u5c06\u4e0a\u8ff0\u65b9\u7a0b\u8fdb\u884c\u9002\u5f53\u79fb\u9879\uff0c\u5c06\u542b\u6709\u53d8\u91cf\u3001\u51fd\u6570\u7684\u9879\u79fb\u52a8\u5230\u7b49\u5f0f\u5de6\u4fa7\uff0c\u542b\u6709\u5e38\u6570\u7684\u9879\u79fb\u52a8\u5230\u7b49\u5f0f\u53f3\u4fa7\uff0c\u65b9\u4fbf\u540e\u7eed\u8f6c\u6362\u6210\u7a0b\u5e8f\u4ee3\u7801\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> \\[ \\begin{cases}     \\begin{align}         \\dfrac{\\partial u}{\\partial x} +  \\dfrac{\\partial u}{\\partial y} - u &amp;= 1, \\tag{3}\\\\         \\dfrac{\\partial v}{\\partial x} +  \\dfrac{\\partial v}{\\partial y} - v &amp;= 0. \\tag{4}     \\end{align} \\end{cases} \\] <p>\u7136\u540e\u5c31\u53ef\u4ee5\u5c06\u4e0a\u8ff0\u79fb\u9879\u540e\u7684\u65b9\u7a0b\u7ec4\u6839\u636e\u4ee5\u4e0b\u6b65\u9aa4\u8f6c\u6362\u6210\u5bf9\u5e94\u7684\u7a0b\u5e8f\u4ee3\u7801\u3002</p> <ol> <li> <p>\u5728 <code>ppsci/equation/pde/</code> \u4e0b\u65b0\u5efa\u65b9\u7a0b\u6587\u4ef6\u3002\u5982\u679c\u60a8\u7684\u65b9\u7a0b\u5e76\u4e0d\u662f PDE \u65b9\u7a0b\uff0c\u90a3\u4e48\u9700\u8981\u65b0\u5efa\u4e00\u4e2a\u65b9\u7a0b\u7c7b\u6587\u4ef6\u5939\uff0c\u6bd4\u5982\u5728 <code>ppsci/equation/</code> \u4e0b\u65b0\u5efa <code>ode</code> \u6587\u4ef6\u5939\uff0c\u518d\u5c06\u60a8\u7684\u65b9\u7a0b\u6587\u4ef6\u653e\u5728 <code>ode</code> \u6587\u4ef6\u5939\u4e0b\u3002\u6b64\u5904\u4ee5PDE\u7c7b\u7684\u65b9\u7a0b <code>new_pde.py</code> \u4e3a\u4f8b\u3002</p> </li> <li> <p>\u5728 <code>new_pde.py</code> \u6587\u4ef6\u4e2d\u5bfc\u5165 PaddleScience \u7684\u65b9\u7a0b\u57fa\u7c7b\u6240\u5728\u6a21\u5757 <code>base</code>\uff0c\u5e76\u4ece <code>base.PDE</code> \u6d3e\u751f <code>Class NewPDE</code>\u3002</p> ppsci/equation/pde/new_pde.py<pre><code>from ppsci.equation.pde import base\n\nclass NewPDE(base.PDE):\n</code></pre> </li> <li> <p>\u7f16\u5199 <code>__init__</code> \u4ee3\u7801\uff0c\u7528\u4e8e\u65b9\u7a0b\u521b\u5efa\u65f6\u7684\u521d\u59cb\u5316\uff0c\u5728\u5176\u4e2d\u5b9a\u4e49\u5fc5\u8981\u7684\u53d8\u91cf\u548c\u516c\u5f0f\u8ba1\u7b97\u8fc7\u7a0b\u3002PaddleScience \u652f\u6301\u4f7f\u7528 sympy \u7b26\u53f7\u8ba1\u7b97\u5e93\u521b\u5efa\u65b9\u7a0b\u548c\u76f4\u63a5\u4f7f\u7528 python \u51fd\u6570\u7f16\u5199\u65b9\u7a0b\uff0c\u4e24\u79cd\u65b9\u5f0f\u5982\u4e0b\u6240\u793a\u3002</p> sympy expressionpython function ppsci/equation/pde/new_pde.py<pre><code>from ppsci.equation.pde import base\n\nclass NewPDE(base.PDE):\n    def __init__(self):\n        x, y = self.create_symbols(\"x y\") # \u521b\u5efa\u81ea\u53d8\u91cf x, y\n        u = self.create_function(\"u\", (x, y))  # \u521b\u5efa\u5173\u4e8e\u81ea\u53d8\u91cf (x, y) \u7684\u51fd\u6570 u(x,y)\n        v = self.create_function(\"v\", (x, y))  # \u521b\u5efa\u5173\u4e8e\u81ea\u53d8\u91cf (x, y) \u7684\u51fd\u6570 v(x,y)\n\n        expr1 = u.diff(x) + u.diff(y) - u  # \u5bf9\u5e94\u7b49\u5f0f(3)\u5de6\u4fa7\u8868\u8fbe\u5f0f\n        expr2 = v.diff(x) + v.diff(y) - v  # \u5bf9\u5e94\u7b49\u5f0f(4)\u5de6\u4fa7\u8868\u8fbe\u5f0f\n\n        self.add_equation(\"expr1\", expr1)  # \u5c06expr1 \u7684 sympy \u8868\u8fbe\u5f0f\u5bf9\u8c61\u6dfb\u52a0\u5230 NewPDE \u5bf9\u8c61\u7684\u516c\u5f0f\u96c6\u5408\u4e2d\n        self.add_equation(\"expr2\", expr2)  # \u5c06expr2 \u7684 sympy \u8868\u8fbe\u5f0f\u5bf9\u8c61\u6dfb\u52a0\u5230 NewPDE \u5bf9\u8c61\u7684\u516c\u5f0f\u96c6\u5408\u4e2d\n</code></pre> ppsci/equation/pde/new_pde.py<pre><code>from ppsci.autodiff import jacobian\n\nfrom ppsci.equation.pde import base\n\nclass NewPDE(base.PDE):\n    def __init__(self):\n        def expr1_compute_func(out):\n            x, y = out[\"x\"], out[\"y\"]  # \u4ece out \u6570\u636e\u5b57\u5178\u4e2d\u53d6\u51fa\u81ea\u53d8\u91cf x, y \u7684\u6570\u636e\u503c\n            u = out[\"u\"]  # \u4ece out \u6570\u636e\u5b57\u5178\u4e2d\u53d6\u51fa\u56e0\u53d8\u91cf u \u7684\u51fd\u6570\u503c\n\n            expr1 = jacobian(u, x) + jacobian(u, y) - u  # \u5bf9\u5e94\u7b49\u5f0f(3)\u5de6\u4fa7\u8868\u8fbe\u5f0f\u8ba1\u7b97\u8fc7\u7a0b\n            return expr1  # \u8fd4\u56de\u8ba1\u7b97\u7ed3\u679c\u503c\n\n        def expr2_compute_func(out):\n            x, y = out[\"x\"], out[\"y\"]  # \u4ece out \u6570\u636e\u5b57\u5178\u4e2d\u53d6\u51fa\u81ea\u53d8\u91cf x, y \u7684\u6570\u636e\u503c\n            v = out[\"v\"]  # \u4ece out \u6570\u636e\u5b57\u5178\u4e2d\u53d6\u51fa\u56e0\u53d8\u91cf v \u7684\u51fd\u6570\u503c\n\n            expr2 = jacobian(v, x) + jacobian(v, y) - v  # \u5bf9\u5e94\u7b49\u5f0f(4)\u5de6\u4fa7\u8868\u8fbe\u5f0f\u8ba1\u7b97\u8fc7\u7a0b\n            return expr2\n\n        self.add_equation(\"expr1\", expr1_compute_func)  # \u5c06 expr1 \u7684\u8ba1\u7b97\u51fd\u6570\u6dfb\u52a0\u5230 NewPDE \u5bf9\u8c61\u7684\u516c\u5f0f\u96c6\u5408\u4e2d\n        self.add_equation(\"expr2\", expr2_compute_func)  # \u5c06 expr2 \u7684\u8ba1\u7b97\u51fd\u6570\u6dfb\u52a0\u5230 NewPDE \u5bf9\u8c61\u7684\u516c\u5f0f\u96c6\u5408\u4e2d\n</code></pre> </li> <li> <p>\u5728 <code>ppsci/equation/__init__.py</code> \u4e2d\u5bfc\u5165\u7f16\u5199\u7684\u65b0\u65b9\u7a0b\u7c7b\uff0c\u5e76\u6dfb\u52a0\u5230 <code>__all__</code> \u4e2d</p> ppsci/equation/__init__.py<pre><code>...\n...\nfrom ppsci.equation.pde.new_pde import NewPDE\n\n__all__ = [\n    ...,\n    ...,\n    \"NewPDE\",\n]\n</code></pre> </li> </ol> <p>\u5b8c\u6210\u4e0a\u8ff0\u65b0\u65b9\u7a0b\u4ee3\u7801\u7f16\u5199\u7684\u5de5\u4f5c\u4e4b\u540e\uff0c\u6211\u4eec\u5c31\u80fd\u50cf PaddleScience \u5185\u7f6e\u65b9\u7a0b\u4e00\u6837\uff0c\u4ee5 <code>ppsci.equation.NewPDE</code> \u7684\u65b9\u5f0f\uff0c\u8c03\u7528\u6211\u4eec\u7f16\u5199\u7684\u65b0\u65b9\u7a0b\u7c7b\uff0c\u5e76\u7528\u4e8e\u521b\u5efa\u65b9\u7a0b\u5b9e\u4f8b\u3002</p> <p>\u5728\u65b9\u7a0b\u6784\u5efa\u5b8c\u6bd5\u540e\u4e4b\u540e\uff0c\u6211\u4eec\u9700\u8981\u5c06\u6240\u6709\u65b9\u7a0b\u5305\u88c5\u4e3a\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d</p> examples/demo/demo.py<pre><code>new_pde = ppsci.equation.NewPDE(...)\nequation = {..., \"newpde\": new_pde}\n</code></pre>"},{"location":"zh/development/#25","title":"2.5 \u6784\u5efa\u51e0\u4f55\u6a21\u5757[\u53ef\u9009]","text":"<p>\u6a21\u578b\u8bad\u7ec3\u3001\u9a8c\u8bc1\u65f6\u6240\u7528\u7684\u8f93\u5165\u3001\u6807\u7b7e\u6570\u636e\u7684\u6765\u6e90\uff0c\u6839\u636e\u5177\u4f53\u6848\u4f8b\u573a\u666f\u7684\u4e0d\u540c\u800c\u53d8\u5316\u3002\u5927\u90e8\u5206\u57fa\u4e8e PINN \u7684\u6848\u4f8b\uff0c\u5176\u6570\u636e\u6765\u81ea\u51e0\u4f55\u5f62\u72b6\u5185\u90e8\u3001\u8868\u9762\u91c7\u6837\u5f97\u5230\u7684\u5750\u6807\u70b9\u3001\u6cd5\u5411\u91cf\u3001SDF \u503c\uff1b\u800c\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5176\u8f93\u5165\u3001\u6807\u7b7e\u6570\u636e\u5927\u591a\u6570\u6765\u81ea\u4e8e\u5916\u90e8\u6587\u4ef6\uff0c\u6216\u901a\u8fc7 numpy \u7b49\u7b2c\u4e09\u65b9\u5e93\u6784\u9020\u7684\u5b58\u653e\u5728\u5185\u5b58\u4e2d\u7684\u6570\u636e\u3002\u672c\u7ae0\u8282\u4e3b\u8981\u5bf9\u7b2c\u4e00\u79cd\u60c5\u51b5\u6240\u9700\u7684\u51e0\u4f55\u6a21\u5757\u8fdb\u884c\u4ecb\u7ecd\uff0c\u7b2c\u4e8c\u79cd\u60c5\u51b5\u5219\u4e0d\u4e00\u5b9a\u9700\u8981\u51e0\u4f55\u6a21\u5757\uff0c\u5176\u6784\u9020\u65b9\u5f0f\u53ef\u4ee5\u53c2\u8003 #2.6 \u6784\u5efa\u7ea6\u675f\u6761\u4ef6\u3002</p>"},{"location":"zh/development/#251","title":"2.5.1 \u6784\u5efa\u5df2\u6709\u51e0\u4f55","text":"<p>PaddleScience \u5185\u7f6e\u4e86\u51e0\u7c7b\u5e38\u7528\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5305\u62ec\u7b80\u5355\u51e0\u4f55\u3001\u590d\u6742\u51e0\u4f55\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> \u51e0\u4f55\u8c03\u7528\u65b9\u5f0f \u542b\u4e49 <code>ppsci.geometry.Interval</code> 1 \u7ef4\u7ebf\u6bb5\u51e0\u4f55 <code>ppsci.geometry.Disk</code> 2 \u7ef4\u5706\u9762\u51e0\u4f55 <code>ppsci.geometry.Polygon</code> 2 \u7ef4\u591a\u8fb9\u5f62\u51e0\u4f55 <code>ppsci.geometry.Rectangle</code> 2 \u7ef4\u77e9\u5f62\u51e0\u4f55 <code>ppsci.geometry.Triangle</code> 2 \u7ef4\u4e09\u89d2\u5f62\u51e0\u4f55 <code>ppsci.geometry.Cuboid</code> 3 \u7ef4\u7acb\u65b9\u4f53\u51e0\u4f55 <code>ppsci.geometry.Sphere</code> 3 \u7ef4\u5706\u7403\u51e0\u4f55 <code>ppsci.geometry.Mesh</code> 3 \u7ef4 Mesh \u51e0\u4f55 <code>ppsci.geometry.PointCloud</code> \u70b9\u4e91\u51e0\u4f55 <code>ppsci.geometry.TimeDomain</code> 1 \u7ef4\u65f6\u95f4\u51e0\u4f55(\u5e38\u7528\u4e8e\u77ac\u6001\u95ee\u9898) <code>ppsci.geometry.TimeXGeometry</code> 1 + N \u7ef4\u5e26\u6709\u65f6\u95f4\u7684\u51e0\u4f55(\u5e38\u7528\u4e8e\u77ac\u6001\u95ee\u9898) <p>\u4ee5\u8ba1\u7b97\u57df\u4e3a 2 \u7ef4\u77e9\u5f62\u51e0\u4f55\u4e3a\u4f8b\uff0c\u5b9e\u4f8b\u5316\u4e00\u4e2a x \u8f74\u8fb9\u957f\u4e3a2\uff0cy \u8f74\u8fb9\u957f\u4e3a 1\uff0c\u4e14\u5de6\u4e0b\u89d2\u4e3a\u70b9 (-1,-3) \u7684\u77e9\u5f62\u51e0\u4f55\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/demo/demo.py<pre><code>LEN_X, LEN_Y = 2, 1  # \u5b9a\u4e49\u77e9\u5f62\u8fb9\u957f\nrect = ppsci.geometry.Rectangle([-1, -3], [-1 + LEN_X, -3 + LEN_Y])  # \u901a\u8fc7\u5de6\u4e0b\u89d2\u3001\u53f3\u4e0a\u89d2\u5bf9\u89d2\u7ebf\u5750\u6807\u6784\u9020\u77e9\u5f62\n</code></pre> <p>\u5176\u4f59\u7684\u51e0\u4f55\u4f53\u6784\u9020\u65b9\u6cd5\u7c7b\u4f3c\uff0c\u53c2\u8003 API \u6587\u6863\u7684 ppsci.geometry \u90e8\u5206\u5373\u53ef\u3002</p>"},{"location":"zh/development/#252","title":"2.5.2 \u6784\u5efa\u65b0\u7684\u51e0\u4f55","text":"<p>\u4e0b\u9762\u4ee5\u6784\u5efa\u4e00\u4e2a\u65b0\u7684\u51e0\u4f55\u4f53 \u2014\u2014 2 \u7ef4\u692d\u5706\uff08\u65e0\u65cb\u8f6c\uff09\u4e3a\u4f8b\u8fdb\u884c\u4ecb\u7ecd\u3002</p> <ol> <li> <p>\u9996\u5148\u6211\u4eec\u9700\u8981\u5728\u4e8c\u7ef4\u51e0\u4f55\u7684\u4ee3\u7801\u6587\u4ef6 <code>ppsci/geometry/geometry_2d.py</code> \u4e2d\u65b0\u5efa\u692d\u5706\u7c7b <code>Ellipse</code>\uff0c\u5e76\u5236\u5b9a\u5176\u76f4\u63a5\u7236\u7c7b\u4e3a <code>geometry.Geometry</code> \u51e0\u4f55\u57fa\u7c7b\u3002 \u7136\u540e\u6839\u636e\u692d\u5706\u7684\u4ee3\u6570\u8868\u793a\u516c\u5f0f\uff1a\\(\\dfrac{x^2}{a^2} + \\dfrac{y^2}{b^2} = 1\\)\uff0c\u53ef\u4ee5\u53d1\u73b0\u8868\u793a\u4e00\u4e2a\u692d\u5706\u9700\u8981\u8bb0\u5f55\u5176\u5706\u5fc3\u5750\u6807 \\((x_0,y_0)\\)\u3001\\(x\\) \u8f74\u534a\u5f84 \\(a\\)\u3001\\(y\\) \u8f74\u534a\u5f84 \\(b\\)\u3002\u56e0\u6b64\u8be5\u692d\u5706\u7c7b\u7684\u4ee3\u7801\u5982\u4e0b\u6240\u793a\u3002</p> ppsci/geometry/geometry_2d.py<pre><code>class Ellipse(geometry.Geometry):\n    def __init__(self, x0: float, y0: float, a: float, b: float)\n        self.center = np.array((x0, y0), dtype=paddle.get_default_dtype())\n        self.a = a\n        self.b = b\n</code></pre> </li> <li> <p>\u4e3a\u692d\u5706\u7c7b\u7f16\u5199\u5fc5\u8981\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <ul> <li> <p>\u5224\u65ad\u7ed9\u5b9a\u70b9\u96c6\u662f\u5426\u5728\u692d\u5706\u5185\u90e8</p> ppsci/geometry/geometry_2d.py<pre><code>def is_inside(self, x):\n    return ((x / self.center) ** 2).sum(axis=1) &lt; 1\n</code></pre> </li> <li> <p>\u5224\u65ad\u7ed9\u5b9a\u70b9\u96c6\u662f\u5426\u5728\u692d\u5706\u8fb9\u754c\u4e0a</p> ppsci/geometry/geometry_2d.py<pre><code>def on_boundary(self, x):\n    return np.isclose(((x / self.center) ** 2).sum(axis=1), 1)\n</code></pre> </li> <li> <p>\u5728\u692d\u5706\u5185\u90e8\u70b9\u968f\u673a\u91c7\u6837(\u6b64\u5904\u4f7f\u7528\u201c\u62d2\u7edd\u91c7\u6837\u6cd5\u201d\u5b9e\u73b0)</p> ppsci/geometry/geometry_2d.py<pre><code>def random_points(self, n, random=\"pseudo\"):\n    res_n = n\n    result = []\n    max_radius = self.center.max()\n    while (res_n &lt; n):\n        rng = sampler.sample(n, 2, random)\n        r, theta = rng[:, 0], 2 * np.pi * rng[:, 1]\n        x = np.sqrt(r) * np.cos(theta)\n        y = np.sqrt(r) * np.sin(theta)\n        candidate = max_radius * np.stack((x, y), axis=1) + self.center\n        candidate = candidate[self.is_inside(candidate)]\n        if len(candidate) &gt; res_n:\n            candidate = candidate[: res_n]\n\n        result.append(candidate)\n        res_n -= len(candidate)\n    result = np.concatenate(result, axis=0)\n    return result\n</code></pre> </li> <li> <p>\u5728\u692d\u5706\u8fb9\u754c\u968f\u673a\u91c7\u6837(\u6b64\u5904\u57fa\u4e8e\u692d\u5706\u53c2\u6570\u65b9\u7a0b\u5b9e\u73b0)</p> ppsci/geometry/geometry_2d.py<pre><code>def random_boundary_points(self, n, random=\"pseudo\"):\n    theta = 2 * np.pi * sampler.sample(n, 1, random)\n    X = np.concatenate((self.a * np.cos(theta),self.b * np.sin(theta)), axis=1)\n    return X + self.center\n</code></pre> </li> </ul> </li> <li> <p>\u5728 <code>ppsci/geometry/__init__.py</code> \u4e2d\u52a0\u5165\u692d\u5706\u7c7b <code>Ellipse</code>\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> ppsci/geometry/__init__.py<pre><code>...\n...\nfrom ppsci.geometry.geometry_2d import Ellipse\n\n__all__ = [\n    ...,\n    ...,\n    \"Ellipse\",\n]\n</code></pre> </li> </ol> <p>\u5b8c\u6210\u4e0a\u8ff0\u5b9e\u73b0\u4e4b\u540e\uff0c\u6211\u4eec\u5c31\u80fd\u4ee5\u5982\u4e0b\u65b9\u5f0f\u5b9e\u4f8b\u5316\u692d\u5706\u7c7b\u3002\u540c\u6837\u5730\uff0c\u5efa\u8bae\u5c06\u6240\u6709\u51e0\u4f55\u7c7b\u5b9e\u4f8b\u5305\u88c5\u5728\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u7d22\u5f15\u3002</p> examples/demo/demo.py<pre><code>ellipse = ppsci.geometry.Ellipse(0, 0, 2, 1)\ngeom = {..., \"ellipse\": ellipse}\n</code></pre>"},{"location":"zh/development/#26","title":"2.6 \u6784\u5efa\u7ea6\u675f\u6761\u4ef6","text":"<p>\u65e0\u8bba\u662f PINNs \u65b9\u6cd5\u8fd8\u662f\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5b83\u4eec\u603b\u662f\u9700\u8981\u5229\u7528\u6570\u636e\u6765\u6307\u5bfc\u7f51\u7edc\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u800c\u8fd9\u4e00\u8fc7\u7a0b\u5728 PaddleScience \u4e2d\u7531 <code>Constraint</code>\uff08\u7ea6\u675f\uff09\u6a21\u5757\u8d1f\u8d23\u3002</p>"},{"location":"zh/development/#261","title":"2.6.1 \u6784\u5efa\u5df2\u6709\u7ea6\u675f","text":"<p>PaddleScience \u5185\u7f6e\u4e86\u4e00\u4e9b\u5e38\u89c1\u7684\u7ea6\u675f\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> \u7ea6\u675f\u540d\u79f0 \u529f\u80fd <code>ppsci.constraint.BoundaryConstraint</code> \u8fb9\u754c\u7ea6\u675f <code>ppsci.constraint.InitialConstraint</code> \u5185\u90e8\u70b9\u521d\u503c\u7ea6\u675f <code>ppsci.constraint.IntegralConstraint</code> \u8fb9\u754c\u79ef\u5206\u7ea6\u675f <code>ppsci.constraint.InteriorConstraint</code> \u5185\u90e8\u70b9\u7ea6\u675f <code>ppsci.constraint.PeriodicConstraint</code> \u8fb9\u754c\u5468\u671f\u7ea6\u675f <code>ppsci.constraint.SupervisedConstraint</code> \u76d1\u7763\u6570\u636e\u7ea6\u675f <p>\u5982\u679c\u60a8\u60f3\u4f7f\u7528\u8fd9\u4e9b\u5185\u7f6e\u7684\u7ea6\u675f\uff0c\u53ef\u4ee5\u76f4\u63a5\u8c03\u7528 <code>ppsci.constraint.*</code> \u4e0b\u7684 API\uff0c\u5e76\u586b\u5165\u7ea6\u675f\u5b9e\u4f8b\u5316\u6240\u9700\u7684\u53c2\u6570\uff0c\u5373\u53ef\u5feb\u901f\u6784\u5efa\u7ea6\u675f\u6761\u4ef6\u3002</p> examples/demo/demo.py<pre><code># create a SupervisedConstraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.MSELoss(\"mean\"),\n    {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n    name=\"Sup\",\n)\n</code></pre> <p>\u7ea6\u675f\u7684\u53c2\u6570\u586b\u5199\u65b9\u5f0f\uff0c\u8bf7\u53c2\u8003\u5bf9\u5e94\u7684 API \u6587\u6863\u53c2\u6570\u8bf4\u660e\u548c\u6837\u4f8b\u4ee3\u7801\u3002</p>"},{"location":"zh/development/#262","title":"2.6.2 \u6784\u5efa\u65b0\u7684\u7ea6\u675f","text":"<p>\u5f53 PaddleScience \u5185\u7f6e\u7684\u7ea6\u675f\u65e0\u6cd5\u6ee1\u8db3\u60a8\u7684\u9700\u6c42\u65f6\uff0c\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7\u65b0\u589e\u7ea6\u675f\u6587\u4ef6\u5e76\u7f16\u5199\u7ea6\u675f\u4ee3\u7801\u7684\u65b9\u5f0f\uff0c\u4f7f\u7528\u60a8\u81ea \u5b9a\u4e49\u7684\u7ea6\u675f\uff0c\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <ol> <li> <p>\u5728 <code>ppsci/constraint</code> \u4e0b\u65b0\u5efa\u7ea6\u675f\u6587\u4ef6\uff08\u6b64\u5904\u4ee5\u7ea6\u675f <code>new_constraint.py</code> \u4e3a\u4f8b\uff09</p> </li> <li> <p>\u5728 <code>new_constraint.py</code> \u6587\u4ef6\u4e2d\u5bfc\u5165 PaddleScience \u7684\u7ea6\u675f\u57fa\u7c7b\u6240\u5728\u6a21\u5757 <code>base</code>\uff0c\u5e76\u8ba9\u521b\u5efa\u7684\u65b0\u7ea6\u675f \u7c7b\uff08\u4ee5 <code>Class NewConstraint</code> \u4e3a\u4f8b\uff09\u4ece <code>base.PDE</code> \u7ee7\u627f</p> ppsci/constraint/new_constraint.py<pre><code>from ppsci.constraint import base\n\nclass NewConstraint(base.Constraint):\n</code></pre> </li> <li> <p>\u7f16\u5199 <code>__init__</code> \u65b9\u6cd5\uff0c\u7528\u4e8e\u7ea6\u675f\u521b\u5efa\u65f6\u7684\u521d\u59cb\u5316\u3002</p> ppsci/constraint/new_constraint.py<pre><code>from ppsci.constraint import base\n\nclass NewConstraint(base.Constraint):\n    def __init__(self, ...):\n        ...\n        # initialization\n</code></pre> </li> <li> <p>\u5728 <code>ppsci/constraint/__init__.py</code> \u4e2d\u5bfc\u5165\u7f16\u5199\u7684\u65b0\u7ea6\u675f\u7c7b\uff0c\u5e76\u6dfb\u52a0\u5230 <code>__all__</code> \u4e2d</p> ppsci/constraint/__init__.py<pre><code>...\n...\nfrom ppsci.constraint.new_constraint import NewConstraint\n\n__all__ = [\n    ...,\n    ...,\n    \"NewConstraint\",\n]\n</code></pre> </li> </ol> <p>\u5b8c\u6210\u4e0a\u8ff0\u65b0\u7ea6\u675f\u4ee3\u7801\u7f16\u5199\u7684\u5de5\u4f5c\u4e4b\u540e\uff0c\u6211\u4eec\u5c31\u80fd\u50cf PaddleScience \u5185\u7f6e\u7ea6\u675f\u4e00\u6837\uff0c\u4ee5 <code>ppsci.constraint.NewConstraint</code> \u7684\u65b9\u5f0f\uff0c\u8c03\u7528\u6211\u4eec\u7f16\u5199\u7684\u65b0\u7ea6\u675f\u7c7b\uff0c\u5e76\u7528\u4e8e\u521b\u5efa\u7ea6\u675f\u5b9e\u4f8b\u3002</p> examples/demo/demo.py<pre><code>new_constraint = ppsci.constraint.NewConstraint(...)\nconstraint = {..., new_constraint.name: new_constraint}\n</code></pre>"},{"location":"zh/development/#27","title":"2.7 \u5b9a\u4e49\u8d85\u53c2\u6570","text":"<p>\u5728\u6a21\u578b\u5f00\u59cb\u8bad\u7ec3\u524d\uff0c\u9700\u8981\u5b9a\u4e49\u4e00\u4e9b\u8bad\u7ec3\u76f8\u5173\u7684\u8d85\u53c2\u6570\uff0c\u5982\u8bad\u7ec3\u8f6e\u6570\u3001\u5b66\u4e60\u7387\u7b49\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> examples/demo/demo.py<pre><code>EPOCHS = 10000\nLEARNING_RATE = 0.001\n</code></pre>"},{"location":"zh/development/#28","title":"2.8 \u6784\u5efa\u4f18\u5316\u5668","text":"<p>\u6a21\u578b\u8bad\u7ec3\u65f6\u9664\u4e86\u6a21\u578b\u672c\u8eab\uff0c\u8fd8\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u7528\u4e8e\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u7684\u4f18\u5316\u5668\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> examples/demo/demo.py<pre><code>optimizer = ppsci.optimizer.Adam(0.001)(model)\n</code></pre>"},{"location":"zh/development/#29","title":"2.9 \u6784\u5efa\u8bc4\u4f30\u5668[\u53ef\u9009]","text":""},{"location":"zh/development/#291","title":"2.9.1 \u6784\u5efa\u5df2\u6709\u8bc4\u4f30\u5668","text":"<p>PaddleScience \u5185\u7f6e\u4e86\u4e00\u4e9b\u5e38\u89c1\u7684\u8bc4\u4f30\u5668\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> \u8bc4\u4f30\u5668\u540d\u79f0 \u529f\u80fd <code>ppsci.validator.GeometryValidator</code> \u51e0\u4f55\u8bc4\u4f30\u5668 <code>ppsci.validator.SupervisedValidator</code> \u76d1\u7763\u6570\u636e\u8bc4\u4f30\u5668 <p>\u5982\u679c\u60a8\u60f3\u4f7f\u7528\u8fd9\u4e9b\u5185\u7f6e\u7684\u8bc4\u4f30\u5668\uff0c\u53ef\u4ee5\u76f4\u63a5\u8c03\u7528 <code>ppsci.validate.*</code> \u4e0b\u7684 API\uff0c\u5e76\u586b\u5165\u8bc4\u4f30\u5668\u5b9e\u4f8b\u5316\u6240\u9700\u7684\u53c2\u6570\uff0c\u5373\u53ef\u5feb\u901f\u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> examples/demo/demo.py<pre><code># create a SupervisedValidator\neta_mse_validator = ppsci.validate.SupervisedValidator(\n    valid_dataloader_cfg,\n    ppsci.loss.MSELoss(\"mean\"),\n    {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"eta_mse\",\n)\n</code></pre>"},{"location":"zh/development/#292","title":"2.9.2 \u6784\u5efa\u65b0\u7684\u8bc4\u4f30\u5668","text":"<p>\u5f53 PaddleScience \u5185\u7f6e\u7684\u8bc4\u4f30\u5668\u65e0\u6cd5\u6ee1\u8db3\u60a8\u7684\u9700\u6c42\u65f6\uff0c\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7\u65b0\u589e\u8bc4\u4f30\u5668\u6587\u4ef6\u5e76\u7f16\u5199\u8bc4\u4f30\u5668\u4ee3\u7801\u7684\u65b9\u5f0f\uff0c\u4f7f \u7528\u60a8\u81ea\u5b9a\u4e49\u7684\u8bc4\u4f30\u5668\uff0c\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <ol> <li> <p>\u5728 <code>ppsci/validate</code> \u4e0b\u65b0\u5efa\u8bc4\u4f30\u5668\u6587\u4ef6\uff08\u6b64\u5904\u4ee5 <code>new_validator.py</code> \u4e3a\u4f8b\uff09\u3002</p> </li> <li> <p>\u5728 <code>new_validator.py</code> \u6587\u4ef6\u4e2d\u5bfc\u5165 PaddleScience \u7684\u8bc4\u4f30\u5668\u57fa\u7c7b\u6240\u5728\u6a21\u5757 <code>base</code>\uff0c\u5e76\u8ba9\u521b\u5efa\u7684\u65b0\u8bc4\u4f30\u5668\u7c7b\uff08\u4ee5 <code>Class NewValidator</code> \u4e3a\u4f8b\uff09\u4ece <code>base.Validator</code> \u7ee7\u627f\u3002</p> ppsci/validate/new_validator.py<pre><code>from ppsci.validate import base\n\nclass NewValidator(base.Validator):\n</code></pre> </li> <li> <p>\u7f16\u5199 <code>__init__</code> \u4ee3\u7801\uff0c\u7528\u4e8e\u8bc4\u4f30\u5668\u521b\u5efa\u65f6\u7684\u521d\u59cb\u5316</p> ppsci/validate/new_validator.py<pre><code>from ppsci.validate import base\n\nclass NewValidator(base.Validator):\n    def __init__(self, ...):\n        ...\n        # initialization\n</code></pre> </li> <li> <p>\u5728 <code>ppsci/validate/__init__.py</code> \u4e2d\u5bfc\u5165\u7f16\u5199\u7684\u65b0\u8bc4\u4f30\u5668\u7c7b\uff0c\u5e76\u6dfb\u52a0\u5230 <code>__all__</code> \u4e2d\u3002</p> ppsci/validate/__init__.py<pre><code>...\n...\nfrom ppsci.validate.new_validator import NewValidator\n\n__all__ = [\n    ...,\n    ...,\n    \"NewValidator\",\n]\n</code></pre> </li> </ol> <p>\u5b8c\u6210\u4e0a\u8ff0\u65b0\u8bc4\u4f30\u5668\u4ee3\u7801\u7f16\u5199\u7684\u5de5\u4f5c\u4e4b\u540e\uff0c\u6211\u4eec\u5c31\u80fd\u50cf PaddleScience \u5185\u7f6e\u8bc4\u4f30\u5668\u4e00\u6837\uff0c\u4ee5 <code>ppsci.validate.NewValidator</code> \u7684\u65b9\u5f0f\uff0c\u8c03\u7528\u6211\u4eec\u7f16\u5199\u7684\u65b0\u8bc4\u4f30\u5668\u7c7b\uff0c\u5e76\u7528\u4e8e\u521b\u5efa\u8bc4\u4f30\u5668\u5b9e\u4f8b\u3002\u540c\u6837\u5730\uff0c\u5728\u8bc4\u4f30\u5668\u6784\u5efa\u5b8c\u6bd5\u540e\u4e4b\u540e\uff0c\u5efa\u8bae\u5c06\u6240\u6709\u8bc4\u4f30\u5668\u5305\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\u65b9\u4fbf\u540e\u7eed\u7d22\u5f15\u3002</p> examples/demo/demo.py<pre><code>new_validator = ppsci.validate.NewValidator(...)\nvalidator = {..., new_validator.name: new_validator}\n</code></pre>"},{"location":"zh/development/#210","title":"2.10 \u6784\u5efa\u53ef\u89c6\u5316\u5668[\u53ef\u9009]","text":"<p>PaddleScience \u5185\u7f6e\u4e86\u4e00\u4e9b\u5e38\u89c1\u7684\u53ef\u89c6\u5316\u5668\uff0c\u5982 <code>VisualizerVtu</code> \u53ef\u89c6\u5316\u5668\u7b49\uff0c\u5982\u679c\u60a8\u60f3\u4f7f\u7528\u8fd9\u4e9b\u5185\u7f6e\u7684\u53ef\u89c6 \u5316\u5668\uff0c\u53ef\u4ee5\u76f4\u63a5\u8c03\u7528 <code>ppsci.visualizer.*</code> \u4e0b\u7684 API\uff0c\u5e76\u586b\u5165\u53ef\u89c6\u5316\u5668\u5b9e\u4f8b\u5316\u6240\u9700\u7684 \u53c2\u6570\uff0c\u5373\u53ef\u5feb\u901f\u6784\u5efa\u6a21\u578b\u3002</p> examples/demo/demo.py<pre><code># manually collate input data for visualization,\n# interior+boundary\nvis_points = {}\nfor key in vis_interior_points:\n    vis_points[key] = np.concatenate(\n        (vis_interior_points[key], vis_boundary_points[key])\n    )\n\nvisualizer = {\n    \"visualize_u_v\": ppsci.visualize.VisualizerVtu(\n        vis_points,\n        {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"p\": lambda d: d[\"p\"]},\n        prefix=\"result_u_v\",\n    )\n}\n</code></pre> <p>\u5982\u9700\u65b0\u589e\u53ef\u89c6\u5316\u5668\uff0c\u6b65\u9aa4\u4e0e\u5176\u4ed6\u6a21\u5757\u7684\u65b0\u589e\u65b9\u6cd5\u7c7b\u4f3c\uff0c\u6b64\u5904\u4e0d\u518d\u8d58\u8ff0\u3002</p>"},{"location":"zh/development/#211-solver","title":"2.11 \u6784\u5efaSolver","text":"<p><code>Solver</code> \u662f PaddleScience \u8d1f\u8d23\u8c03\u7528\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u7684\u5168\u5c40\u7ba1\u7406\u7c7b\u3002\u5728\u8bad\u7ec3\u5f00\u59cb\u524d\uff0c\u9700\u8981\u628a\u6784\u5efa\u597d\u7684\u6a21\u578b\u3001\u7ea6\u675f\u3001\u4f18\u5316\u5668\u7b49\u5b9e\u4f8b\u4f20\u7ed9 <code>Solver</code> \u4ee5\u5b9e\u4f8b\u5316\uff0c\u518d\u8c03\u7528\u5b83\u7684\u5185\u7f6e\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> examples/demo/demo.py<pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    output_dir,\n    optimizer,\n    lr_scheduler,\n    EPOCHS,\n    iters_per_epoch,\n    eval_during_train=True,\n    eval_freq=eval_freq,\n    equation=equation,\n    validator=validator,\n    visualizer=visualizer,\n)\n</code></pre>"},{"location":"zh/development/#212","title":"2.12 \u7f16\u5199\u914d\u7f6e\u6587\u4ef6[\u91cd\u8981]","text":"\u5185\u5bb9\u8f83\u957f\uff0c\u70b9\u51fb\u5c55\u5f00 <p>\u7ecf\u8fc7\u4e0a\u8ff0\u6b65\u9aa4\u7684\u5f00\u53d1\uff0c\u6848\u4f8b\u4ee3\u7801\u7684\u4e3b\u8981\u90e8\u5206\u5df2\u7ecf\u5b8c\u6210\u3002 \u5f53\u6211\u4eec\u60f3\u57fa\u4e8e\u8fd9\u4efd\u4ee3\u7801\u8fd0\u884c\u4e00\u4e9b\u8c03\u4f18\u5b9e\u9a8c\uff0c\u4ece\u800c\u5f97\u5230\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u6216\u66f4\u597d\u5730\u5bf9\u8fd0\u884c\u53c2\u6570\u8bbe\u7f6e\u8fdb\u884c\u7ba1\u7406\uff0c \u5219\u53ef\u4ee5\u5229\u7528 PaddleScience \u63d0\u4f9b\u7684\u914d\u7f6e\u7ba1\u7406\u7cfb\u7edf\uff0c\u5c06\u5b9e\u9a8c\u8fd0\u884c\u53c2\u6570\u4ece\u4ee3\u7801\u4e2d\u5206\u79bb\u51fa\u6765\uff0c\u5199\u5230 <code>yaml</code> \u683c\u5f0f\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u4ece\u800c\u66f4\u597d\u7684\u7ba1\u7406\u3001\u8bb0\u5f55\u3001\u8c03\u4f18\u5b9e\u9a8c\u3002</p> <p>\u4ee5 <code>viv</code> \u6848\u4f8b\u4ee3\u7801\u4e3a\u4f8b\uff0c\u5728\u8fd0\u884c\u65f6\u6211\u4eec\u9700\u8981\u5728\u9002\u5f53\u7684\u4f4d\u7f6e\u8bbe\u7f6e\u65b9\u7a0b\u53c2\u6570\u3001STL \u6587\u4ef6\u8def\u5f84\u3001\u8bad\u7ec3\u8f6e\u6570\u3001<code>batch_size</code>\u3001\u968f\u673a\u79cd\u5b50\u3001\u5b66\u4e60\u7387\u7b49\u8d85\u53c2\u6570\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>...\n# set dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"MatDataset\",\n        \"file_path\": cfg.VIV_DATA_PATH,\n        \"input_keys\": (\"t_f\",),\n        \"label_keys\": (\"eta\", \"f\"),\n        \"weight_dict\": {\"eta\": 100},\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": True,\n    },\n}\n...\n...\n# set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.Step(**cfg.TRAIN.lr_scheduler)()\n...\n</code></pre> <p>\u8fd9\u4e9b\u53c2\u6570\u5728\u5b9e\u9a8c\u8fc7\u7a0b\u4e2d\u968f\u65f6\u53ef\u80fd\u4f5c\u4e3a\u53d8\u91cf\u800c\u88ab\u624b\u52a8\u8c03\u6574\uff0c\u5728\u8c03\u6574\u8fc7\u7a0b\u4e2d\u5982\u4f55\u907f\u514d\u9891\u7e41\u4fee\u6539\u6e90\u4ee3\u7801\u5bfc\u81f4\u8bd5\u9a8c\u8bb0\u5f55\u6df7\u4e71\u3001\u4fdd\u969c\u8bb0\u5f55\u5b8c\u6574\u53ef\u8ffd\u6eaf\u4fbf\u662f\u4e00\u5927\u95ee\u9898\uff0c\u56e0\u6b64 PaddleScience \u63d0\u4f9b\u4e86\u57fa\u4e8e hydra + omegaconf \u7684 \u914d\u7f6e\u6587\u4ef6\u7ba1\u7406\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002</p> <p>\u5c06\u5df2\u6709\u7684\u4ee3\u7801\u4fee\u6539\u6210\u914d\u7f6e\u6587\u4ef6\u63a7\u5236\u7684\u65b9\u5f0f\u975e\u5e38\u7b80\u5355\uff0c\u53ea\u9700\u8981\u5c06\u5fc5\u8981\u7684\u53c2\u6570\u5199\u5230 <code>yaml</code> \u6587\u4ef6\u4e2d\uff0c\u7136\u540e\u901a\u8fc7 hydra \u5728\u7a0b\u5e8f\u8fd0\u884c\u65f6\u8bfb\u53d6\u3001\u89e3\u6790\u8be5\u6587\u4ef6\uff0c\u901a\u8fc7\u5176\u5185\u5bb9\u63a7\u5236\u5b9e\u9a8c\u8fd0\u884c\u5373\u53ef\uff0c\u4ee5 <code>viv</code> \u6848\u4f8b\u4e3a\u4f8b\uff0c\u5177\u4f53\u5305\u542b\u4ee5\u4e0b\u51e0\u4e2a\u6b65\u9aa4\u3002</p> <ol> <li> <p>\u5219\u9700\u5728\u4ee3\u7801\u6587\u4ef6 <code>viv.py</code> \u6240\u5728\u76ee\u5f55\u4e0b\u65b0\u5efa <code>conf</code> \u6587\u4ef6\u5939\uff0c\u5e76\u5728 <code>conf</code> \u4e0b\u65b0\u5efa\u4e0e <code>viv.py</code> \u540c\u540d\u7684 <code>viv.yaml</code> \u6587\u4ef6\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>PaddleScience/examples/fsi/\n\u251c\u2500\u2500 viv.py\n\u2514\u2500\u2500 conf\n    \u2514\u2500\u2500 viv.yaml\n</code></pre> </li> <li> <p>\u5c06 <code>viv.py</code> \u4e2d\u5fc5\u8981\u7684\u8d85\u53c2\u6570\u6309\u7167\u5176\u8bed\u4e49\u586b\u5199\u5230 <code>viv.yaml</code> \u7684\u5404\u4e2a\u5c42\u7ea7\u7684\u914d\u7f6e\u4e2d\uff0c\u5982\u901a\u7528\u53c2\u6570 <code>mode</code>\u3001<code>output_dir</code>\u3001<code>seed</code>\u3001\u65b9\u7a0b\u53c2\u6570\u3001\u6587\u4ef6\u8def\u5f84\u7b49\uff0c\u76f4\u63a5\u586b\u5199\u5728\u4e00\u7ea7\u5c42\u7ea7\uff1b\u800c\u53ea\u4e0e\u6a21\u578b\u3001\u8bad\u7ec3\u76f8\u5173\u7684\u6a21\u578b\u7ed3\u6784\u53c2\u6570\u3001\u8bad\u7ec3\u8f6e\u6570\u7b49\uff0c\u53ea\u9700\u5206\u522b\u586b\u5199\u5728 <code>MODEL</code>\u3001<code>TRAIN</code> \u5c42\u7ea7\u4e0b\u5373\u53ef\uff08<code>EVAL</code> \u5c42\u7ea7\u540c\u7406\uff09\u3002</p> </li> <li> <p>\u5c06\u5df2\u6709\u7684 <code>train</code> \u548c <code>evaluate</code> \u51fd\u6570\u4fee\u6539\u4e3a\u63a5\u53d7\u4e00\u4e2a\u53c2\u6570 <code>cfg</code>\uff08<code>cfg</code> \u5373\u4e3a\u8bfb\u53d6\u8fdb\u6765\u7684 <code>yaml</code> \u6587\u4ef6\u91cc\u7684\u5185\u5bb9\uff0c\u5e76\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b58\u50a8\uff09\uff0c\u5e76\u5c06\u5176\u5185\u90e8\u7684\u8d85\u53c2\u6570\u7edf\u4e00\u6539\u4e3a\u901a\u8fc7 <code>cfg.xxx</code> \u83b7\u53d6\u800c\u975e\u539f\u5148\u7684\u76f4\u63a5\u8bbe\u7f6e\u4e3a\u6570\u5b57\u6216\u5b57\u7b26\u4e32\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>from omegaconf import DictConfig\n\ndef train(cfg: DictConfig):\n    # \u8bad\u7ec3\u4ee3\u7801...\n\ndef evaluate(cfg: DictConfig):\n    # \u8bc4\u4f30\u4ee3\u7801...\n</code></pre> </li> <li> <p>\u65b0\u5efa\u4e00\u4e2a <code>main</code> \u51fd\u6570\uff08\u540c\u6837\u63a5\u53d7\u4e14\u53ea\u63a5\u53d7\u4e00\u4e2a <code>cfg</code> \u53c2\u6570\uff09\uff0c\u5b83\u8d1f\u8d23\u6839\u636e <code>cfg.mode</code> \u6765\u8c03\u7528 <code>train</code> \u6216 <code>evaluate</code> \u51fd\u6570\uff0c\u5e76 <code>main</code> \u51fd\u6570\u52a0\u4e0a\u88c5\u9970\u5668 <code>@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"viv.yaml\")</code>\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"viv.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n</code></pre> </li> <li> <p>\u5728\u4e3b\u7a0b\u5e8f\u7684\u542f\u52a8\u5165\u53e3 <code>if __name__ == \"__main__\":</code> \u4e2d\u542f\u52a8 <code>main()</code> \u5373\u53ef\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>if __name__ == \"__main__\":\n    main()\n</code></pre> </li> </ol> <p>\u5168\u90e8\u6539\u9020\u5b8c\u6bd5\u540e\uff0c<code>viv.py</code> \u548c <code>viv.yaml</code> \u5982\u4e0b\u6240\u793a\u3002</p> examples/fsi/viv.pyexamples/fsi/conf/viv.yaml <pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hydra\nfrom omegaconf import DictConfig\n\nimport ppsci\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"VIV\": ppsci.equation.Vibration(2, -4, 0)}\n\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"MatDataset\",\n                \"file_path\": cfg.VIV_DATA_PATH,\n                \"input_keys\": (\"t_f\",),\n                \"label_keys\": (\"eta\", \"f\"),\n                \"weight_dict\": {\"eta\": 100},\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n        name=\"Sup\",\n    )\n    # wrap constraints together\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Step(**cfg.TRAIN.lr_scheduler)()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)((model,) + tuple(equation.values()))\n\n    # set validator\n    eta_l2_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"MatDataset\",\n                \"file_path\": cfg.VIV_DATA_PATH,\n                \"input_keys\": (\"t_f\",),\n                \"label_keys\": (\"eta\", \"f\"),\n            },\n            \"batch_size\": cfg.EVAL.batch_size,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n        metric={\"MSE\": ppsci.metric.L2Rel()},\n        name=\"eta_l2\",\n    )\n    validator = {eta_l2_validator.name: eta_l2_validator}\n\n    # set visualizer(optional)\n    visu_mat = ppsci.utils.reader.load_mat_file(\n        cfg.VIV_DATA_PATH,\n        (\"t_f\", \"eta_gt\", \"f_gt\"),\n        alias_dict={\"eta_gt\": \"eta\", \"f_gt\": \"f\"},\n    )\n    visualizer = {\n        \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n            visu_mat,\n            (\"t_f\",),\n            {\n                r\"$\\eta$\": lambda d: d[\"eta\"],  # plot with latex title\n                r\"$\\eta_{gt}$\": lambda d: d[\"eta_gt\"],  # plot with latex title\n                r\"$f$\": equation[\"VIV\"].equations[\"f\"],  # plot with latex title\n                r\"$f_{gt}$\": lambda d: d[\"f_gt\"],  # plot with latex title\n            },\n            num_timestamps=1,\n            prefix=\"viv_pred\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        optimizer=optimizer,\n        equation=equation,\n        validator=validator,\n        visualizer=visualizer,\n        cfg=cfg,\n    )\n\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"VIV\": ppsci.equation.Vibration(2, -4, 0)}\n\n    # set validator\n    eta_l2_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"MatDataset\",\n                \"file_path\": cfg.VIV_DATA_PATH,\n                \"input_keys\": (\"t_f\",),\n                \"label_keys\": (\"eta\", \"f\"),\n            },\n            \"batch_size\": cfg.EVAL.batch_size,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n        metric={\"MSE\": ppsci.metric.L2Rel()},\n        name=\"eta_l2\",\n    )\n    validator = {eta_l2_validator.name: eta_l2_validator}\n\n    # set visualizer(optional)\n    visu_mat = ppsci.utils.reader.load_mat_file(\n        cfg.VIV_DATA_PATH,\n        (\"t_f\", \"eta_gt\", \"f_gt\"),\n        alias_dict={\"eta_gt\": \"eta\", \"f_gt\": \"f\"},\n    )\n\n    visualizer = {\n        \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n            visu_mat,\n            (\"t_f\",),\n            {\n                r\"$\\eta$\": lambda d: d[\"eta\"],  # plot with latex title\n                r\"$\\eta_{gt}$\": lambda d: d[\"eta_gt\"],  # plot with latex title\n                r\"$f$\": equation[\"VIV\"].equations[\"f\"],  # plot with latex title\n                r\"$f_{gt}$\": lambda d: d[\"f_gt\"],  # plot with latex title\n            },\n            num_timestamps=1,\n            prefix=\"viv_pred\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        equation=equation,\n        validator=validator,\n        visualizer=visualizer,\n        cfg=cfg,\n    )\n\n    # evaluate\n    solver.eval()\n    # visualize prediction\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    from paddle import nn\n    from paddle.static import InputSpec\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n    # initialize equation\n    equation = {\"VIV\": ppsci.equation.Vibration(2, -4, 0)}\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        equation=equation,\n        cfg=cfg,\n    )\n    # Convert equation to func\n    f_func = ppsci.lambdify(\n        solver.equation[\"VIV\"].equations[\"f\"],\n        solver.model,\n        list(solver.equation[\"VIV\"].learnable_parameters),\n    )\n\n    class Wrapped_Model(nn.Layer):\n        def __init__(self, model, func):\n            super().__init__()\n            self.model = model\n            self.func = func\n\n        def forward(self, x):\n            model_out = self.model(x)\n            func_out = self.func(x)\n            return {**model_out, \"f\": func_out}\n\n    solver.model = Wrapped_Model(model, f_func)\n    # export models\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path, skip_prune_program=True)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    # set model predictor\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    infer_mat = ppsci.utils.reader.load_mat_file(\n        cfg.VIV_DATA_PATH,\n        (\"t_f\", \"eta_gt\", \"f_gt\"),\n        alias_dict={\"eta_gt\": \"eta\", \"f_gt\": \"f\"},\n    )\n\n    input_dict = {key: infer_mat[key] for key in cfg.INFER.input_keys}\n\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.INFER.output_keys, output_dict.keys())\n    }\n    infer_mat.update(output_dict)\n\n    ppsci.visualize.plot.save_plot_from_1d_dict(\n        \"./viv_pred\", infer_mat, (\"t_f\",), (\"eta\", \"eta_gt\", \"f\", \"f_gt\")\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"viv.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>defaults: # (1)\n  - ppsci_default # (2)\n  - TRAIN: train_default # (3)\n  - TRAIN/ema: ema_default # (4)\n  - TRAIN/swa: swa_default # (5)\n  - EVAL: eval_default # (6)\n  - INFER: infer_default # (7)\n  - _self_ # (8)\n\nhydra:\n  run:\n    # dynamic output directory according to running time and override name\n    dir: outputs_VIV/${now:%Y-%m-%d}/${now:%H-%M-%S}/${hydra.job.override_dirname} # (9)\n  job:\n    name: ${mode} # name of logfile\n    chdir: false # keep current working directory unchanged\n  callbacks:\n    init_callback: # (10)\n      _target_: ppsci.utils.callbacks.InitCallback # (11)\n  sweep:\n    # output directory for multirun\n    dir: ${hydra.run.dir}\n    subdir: ./\n\n# general settings\nmode: train # running mode: train/eval # (12)\nseed: 42 # (13)\noutput_dir: ${hydra:run.dir} # (14)\nlog_freq: 20 # (15)\nuse_tbd: false # (16)\n\nVIV_DATA_PATH: \"./VIV_Training_Neta100.mat\" # (17)\n\n# model settings\nMODEL: # (18)\n  input_keys: [\"t_f\"] # (19)\n  output_keys: [\"eta\"] # (20)\n  num_layers: 5 # (21)\n  hidden_size: 50 # (22)\n  activation: \"tanh\" # (23)\n\n# training settings\nTRAIN: # (24)\n  epochs: 100000 # (25)\n  iters_per_epoch: 1 # (26)\n  save_freq: 10000 # (27)\n  eval_during_train: true # (28)\n  eval_freq: 1000 # (29)\n  batch_size: 100 # (30)\n  lr_scheduler: # (31)\n    epochs: ${TRAIN.epochs} # (32)\n    iters_per_epoch: ${TRAIN.iters_per_epoch} # (33)\n    learning_rate: 0.001 # (34)\n    step_size: 20000 # (35)\n    gamma: 0.9 # (36)\n  pretrained_model_path: null # (37)\n  checkpoint_path: null # (38)\n\n# evaluation settings\nEVAL: # (39)\n  pretrained_model_path: null # (40)\n  batch_size: 32 # (41)\n\n# inference settings\nINFER: # (42)\n  pretrained_model_path: \"https://paddle-org.bj.bcebos.com/paddlescience/models/viv/viv_pretrained.pdparams\" # (43)\n  export_path: ./inference/viv # (44)\n  pdmodel_path: ${INFER.export_path}.pdmodel # (45)\n  pdiparams_path: ${INFER.export_path}.pdiparams # (46)\n  input_keys: ${MODEL.input_keys} # (47)\n  output_keys: [\"eta\", \"f\"] # (48)\n  device: gpu # (49)\n  engine: native # (50)\n  precision: fp32 # (51)\n  onnx_path: ${INFER.export_path}.onnx # (52)\n  ir_optim: true # (53)\n  min_subgraph_size: 10 # (54)\n  gpu_mem: 4000 # (55)\n  gpu_id: 0 # (56)\n  max_batch_size: 64 # (57)\n  num_cpu_threads: 4 # (58)\n  batch_size: 16 # (59)\n</code></pre> <ol> <li><code>defaults:</code> - \u5b9a\u4e49\u4e00\u7cfb\u5217\u9ed8\u8ba4\u914d\u7f6e\u9879\u7684\u5f00\u5934\u3002</li> <li><code>- ppsci_default</code> - \u4f7f\u7528\u540d\u4e3a<code>ppsci_default</code>\u7684\u9ed8\u8ba4\u914d\u7f6e\u3002</li> <li><code>- TRAIN: train_default</code> - \u5728<code>TRAIN</code>\u547d\u540d\u7a7a\u95f4\u4e0b\u5e94\u7528<code>train_default</code>\u9ed8\u8ba4\u914d\u7f6e\u3002</li> <li><code>- TRAIN/ema: ema_default</code> - \u5728<code>TRAIN</code>\u7684<code>ema</code>\u5b50\u547d\u540d\u7a7a\u95f4\u4e0b\u5e94\u7528<code>ema_default</code>\u914d\u7f6e\u3002</li> <li><code>- TRAIN/swa: swa_default</code> - \u5728<code>TRAIN</code>\u7684<code>swa</code>\u5b50\u547d\u540d\u7a7a\u95f4\u4e0b\u5e94\u7528<code>swa_default</code>\u914d\u7f6e\u3002</li> <li><code>- EVAL: eval_default</code> - \u5728<code>EVAL</code>\u547d\u540d\u7a7a\u95f4\u4e0b\u5e94\u7528<code>eval_default</code>\u9ed8\u8ba4\u914d\u7f6e\u3002</li> <li><code>- INFER: infer_default</code> - \u5728<code>INFER</code>\u547d\u540d\u7a7a\u95f4\u4e0b\u5e94\u7528<code>infer_default</code>\u9ed8\u8ba4\u914d\u7f6e\u3002</li> <li><code>- _self_</code> - \u8868\u793a\u5f53\u524d\u6587\u4ef6\u672c\u8eab\u7684\u914d\u7f6e\u5c06\u88ab\u5305\u542b\uff0c\u7528\u4e8e\u8986\u76d6\u6216\u6dfb\u52a0\u9ed8\u8ba4\u8bbe\u7f6e\u3002</li> <li><code>dir: outputs_VIV/...</code> - \u8bbe\u7f6e\u52a8\u6001\u8f93\u51fa\u76ee\u5f55\uff0c\u57fa\u4e8e\u5f53\u524d\u65f6\u95f4\u4ee5\u53ca\u8986\u76d6\u540d\u79f0\u3002</li> <li><code>init_callback:</code> - \u5b9a\u4e49\u521d\u59cb\u5316\u56de\u8c03\u7684\u8bbe\u7f6e\u3002</li> <li><code>_target_: ppsci.utils.callbacks.InitCallback</code> - \u6307\u5b9a\u521d\u59cb\u5316\u56de\u8c03\u7684\u5177\u4f53\u5b9e\u73b0\u7c7b\u6216\u51fd\u6570\u3002</li> <li><code>mode: train</code> - \u8bbe\u7f6e\u5f53\u524d\u8fd0\u884c\u6a21\u5f0f\u4e3a\u8bad\u7ec3\u6a21\u5f0f\u3002</li> <li><code>seed: 42</code> - \u8bbe\u7f6e\u5168\u5c40\u968f\u673a\u79cd\u5b50\u4e3a42\uff0c\u4ee5\u786e\u4fdd\u5b9e\u9a8c\u53ef\u91cd\u590d\u6027\u3002</li> <li><code>output_dir: ${hydra:run.dir}</code> - \u8bbe\u7f6e\u8f93\u51fa\u76ee\u5f55\uff0c\u4e0eHydra\u914d\u7f6e\u7684\u8fd0\u884c\u76ee\u5f55\u76f8\u540c\u3002</li> <li><code>log_freq: 20</code> - \u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u9891\u7387\uff0c\u6bcf20\u6b21\u8fdb\u884c\u8bb0\u5f55\u3002</li> <li><code>use_tbd: false</code> - \u5173\u95ed tensorboard \u529f\u80fd\u3002</li> <li><code>VIV_DATA_PATH: \"./VIV_Training_Neta100.mat\"</code> - \u6307\u5b9aVIV\u7684\u6570\u636e\u6587\u4ef6\u8def\u5f84\uff08\u53ef\u4ee5\u5176\u5b83\u6570\u636e\u96c6\u6216\u8d85\u53c2\u503c\uff09\u3002</li> <li><code>MODEL:</code> - \u5f00\u59cb\u5b9a\u4e49\u6a21\u578b\u76f8\u5173\u8bbe\u7f6e\u3002</li> <li><code>input_keys: [\"t_f\"]</code> - \u8bbe\u7f6e\u6a21\u578b\u7684\u8f93\u5165\u952e\u4e3a<code>t_f</code>\u3002</li> <li><code>output_keys: [\"eta\"]</code> - \u8bbe\u7f6e\u6a21\u578b\u7684\u8f93\u51fa\u952e\u4e3a<code>eta</code>\u3002</li> <li><code>num_layers: 5</code> - \u8bbe\u7f6e\u6a21\u578b\u5c42\u6570\u4e3a5\u3002</li> <li><code>hidden_size: 50</code> - \u8bbe\u7f6e\u6a21\u578b\u9690\u85cf\u5c42\u7684\u5927\u5c0f\u4e3a50\u3002</li> <li><code>activation: \"tanh\"</code> - \u4f7f\u7528\u53cc\u66f2\u6b63\u5207\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u3002</li> <li><code>TRAIN:</code> - \u5f00\u59cb\u5b9a\u4e49\u8bad\u7ec3\u76f8\u5173\u8bbe\u7f6e\u3002</li> <li><code>epochs: 100000</code> - \u8bbe\u7f6e\u8bad\u7ec3\u7684\u603b\u8f6e\u6570\u4e3a100000\u8f6e\u3002</li> <li><code>iters_per_epoch: 1</code> - \u6bcf\u4e2a\u8f6e\u6b21\u5305\u542b1\u6b21\u8fed\u4ee3\u3002</li> <li><code>save_freq: 10000</code> - \u6bcf10000\u6b21\u8fed\u4ee3\u4fdd\u5b58\u4e00\u6b21\u6a21\u578b\u6216\u68c0\u67e5\u70b9\u3002</li> <li><code>eval_during_train: true</code> - \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u542f\u7528\u8bc4\u4f30\u3002</li> <li><code>eval_freq: 1000</code> - \u6bcf1000\u6b21\u8fed\u4ee3\u8fdb\u884c\u4e00\u6b21\u8bc4\u4f30\u3002</li> <li><code>batch_size: 100</code> - \u8bbe\u7f6e\u8bad\u7ec3\u6279\u6b21\u5927\u5c0f\u4e3a100\u3002</li> <li><code>lr_scheduler:</code> - \u5f00\u59cb\u5b9a\u4e49\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u7684\u8bbe\u7f6e\u3002</li> <li><code>epochs: ${TRAIN.epochs}</code> - \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u4f7f\u7528\u7684\u8f6e\u6570\u4e0e\u8bad\u7ec3\u7684\u8f6e\u6570\u76f8\u540c\u3002</li> <li><code>iters_per_epoch: ${TRAIN.iters_per_epoch}</code> - \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u4f7f\u7528\u7684\u6bcf\u4e2a\u8f6e\u6b21\u7684\u8fed\u4ee3\u6570\u4e0e\u8bad\u7ec3\u7684\u76f8\u540c\u3002</li> <li><code>learning_rate: 0.001</code> - \u8bbe\u7f6e\u521d\u59cb\u5b66\u4e60\u7387\u4e3a0.001\u3002</li> <li><code>step_size: 20000</code> - \u6bcf20000\u6b65\u8c03\u6574\u5b66\u4e60\u7387\u3002</li> <li><code>gamma: 0.9</code> - \u5b66\u4e60\u7387\u8c03\u6574\u7684\u6bd4\u7387\u4e3a0.9\u3002</li> <li><code>pretrained_model_path: null</code> - \u521d\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\u8def\u5f84\uff0c\u53ef\u4ee5\u662f\u5b9e\u9645\u8def\u5f84\u6216 url\uff0c\u9ed8\u8ba4\u4e3a\u7a7a\u8868\u793a\u4e0d\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u3002</li> <li><code>checkpoint_path: null</code> - \u68c0\u67e5\u70b9\u8def\u5f84\uff0c\u7528\u4e8e\u6307\u5b9a\u6a21\u578b\u52a0\u8f7d\u7684\u68c0\u67e5\u70b9\uff0c\u9ed8\u8ba4\u4e3a\u7a7a\u8868\u793a\u4e0d\u52a0\u8f7d\u68c0\u67e5\u70b9\u3002</li> <li><code>EVAL:</code> - \u5f00\u59cb\u5b9a\u4e49\u8bc4\u4f30\u76f8\u5173\u8bbe\u7f6e\u3002</li> <li><code>pretrained_model_path: null</code> - \u8bc4\u4f30\u65f6\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3a\u7a7a\u8868\u793a\u4e0d\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u3002</li> <li><code>batch_size: 32</code> - \u8bc4\u4f30\u65f6\u7684\u6279\u6b21</li> <li><code>INFER:</code> - \u5f00\u59cb\u5b9a\u4e49\u63a8\u7406\uff08\u63a8\u65ad\uff09\u76f8\u5173\u7684\u8bbe\u7f6e\u3002</li> <li><code>pretrained_model_path: \"https://...\"</code> - \u6307\u5b9a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8def\u5f84\uff0c\u53ef\u4ee5\u662f\u5b9e\u9645\u8def\u5f84\u6216 url\uff0c\u7528\u4e8e\u63a8\u7406\u3002</li> <li><code>export_path: ./inference/viv</code> - \u8bbe\u7f6e\u6a21\u578b\u5bfc\u51fa\u8def\u5f84\uff0c\u5373\u63a8\u7406\u6240\u9700\u7684\u6a21\u578b\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u3002</li> <li><code>pdmodel_path: ${INFER.export_path}.pdmodel</code> - \u6307\u5b9aPaddle\u6a21\u578b\u7684\u7ed3\u6784\u6587\u4ef6\u8def\u5f84\u3002</li> <li><code>pdiparams_path: ${INFER.export_path}.pdiparams</code> - \u6307\u5b9aPaddle\u6a21\u578b\u7684\u53c2\u6570\u6587\u4ef6\u8def\u5f84\u3002</li> <li><code>input_keys: ${MODEL.input_keys}</code> - \u63a8\u7406\u65f6\u4f7f\u7528\u7684\u8f93\u5165\u952e\u4e0e\u6a21\u578b\u5b9a\u4e49\u65f6\u76f8\u540c\u3002</li> <li><code>output_keys: [\"eta\", \"f\"]</code> - \u8bbe\u7f6e\u63a8\u7406\u65f6\u7684\u8f93\u51fa\u952e\uff0c\u5305\u62ec\"eta\"\u548c\"f\"\u3002</li> <li><code>device: gpu</code> - \u6307\u5b9a\u63a8\u7406\u4f7f\u7528\u7684\u8bbe\u5907\u4e3aGPU\u3002</li> <li><code>engine: native</code> - \u8bbe\u7f6e\u63a8\u7406\u5f15\u64ce\u4e3a\u539f\u751f\uff08\u53ef\u80fd\u662f\u6307\u4f7f\u7528\u7279\u5b9a\u5e93\u6216\u6846\u67b6\u7684\u9ed8\u8ba4\u5f15\u64ce\uff09\u3002</li> <li><code>precision: fp32</code> - \u8bbe\u7f6e\u63a8\u7406\u7684\u7cbe\u5ea6\u4e3a32\u4f4d\u6d6e\u70b9\u6570\u3002</li> <li><code>onnx_path: ${INFER.export_path}.onnx</code> - \u5982\u679c\u652f\u6301\uff0c\u6307\u5b9aONNX\u6a21\u578b\u6587\u4ef6\u7684\u8def\u5f84\u3002</li> <li><code>ir_optim: true</code> - \u542f\u7528\u4e2d\u95f4\u8868\u793a\uff08Intermediate Representation\uff09\u7684\u4f18\u5316\u3002</li> <li><code>min_subgraph_size: 10</code> - \u8bbe\u7f6e\u5b50\u56fe\u4f18\u5316\u7684\u6700\u5c0f\u5927\u5c0f\uff0c\u7528\u4e8e\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002</li> <li><code>gpu_mem: 4000</code> - \u6307\u5b9a\u63a8\u7406\u65f6GPU\u53ef\u4f7f\u7528\u7684\u5185\u5b58\u91cf\uff08\u5355\u4f4d\u53ef\u80fd\u662fMB\uff09\u3002</li> <li><code>gpu_id: 0</code> - \u6307\u5b9a\u4f7f\u7528\u54ea\u4e2aGPU\u8fdb\u884c\u63a8\u7406\uff0c\u8fd9\u91cc\u662f\u7b2c\u4e00\u4e2aGPU\u3002</li> <li><code>max_batch_size: 64</code> - \u8bbe\u7f6e\u63a8\u7406\u65f6\u652f\u6301\u7684\u6700\u5927\u6279\u6b21\u5927\u5c0f\u3002</li> <li><code>num_cpu_threads: 4</code> - \u6307\u5b9a\u7528\u4e8e\u63a8\u7406\u7684CPU\u7ebf\u7a0b\u6570\u91cf\u3002</li> <li><code>batch_size: 16</code> - \u8bbe\u7f6e\u63a8\u7406\u65f6\u7684\u5b9e\u9645\u6279\u6b21\u5927\u5c0f\u4e3a16\u3002</li> </ol>"},{"location":"zh/development/#213","title":"2.13 \u8bad\u7ec3","text":"<p>PaddleScience \u6a21\u578b\u7684\u8bad\u7ec3\u53ea\u9700\u8c03\u7528\u4e00\u884c\u4ee3\u7801\u3002</p> examples/demo/demo.py<pre><code>solver.train()\n</code></pre>"},{"location":"zh/development/#214","title":"2.14 \u8bc4\u4f30","text":"<p>PaddleScience \u6a21\u578b\u7684\u8bc4\u4f30\u53ea\u9700\u8c03\u7528\u4e00\u884c\u4ee3\u7801\u3002</p> examples/demo/demo.py<pre><code>solver.eval()\n</code></pre>"},{"location":"zh/development/#215","title":"2.15 \u53ef\u89c6\u5316[\u53ef\u9009]","text":"<p>\u82e5 <code>Solver</code> \u5b9e\u4f8b\u5316\u65f6\u4f20\u5165\u4e86 <code>visualizer</code> \u53c2\u6570\uff0c\u5219 PaddleScience \u6a21\u578b\u7684\u53ef\u89c6\u5316\u53ea\u9700\u8c03\u7528\u4e00\u884c\u4ee3\u7801\u3002</p> examples/demo/demo.py<pre><code>solver.visualize()\n</code></pre> <p>\u53ef\u89c6\u5316\u65b9\u6848</p> <p>\u5bf9\u4e8e\u4e00\u4e9b\u590d\u6742\u7684\u6848\u4f8b\uff0c<code>Visualizer</code> \u7684\u7f16\u5199\u6210\u672c\u5e76\u4e0d\u4f4e\uff0c\u5e76\u4e14\u4e0d\u662f\u4efb\u4f55\u6570\u636e\u7c7b\u578b\u90fd\u53ef\u4ee5\u8fdb\u884c\u65b9\u4fbf\u7684\u53ef\u89c6\u5316\u3002\u56e0\u6b64\u53ef\u4ee5\u5728\u8bad\u7ec3\u5b8c\u6210\u4e4b\u540e\uff0c\u624b\u52a8\u6784\u5efa\u7528\u4e8e\u9884\u6d4b\u7684\u6570\u636e\u5b57\u5178\uff0c\u518d\u4f7f\u7528 <code>solver.predict</code> \u5f97\u5230\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff0c\u6700\u540e\u5229\u7528 <code>matplotlib</code> \u7b49\u7b2c\u4e09\u65b9\u5e93\uff0c\u5bf9\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u5e76\u4fdd\u5b58\u3002</p>"},{"location":"zh/development/#3","title":"3. \u7f16\u5199\u6587\u6863","text":"<p>\u9664\u4e86\u6848\u4f8b\u4ee3\u7801\uff0cPaddleScience \u540c\u65f6\u5b58\u653e\u4e86\u5bf9\u5e94\u6848\u4f8b\u7684\u8be6\u7ec6\u6587\u6863\uff0c\u4f7f\u7528 Markdown + Mkdocs + Mkdocs-Material \u8fdb\u884c\u7f16\u5199\u548c\u6e32\u67d3\uff0c\u64b0\u5199\u6587\u6863\u6b65\u9aa4\u5982\u4e0b\u3002</p>"},{"location":"zh/development/#31","title":"3.1 \u5b89\u88c5\u5fc5\u8981\u4f9d\u8d56\u5305","text":"<p>\u6587\u6863\u64b0\u5199\u8fc7\u7a0b\u4e2d\u9700\u8fdb\u884c\u5373\u65f6\u6e32\u67d3\uff0c\u9884\u89c8\u6587\u6863\u5185\u5bb9\u4ee5\u68c0\u67e5\u64b0\u5199\u7684\u5185\u5bb9\u662f\u5426\u6709\u8bef\u3002\u56e0\u6b64\u9700\u8981\u6309\u7167\u5982\u4e0b\u547d\u4ee4\uff0c\u5b89\u88c5 mkdocs \u76f8\u5173\u4f9d\u8d56\u5305\u3002</p> <pre><code>pip install -r docs/requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre>"},{"location":"zh/development/#32","title":"3.2 \u64b0\u5199\u6587\u6863\u5185\u5bb9","text":"<p>PaddleScience \u6587\u6863\u57fa\u4e8e Mkdocs-Material\u3001PyMdown \u7b49\u63d2\u4ef6\u8fdb\u884c\u7f16\u5199\uff0c\u5176\u5728 Markdown \u8bed\u6cd5\u57fa\u7840\u4e0a\u652f\u6301\u4e86\u591a\u79cd\u6269\u5c55\u6027\u529f\u80fd\uff0c\u80fd\u6781\u5927\u5730\u63d0\u5347\u6587\u6863\u7684\u7f8e\u89c2\u7a0b\u5ea6\u548c\u9605\u8bfb\u4f53\u9a8c\u3002\u5efa\u8bae\u53c2\u8003\u8d85\u94fe\u63a5\u5185\u7684\u6587\u6863\u5185\u5bb9\uff0c\u9009\u62e9\u5408\u9002\u7684\u529f\u80fd\u8f85\u52a9\u6587\u6863\u64b0\u5199\u3002</p>"},{"location":"zh/development/#33-markdownlint","title":"3.3 \u4f7f\u7528 markdownlint \u683c\u5f0f\u5316\u6587\u6863[\u53ef\u9009]","text":"<p>\u5982\u679c\u60a8\u4f7f\u7528\u7684\u5f00\u53d1\u73af\u5883\u4e3a VSCode\uff0c\u5219\u63a8\u8350\u5b89\u88c5 markdownlint \u6269\u5c55\u3002\u5b89\u88c5\u5b8c\u6bd5\u540e\u5728\u7f16\u5199\u5b8c\u7684\u6587\u6863\u5185\uff1a\u70b9\u51fb\u53f3\u952e--&gt;\u683c\u5f0f\u5316\u6587\u6863\u5373\u53ef\u3002</p>"},{"location":"zh/development/#34","title":"3.4 \u9884\u89c8\u6587\u6863","text":"<p>\u5728 <code>PaddleScience/</code> \u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7b49\u5f85\u6784\u5efa\u5b8c\u6210\u540e\uff0c\u70b9\u51fb\u663e\u793a\u7684\u94fe\u63a5\u8fdb\u5165\u672c\u5730\u7f51\u9875\u9884\u89c8\u6587\u6863\u5185\u5bb9\u3002</p> <pre><code>mkdocs serve\n</code></pre> <pre><code># ====== \u7ec8\u7aef\u6253\u5370\u4fe1\u606f\u5982\u4e0b ======\n# INFO     -  Building documentation...\n# INFO     -  Cleaning site directory\n# INFO     -  Documentation built in 20.95 seconds\n# INFO     -  [07:39:35] Watching paths for changes: 'docs', 'mkdocs.yml'\n# INFO     -  [07:39:35] Serving on http://127.0.0.1:8000/PaddlePaddle/PaddleScience/\n# INFO     -  [07:39:41] Browser connected: http://127.0.0.1:58903/PaddlePaddle/PaddleScience/\n# INFO     -  [07:40:41] Browser connected: http://127.0.0.1:58903/PaddlePaddle/PaddleScience/zh/development/\n</code></pre> <p>\u624b\u52a8\u6307\u5b9a\u670d\u52a1\u5730\u5740\u548c\u7aef\u53e3\u53f7</p> <p>\u82e5\u9ed8\u8ba4\u7aef\u53e3\u53f7 8000 \u88ab\u5360\u7528\uff0c\u5219\u53ef\u4ee5\u624b\u52a8\u6307\u5b9a\u670d\u52a1\u90e8\u7f72\u7684\u5730\u5740\u548c\u7aef\u53e3\uff0c\u793a\u4f8b\u5982\u4e0b\u3002</p> <pre><code># \u6307\u5b9a 127.0.0.1 \u4e3a\u5730\u5740\uff0c8687 \u4e3a\u7aef\u53e3\u53f7\nmkdocs serve -a 127.0.0.1:8687\n</code></pre>"},{"location":"zh/development/#4","title":"4. \u6574\u7406\u4ee3\u7801\u5e76\u63d0\u4ea4","text":""},{"location":"zh/development/#41-pre-commit","title":"4.1 \u5b89\u88c5 pre-commit","text":"<p>PaddleScience \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4ee3\u7801\u5e93\uff0c\u7531\u591a\u4eba\u5171\u540c\u53c2\u4e0e\u5f00\u53d1\uff0c\u56e0\u6b64\u4e3a\u4e86\u4fdd\u6301\u6700\u7ec8\u5408\u5165\u7684\u4ee3\u7801\u98ce\u683c\u6574\u6d01\u3001\u4e00\u81f4\uff0c PaddleScience \u4f7f\u7528\u4e86\u5305\u62ec isort\u3001black \u7b49\u81ea\u52a8\u5316\u4ee3\u7801\u68c0\u67e5\u3001\u683c\u5f0f\u5316\u63d2\u4ef6\uff0c \u8ba9 commit \u7684\u4ee3\u7801\u9075\u5faa python PEP8 \u4ee3\u7801\u98ce\u683c\u89c4\u8303\u3002</p> <p>\u56e0\u6b64\u5728 commit \u60a8\u7684\u4ee3\u7801\u4e4b\u524d\uff0c\u8bf7\u52a1\u5fc5\u5148\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 <code>pre-commit</code>\uff0c\u5426\u5219\u63d0\u4ea4\u7684 PR \u4f1a\u88ab code-style \u68c0\u6d4b\u5230\u4ee3\u7801\u672a\u683c\u5f0f\u5316\u800c\u65e0\u6cd5\u5408\u5165\u3002</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>\u5173\u4e8e pre-commit \u7684\u8be6\u60c5\u8bf7\u53c2\u8003 Paddle \u4ee3\u7801\u98ce\u683c\u68c0\u67e5\u6307\u5357</p>"},{"location":"zh/development/#42","title":"4.2 \u6574\u7406\u4ee3\u7801","text":"<p>\u5728\u5b8c\u6210\u8303\u4f8b\u7f16\u5199\u4e0e\u8bad\u7ec3\u540e\uff0c\u786e\u8ba4\u7ed3\u679c\u65e0\u8bef\uff0c\u5c31\u53ef\u4ee5\u6574\u7406\u4ee3\u7801\u3002 \u4f7f\u7528 git \u547d\u4ee4\u5c06\u6240\u6709\u65b0\u589e\u3001\u4fee\u6539\u7684\u4ee3\u7801\u6587\u4ef6\u4ee5\u53ca\u5fc5\u8981\u7684\u6587\u6863\u3001\u56fe\u7247\u7b49\u4e00\u5e76\u4e0a\u4f20\u5230\u81ea\u5df1\u4ed3\u5e93\u7684 <code>dev_model</code> \u5206\u652f\u4e0a\u3002</p>"},{"location":"zh/development/#43-pull-request","title":"4.3 \u63d0\u4ea4 pull request","text":"<p>\u5728 github \u7f51\u9875\u7aef\u5207\u6362\u5230 <code>dev_model</code> \u5206\u652f\uff0c\u5e76\u70b9\u51fb \"Contribute\"\uff0c\u518d\u70b9\u51fb \"Open pull request\" \u6309\u94ae\uff0c \u5c06\u542b\u6709\u60a8\u7684\u4ee3\u7801\u3001\u6587\u6863\u3001\u56fe\u7247\u7b49\u5185\u5bb9\u7684 <code>dev_model</code> \u5206\u652f\u4f5c\u4e3a\u5408\u5165\u8bf7\u6c42\u8d21\u732e\u5230 PaddleScience\u3002</p>"},{"location":"zh/install_setup/","title":"\u5b89\u88c5\u4f7f\u7528","text":""},{"location":"zh/install_setup/#paddlescience","title":"\u6b22\u8fce\u4f7f\u7528 PaddleScience","text":""},{"location":"zh/install_setup/#1","title":"1. \u5f00\u59cb\u5b89\u88c5","text":""},{"location":"zh/install_setup/#11-docker","title":"1.1 \u4ece docker \u955c\u50cf\u542f\u52a8[\u53ef\u9009]","text":"\u4ece DockerHub \u62c9\u53d6\u955c\u50cf\u901a\u8fc7 Dockerfile \u6784\u5efa\u955c\u50cf <pre><code># pull image\ndocker pull hydrogensulfate/paddlescience\n\n# create a container named 'paddlescience' based on pulled image\n## docker version &lt; 19.03\nnvidia-docker run --name paddlescience_container --network=host -it --shm-size 64g hydrogensulfate/paddlescience:latest /bin/bash\n\n## docker version &gt;= 19.03\n# docker run --name paddlescience_container --gpus all --network=host -it shm-size 64g hydrogensulfate/paddlescience:latest /bin/bash\n</code></pre> <p>Note</p> <p>Dockerhub \u62c9\u53d6\u7684\u955c\u50cf\u4ec5\u9884\u88c5\u4e86\u8fd0\u884c PaddleScience \u6240\u9700\u7684\u4f9d\u8d56\u5305\uff0c\u5982 pymesh\u3001open3d\uff0c\u5e76\u4e0d\u5305\u542b PaddleScience\u3002 \u56e0\u6b64\u8bf7\u5728\u955c\u50cf\u62c9\u53d6\u548c\u5bb9\u5668\u6784\u5efa\u5b8c\u6210\u540e\uff0c\u53c2\u8003 1.4 \u5b89\u88c5 PaddleScience \u4e2d\u7684\u6b65\u9aa4\uff0c\u5728\u5bb9\u5668\u4e2d\u5b89\u88c5 PaddleScience\u3002</p> <pre><code>git clone https://github.com/PaddlePaddle/PaddleScience.git\ncd PaddleScience/docker/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/docker/pymesh.tar.xz\nbash run.sh\n</code></pre> <p>\u5982\u679c\u51fa\u73b0\u56e0\u7f51\u7edc\u95ee\u9898\u5bfc\u81f4\u7684 docker \u6784\u5efa\u65f6 apt \u4e0b\u8f7d\u62a5\u9519\uff0c\u5219\u91cd\u590d\u6267\u884c <code>bash run.sh</code> \u76f4\u81f3\u6784\u5efa\u5b8c\u6210\u3002</p> <p>\u66f4\u591a\u5173\u4e8e Paddle Docker \u7684\u5b89\u88c5\u548c\u4f7f\u7528\uff0c\u8bf7\u53c2\u8003 Docker \u5b89\u88c5\u3002</p>"},{"location":"zh/install_setup/#12-python","title":"1.2 python \u73af\u5883\u5b89\u88c5[\u53ef\u9009]","text":"<p>\u5982\u679c\u4f60\u8fd8\u6ca1\u6709 python \u73af\u5883\u6216\u8005 python \u7248\u672c\u5c0f\u4e8e 3.9\uff0c\u5219\u63a8\u8350\u4f7f\u7528 Anaconda \u5b89\u88c5\u5e76\u914d\u7f6e python \u73af\u5883\uff0c\u5426\u5219\u53ef\u4ee5\u5ffd\u7565\u672c\u6b65\u9aa4\u3002</p> <ol> <li>\u6839\u636e\u7cfb\u7edf\u73af\u5883\uff0c\u4ece https://repo.anaconda.com/archive/ \u4e2d\u4e0b\u8f7d\u5bf9\u5e94\u7684 Anaconda3 \u5b89\u88c5\u5305\uff0c\u5e76\u624b\u52a8\u5b89\u88c5\u3002</li> <li> <p>\u521b\u5efa python 3.9 \u73af\u5883\uff0c\u5e76\u8fdb\u5165\u8be5\u73af\u5883\u3002</p> <pre><code># \u4f7f\u7528 conda \u521b\u5efa python \u73af\u5883\uff0c\u5e76\u547d\u540d\u4e3a \"ppsci_py39\"\nconda create -n ppsci_py39 python=3.9\n\n# \u8fdb\u5165\u521b\u5efa\u597d\u7684 \"ppsci_py39\" \u73af\u5883\nconda activate ppsci_py39\n</code></pre> </li> </ol>"},{"location":"zh/install_setup/#13-paddlepaddle","title":"1.3 \u5b89\u88c5 PaddlePaddle","text":"<p>\u8bf7\u5728 PaddlePaddle \u5b98\u7f51\u6309\u7167\u60a8\u7684\u8fd0\u884c\u73af\u5883\uff0c\u5b89\u88c5 develop \u7248\u7684 PaddlePaddle\u3002</p> <p>\u5b89\u88c5\u5b8c\u6bd5\u4e4b\u540e\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u9a8c\u8bc1 Paddle \u662f\u5426\u5b89\u88c5\u6210\u529f\u3002</p> <pre><code>python -c \"import paddle; paddle.utils.run_check()\"\n</code></pre> <p>\u5982\u679c\u51fa\u73b0 <code>PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.</code> \u4fe1\u606f\uff0c\u8bf4\u660e\u60a8\u5df2\u6210\u529f\u5b89\u88c5\uff0c\u53ef\u4ee5\u7ee7\u7eed\u5b89\u88c5 PaddleScience\u3002</p>"},{"location":"zh/install_setup/#14-paddlescience","title":"1.4 \u5b89\u88c5 PaddleScience","text":""},{"location":"zh/install_setup/#141","title":"1.4.1 \u5b89\u88c5\u57fa\u7840\u529f\u80fd","text":"<p>\u4ece\u4ee5\u4e0b\u4e09\u79cd\u5b89\u88c5\u65b9\u5f0f\u4e2d\u4efb\u9009\u4e00\u79cd\u3002</p> git \u6e90\u7801\u5b89\u88c5[\u63a8\u8350]pip \u5b89\u88c5\u8bbe\u7f6e PYTHONPATH <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u4ece github \u4e0a clone PaddleScience \u6e90\u4ee3\u7801\uff0c\u5e76\u4ee5 editable \u7684\u65b9\u5f0f\u5b89\u88c5 PaddleScience\u3002</p> <pre><code>git clone -b develop https://github.com/PaddlePaddle/PaddleScience.git\n# \u82e5 github clone \u901f\u5ea6\u6bd4\u8f83\u6162\uff0c\u53ef\u4ee5\u4f7f\u7528 gitee clone\n# git clone -b develop https://gitee.com/paddlepaddle/PaddleScience.git\n\ncd PaddleScience\n\n# install paddlesci with editable mode\npip install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ee5 pip \u7684\u65b9\u5f0f\u5b89\u88c5\u6700\u65b0\u7248\u672c\u7684 PaddleScience\u3002</p> <pre><code>pip install -U paddlesci\n</code></pre> <p>\u5982\u679c\u5728\u60a8\u7684\u73af\u5883\u4e2d\uff0c\u4e0a\u8ff0\u4e24\u79cd\u65b9\u5f0f\u90fd\u65e0\u6cd5\u6b63\u5e38\u5b89\u88c5\uff0c\u5219\u53ef\u4ee5\u9009\u62e9\u672c\u65b9\u5f0f\uff0c\u5728\u7ec8\u7aef\u5185\u5c06\u73af\u5883\u53d8\u91cf <code>PYTHONPATH</code> \u4e34\u65f6\u8bbe\u7f6e\u4e3a <code>PaddleScience</code> \u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> LinuxWindows <pre><code>cd PaddleScience/\nexport PYTHONPATH=$PYTHONPATH:$PWD\n</code></pre> <pre><code>cd PaddleScience/\nset PYTHONPATH=%cd%\n</code></pre> <p>\u4e0a\u8ff0\u65b9\u5f0f\u7684\u4f18\u70b9\u662f\u6b65\u9aa4\u7b80\u5355\u65e0\u9700\u5b89\u88c5\uff0c\u7f3a\u70b9\u662f\u5f53\u73af\u5883\u53d8\u91cf\u751f\u6548\u7684\u7ec8\u7aef\u88ab\u5173\u95ed\u540e\uff0c\u9700\u8981\u91cd\u65b0\u6267\u884c\u4e0a\u8ff0\u547d\u4ee4\u8bbe\u7f6e <code>PYTHONPATH</code> \u624d\u80fd\u518d\u6b21\u4f7f\u7528 PaddleScience\uff0c\u8f83\u4e3a\u7e41\u7410\u3002</p>"},{"location":"zh/install_setup/#142","title":"1.4.2 \u5b89\u88c5\u989d\u5916\u529f\u80fd[\u53ef\u9009]","text":"<p>\u5982\u9700\u4f7f\u7528 <code>.obj</code>, <code>.ply</code>, <code>.off</code>, <code>.stl</code>, <code>.mesh</code>, <code>.node</code>, <code>.poly</code> and <code>.msh</code> \u7b49\u590d\u6742\u51e0\u4f55\u6587\u4ef6\u6784\u5efa\u51e0\u4f55\uff08\u8ba1\u7b97\u57df\uff09\uff0c\u4ee5\u53ca\u4f7f\u7528\u52a0\u5bc6\u91c7\u6837\u7b49\u529f\u80fd\uff0c\u5219\u9700\u6309\u7167\u4e0b\u65b9\u7ed9\u51fa\u7684\u547d\u4ee4\uff0c\u5b89\u88c5 open3d\u3001 pybind11\u3001pysdf\u3001PyMesh \u56db\u4e2a\u4f9d\u8d56\u5e93\uff08\u4e0a\u8ff01.1 \u4ece docker \u955c\u50cf\u542f\u52a8\u4e2d\u5df2\u5b89\u88c5\u4e0a\u8ff0\u4f9d\u8d56\u5e93)\u3002</p> <p>\u5426\u5219\u65e0\u6cd5\u4f7f\u7528 <code>ppsci.geometry.Mesh</code> \u7b49\u57fa\u4e8e\u590d\u6742\u51e0\u4f55\u6587\u4ef6\u7684 API\uff0c\u56e0\u6b64\u4e5f\u65e0\u6cd5\u8fd0\u884c\u5982 Aneurysm \u7b49\u4f9d\u8d56 <code>ppsci.geometry.Mesh</code> API \u7684\u590d\u6742\u6848\u4f8b\u3002</p> open3d \u5b89\u88c5\u547d\u4ee4pybind11 \u5b89\u88c5\u547d\u4ee4pysdf \u5b89\u88c5\u547d\u4ee4PyMesh \u5b89\u88c5\u547d\u4ee4 <pre><code>pip install open3d -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre> <pre><code>pip install pybind11 -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre> <pre><code>pip install pysdf\n</code></pre> <p>\u5728\u5b89\u88c5 PyMesh \u4e4b\u524d\uff0c\u9996\u5148\u9700\u901a\u8fc7 <code>cmake --version</code> \u786e\u8ba4\u73af\u5883\u4e2d\u662f\u5426\u5df2\u5b89\u88c5 cmake\u3002 \u5982\u672a\u5b89\u88c5\uff0c\u53ef\u6309\u7167\u4e0b\u5217\u547d\u4ee4\u4e0b\u8f7d\u3001\u89e3\u538b cmake \u5305\uff0c\u5e76\u6dfb\u52a0\u5230 <code>PATH</code> \u53d8\u91cf\u4e2d\u5373\u53ef\u5b8c\u6210\u5b89\u88c5\u3002</p> <pre><code>wget -nc https://paddle-org.bj.bcebos.com/paddlescience/cmake-3.23.0-linux-x86_64.tar.gz\ntar -zxvf cmake-3.23.0-linux-x86_64.tar.gz\nrm -f cmake-3.23.0-linux-x86_64.tar.gz\nPATH=$PWD/cmake-3.23.0-linux-x86_64/bin:$PATH\n\n# cmake --version\n# cmake version 3.23.0\n\n# CMake suite maintained and supported by Kitware (kitware.com/cmake).\n</code></pre> <p>PyMesh \u5e93\u9700\u8981\u4ee5 setup \u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>wget -nc https://paddle-org.bj.bcebos.com/paddlescience/PyMesh.tar.gz\ntar -zxvf PyMesh.tar.gz\n\n# \u4e5f\u53ef\u4ee5\u4f7f\u7528 git \u547d\u4ee4\u4e0b\u8f7d\uff0c\u901f\u5ea6\u53ef\u80fd\u4f1a\u6bd4\u8f83\u6162\n# git clone https://github.com/PyMesh/PyMesh.git\n# git submodule update --init --recursive --progress\n\ncd PyMesh\nexport PYMESH_PATH=`pwd`\n\napt-get install \\\n    libeigen3-dev \\\n    libgmp-dev \\\n    libgmpxx4ldbl \\\n    libmpfr-dev \\\n    libboost-dev \\\n    libboost-thread-dev \\\n    libtbb-dev \\\n    python3-dev\n\npython -m pip install --user -r $PYMESH_PATH/python/requirements.txt\npython setup.py build\npython setup.py install --user\n\n# test whether installed successfully\npython -c \"import pymesh; pymesh.test()\"\n\n# Ran 175 tests in 3.150s\n\n# OK (SKIP=2)\n</code></pre> <p>\u5b89\u88c5\u6ce8\u610f\u4e8b\u9879</p> <p>\u5982\u679c\u4f7f\u7528 git \u547d\u4ee4\u4e0b\u8f7d PyMesh \u9879\u76ee\u6587\u4ef6\uff0c\u5219\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4f1a\u51fa\u73b0\u4e24\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u6309\u7167\u4ee5\u4e0b\u65b9\u5f0f\u89e3\u51b3\uff1a</p> <ol> <li> <p>\u7531\u4e8e\u7f51\u7edc\u95ee\u9898\uff0c<code>git submodule update</code> \u8fc7\u7a0b\u4e2d\u53ef\u80fd\u67d0\u4e9b submodule \u4f1a clone \u5931\u8d25\uff0c\u6b64\u65f6\u53ea\u9700 \u53cd\u590d\u6267\u884c <code>git submodule update --init --recursive --progress</code> \u76f4\u5230\u6240\u6709\u5e93\u90fd clone \u6210\u529f\u5373\u53ef\u3002</p> </li> <li> <p>\u6240\u6709 submodule \u90fd clone \u6210\u529f\u540e\uff0c\u8bf7\u68c0\u67e5 <code>PyMesh/third_party/</code> \u4e0b\u662f\u5426\u6709\u7a7a\u6587\u4ef6\u5939\uff0c\u82e5\u6709\u5219\u9700 \u624b\u52a8\u627e\u5230\u5e76\u5220\u9664\u8fd9\u4e9b\u7a7a\u6587\u4ef6\u5939\uff0c\u518d\u6267\u884c <code>git submodule update --init --recursive --progress</code> \u547d \u4ee4\u5373\u53ef\u6062\u590d\u8fd9\u4e9b\u7a7a\u6587\u4ef6\u5939\u81f3\u6b63\u5e38\u542b\u6709\u6587\u4ef6\u7684\u72b6\u6001\uff0c\u6b64\u65f6\u518d\u7ee7\u7eed\u6267\u884c\u5269\u4f59\u5b89\u88c5\u547d\u4ee4\u5373\u53ef\u3002</p> </li> </ol>"},{"location":"zh/install_setup/#2","title":"2. \u9a8c\u8bc1\u5b89\u88c5","text":"<ul> <li> <p>\u6267\u884c\u4ee5\u4e0b\u4ee3\u7801\uff0c\u9a8c\u8bc1\u5b89\u88c5\u7684 PaddleScience \u57fa\u7840\u529f\u80fd\u662f\u5426\u6b63\u5e38\u3002</p> <pre><code>python -c \"import ppsci; ppsci.run_check()\"\n</code></pre> <p>\u5982\u679c\u51fa\u73b0 <code>PaddleScience is installed successfully.\u2728 \ud83c\udf70 \u2728</code>\uff0c\u5219\u8bf4\u660e\u5b89\u88c5\u9a8c\u8bc1\u6210\u529f\u3002</p> </li> <li> <p>[\u53ef\u9009]\u5982\u679c\u5df2\u6309\u7167 1.4.2 \u5b89\u88c5\u989d\u5916\u4f9d\u8d56 \u6b63\u786e\u5b89\u88c5\u4e86 4 \u4e2a\u989d\u5916\u4f9d\u8d56\u5e93\uff0c\u5219\u53ef\u4ee5\u6267\u884c\u4ee5\u4e0b\u4ee3\u7801\uff0c     \u9a8c\u8bc1 PaddleScience \u7684 <code>ppsci.geometry.Mesh</code> \u6a21\u5757\u662f\u5426\u80fd\u6b63\u5e38\u8fd0\u884c\u3002</p> <pre><code>python -c \"import ppsci; ppsci.run_check_mesh()\"\n</code></pre> <p>\u5982\u679c\u51fa\u73b0 <code>ppsci.geometry.Mesh module running successfully.\u2728 \ud83c\udf70 \u2728</code>\uff0c\u5219\u8bf4\u660e\u8be5\u6a21\u5757\u8fd0\u884c\u6b63\u5e38\u3002</p> </li> </ul>"},{"location":"zh/install_setup/#3","title":"3. \u5f00\u59cb\u4f7f\u7528","text":"<ul> <li> <p>\u8fd0\u884c\u5185\u7f6e\u7684\u6848\u4f8b\uff08\u4ee5 ldc2d_unsteady_Re10.py \u4e3a\u4f8b\uff09</p> <pre><code>cd examples/ldc/\npython ./ldc2d_unsteady_Re10.py\n</code></pre> </li> <li> <p>\u7f16\u5199\u81ea\u5df1\u7684\u6848\u4f8b\uff08\u5047\u8bbe\u6848\u4f8b\u540d\u4e3a demo\uff09</p> <p>\u63a8\u8350\u5728 <code>examples/</code> \u4e0b\u65b0\u5efa <code>demo</code> \u6587\u4ef6\u5939\uff0c\u7136\u540e\u5728 <code>demo</code> \u6587\u4ef6\u5939\u4e0b\u65b0\u5efa <code>demo.py</code>\uff0c\u6700\u540e\u5728 <code>demo.py</code> \u6587\u4ef6\u4e2d\u4f7f\u7528 PaddleScience \u63d0\u4f9b\u7684 API \u7f16\u5199\u4ee3\u7801\u3002</p> examples/demo/demo.py<pre><code>import ppsci\n\n# write your code here...\n</code></pre> <p>\u7f16\u5199\u5b8c\u6bd5\u540e\u8fd0\u884c\u4f60\u7684\u4ee3\u7801</p> <pre><code>cd examples/demo\npython ./demo.py\n</code></pre> <p>\u5982\u4e0d\u4e86\u89e3\u63a5\u4e0b\u6765\u8be5\u5982\u4f55\u57fa\u4e8e PaddleScience \u7f16\u5199\u4ee3\u7801\uff0c\u5219\u63a8\u8350\u53c2\u8003 \u5feb\u901f\u5f00\u59cb \u548c\u5176\u4ed6\u6848\u4f8b\u7684\u6587\u6863\u3001\u4ee3\u7801\uff0c\u8fdb\u4e00\u6b65\u4e86\u89e3\u5982\u4f55\u4f7f\u7528 <code>ppsci</code> \u4e0b\u7684\u6a21\u5757\u6765\u7f16\u5199\u81ea\u5df1\u7684\u6848\u4f8b\u3002</p> </li> </ul>"},{"location":"zh/overview/","title":"\u529f\u80fd\u4ecb\u7ecd","text":""},{"location":"zh/overview/#paddlescience","title":"PaddleScience \u6a21\u5757\u4ecb\u7ecd","text":"<p>PaddleScience \u5728\u4ee3\u7801\u7ed3\u6784\u4e0a\u5212\u5206\u4e3a 12 \u4e2a\u6a21\u5757\u3002\u4ece\u4e00\u822c\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9 12 \u4e2a\u6a21\u5757\u5206\u522b\u8d1f\u8d23\u6784\u5efa\u8f93\u5165\u6570\u636e\u3001\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3001\u6784\u5efa\u635f\u5931\u51fd\u6570\u3001\u6784\u5efa\u4f18\u5316\u5668\uff0c\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u7b49\u529f\u80fd\u3002\u4ece\u79d1\u5b66\u8ba1\u7b97\u89d2\u5ea6\u6765\u770b\uff0c\u90e8\u5206\u6a21\u5757\u627f\u62c5\u4e86\u4e0d\u540c\u4e8e CV\u3001NLP \u4efb\u52a1\u7684\u529f\u80fd\uff0c\u6bd4\u5982\u7528\u4e8e\u7269\u7406\u673a\u7406\u9a71\u52a8\u7684 Equation \u6a21\u5757\uff0c\u5b9a\u4e49\u65b9\u7a0b\u516c\u5f0f\u548c\u8f85\u52a9\u9ad8\u9636\u5fae\u5206\u8ba1\u7b97\uff1b\u7528\u4e8e\u6d89\u53ca\u51e0\u4f55\u573a\u666f\u91c7\u6837\u7684 Geometry \u6a21\u5757\uff0c\u5b9a\u4e49\u7b80\u5355\u3001\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u5e76\u5728\u5176\u5185\u90e8\u3001\u8fb9\u754c\u91c7\u6837\u6784\u9020\u6570\u636e\uff1bConstraint \u6a21\u5757\u5c06\u4e0d\u540c\u7684\u4f18\u5316\u76ee\u6807\u89c6\u4e3a\u4e00\u79cd\u201c\u7ea6\u675f\u201d\uff0c\u4f7f\u5f97\u5957\u4ef6\u80fd\u7528\u4e00\u5957\u8bad\u7ec3\u4ee3\u7801\u7edf\u4e00\u7269\u7406\u673a\u7406\u9a71\u52a8\u3001\u6570\u636e\u9a71\u52a8\u3001\u6570\u7406\u878d\u5408\u4e09\u79cd\u4e0d\u540c\u7684\u6c42\u89e3\u6d41\u7a0b\u3002</p> <p></p>"},{"location":"zh/overview/#1","title":"1. \u6574\u4f53\u5de5\u4f5c\u6d41","text":"<p>\u4e0a\u56fe\u662f PaddleScience \u7684 workflow \u793a\u610f\u56fe\uff08\u4ee5\u57fa\u4e8e\u51e0\u4f55\u7684\u95ee\u9898\u6c42\u89e3\u4e3a\u4f8b\uff09\uff0c\u6d41\u7a0b\u63cf\u8ff0\u5982\u4e0b</p> <ol> <li>Geometry \u8d1f\u8d23\u6784\u5efa\u51e0\u4f55\u5e76\u5728\u51e0\u4f55\u4e0a\u91c7\u6837\uff0c\u5b8c\u6210\u6570\u636e\u6784\u5efa\uff1b</li> <li>\u7528 Model \u6a21\u5757\u63a5\u53d7\u8f93\u5165\uff0c\u5f97\u5230\u6a21\u578b\u8f93\u51fa\uff1b</li> <li>\u79d1\u5b66\u8ba1\u7b97\u4efb\u52a1\u5177\u6709\u7279\u6b8a\u6027\uff0c\u6a21\u578b\u8f93\u51fa\u5f80\u5f80\u5e76\u4e0d\u662f\u524d\u5411\u8ba1\u7b97\u7684\u7ec8\u70b9\uff0c\u8fd8\u9700\u8981\u8fdb\u4e00\u6b65\u6309\u7167 Equation\uff0c\u8ba1\u7b97\u51fa\u65b9\u7a0b\u516c\u5f0f\u6240\u9700\u7684\u53d8\u91cf\uff1b</li> <li>\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5229\u7528\u6846\u67b6\u7684\u81ea\u52a8\u5fae\u5206\u673a\u5236\uff0c\u6c42\u51fa\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\uff1b</li> <li>\u4e0a\u8ff0\u7684\u4f18\u5316\u76ee\u6807\u53ef\u4ee5\u65bd\u52a0\u5728\u51e0\u4f55\u7684\u4e0d\u540c\u533a\u57df\u4e0a\uff0c\u6bd4\u5982interior\u3001boundary\u533a\u57df\uff0c\u56e0\u6b64\u4e0a\u56fe\u4e2d\u7684 Constraint \u53ef\u4ee5\u6709\u591a\u4e2a\uff1b</li> <li>\u5c06\u6240\u6709 Constraint \u8d21\u732e\u7684\u68af\u5ea6\u7d2f\u52a0\uff0c\u5e76\u7528\u4e8e\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff1b</li> <li>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5982\u679c\u5f00\u542f\u4e86\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u529f\u80fd\uff0c\u5219\u4f1a\u6309\u4e00\u5b9a\u9891\u7387\u81ea\u52a8\u5bf9\u5f53\u524d\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u548c\u9884\u6d4b\u7ed3\u679c\u53ef\u89c6\u5316\uff1b</li> <li>Solver \u662f\u6574\u4e2a\u5957\u4ef6\u8fd0\u884c\u7684\u5168\u5c40\u8c03\u5ea6\u6a21\u5757\uff0c\u8d1f\u8d23\u5c06\u4e0a\u8ff0\u8fc7\u7a0b\u6309\u7528\u6237\u6307\u5b9a\u7684\u8f6e\u6570\u548c\u9891\u7387\u91cd\u590d\u8fd0\u884c\u3002</li> </ol>"},{"location":"zh/overview/#2","title":"2. \u6a21\u5757\u7b80\u4ecb","text":""},{"location":"zh/overview/#21-arch","title":"2.1 Arch","text":"<p>Arch \u6a21\u5757\u8d1f\u8d23\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u7ec4\u7f51\u3001\u53c2\u6570\u521d\u59cb\u5316\u3001\u524d\u5411\u8ba1\u7b97\u7b49\u529f\u80fd\uff0c\u5185\u7f6e\u4e86\u591a\u79cd\u6a21\u578b\u4f9b\u7528\u6237\u4f7f\u7528\u3002</p>"},{"location":"zh/overview/#22-autodiff","title":"2.2 AutoDiff","text":"<p>AutoDiff \u6a21\u5757\u8d1f\u8d23\u8ba1\u7b97\u9ad8\u9636\u5fae\u5206\u529f\u80fd\uff0c\u5185\u7f6e\u57fa\u4e8e Paddle \u81ea\u52a8\u5fae\u5206\u673a\u5236\u7684\u5168\u5c40\u5355\u4f8b <code>jacobian</code>\u3001<code>hessian</code> \u4f9b\u7528\u6237\u4f7f\u7528\u3002</p>"},{"location":"zh/overview/#23-constraint","title":"2.3 Constraint","text":"<p>\u4e3a\u4e86\u5728\u5957\u4ef6\u4e2d\u7edf\u4e00\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u3001\u6570\u636e\u9a71\u52a8\u3001\u6570\u7406\u878d\u5408\u4e09\u79cd\u6c42\u89e3\u65b9\u5f0f\uff0c\u6211\u4eec\u5c06\u6570\u636e\u6784\u9020\u3001\u8f93\u5165\u5230\u8f93\u51fa\u7684\u8ba1\u7b97\u8fc7\u7a0b\u3001\u635f\u5931\u51fd\u6570\u7b49\u5fc5\u8981\u63a5\u53e3\u5728\u5176\u5b9a\u4e49\u5b8c\u6bd5\u4e4b\u540e\uff0c\u7edf\u4e00\u8bb0\u5f55\u5728 Constraint \u8fd9\u4e00\u6a21\u5757\u4e2d\uff0c\u6709\u4e86\u8fd9\u4e9b\u63a5\u53e3\uff0cConstraint \u5c31\u80fd\u8868\u793a\u4e0d\u540c\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u5982\uff1a</p> <ul> <li><code>InteriorConstraint</code> \u5b9a\u4e49\u4e86\u5728\u7ed9\u5b9a\u7684\u51e0\u4f55\u533a\u57df\u5185\u90e8\uff0c\u6309\u7167\u7ed9\u5b9a\u8f93\u5165\u5230\u8f93\u51fa\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5229\u7528\u635f\u5931\u51fd\u6570\u4f18\u5316\u6a21\u578b\u53c2\u6570\uff0c\u4f7f\u5f97\u6a21\u578b\u8f93\u51fa\u6ee1\u8db3\u7ed9\u5b9a\u7684\u6761\u4ef6\uff1b</li> <li><code>BoundaryConstraint</code> \u5b9a\u4e49\u4e86\u5728\u7ed9\u5b9a\u7684\u51e0\u4f55\u533a\u57df\u8fb9\u754c\u4e0a\uff0c\u6309\u7167\u7ed9\u5b9a\u8f93\u5165\u5230\u8f93\u51fa\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5229\u7528\u635f\u5931\u51fd\u6570\u4f18\u5316\u6a21\u578b\u53c2\u6570\uff0c\u4f7f\u5f97\u6a21\u578b\u8f93\u51fa\u6ee1\u8db3\u7ed9\u5b9a\u7684\u6761\u4ef6\uff1b</li> <li><code>SupervisedConstraint</code> \u5b9a\u4e49\u4e86\u5728\u7ed9\u5b9a\u7684\u76d1\u7763\u6570\u636e\uff08\u76f8\u5f53\u4e8eCV\u3001NLP\u4e2d\u7684\u76d1\u7763\u8bad\u7ec3\uff09\u4e0a\uff0c\u6309\u7167\u7ed9\u5b9a\u8f93\u5165\u5230\u8f93\u51fa\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5229\u7528\u635f\u5931\u51fd\u6570\u4f18\u5316\u6a21\u578b\u53c2\u6570\uff0c\u4f7f\u5f97\u6a21\u578b\u8f93\u51fa\u6ee1\u8db3\u7ed9\u5b9a\u7684\u6761\u4ef6\u3002</li> <li>...</li> </ul> <p>\u8fd9\u4e00\u6a21\u5757\u6709\u4e24\u4e2a\u4e3b\u8981\u4f5c\u7528\uff0c\u4e00\u662f\u5728\u4ee3\u7801\u6d41\u7a0b\u4e0a\u7edf\u4e00\u4e86\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u3001\u6570\u636e\u9a71\u52a8\u4e24\u4e2a\u4e0d\u540c\u7684\u4f18\u5316\u8303\u5f0f\uff08\u524d\u8005\u7c7b\u4f3c\u76d1\u7763\u8bad\u7ec3\u65b9\u5f0f\uff0c\u540e\u8005\u7c7b\u4f3c\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u5f0f\uff09\uff0c\u4e8c\u662f\u4f7f\u5f97\u5957\u4ef6\u80fd\u5e94\u7528\u5728\u6570\u7406\u878d\u5408\u7684\u573a\u666f\u4e2d\uff0c\u53ea\u9700\u5206\u522b\u6784\u9020\u4e0d\u540c\u7684 Constraint \u5e76\u8ba9\u5b83\u4eec\u5171\u540c\u53c2\u4e0e\u8bad\u7ec3\u5373\u53ef\u3002</p>"},{"location":"zh/overview/#24-data","title":"2.4 Data","text":"<p>Data \u6a21\u5757\u8d1f\u8d23\u6570\u636e\u7684\u8bfb\u53d6\u3001\u5305\u88c5\u548c\u9884\u5904\u7406\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> \u5b50\u6a21\u5757\u540d\u79f0 \u5b50\u6a21\u5757\u529f\u80fd ppsci.data.dataset \u6570\u636e\u96c6\u76f8\u5173 ppsci.data.transform \u5355\u4e2a\u6570\u636e\u6837\u672c\u9884\u5904\u7406\u76f8\u5173\u65b9\u6cd5 ppsci.data.batch_transform \u6279\u6570\u636e\u9884\u5904\u7406\u76f8\u5173\u65b9\u6cd5"},{"location":"zh/overview/#25-equation","title":"2.5 Equation","text":"<p>Equation \u6a21\u5757\u8d1f\u8d23\u5b9a\u4e49\u5404\u79cd\u5e38\u89c1\u65b9\u7a0b\u7684\u8ba1\u7b97\u51fd\u6570\uff0c\u5982 <code>NavierStokes</code> \u8868\u793a N-S \u65b9\u7a0b\uff0c<code>Vibration</code> \u8868\u793a\u632f\u52a8\u65b9\u7a0b\uff0c\u6bcf\u4e2a\u65b9\u7a0b\u5185\u90e8\u542b\u6709\u76f8\u5173\u53d8\u91cf\u7684\u8ba1\u7b97\u51fd\u6570\u3002</p>"},{"location":"zh/overview/#26-geometry","title":"2.6 Geometry","text":"<p>Geometry \u6a21\u5757\u8d1f\u8d23\u5b9a\u4e49\u5404\u79cd\u5e38\u89c1\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5982 <code>Interval</code> \u7ebf\u6bb5\u51e0\u4f55\u3001<code>Rectangle</code> \u77e9\u5f62\u51e0\u4f55\u3001<code>Sphere</code> \u7403\u9762\u51e0\u4f55\u3002</p>"},{"location":"zh/overview/#27-loss","title":"2.7 Loss","text":"<p>Loss \u6a21\u5757\u5305\u542b <code>ppsci.loss.loss</code> \u4e0e <code>ppsci.loss.mtl</code> \u4e24\u4e2a\u5b50\u6a21\u5757\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> \u5b50\u6a21\u5757\u540d\u79f0 \u5b50\u6a21\u5757\u529f\u80fd ppsci.loss.loss \u635f\u5931\u51fd\u6570\u76f8\u5173 ppsci.loss.mtl \u591a\u76ee\u6807\u4f18\u5316\u76f8\u5173"},{"location":"zh/overview/#28-optimizer","title":"2.8 Optimizer","text":"<p>Optimizer \u6a21\u5757\u5305\u542b <code>ppsci.optimizer.optimizer</code> \u4e0e <code>ppsci.optimizer.lr_scheduler</code> \u4e24\u4e2a\u5b50\u6a21\u5757\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> \u5b50\u6a21\u5757\u540d\u79f0 \u5b50\u6a21\u5757\u529f\u80fd ppsci.utils.optimizer \u4f18\u5316\u5668\u76f8\u5173 ppsci.utils.lr_scheduler \u5b66\u4e60\u7387\u8c03\u8282\u5668\u76f8\u5173"},{"location":"zh/overview/#29-solver","title":"2.9 Solver","text":"<p>Solver \u6a21\u5757\u8d1f\u8d23\u5b9a\u4e49\u6c42\u89e3\u5668\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u63a8\u7406\u3001\u53ef\u89c6\u5316\u7684\u542f\u52a8\u548c\u7ba1\u7406\u5f15\u64ce\u3002</p>"},{"location":"zh/overview/#210-utils","title":"2.10 Utils","text":"<p>Utils \u6a21\u5757\u5185\u90e8\u5b58\u653e\u4e86\u4e00\u4e9b\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u4e0b\u7684\u5de5\u5177\u7c7b\u3001\u51fd\u6570\uff0c\u4f8b\u5982\u5728 <code>reader.py</code> \u4e0b\u7684\u6570\u636e\u8bfb\u53d6\u51fd\u6570\uff0c\u5728 <code>logger.py</code> \u4e0b\u7684\u65e5\u5fd7\u6253\u5370\u51fd\u6570\uff0c\u4ee5\u53ca\u5728 <code>expression.py</code> \u4e0b\u7684\u65b9\u7a0b\u8ba1\u7b97\u7c7b\u3002</p> <p>\u6839\u636e\u5176\u529f\u80fd\u7ec6\u5206\u4e3a\u4ee5\u4e0b 8 \u4e2a\u5b50\u6a21\u5757</p> \u5b50\u6a21\u5757\u540d\u79f0 \u5b50\u6a21\u5757\u529f\u80fd ppsci.utils.checker ppsci \u5b89\u88c5\u529f\u80fd\u68c0\u67e5\u76f8\u5173 ppsci.utils.expression \u8d1f\u8d23\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u8fc7\u7a0b\u4e2d\u6d89\u53ca\u6a21\u578b\u3001\u65b9\u7a0b\u7684\u524d\u5411\u8ba1\u7b97 ppsci.utils.initializer \u5e38\u7528\u53c2\u6570\u521d\u59cb\u5316\u65b9\u6cd5 ppsci.utils.logger \u65e5\u5fd7\u6253\u5370\u6a21\u5757 ppsci.utils.misc \u5b58\u653e\u901a\u7528\u51fd\u6570 ppsci.utils.reader \u6587\u4ef6\u8bfb\u53d6\u6a21\u5757 ppsci.utils.writer \u6587\u4ef6\u5199\u5165\u6a21\u5757 ppsci.utils.save_load \u6a21\u578b\u53c2\u6570\u4fdd\u5b58\u4e0e\u52a0\u8f7d ppsci.utils.symbolic sympy \u7b26\u53f7\u8ba1\u7b97\u529f\u80fd\u76f8\u5173"},{"location":"zh/overview/#211-validate","title":"2.11 Validate","text":"<p>Validator \u6a21\u5757\u8d1f\u8d23\u5b9a\u4e49\u5404\u79cd\u8bc4\u4f30\u5668\uff0c\u7528\u4e8e\u5728\u6307\u5b9a\u6570\u636e\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff08\u53ef\u9009\uff0c\u9ed8\u8ba4\u4e0d\u5f00\u542f\u8bad\u7ec3\u65f6\u8bc4\u4f30\uff09\uff0c\u5e76\u5f97\u5230\u8bc4\u4f30\u6307\u6807\u3002</p>"},{"location":"zh/overview/#212-visualize","title":"2.12 Visualize","text":"<p>Visualizer \u6a21\u5757\u8d1f\u8d23\u5b9a\u4e49\u5404\u79cd\u53ef\u89c6\u5316\u5668\uff0c\u7528\u4e8e\u6a21\u578b\u8bc4\u4f30\u5b8c\u540e\u5728\u6307\u5b9a\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u6d4b\uff08\u53ef\u9009\uff0c\u9ed8\u8ba4\u4e0d\u5f00\u542f\u8bad\u7ec3\u65f6\u53ef\u89c6\u5316\uff09\u5e76\u5c06\u7ed3\u679c\u4fdd\u5b58\u6210\u53ef\u89c6\u5316\u7684\u6587\u4ef6\u3002</p>"},{"location":"zh/quickstart/","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"zh/quickstart/#_1","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> <p>\u672c\u6587\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684 demo \u53ca\u5176\u6269\u5c55\u95ee\u9898\uff0c\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 PaddleScience \u8bad\u7ec3\u6a21\u578b\uff0c\u89e3\u51b3\u4e00\u7c7b\u65b9\u7a0b\u5b66\u4e60\u4e0e\u9884\u6d4b\u95ee\u9898\uff0c\u5e76\u53ef\u89c6\u5316\u9884\u6d4b\u7ed3\u679c\u3002</p>"},{"location":"zh/quickstart/#1","title":"1. \u95ee\u9898\u7b80\u4ecb","text":"<p>\u5047\u8bbe\u6211\u4eec\u5e0c\u671b\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u53bb\u62df\u5408 \\(x \\in [-\\pi, \\pi]\\) \u533a\u95f4\u5185\uff0c\\(u=\\sin(x)\\) \u8fd9\u4e00\u51fd\u6570\u3002\u5728\u62df\u5408\u51fd\u6570\u5df2\u77e5\u548c\u672a\u77e5\u4e24\u79cd\u60c5\u5f62\u4e0b\uff0c\u5982\u4f55\u53bb\u5c3d\u53ef\u80fd\u5730\u51c6\u786e\u62df\u5408 \\(u=\\sin(x)\\)\u3002</p> <p>\u7b2c\u4e00\u79cd\u573a\u666f\u4e0b\uff0c\u5047\u8bbe\u5df2\u77e5\u76ee\u6807\u51fd\u6570 \\(u\\) \u7684\u89e3\u6790\u89e3\u5c31\u662f \\(u=\\sin(x)\\)\uff0c\u6211\u4eec\u91c7\u7528\u76d1\u7763\u8bad\u7ec3\u7684\u601d\u8def\uff0c\u76f4\u63a5\u7528\u8be5\u516c\u5f0f\u751f\u6210\u6807\u7b7e\u56e0\u53d8\u91cf \\(u\\)\uff0c\u4e0e\u81ea\u53d8\u91cf \\(x\\) \u5171\u540c\u4f5c\u4e3a\u76d1\u7763\u6570\u636e\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002</p> <p>\u7b2c\u4e8c\u79cd\u573a\u666f\u4e0b\uff0c\u5047\u8bbe\u4e0d\u77e5\u9053\u76ee\u6807\u51fd\u6570 \\(u\\) \u7684\u89e3\u6790\u89e3\uff0c\u4f46\u6211\u4eec\u77e5\u9053\u5176\u6ee1\u8db3\u67d0\u79cd\u5fae\u5206\u5173\u7cfb\uff0c\u6211\u4eec\u8fd9\u91cc\u4ee5\u5176\u4e2d\u4e00\u4e2a\u6ee1\u8db3\u6761\u4ef6\u7684\u5fae\u5206\u65b9\u7a0b \\(\\dfrac{\\partial u} {\\partial x}=\\cos(x)\\) \u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u751f\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002</p>"},{"location":"zh/quickstart/#2","title":"2. \u573a\u666f\u4e00","text":"<p>\u76ee\u6807\u62df\u5408\u51fd\u6570\uff1a</p> \\[ u=\\sin(x), x \\in [-\\pi, \\pi]. \\] <p>\u6211\u4eec\u751f\u6210 \\(N\\) \u7ec4\u6570\u636e\u5bf9 \\((x_i, u_i), i=1,...,N\\) \u4f5c\u4e3a\u76d1\u7763\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u5373\u53ef\u3002</p> <p>\u5728\u64b0\u5199\u4ee3\u7801\u4e4b\u524d\uff0c\u6211\u4eec\u9996\u5148\u5bfc\u5165\u5fc5\u8981\u7684\u5305\u3002</p> <pre><code>import numpy as np\n\nimport ppsci\nfrom ppsci.utils import logger\n</code></pre> <p>\u7136\u540e\u521b\u5efa\u65e5\u5fd7\u548c\u6a21\u578b\u4fdd\u5b58\u76ee\u5f55\u4f9b\u8bad\u7ec3\u8fc7\u7a0b\u8bb0\u5f55\u548c\u4fdd\u5b58\u4f7f\u7528\uff0c\u8fd9\u4e00\u6b65\u662f\u7edd\u5927\u90e8\u5206\u6848\u4f8b\u5728\u6b63\u5f0f\u5f00\u59cb\u524d\u90fd\u9700\u8981\u8fdb\u884c\u7684\u64cd\u4f5c\u3002</p> <pre><code># set random seed(42) for reproducibility\nppsci.utils.misc.set_random_seed(42)\n\n# set output directory\nOUTPUT_DIR = \"./output_quick_start_case1\"\n\n# initialize logger while create output directory\nlogger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n</code></pre> <p>\u63a5\u4e0b\u6765\u6b63\u5f0f\u5f00\u59cb\u64b0\u5199\u4ee3\u7801\u3002</p> <p>\u9996\u5148\u5b9a\u4e49\u95ee\u9898\u533a\u95f4\uff0c\u6211\u4eec\u4f7f\u7528 <code>ppsci.geometry.Interval</code> \u5b9a\u4e49\u4e00\u4e2a\u7ebf\u6bb5\u51e0\u4f55\u5f62\u72b6\uff0c\u65b9\u4fbf\u540e\u7eed\u5728\u8be5\u7ebf\u6bb5\u4e0a\u5bf9 \\(x\\) \u8fdb\u884c\u91c7\u6837\u3002</p> <pre><code># set 1D-geometry domain([-\u03c0, \u03c0])\nl_limit, r_limit = -np.pi, np.pi\nx_domain = ppsci.geometry.Interval(l_limit, r_limit)\n</code></pre> <p>\u7136\u540e\u5b9a\u4e49\u4e00\u4e2a\u7b80\u5355\u7684 3 \u5c42 MLP \u6a21\u578b\u3002</p> <pre><code># set model to 3-layer MLP\nmodel = ppsci.arch.MLP((\"x\",), (\"u\",), 3, 64)\n</code></pre> <p>\u4e0a\u8ff0\u4ee3\u7801\u8868\u793a\u6a21\u578b\u63a5\u53d7\u81ea\u53d8\u91cf \\(x\\) \u4f5c\u4e3a\u8f93\u5165\uff0c\u8f93\u51fa\u9884\u6d4b\u7ed3\u679c \\(\\hat{u}\\)</p> <p>\u7136\u540e\u6211\u4eec\u5b9a\u4e49\u5df2\u77e5\u7684 \\(u=\\sin(x)\\) \u8ba1\u7b97\u51fd\u6570\uff0c\u4f5c\u4e3a <code>ppsci.constraint.InteriorConstraint</code> \u7684\u53c2\u6570\uff0c\u7528\u4e8e\u8ba1\u7b97\u6807\u7b7e\u6570\u636e\uff0c<code>InteriorConstraint</code> \u8868\u793a\u4ee5\u7ed9\u5b9a\u7684\u51e0\u4f55\u5f62\u72b6\u6216\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u8054\u5408\u7ed9\u5b9a\u7684\u6807\u7b7e\u6570\u636e\uff0c\u6307\u5bfc\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u3002</p> <pre><code># standard solution of sin(x)\ndef sin_compute_func(data: dict):\n    return np.sin(data[\"x\"])\n\n\n# set constraint on 1D-geometry([-\u03c0, \u03c0])\nITERS_PER_EPOCH = 100  # use 100 iterations per training epoch\ninterior_constraint = ppsci.constraint.InteriorConstraint(\n    output_expr={\"u\": lambda out: out[\"u\"]},\n    label_dict={\"u\": sin_compute_func},\n    geom=x_domain,\n    dataloader_cfg={\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"shuffle\": True,\n        },\n        \"batch_size\": 32,  # use 32 samples(points) per iteration for interior constraint\n    },\n    loss=ppsci.loss.MSELoss(),\n)\n# wrap constraint(s) into one dict\nconstraint = {\n    interior_constraint.name: interior_constraint,\n}\n</code></pre> <p>\u6b64\u5904\u7684 <code>interior_constraint</code> \u8868\u793a\u4e00\u4e2a\u8bad\u7ec3\u76ee\u6807\uff0c\u5373\u6211\u4eec\u5e0c\u671b\u5728 \\([-\\pi, \\pi]\\) \u8fd9\u6bb5\u533a\u95f4\u5185\uff0c\u4f18\u5316\u6a21\u578b\u8ba9\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c \\(\\hat{u}\\) \u5c3d\u53ef\u80fd\u5730\u63a5\u8fd1\u5b83\u7684\u6807\u7b7e\u503c \\(u\\)\u3002</p> <p>\u63a5\u4e0b\u6765\u5c31\u53ef\u4ee5\u5f00\u59cb\u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u6bd4\u5982\u8bad\u7ec3\u8f6e\u6570\u3001\u4f18\u5316\u5668\u3001\u53ef\u89c6\u5316\u5668\u3002</p> <pre><code># set training hyper-parameters\nEPOCHS = 10\n# set optimizer\noptimizer = ppsci.optimizer.Adam(2e-3)(model)\n\n# set visualizer\nvisual_input_dict = {\n    \"x\": np.linspace(l_limit, r_limit, 1000, dtype=\"float32\").reshape(1000, 1)\n}\nvisual_input_dict[\"u_ref\"] = np.sin(visual_input_dict[\"x\"])\nvisualizer = {\n    \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n        visual_input_dict,\n        (\"x\",),\n        {\"u_pred\": lambda out: out[\"u\"], \"u_ref\": lambda out: out[\"u_ref\"]},\n        prefix=\"u=sin(x)\",\n    ),\n}\n</code></pre> <p>\u6700\u540e\u5c06\u4e0a\u8ff0\u5b9a\u4e49\u7684\u5bf9\u8c61\u4f20\u9012\u7ed9\u8bad\u7ec3\u8c03\u5ea6\u7c7b <code>Solver</code>\uff0c\u5373\u53ef\u5f00\u59cb\u6a21\u578b\u8bad\u7ec3</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    OUTPUT_DIR,\n    optimizer,\n    epochs=EPOCHS,\n    iters_per_epoch=ITERS_PER_EPOCH,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n</code></pre> <p>\u8bad\u7ec3\u5b8c\u6bd5\u540e\u518d\u7528\u521a\u624d\u53d6\u7684 1000 \u4e2a\u70b9\u4e0e\u6807\u51c6\u89e3\u8ba1\u7b97 L2-\u76f8\u5bf9\u8bef\u5dee</p> <pre><code># compute l2-relative error of trained model\npred_u = solver.predict(visual_input_dict, return_numpy=True)[\"u\"]\nl2_rel = np.linalg.norm(pred_u - visual_input_dict[\"u_ref\"]) / np.linalg.norm(\n    visual_input_dict[\"u_ref\"]\n)\nlogger.info(f\"l2_rel = {l2_rel:.5f}\")\n</code></pre> <p>\u518d\u5bf9\u8fd9 1000 \u4e2a\u70b9\u7684\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316</p> <pre><code># visualize prediction after finished training\nsolver.visualize()\n</code></pre> <p>\u8bad\u7ec3\u8bb0\u5f55\u4e0b\u6240\u793a</p> <pre><code>...\n...\nppsci INFO: [Train][Epoch  9/10][Iter  80/100] lr: 0.00200, loss: 0.00663, EQ: 0.00663, batch_cost: 0.00180s, reader_cost: 0.00011s, ips: 17756.64, eta: 0:00:00\nppsci INFO: [Train][Epoch  9/10][Iter  90/100] lr: 0.00200, loss: 0.00598, EQ: 0.00598, batch_cost: 0.00180s, reader_cost: 0.00011s, ips: 17793.97, eta: 0:00:00\nppsci INFO: [Train][Epoch  9/10][Iter 100/100] lr: 0.00200, loss: 0.00547, EQ: 0.00547, batch_cost: 0.00179s, reader_cost: 0.00011s, ips: 17864.08, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  10/100] lr: 0.00200, loss: 0.00079, EQ: 0.00079, batch_cost: 0.00182s, reader_cost: 0.00012s, ips: 17547.05, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  20/100] lr: 0.00200, loss: 0.00075, EQ: 0.00075, batch_cost: 0.00183s, reader_cost: 0.00011s, ips: 17482.92, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  30/100] lr: 0.00200, loss: 0.00077, EQ: 0.00077, batch_cost: 0.00182s, reader_cost: 0.00011s, ips: 17539.51, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  40/100] lr: 0.00200, loss: 0.00074, EQ: 0.00074, batch_cost: 0.00182s, reader_cost: 0.00011s, ips: 17587.51, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  50/100] lr: 0.00200, loss: 0.00071, EQ: 0.00071, batch_cost: 0.00182s, reader_cost: 0.00011s, ips: 17563.59, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  60/100] lr: 0.00200, loss: 0.00070, EQ: 0.00070, batch_cost: 0.00182s, reader_cost: 0.00011s, ips: 17604.60, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  70/100] lr: 0.00200, loss: 0.00074, EQ: 0.00074, batch_cost: 0.00181s, reader_cost: 0.00011s, ips: 17699.28, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  80/100] lr: 0.00200, loss: 0.00077, EQ: 0.00077, batch_cost: 0.00180s, reader_cost: 0.00011s, ips: 17764.92, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  90/100] lr: 0.00200, loss: 0.00075, EQ: 0.00075, batch_cost: 0.00180s, reader_cost: 0.00011s, ips: 17795.87, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter 100/100] lr: 0.00200, loss: 0.00071, EQ: 0.00071, batch_cost: 0.00179s, reader_cost: 0.00011s, ips: 17872.00, eta: 0:00:00\n</code></pre> <p>\u8bad\u7ec3\u5b8c\u6bd5\u540e\u518d\u7528\u521a\u624d\u53d6\u7684 1000 \u4e2a\u70b9\u4e0e\u6807\u51c6\u89e3\u8ba1\u7b97 L2-\u76f8\u5bf9\u8bef\u5dee</p> <pre><code># compute l2-relative error of trained model\npred_u = solver.predict(visual_input_dict, return_numpy=True)[\"u\"]\nl2_rel = np.linalg.norm(pred_u - visual_input_dict[\"u_ref\"]) / np.linalg.norm(\n    visual_input_dict[\"u_ref\"]\n)\nlogger.info(f\"l2_rel = {l2_rel:.5f}\")\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230\u5229\u7528\u6807\u51c6\u89e3\u76d1\u7763\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u6807\u51c6\u89e3\u9644\u8fd1\u4ecd\u6709\u5f88\u597d\u7684\u9884\u6d4b\u80fd\u529b\uff0cL2-\u76f8\u5bf9\u8bef\u5dee\u4e3a 0.02677\u3002</p> <p>\u9884\u6d4b\u7ed3\u679c\u53ef\u89c6\u5316\u5982\u4e0b\u6240\u793a</p> <p></p> <p>\u573a\u666f\u4e00\u7684\u5b8c\u6574\u4ee3\u7801\u5982\u4e0b\u6240\u793a</p> examples/quick_start/case1.py<pre><code>import numpy as np\n\nimport ppsci\nfrom ppsci.utils import logger\n\n# set random seed(42) for reproducibility\nppsci.utils.misc.set_random_seed(42)\n\n# set output directory\nOUTPUT_DIR = \"./output_quick_start_case1\"\n\n# initialize logger while create output directory\nlogger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n\n# set 1D-geometry domain([-\u03c0, \u03c0])\nl_limit, r_limit = -np.pi, np.pi\nx_domain = ppsci.geometry.Interval(l_limit, r_limit)\n\n# set model to 3-layer MLP\nmodel = ppsci.arch.MLP((\"x\",), (\"u\",), 3, 64)\n\n# standard solution of sin(x)\ndef sin_compute_func(data: dict):\n    return np.sin(data[\"x\"])\n\n\n# set constraint on 1D-geometry([-\u03c0, \u03c0])\nITERS_PER_EPOCH = 100  # use 100 iterations per training epoch\ninterior_constraint = ppsci.constraint.InteriorConstraint(\n    output_expr={\"u\": lambda out: out[\"u\"]},\n    label_dict={\"u\": sin_compute_func},\n    geom=x_domain,\n    dataloader_cfg={\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"shuffle\": True,\n        },\n        \"batch_size\": 32,  # use 32 samples(points) per iteration for interior constraint\n    },\n    loss=ppsci.loss.MSELoss(),\n)\n# wrap constraint(s) into one dict\nconstraint = {\n    interior_constraint.name: interior_constraint,\n}\n\n# set training hyper-parameters\nEPOCHS = 10\n# set optimizer\noptimizer = ppsci.optimizer.Adam(2e-3)(model)\n\n# set visualizer\nvisual_input_dict = {\n    \"x\": np.linspace(l_limit, r_limit, 1000, dtype=\"float32\").reshape(1000, 1)\n}\nvisual_input_dict[\"u_ref\"] = np.sin(visual_input_dict[\"x\"])\nvisualizer = {\n    \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n        visual_input_dict,\n        (\"x\",),\n        {\"u_pred\": lambda out: out[\"u\"], \"u_ref\": lambda out: out[\"u_ref\"]},\n        prefix=\"u=sin(x)\",\n    ),\n}\n\n# initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    OUTPUT_DIR,\n    optimizer,\n    epochs=EPOCHS,\n    iters_per_epoch=ITERS_PER_EPOCH,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n\n# compute l2-relative error of trained model\npred_u = solver.predict(visual_input_dict, return_numpy=True)[\"u\"]\nl2_rel = np.linalg.norm(pred_u - visual_input_dict[\"u_ref\"]) / np.linalg.norm(\n    visual_input_dict[\"u_ref\"]\n)\nlogger.info(f\"l2_rel = {l2_rel:.5f}\")\n\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>"},{"location":"zh/quickstart/#3","title":"3. \u573a\u666f\u4e8c","text":"<p>\u53ef\u4ee5\u770b\u5230\u573a\u666f\u4e00\u7684\u76d1\u7763\u8bad\u7ec3\u65b9\u5f0f\u80fd\u8f83\u597d\u5730\u89e3\u51b3\u51fd\u6570\u62df\u5408\u95ee\u9898\uff0c\u4f46\u4e00\u822c\u60c5\u51b5\u4e0b\u6211\u4eec\u662f\u65e0\u6cd5\u5f97\u77e5\u62df\u5408\u51fd\u6570\u672c\u8eab\u7684\u89e3\u6790\u5f0f\u7684\uff0c\u56e0\u6b64\u4e5f\u65e0\u6cd5\u76f4\u63a5\u6784\u9020\u56e0\u53d8\u91cf\u7684\u76d1\u7763\u6570\u636e\u3002</p> <p>\u867d\u7136\u65e0\u6cd5\u6c42\u51fa\u89e3\u6790\u5f0f\u76f4\u63a5\u6784\u9020\u76d1\u7763\u6570\u636e\uff0c\u4f46\u5f80\u5f80\u53ef\u4ee5\u5229\u7528\u76f8\u5173\u6570\u5b66\u77e5\u8bc6\uff0c\u63a8\u5bfc\u51fa\u76ee\u6807\u62df\u5408\u51fd\u6570\u7b26\u5408\u7684\u67d0\u79cd\u6570\u5b66\u5173\u7cfb\uff0c\u4ee5\u8bad\u7ec3\u6a21\u578b\u4ee5\u6ee1\u8db3\u8fd9\u79cd\u6570\u5b66\u5173\u7cfb\u7684\u65b9\u5f0f\uff0c\u8fbe\u5230\u4ee5\u201c\u95f4\u63a5\u76d1\u7763\u201d\u7684\u65b9\u5f0f\u4f18\u5316\u6a21\u578b\u7684\u76ee\u7684\u3002</p> <p>\u5047\u8bbe\u6211\u4eec\u4e0d\u518d\u4f7f\u7528 \\(u=\\sin(x)\\) \u8fd9\u4e00\u5148\u9a8c\u516c\u5f0f\uff0c\u56e0\u800c\u65e0\u6cd5\u8ba1\u7b97\u6807\u7b7e\u6570\u636e \\(u\\)\u3002\u56e0\u6b64\u6211\u4eec\u4f7f\u7528\u5982\u4e0b\u65b9\u7a0b\u7ec4\uff0c\u5176\u542b\u6709\u4e00\u4e2a\u504f\u5fae\u5206\u65b9\u7a0b\u548c\u8fb9\u754c\u6761\u4ef6</p> \\[ \\begin{cases} \\begin{aligned}     \\dfrac{\\partial u} {\\partial x} &amp;= \\cos(x) \\\\     u(-\\pi) &amp;= 2 \\end{aligned} \\end{cases} \\] <p>\u6784\u9020\u6570\u636e\u5bf9 \\((x_i, \\cos(x_i)), i=1,...,N\\)\u3002 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u4ecd\u7136\u80fd\u4fdd\u6301\u6a21\u578b\u7684\u8f93\u5165\u3001\u8f93\u51fa\u4e0d\u53d8\uff0c\u4f46\u4f18\u5316\u76ee\u6807\u53d8\u6210\u4e86\uff1a\u8ba9 \\(\\dfrac{\\partial \\hat{u}} {\\partial x}\\) \u5c3d\u53ef\u80fd\u5730\u63a5\u8fd1 \\(\\cos(x)\\)\uff0c\u4e14 \\(\\hat{u}(-\\pi)\\) \u4e5f\u8981\u5c3d\u53ef\u80fd\u5730\u63a5\u8fd1 \\(2\\)\u3002</p> <p>\u57fa\u4e8e\u4ee5\u4e0a\u7406\u8bba\uff0c\u6211\u4eec\u5bf9\u573a\u666f\u4e00\u7684\u4ee3\u7801\u8fdb\u884c\u5c11\u91cf\u7684\u6539\u5199\u5373\u53ef\u5f97\u5230\u672c\u573a\u666f\u4e8c\u7684\u4ee3\u7801\u3002</p> <p>\u9996\u5148\u7531\u4e8e\u6211\u4eec\u9700\u8981\u4f7f\u7528\u4e00\u9636\u5fae\u5206\u8fd9\u4e00\u64cd\u4f5c\uff0c\u56e0\u6b64\u5728\u4ee3\u7801\u5f00\u5934\u5904\u9700\u5bfc\u5165\u4e00\u9636\u5fae\u5206 API</p> <pre><code>import numpy as np\n\nimport ppsci\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\n</code></pre> <p>\u7136\u540e\u5728\u539f\u6765\u7684\u6807\u7b7e\u8ba1\u7b97\u51fd\u6570\u4e0b\u65b9\uff0c\u65b0\u589e\u4e00\u4e2a\u5fae\u5206\u6807\u7b7e\u503c\u8ba1\u7b97\u51fd\u6570</p> <pre><code># standard solution of cos(x)\ndef cos_compute_func(data: dict):\n    return np.cos(data[\"x\"])\n</code></pre> <p>\u63a5\u7740\u5c06 <code>interior_constraint</code> \u8fd9\u4e00\u7ea6\u675f\u6761\u4ef6\u4ece\u7ea6\u675f\u201c\u6a21\u578b\u8f93\u51fa\u201d\uff0c\u6539\u4e3a\u7ea6\u675f\u201c\u6a21\u578b\u8f93\u51fa\u5bf9\u8f93\u5165\u7684\u4e00\u9636\u5fae\u5206\u201d</p> <pre><code># set constraint on 1D-geometry([-\u03c0, \u03c0])\nITERS_PER_EPOCH = 100  # use 100 iterations per training epoch\ninterior_constraint = ppsci.constraint.InteriorConstraint(\n    output_expr={\"du_dx\": lambda out: jacobian(out[\"u\"], out[\"x\"])},\n    label_dict={\"du_dx\": cos_compute_func},\n    geom=x_domain,\n    dataloader_cfg={\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"shuffle\": True,\n        },\n        \"batch_size\": 32,  # use 32 samples(points) per iteration for interior constraint\n    },\n    loss=ppsci.loss.MSELoss(),\n)\n</code></pre> <p>\u8003\u8651\u5230\u4e00\u822c\u60c5\u51b5\u4e0b\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u89e3\u4f1a\u5b58\u5728\u5f85\u5b9a\u7cfb\u6570\uff0c\u9700\u901a\u8fc7\u5b9a\u89e3\u6761\u4ef6\uff08\u521d\uff08\u8fb9\uff09\u503c\u6761\u4ef6\uff09\u6765\u786e\u5b9a\uff0c\u56e0\u6b64\u9700\u8981\u5728 <code>interior_constraint</code> \u6784\u5efa\u4ee3\u7801\u7684\u540e\u9762\uff0c\u989d\u5916\u6dfb\u52a0\u4e00\u4e2a\u8fb9\u754c\u6761\u4ef6\u7ea6\u675f <code>bc_constraint</code>\uff0c\u5982\u4e0b\u6240\u793a</p> <pre><code>bc_constraint = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda d: d[\"u\"]},\n    {\"u\": lambda d: sin_compute_func(d) + 2},  # (1)\n    x_domain,\n    dataloader_cfg={\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"shuffle\": True,\n        },\n        \"batch_size\": 1,  # use 1 sample(point) per iteration for boundary constraint\n    },\n    loss=ppsci.loss.MSELoss(),\n    criteria=lambda x: np.isclose(x, l_limit),\n)\n</code></pre> <ol> <li>\u5bf9\u5e94\u8fb9\u754c\u6761\u4ef6 \\(u(x_0)=sin(x_0)+2\\)</li> </ol> <p>\u7136\u540e\u5c06\u8be5\u8fb9\u754c\u7ea6\u675f <code>bc_constraint</code> \u6dfb\u52a0\u5230 <code>constraint</code> \u4e2d</p> <pre><code># wrap constraint(s) into one dict\nconstraint = {\n    interior_constraint.name: interior_constraint,\n    bc_constraint.name: bc_constraint,\n}\n</code></pre> <p>\u540c\u6837\u5730\uff0c\u4fee\u6539 Visualizer \u7ed8\u5236\u7684\u6807\u51c6\u89e3\u4e3a \\(sin(x)+2\\)</p> <pre><code># set visualizer\nvisual_input_dict = {\n    \"x\": np.linspace(l_limit, r_limit, 1000, dtype=\"float32\").reshape(1000, 1)\n}\nvisual_input_dict[\"u_ref\"] = np.sin(visual_input_dict[\"x\"]) + 2.0\nvisualizer = {\n    \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n        visual_input_dict,\n        (\"x\",),\n        {\"u_pred\": lambda out: out[\"u\"], \"u_ref\": lambda out: out[\"u_ref\"]},\n        prefix=\"u=sin(x)\",\n    ),\n}\n</code></pre> <p>\u4fee\u6539\u5b8c\u6bd5\u540e\u6267\u884c\u8bad\u7ec3</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    OUTPUT_DIR,\n    optimizer,\n    epochs=EPOCHS,\n    iters_per_epoch=ITERS_PER_EPOCH,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n</code></pre> <p>\u8bad\u7ec3\u65e5\u5fd7\u5982\u4e0b\u6240\u793a</p> <pre><code>...\n...\nppsci INFO: [Train][Epoch  9/10][Iter  90/100] lr: 0.00200, loss: 0.00176, EQ: 0.00087, BC: 0.00088, batch_cost: 0.00346s, reader_cost: 0.00024s, ips: 9527.80, eta: 0:00:00\nppsci INFO: [Train][Epoch  9/10][Iter 100/100] lr: 0.00200, loss: 0.00170, EQ: 0.00087, BC: 0.00083, batch_cost: 0.00349s, reader_cost: 0.00024s, ips: 9452.07, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  10/100] lr: 0.00200, loss: 0.00107, EQ: 0.00072, BC: 0.00035, batch_cost: 0.00350s, reader_cost: 0.00025s, ips: 9424.75, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  20/100] lr: 0.00200, loss: 0.00116, EQ: 0.00083, BC: 0.00033, batch_cost: 0.00350s, reader_cost: 0.00025s, ips: 9441.33, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  30/100] lr: 0.00200, loss: 0.00103, EQ: 0.00079, BC: 0.00024, batch_cost: 0.00355s, reader_cost: 0.00025s, ips: 9291.90, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  40/100] lr: 0.00200, loss: 0.00108, EQ: 0.00078, BC: 0.00030, batch_cost: 0.00353s, reader_cost: 0.00025s, ips: 9348.09, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  50/100] lr: 0.00200, loss: 0.00163, EQ: 0.00082, BC: 0.00082, batch_cost: 0.00350s, reader_cost: 0.00024s, ips: 9416.24, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  60/100] lr: 0.00200, loss: 0.00160, EQ: 0.00083, BC: 0.00077, batch_cost: 0.00353s, reader_cost: 0.00024s, ips: 9345.73, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  70/100] lr: 0.00200, loss: 0.00150, EQ: 0.00082, BC: 0.00068, batch_cost: 0.00351s, reader_cost: 0.00024s, ips: 9393.89, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  80/100] lr: 0.00200, loss: 0.00146, EQ: 0.00081, BC: 0.00064, batch_cost: 0.00350s, reader_cost: 0.00024s, ips: 9424.81, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter  90/100] lr: 0.00200, loss: 0.00138, EQ: 0.00081, BC: 0.00058, batch_cost: 0.00349s, reader_cost: 0.00024s, ips: 9444.12, eta: 0:00:00\nppsci INFO: [Train][Epoch 10/10][Iter 100/100] lr: 0.00200, loss: 0.00133, EQ: 0.00079, BC: 0.00054, batch_cost: 0.00349s, reader_cost: 0.00024s, ips: 9461.54, eta: 0:00:00\n</code></pre> <p>\u8bad\u7ec3\u5b8c\u6bd5\u540e\u518d\u7528\u521a\u624d\u53d6\u7684 1000 \u4e2a\u70b9\u4e0e\u6807\u51c6\u89e3\u8ba1\u7b97 L2-\u76f8\u5bf9\u8bef\u5dee</p> <pre><code># compute l2-relative error of trained model\npred_u = solver.predict(visual_input_dict, return_numpy=True)[\"u\"]\nl2_rel = np.linalg.norm(pred_u - visual_input_dict[\"u_ref\"]) / np.linalg.norm(\n    visual_input_dict[\"u_ref\"]\n)\nlogger.info(f\"l2_rel = {l2_rel:.5f}\")\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230\u5229\u7528\u5fae\u5206\u65b9\u7a0b\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u6807\u51c6\u89e3\u9644\u8fd1\u4ecd\u6709\u5f88\u597d\u7684\u9884\u6d4b\u80fd\u529b\uff0cL2-\u76f8\u5bf9\u8bef\u5dee\u4e3a 0.00564\u3002</p> <p>\u9884\u6d4b\u7ed3\u679c\u53ef\u89c6\u5316\u5982\u4e0b\u6240\u793a</p> <p></p> <p>\u53ef\u4ee5\u53d1\u73b0\u5229\u7528\u5fae\u5206\u5173\u7cfb\u8bad\u7ec3\u7684\u6a21\u578b\u4ecd\u7136\u5177\u5907\u826f\u597d\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u4e14\u7ed3\u5408\u5b9a\u89e3\u6761\u4ef6\uff0c\u80fd\u5b66\u4e60\u51fa\u540c\u65f6\u7b26\u5408\u5fae\u5206\u65b9\u7a0b\u548c\u5b9a\u89e3\u6761\u4ef6\u7684\u6b63\u786e\u89e3\u6a21\u578b\u3002</p> <p>\u573a\u666f\u4e8c\u7684\u5b8c\u6574\u4ee3\u7801\u5982\u4e0b\u6240\u793a</p> <pre><code>import numpy as np\n\nimport ppsci\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\n\n# set random seed(42) for reproducibility\nppsci.utils.misc.set_random_seed(42)\n\n# set output directory\nOUTPUT_DIR = \"./output_quick_start_case2\"\n\n# initialize logger while create output directory\nlogger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n\n# set 1D-geometry domain([-\u03c0, \u03c0])\nl_limit, r_limit = -np.pi, np.pi\nx_domain = ppsci.geometry.Interval(l_limit, r_limit)\n\n# set model to 3-layer MLP\nmodel = ppsci.arch.MLP((\"x\",), (\"u\",), 3, 64)\n\n# standard solution of sin(x)\ndef sin_compute_func(data: dict):\n    return np.sin(data[\"x\"])\n\n\n# standard solution of cos(x)\ndef cos_compute_func(data: dict):\n    return np.cos(data[\"x\"])\n\n\n# set constraint on 1D-geometry([-\u03c0, \u03c0])\nITERS_PER_EPOCH = 100  # use 100 iterations per training epoch\ninterior_constraint = ppsci.constraint.InteriorConstraint(\n    output_expr={\"du_dx\": lambda out: jacobian(out[\"u\"], out[\"x\"])},\n    label_dict={\"du_dx\": cos_compute_func},\n    geom=x_domain,\n    dataloader_cfg={\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"shuffle\": True,\n        },\n        \"batch_size\": 32,  # use 32 samples(points) per iteration for interior constraint\n    },\n    loss=ppsci.loss.MSELoss(),\n)\nbc_constraint = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda d: d[\"u\"]},\n    {\"u\": lambda d: sin_compute_func(d) + 2},  # (1)\n    x_domain,\n    dataloader_cfg={\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"shuffle\": True,\n        },\n        \"batch_size\": 1,  # use 1 sample(point) per iteration for boundary constraint\n    },\n    loss=ppsci.loss.MSELoss(),\n    criteria=lambda x: np.isclose(x, l_limit),\n)\n# wrap constraint(s) into one dict\nconstraint = {\n    interior_constraint.name: interior_constraint,\n    bc_constraint.name: bc_constraint,\n}\n\n# set training hyper-parameters\nEPOCHS = 10\n# set optimizer\noptimizer = ppsci.optimizer.Adam(2e-3)(model)\n\n# set visualizer\nvisual_input_dict = {\n    \"x\": np.linspace(l_limit, r_limit, 1000, dtype=\"float32\").reshape(1000, 1)\n}\nvisual_input_dict[\"u_ref\"] = np.sin(visual_input_dict[\"x\"]) + 2.0\nvisualizer = {\n    \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n        visual_input_dict,\n        (\"x\",),\n        {\"u_pred\": lambda out: out[\"u\"], \"u_ref\": lambda out: out[\"u_ref\"]},\n        prefix=\"u=sin(x)\",\n    ),\n}\n\n# initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    OUTPUT_DIR,\n    optimizer,\n    epochs=EPOCHS,\n    iters_per_epoch=ITERS_PER_EPOCH,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n\n# compute l2-relative error of trained model\npred_u = solver.predict(visual_input_dict, return_numpy=True)[\"u\"]\nl2_rel = np.linalg.norm(pred_u - visual_input_dict[\"u_ref\"]) / np.linalg.norm(\n    visual_input_dict[\"u_ref\"]\n)\nlogger.info(f\"l2_rel = {l2_rel:.5f}\")\n\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>"},{"location":"zh/reproduction/","title":"\u590d\u73b0\u6307\u5357","text":""},{"location":"zh/reproduction/#_1","title":"\u6a21\u578b\u590d\u73b0\u6d41\u7a0b\u53ca\u9a8c\u6536\u6807\u51c6","text":"<p>\u672c\u6587\u6863\u4ecb\u7ecd\u5982\u4f55\u57fa\u4e8e PaddleScience \u5957\u4ef6\u8fdb\u884c\u6a21\u578b\u590d\u73b0\u5e76\u6700\u7ec8\u8d21\u732e\u5230 PaddleScience \u5957\u4ef6\u4e2d</p>"},{"location":"zh/reproduction/#1","title":"1. \u80cc\u666f\u8bf4\u660e","text":""},{"location":"zh/reproduction/#11","title":"1.1 \u590d\u73b0\u76ee\u6807","text":"<p>\u5728 AI for Science (\u4ee5\u4e0b\u7b80\u79f0 AI4S) \u9886\u57df\u7684\u6a21\u578b\u590d\u73b0\u4efb\u52a1\u4e2d\uff0c\u901a\u5e38\u9700\u8981\u5bf9\u6807\u51c6\u8bba\u6587\u4e2d\u7684\u539f\u59cb\u4ee3\u7801\u57fa\u4e8e\u98de\u6868\u8fdb\u884c\u590d\u73b0\uff0c\u540c\u65f6\u4e5f\u53ef\u80fd\u4f1a\u6d89\u53ca\u4e00\u4e9b\u5f00\u653e\u6027\u7684\u4efb\u52a1\uff0c\u5982\u4f1a\u63d0\u4f9b\u5efa\u8bbe\u76ee\u6807\uff0c\u9009\u624b\u81ea\u884c\u4f7f\u7528\u98de\u6868\u8fdb\u884c\u5168\u90e8\u4ee3\u7801\u7684\u521b\u5efa\u3002\u56f4\u7ed5\u4ee5\u4e0a\u4efb\u52a1\uff0c\u5236\u5b9a\u5982\u4e0b\u6a21\u578b\u590d\u73b0\u6d41\u7a0b\u3001\u9a8c\u6536\u6807\u51c6\u4e0e\u65b9\u6cd5\uff0c\u5176\u4e2d\u6bcf\u4e2a\u9636\u6bb5\u7684\u6bcf\u4e2a\u8fc7\u7a0b\u5747\u9700\u8981\u5f62\u6210\u5bf9\u5e94\u7684\u8bf4\u660e\u6587\u6863\u3002</p>"},{"location":"zh/reproduction/#12-paddlescience","title":"1.2 \u98de\u6868\u4e0e PaddleScience","text":"<p>\u9664\u7279\u6b8a\u8bf4\u660e\u5916\uff0c\u6a21\u578b\u590d\u73b0\u9ed8\u8ba4\u8981\u6c42\u57fa\u4e8e PaddleScience</p> <ul> <li> <p>PaddleScience</p> <p>\u57fa\u4e8e\u98de\u6868\u7684 AI4S \u5957\u4ef6\uff0c\u63d0\u4f9b\u9762\u5411 AI4S \u9886\u57df\u901a\u7528\u529f\u80fd\uff0c\u5982\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u89e3\u6790\u3001\u901a\u7528\u5fae\u5206\u65b9\u7a0b\u3001\u6570\u636e\u9a71\u52a8/\u7269\u7406\u673a\u7406/\u6570\u7406\u878d\u5408\u7b49\u6c42\u89e3\u5668\uff0c\u65b9\u4fbf\u5f00\u53d1 AI4S \u9886\u57df\u76f8\u5173\u6a21\u578b\uff0c\u5177\u4f53\u53c2\u8003 PaddleScience \u6587\u6863</p> </li> <li> <p>\u98de\u6868PaddlePaddle</p> <p>\u56fd\u4ea7\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u5e73\u53f0\uff0c\u63d0\u4f9b\u9762\u5411\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7840\u529f\u80fd\u4e0e\u901a\u7528API\uff0c\u65b9\u4fbf\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5177\u4f53\u53c2\u8003 Paddle API \u6587\u6863</p> </li> </ul>"},{"location":"zh/reproduction/#2","title":"2. \u590d\u73b0\u6b65\u9aa4","text":"<p>\u4f18\u5148\u57fa\u4e8e PaddleScience \u590d\u73b0\uff0c\u5982\u590d\u73b0\u4ee3\u7801\u7684\u5b9e\u73b0\u903b\u8f91\u548c PaddleScience \u5b58\u5728\u4e25\u91cd\u51b2\u7a81\uff0c\u53ef\u8003\u8651\u4f7f\u7528 PaddlePaddle API \u590d\u73b0\u3002</p> <p></p>"},{"location":"zh/reproduction/#21","title":"2.1 \u57fa\u51c6\u8bba\u6587\u7406\u89e3\u3001\u539f\u59cb\u4ee3\u7801\u8dd1\u901a\u3001\u8bba\u6587\u7ed3\u679c\u590d\u73b0","text":"<p>\u8be5\u9636\u6bb5\u9700\u9009\u624b\u8ba4\u771f\u7406\u89e3\u539f\u59cb\u8bba\u6587\u4e0e\u63d0\u4f9b\u7684\u4ee3\u7801\uff0c\u5e76\u6309\u7167\u80cc\u666f\u3001\u65b9\u6cd5\u4e0e\u6a21\u578b\u3001\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u3001\u7ed3\u679c\u3001\u5c55\u671b\u4e0e\u81f4\u8c22\u7b49\u5206\u7ae0\u8282\u8bf4\u660e\u8bba\u6587\u3002\u8fc7\u7a0b\u4e2d\u9700\u8981\u9a8c\u8bc1\u8bba\u6587\u63d0\u4f9b\u7684\u4ee3\u7801\uff0c\u786e\u8ba4\u4ee3\u7801\u5b8c\u6574\u4e14\u53ef\u6267\u884c\uff0c\u4e14\u80fd\u591f\u5f97\u5230\u8bba\u6587\u4e2d\u5217\u51fa\u7684\u7ed3\u679c\u3002 *\u6b64\u90e8\u5206\u4e3b\u8981\u76ee\u7684\u662f\u4e3a\u4e86\u5176\u4ed6\u7528\u6237\u66f4\u597d\u7684\u7406\u89e3\u8be5\u590d\u73b0\u5de5\u4f5c\u7684\u610f\u4e49- \u3002\u5173\u952e\u793a\u610f\u5982\u4e0b\uff1a</p> <ol> <li> <p>\u539f\u59cb\u4ee3\u7801\u9a8c\u8bc1</p> <p>\u521d\u6b65\u5bf9\u7167\u57fa\u51c6\u8bba\u6587\u4e2d\u63d0\u4f9b\u7684\u7ed3\u679c\uff0c\u8bc4\u4f30\u63d0\u4f9b\u7684\u4ee3\u7801\u662f\u5426\u5b8c\u6574\u3001\u53ef\u6267\u884c\uff0c\u5177\u4f53\u4e3a\uff1a</p> <ul> <li>\u82e5\u6709\u95ee\u9898\uff0c\u5219\u9700\u8981\u4e0e\u98de\u6868\u56e2\u961f\u53cd\u9988\u8bba\u6587\u95ee\u9898\uff0c\u7531\u98de\u6868\u56e2\u961f\u8bc4\u4f30\u8bba\u6587/\u6a21\u578b\u590d\u73b0\u7684\u5fc5\u8981\u6027\uff0c\u82e5\u8be5\u8bba\u6587/\u6a21\u578b\u4ef7\u503c\u8f83\u9ad8\uff0c\u4e14\u5177\u5907\u4e86\u590d\u73b0\u7684\u6761\u4ef6\u540e\uff0c\u5219\u53ef\u91cd\u65b0\u8fdb\u5165\u590d\u73b0\u6d41\u7a0b\uff1b</li> <li>\u82e5\u65e0\u95ee\u9898\uff0c\u5219\u6b63\u5f0f\u8fdb\u5165\u8bba\u6587/\u6a21\u578b\u7684\u590d\u73b0\u9636\u6bb5</li> </ul> </li> <li> <p>\u80cc\u666f\u8bf4\u660e</p> <p>\u7ed3\u5408\u7ed9\u5b9a\u7684\u8bba\u6587\uff0c\u9700\u89e3\u91ca\u8bba\u6587\u63cf\u8ff0\u7684\u9886\u57df\u95ee\u9898\uff08eg. \u6d41\u4f53\u3001\u6750\u6599\u7b49\uff09\u3001\u4ecb\u7ecd\u81ea\u5df1\u7684\u590d\u73b0/\u5f00\u53d1\u5de5\u4f5c\uff0c\u5373\u5bf9\u57fa\u51c6\u8bba\u6587\u4e2d\u6458\u8981\u3001\u7b80\u4ecb\u6709\u5b8c\u6574\u7684\u4ecb\u7ecd\uff08\u5f00\u653e\u8bfe\u9898\u5219\u63cf\u8ff0\u8be5\u8bfe\u9898\u7684\u80cc\u666f\uff09\uff0c\u4e4b\u540e\u5bf9\u8bba\u6587\u7684\u65b9\u6cd5\uff0c\u5e94\u7528\u7684\u6a21\u578b\u8fdb\u884c\u8bf4\u660e\u3002</p> </li> <li> <p>\u65b9\u6cd5\u4e0e\u6a21\u578b</p> <p>\u89e3\u91ca\u57fa\u51c6\u8bba\u6587\u6240\u7528\u5230\u7684\u7406\u8bba\u3001\u65b9\u6cd5\u4ee5\u53ca\u6240\u7528\u7f51\u7edc\u6a21\u578b\u7684\u539f\u7406\u7b49\uff0c\u5373\u5bf9\u8bba\u6587\u7684\u6838\u5fc3\u6280\u672f\u90e8\u5206\u8fdb\u884c\u8bf4\u660e\uff0c\u540c\u65f6\u8bf4\u660e\u8bba\u6587\u4e2d\u8be5\u65b9\u6cd5\u3001\u6a21\u578b\u6240\u4f53\u73b0\u7684\u4ef7\u503c\uff08\u6216\u76f8\u5bf9\u4e8e\u4f20\u7edf\u6c42\u89e3\u65b9\u6cd5\u7684\u4f18\u52bf\u7b49\uff09\u3002</p> </li> <li> <p>\u8bad\u7ec3\u4e0e\u9a8c\u8bc1</p> <p>\u5bf9\u57fa\u51c6\u8bba\u6587\u4e2d\u63d0\u4f9b\u7684\u5178\u578b\u6848\u4f8b\uff0c\u6574\u4f53\u8bf4\u660e\u6848\u4f8b\u7684\u539f\u7406\uff08eg. \u6d41\u4f53 N-S \u65b9\u7a0b\u6c42\u89e3\uff09\u3001\u8bf4\u660e\u8bad\u7ec3\u7684\u8fc7\u7a0b\uff08\u5982\u6570\u636e\u96c6\u6765\u6e90\u3001\u5185\u5bb9\uff0c\u8d85\u53c2\u6570\u8bbe\u5b9a\uff0c\u786c\u4ef6\u73af\u5883\u53ca\u8bad\u7ec3\u65f6\u95f4\u7b49\uff09\u3002</p> </li> </ol>"},{"location":"zh/reproduction/#22-paddlescience","title":"2.2 \u57fa\u4e8e PaddleScience \u7684\u4ee3\u7801\u590d\u73b0","text":"<ol> <li> <p>\u5982\u679c\u6e90\u4ee3\u7801\u4f7f\u7528 pytorch \u5b9e\u73b0\uff0c\u53ef\u4f18\u5148\u4f7f\u7528 PaConvert \u4e00\u952e\u81ea\u52a8\u8f6c\u6362 pytorch \u4ee3\u7801\u4e3a paddle \u4ee3\u7801\uff0c\u914d\u5408 PaDiff \u9a8c\u8bc1\u8f6c\u6362\u524d\u540e\u6a21\u578b\u7684\u7cbe\u5ea6\u5bf9\u9f50\u60c5\u51b5\uff1b\u5982\u679c\u6e90\u4ee3\u7801\u4f7f\u7528\u975e pytorch \u5b9e\u73b0\uff08\u5982 tensorflow\uff09\uff0c\u5219\u9700\u8981\u5217\u51fa\u9700\u8981\u57fa\u4e8e\u98de\u6868\u590d\u73b0\u7684 API\uff0c\u540c\u65f6\u9700\u8981\u5f62\u6210 API \u6620\u5c04\u8868\uff0c\u57fa\u4e8e\u8be5\u6620\u5c04\u8868\u624b\u52a8\u8f6c\u6362 tensorflow API \u4e3a paddle API\uff0c\u82e5\u98de\u6868\u7f3a\u5c11\u5bf9\u5e94\u7684API\u53ef\u7740\u91cd\u8bf4\u660e\u3002</p> </li> <li> <p>\u98de\u6868\u4ee3\u7801\u8dd1\u901a</p> <p>\u5b8c\u6210\u57fa\u4e8e API \u590d\u73b0\u540e\uff0c\u9700\u8981\u7ed3\u5408\u8bba\u6587\u4e2d\u7684\u6a21\u578b\uff0c\u5b9e\u73b0\u57fa\u4e8e\u98de\u6868\u7684\u4ee3\u7801\u8dd1\u901a\uff0c\u5e76\u9488\u5bf9\u524d\u53cd\u5411\u4f7f\u80fd\u80fd\u5bf9\u9f50\u4f7f\u7528 PaDiff \u5de5\u5177\u8fdb\u884c\u8fed\u4ee3\u3001\u4fee\u6b63\uff08\u5982\u53ef\u80fd\u9700\u8981\u4e0e\u539f\u59cb\u4ee3\u7801\u9010\u6b65\u5bf9\u9f50\uff09\u3002</p> </li> <li> <p>\u98de\u6868\u4ee3\u7801\u8f6c\u6362\u6210 PaddleScience \u4ee3\u7801</p> <p>\u63d0\u9ad8\u4ee3\u7801\u590d\u7528\u6027\u548c\u7b80\u6d01\u6027\uff0c\u9700\u8981\u5c06 Paddle \u5b9e\u73b0\u8f6c\u6362\u6210 PaddleScience \u5957\u4ef6\u63d0\u4f9b\u7684 API \u5b9e\u73b0\uff0cPaddleScience \u63d0\u4f9b\u4e86\u9762\u5411 AI4S \u9886\u57df\u901a\u7528\u529f\u80fd\u3002\u5177\u4f53\u6d41\u7a0b\u8bf7\u53c2\u8003\uff1aPaddleScience \u5f00\u53d1\u6307\u5357</p> </li> <li> <p>\u590d\u73b0\u8bba\u6587\u4e2d\u6307\u6807\u7ed3\u679c</p> <p>\u7ed3\u5408\u8bba\u6587\u4e2d\u63d0\u5230\u7684\u5178\u578b demo\uff0c\u8fdb\u884c\u5173\u952e\u6307\u6807\u590d\u73b0\uff0c\u5373\u80fd\u591f\u57fa\u4e8e\u98de\u6868\u590d\u73b0\u8bba\u6587\u4e2d\u7684\u6307\u6807\uff0c\u5982\u56fe\u8868\u3001\u6570\u636e\u7b49\uff0c\u5176\u6536\u655b\u8d8b\u52bf\u3001\u6570\u503c\u7b49\u5e94\u57fa\u672c\u4e00\u81f4\uff08\u4e0e\u4f5c\u8005\u63d0\u4f9b\u6307\u6807\u7684\u76f8\u5bf9\u8bef\u5dee\u5728 \u00b110% \u4ee5\u5185\u5373\u53ef\uff09</p> </li> <li> <p>\u4ee3\u7801\u5408\u5165\uff0c\u5b8c\u6210\u590d\u73b0</p> <p>\u82e5\u4ee5\u4e0a\u6d3b\u52a8\u5747\u5b8c\u5168\u8fbe\u6210\uff0c\u5219\u53ef\u4ee5\u5c06\u8bba\u6587/\u6a21\u578b\u590d\u73b0\u7684\u4ee3\u7801\u6309\u7167\u4e0b\u6587\u4e2d\u201c\u4ee3\u7801\u89c4\u8303\u201d\u7684\u8981\u6c42\uff0c\u63d0\u4ea4 PR \u5408\u5165\u5230\u6307\u5b9a\u7684 Repo\uff0c\u901a\u8fc7\u8bc4\u5ba1\u540e\u5373\u53ef\u8ba4\u4e3a\u5b8c\u6210 90% \u7684\u590d\u73b0\u5de5\u4f5c\uff0c\u5e76\u53ef\u8fdb\u884c\u9a8c\u6536\u3002</p> </li> </ol>"},{"location":"zh/reproduction/#3","title":"3 \u9a8c\u6536\u6807\u51c6","text":""},{"location":"zh/reproduction/#31","title":"3.1 \u4ea4\u4ed8\u4ea7\u7269\u5217\u8868","text":"\u4ea7\u7269 \u5177\u4f53\u5185\u5bb9 \u6a21\u578b \u57fa\u4e8e PaddleScience \u590d\u73b0\u7684\u4ee3\u7801\u3001\u6a21\u578b\u9884\u8bad\u7ec3\u53c2\u6570 \u6587\u6863 AIStudio \u6587\u6863\u3001PaddleScience \u6587\u6863"},{"location":"zh/reproduction/#32","title":"3.2 \u5177\u4f53\u8bc4\u4f30\u6807\u51c6","text":""},{"location":"zh/reproduction/#321","title":"3.2.1 \u6a21\u578b\u6b63\u786e\u6027\u8bc4\u4f30","text":"<p>\u5b9a\u6027\u5206\u6790\uff1a\u6a21\u578b\u6536\u655b\u66f2\u7ebf\u3001\u6700\u7ec8\u6548\u679c\u56fe\u793a\u4e0e\u8bba\u6587\u80fd\u591f\u4fdd\u6301\u4e00\u81f4 \u5b9a\u91cf\u5206\u6790\uff1a\u5982\u679c\u8bba\u6587\u5305\u542b\u5b9a\u91cf\u6307\u6807\uff0c\u5219\u590d\u73b0\u6307\u6807\u4e0e\u8bba\u6587\u6307\u6807\u76f8\u5bf9\u8bef\u5dee\u9700\u6ee1\u8db3\uff1a \\(\\dfrac{|\u590d\u73b0\u6307\u6807-\u6e90\u4ee3\u7801\u6307\u6807|}{\u6e90\u4ee3\u7801\u6307\u6807} \\le 10\\%\\)</p>"},{"location":"zh/reproduction/#322","title":"3.2.2 \u4ee3\u7801\u89c4\u8303","text":"<p>\u6574\u4f53\u4ee3\u7801\u89c4\u8303\u9075\u5faa PEP8 https://peps.python.org/pep-0008/ \uff0c\u9664\u6b64\u4e4b\u5916\u9700\u8981\u6ce8\u610f\uff1a</p> <ul> <li>\u6587\u4ef6\u548c\u6587\u4ef6\u5939\u547d\u540d\u4e2d\uff0c\u5c3d\u91cf\u4f7f\u7528\u4e0b\u5212\u7ebf <code>_</code> \u4ee3\u8868\u7a7a\u683c\uff0c\u4e0d\u8981\u4f7f\u7528 <code>-</code>\u3002</li> <li>\u6a21\u578b\u5b9a\u4e49\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u6709\u4e00\u4e2a\u7edf\u4e00\u7684\u53d8\u91cf\uff08parameter\uff09\u547d\u540d\u7ba1\u7406\u624b\u6bb5\uff0c\u5982\u5c3d\u91cf\u624b\u52a8\u58f0\u660e\u6bcf\u4e2a\u53d8\u91cf\u7684\u540d\u5b57\u5e76\u652f\u6301\u540d\u79f0\u53ef\u53d8\uff0c\u7981\u6b62\u5c06\u540d\u79f0\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5e38\u6570\uff08\u5982 \"embedding\"\uff09\uff0c\u907f\u514d\u5728\u590d\u7528\u4ee3\u7801\u9636\u6bb5\u51fa\u73b0\u5404\u79cd\u8be1\u5f02\u7684\u95ee\u9898\u3002</li> <li>\u91cd\u8981\u6587\u4ef6\uff0c\u53d8\u91cf\u7684\u540d\u79f0\u5b9a\u4e49\u8fc7\u7a0b\u4e2d\u9700\u8981\u80fd\u591f\u901a\u8fc7\u540d\u5b57\u8868\u660e\u542b\u4e49\uff0c\u7981\u6b62\u4f7f\u7528\u542b\u6df7\u4e0d\u6e05\u7684\u540d\u79f0\uff0c\u5982 net.py, aaa.py \u7b49\u3002</li> <li>\u5728\u4ee3\u7801\u4e2d\u5b9a\u4e49\u6587\u4ef6(\u5939)\u8def\u5f84\u65f6\uff0c\u9700\u8981\u4f7f\u7528 <code>os.path.join</code> \u5b8c\u6210\uff0c\u7981\u6b62\u4f7f\u7528 <code>string</code> \u76f8\u52a0\u7684\u65b9\u5f0f\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6a21\u578b\u5bf9 <code>windows</code> \u73af\u5883\u7f3a\u4e4f\u652f\u6301\u3002</li> <li> <p>\u5bf9\u4e8e\u4ee3\u7801\u4e2d\u91cd\u8981\u7684\u90e8\u5206\uff0c\u9700\u8981\u52a0\u5165\u6ce8\u91ca\u4ecb\u7ecd\u529f\u80fd\uff0c\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u719f\u6089\u4ee3\u7801\u7ed3\u6784\uff0c\u5305\u62ec\u4f46\u4e0d\u4ec5\u9650\u4e8e\uff1a</p> </li> <li> <p>Dataset\u3001DataLoader\u7684\u5b9a\u4e49\u3002</p> </li> <li>\u6574\u4e2a\u6a21\u578b\u5b9a\u4e49\uff0c\u5305\u62ecinput\uff0c\u8fd0\u7b97\u8fc7\u7a0b\uff0closs\u7b49\u5185\u5bb9\u3002</li> <li>init\uff0csave\uff0cload\uff0c\u7b49io\u90e8\u5206\u3002</li> <li>\u8fd0\u884c\u4e2d\u95f4\u7684\u5173\u952e\u72b6\u6001\uff0c\u5982print loss\uff0csave model\u7b49\u3002</li> <li> <p>\u4e00\u4e2a\u6bd4\u8f83\u7b26\u5408\u4ee3\u7801\u89c4\u8303\u7684\u4f8b\u5b50\u5982\u4e0b\u3002</p> <pre><code>from paddle import io\nfrom paddle.vision import transforms as T\nfrom PIL import Image\nimport numpy as np\n\n\nIMAGE_SIZE = 256\n\n\nclass PetDataset(io.Dataset):\n    \"\"\"\n    Pet \u6570\u636e\u96c6\u5b9a\u4e49\n    \"\"\"\n\n    def __init__(self, mode=\"train\"):\n        \"\"\"\n        \u6784\u9020\u51fd\u6570\n        \"\"\"\n        if mode not in [\"train\", \"test\", \"predict\"]:\n            raise ValueError(\n                f\"mode should be 'train' or 'test' or 'predict', but got {mode}\"\n            )\n        self.image_size = IMAGE_SIZE\n        self.mode = mode\n        self.train_images = []\n        self.label_images = []\n        with open(f\"./{self.mode}.txt\", \"r\") as f:\n            for line in f.readlines():\n                image, label = line.strip().split(\"\\t\")\n                self.train_images.append(image)\n                self.label_images.append(label)\n\n    def _load_img(self, path, color_mode=\"rgb\", transforms=[]):\n        \"\"\"\n        \u7edf\u4e00\u7684\u56fe\u50cf\u5904\u7406\u63a5\u53e3\u5c01\u88c5\uff0c\u7528\u4e8e\u89c4\u6574\u56fe\u50cf\u5927\u5c0f\u548c\u901a\u9053\n        \"\"\"\n        img = Image.open(path)\n        if color_mode == \"grayscale\":\n            # if image is not already an 8-bit, 16-bit or 32-bit grayscale image\n            # convert it to an 8-bit grayscale image.\n            if img.mode not in (\"L\", \"I;16\", \"I\"):\n                img = img.convert(\"L\")\n        elif color_mode == \"rgba\":\n            if img.mode != \"RGBA\":\n                img = img.convert(\"RGBA\")\n        elif color_mode == \"rgb\":\n            if img.mode != \"RGB\":\n                img = img.convert(\"RGB\")\n        else:\n            raise ValueError(\n                f\"color_mode should be 'grayscale', 'rgb', or 'rgba', but got {color_mode}\"\n            )\n        return T.Compose([T.Resize(self.image_size)] + transforms)(img)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        \u8fd4\u56de image, label\n        \"\"\"\n        train_image = self._load_img(\n            self.train_images[idx],\n            transforms=[T.Transpose(), T.Normalize(mean=127.5, std=127.5)],\n        )  # \u52a0\u8f7d\u539f\u59cb\u56fe\u50cf\n        label_image = self._load_img(\n            self.label_images[idx], color_mode=\"grayscale\", transforms=[T.Grayscale()]\n        )  # \u52a0\u8f7dLabel\u56fe\u50cf\n        # \u8fd4\u56deimage, label\n        train_image = np.array(train_image, dtype=\"float32\")\n        label_image = np.array(label_image, dtype=\"int64\")\n        return train_image, label_image\n\n    def __len__(self):\n        \"\"\"\n        \u8fd4\u56de\u6570\u636e\u96c6\u603b\u6570\n        \"\"\"\n        return len(self.train_images)\n</code></pre> </li> <li> <p>\u63d0\u4f9b\u7684\u4ee3\u7801\u80fd\u6b63\u5e38\u8dd1\u901a\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> </li> </ul>"},{"location":"zh/reproduction/#323","title":"3.2.3 \u6587\u6863\u89c4\u8303","text":"<p>AIStudio \u6587\u6863\u53c2\u8003\uff1aPaddleScience-DarcyFlow - \u98de\u6868AI Studio</p> <p>PaddleScience \u5b98\u7f51\u6587\u6863\u9700\u6ee1\u8db3\uff1a</p> <ul> <li>\u590d\u73b0\u5b8c\u6210\u540e\u9700\u64b0\u5199\u6a21\u578b\u6587\u6863\uff0c\u9700\u5305\u542b\u6a21\u578b\u7b80\u4ecb\u3001\u95ee\u9898\u5b9a\u4e49\u3001\u95ee\u9898\u6c42\u89e3\uff08\u9010\u6b65\u8bb2\u89e3\u8bad\u7ec3\u8bc4\u4f30\u4ee5\u53ca\u53ef\u89c6\u5316\u4ee3\u7801\u7684\u7f16\u5199\u8fc7\u7a0b\uff09\u3001\u5b8c\u6574\u4ee3\u7801\u3001\u7ed3\u679c\u5c55\u793a\u56fe\u7b49\u7ae0\u8282\u3002</li> <li>\u5728\u6587\u6863\u7684\u5f00\u59cb\uff0c\u9700\u8981\u6dfb\u52a0\u590d\u73b0\u7684\u8bba\u6587\u9898\u76ee\u3001\u8bba\u6587\u5730\u5740\u4ee5\u53ca\u53c2\u8003\u4ee3\u7801\u7684\u94fe\u63a5\uff0c\u540c\u65f6\u5efa\u8bae\u5bf9\u53c2\u8003\u4ee3\u7801\u7684\u4f5c\u8005\u8868\u793a\u611f\u8c22\u3002</li> <li>\u4ee3\u7801\u5c01\u88c5\u5f97\u5f53\uff0c\u6613\u8bfb\u6027\u597d\uff0c\u4e0d\u7528\u4e00\u4e9b\u968f\u610f\u7684\u53d8\u91cf/\u7c7b/\u51fd\u6570\u547d\u540d\u3002</li> <li>\u6ce8\u91ca\u6e05\u6670\uff0c\u4e0d\u4ec5\u8bf4\u660e\u505a\u4e86\u4ec0\u4e48\uff0c\u4e5f\u8981\u8bf4\u660e\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u505a\u3002</li> <li>\u5982\u679c\u6a21\u578b\u4f9d\u8d56 PaddlePaddle \u672a\u6db5\u76d6\u7684\u4f9d\u8d56\uff08\u5982 pandas\uff09\uff0c\u5219\u9700\u8981\u5728\u6587\u6863\u5f00\u5934\u5bf9\u8bf4\u660e\u9700\u5b89\u88c5\u54ea\u4e9b\u4f9d\u8d56\u9879\u3002</li> <li>\u968f\u673a\u63a7\u5236\uff0c\u9700\u8981\u5c3d\u91cf\u56fa\u5b9a\u542b\u6709\u968f\u673a\u56e0\u7d20\u6a21\u5757\u7684\u968f\u673a\u79cd\u5b50\uff0c\u4fdd\u8bc1\u6a21\u578b\u53ef\u4ee5\u6b63\u5e38\u590d\u73b0\uff08PaddleScience \u5957\u4ef6\u63d0\u4f9b\u4e86 <code>ppsci.utils.misc.set_random_seed(seed_num)</code> \u8bed\u53e5\u6765\u63a7\u5236\u5168\u5c40\u968f\u673a\u6570\uff09\u3002</li> <li>\u8d85\u53c2\u6570\uff1a\u6a21\u578b\u5185\u90e8\u8d85\u53c2\u6570\u7981\u6b62\u5199\u6b7b\uff0c\u5c3d\u91cf\u90fd\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u914d\u7f6e\u3002</li> <li>\u6587\u6863\u672b\u5c3e\u9644\u4e0a\u53c2\u8003\u8bba\u6587\u3001\u53c2\u8003\u4ee3\u7801\u7f51\u5740\u3001\u590d\u73b0\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u53c2\u6570\u4e0b\u8f7d\u94fe\u63a5\u3002 \u6574\u4f53\u6587\u6863\u64b0\u5199\u53ef\u4ee5\u53c2\u8003\uff1a\u6587\u6863\u53c2\u8003\u6837\u4f8b(darcy2d)\u3002</li> </ul>"},{"location":"zh/technical_doc/","title":"\u6280\u672f\u7a3f\u4ef6","text":""},{"location":"zh/technical_doc/#_1","title":"\u6280\u672f\u7a3f\u4ef6","text":"<p>\u8be6\u60c5\u8bf7\u67e5\u770b\u5fae\u4fe1\u516c\u4f17\u53f7 AI for Science</p>"},{"location":"zh/tutorials/","title":"\u5b66\u4e60\u8d44\u6599","text":""},{"location":"zh/tutorials/#_1","title":"\u5b66\u4e60\u8d44\u6599","text":""},{"location":"zh/tutorials/#_2","title":"\u6559\u7a0b\u8bfe\u4ef6","text":"<ul> <li>\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4e0e\u79d1\u5b66\u8ba1\u7b97</li> <li>\u98de\u6868AI for Science\u6d41\u4f53\u529b\u5b66\u516c\u5f00\u8bfe\u7b2c\u4e00\u671f</li> </ul>"},{"location":"zh/user_guide/","title":"\u4f7f\u7528\u6307\u5357","text":""},{"location":"zh/user_guide/#_1","title":"\u4f7f\u7528\u6307\u5357","text":"<p>\u672c\u6587\u6863\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 PaddleScience \u4e2d\u7684\u5e38\u7528\u57fa\u7840\u529f\u80fd\u548c\u8fdb\u9636\u529f\u80fd\uff0c\u57fa\u7840\u529f\u80fd\u5305\u62ec\u65ad\u70b9\u7ee7\u7eed\u8bad\u7ec3\u3001\u8fc1\u79fb\u5b66\u4e60\u3001\u6a21\u578b\u8bc4\u4f30\u3001\u6a21\u578b\u63a8\u7406\uff1b\u8fdb\u9636\u529f\u80fd\u5305\u62ec\u5206\u5e03\u5f0f\u8bad\u7ec3(\u6682\u65f6\u53ea\u652f\u6301\u6570\u636e\u5e76\u884c)\u3001\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3001\u68af\u5ea6\u7d2f\u52a0\u3002</p>"},{"location":"zh/user_guide/#1","title":"1. \u57fa\u7840\u529f\u80fd","text":""},{"location":"zh/user_guide/#11-yaml-hydra","title":"1.1 \u4f7f\u7528 YAML + hydra","text":"<p>PaddleScience \u63a8\u8350\u4f7f\u7528 YAML \u6587\u4ef6\u63a7\u5236\u7a0b\u5e8f\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u63a8\u7406\u7b49\u8fc7\u7a0b\u3002\u5176\u4e3b\u8981\u539f\u7406\u662f\u5229\u7528 hydra \u914d\u7f6e\u7ba1\u7406\u5de5\u5177\uff0c\u4ece <code>*.yaml</code> \u683c\u5f0f\u7684\u6587\u4ef6\u4e2d\u89e3\u6790\u914d\u7f6e\u53c2\u6570\uff0c\u5e76\u4f20\u9012\u7ed9\u8fd0\u884c\u4ee3\u7801\uff0c\u4ee5\u5bf9\u7a0b\u5e8f\u8fd0\u884c\u65f6\u6240\u4f7f\u7528\u7684\u8d85\u53c2\u6570\u7b49\u5b57\u6bb5\u8fdb\u884c\u7075\u6d3b\u914d\u7f6e\uff0c\u63d0\u9ad8\u5b9e\u9a8c\u6548\u7387\u3002\u672c\u7ae0\u8282\u4e3b\u8981\u4ecb\u7ecd hydra \u914d\u7f6e\u7ba1\u7406\u5de5\u5177\u7684\u57fa\u672c\u4f7f\u7528\u65b9\u6cd5\u3002</p> <p>\u5728\u4f7f\u7528 hydra \u914d\u7f6e\u8fd0\u884c\u53c2\u6570\u524d\uff0c\u8bf7\u5148\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5\u662f\u5426\u5df2\u5b89\u88c5 <code>hydra</code>\u3002</p> <pre><code>pip show hydra-core\n</code></pre> <p>\u5982\u672a\u5b89\u88c5\uff0c\u5219\u9700\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 <code>hydra</code>\u3002</p> <pre><code>pip install hydra-core\n</code></pre>"},{"location":"zh/user_guide/#111","title":"1.1.1 \u6253\u5370\u8fd0\u884c\u914d\u7f6e","text":"<p>Warning</p> <p>\u6ce8\u610f\u672c\u6559\u7a0b\u5185\u7684\u6253\u5370\u8fd0\u884c\u914d\u7f6e\u65b9\u6cd5\u53ea\u4f5c\u4e3a\u8c03\u8bd5\u4f7f\u7528\uff0chydra \u9ed8\u8ba4\u5728\u6253\u5370\u5b8c\u914d\u7f6e\u540e\u4f1a\u7acb\u5373\u7ed3\u675f\u7a0b\u5e8f\u3002\u56e0\u6b64\u5728\u6b63\u5e38\u8fd0\u884c\u7a0b\u5e8f\u65f6\u8bf7\u52ff\u52a0\u4e0a <code>-c job</code> \u53c2\u6570\u3002</p> <p>\u4ee5 bracket \u6848\u4f8b\u4e3a\u4f8b\uff0c\u5176\u6b63\u5e38\u8fd0\u884c\u547d\u4ee4\u4e3a\uff1a<code>python bracket.py</code>\u3002\u82e5\u5728\u5176\u8fd0\u884c\u547d\u4ee4\u672b\u5c3e\u52a0\u4e0a  <code>-c job</code>\uff0c\u5219\u53ef\u4ee5\u6253\u5370\u51fa\u4ece\u8fd0\u884c\u914d\u7f6e\u6587\u4ef6 <code>conf/bracket.yaml</code> \u4e2d\u89e3\u6790\u51fa\u7684\u914d\u7f6e\u53c2\u6570\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> $ python bracket.py -c job<pre><code>mode: train\nseed: 2023\noutput_dir: ${hydra:run.dir}\nlog_freq: 20\nNU: 0.3\nE: 100000000000.0\n...\n...\nEVAL:\n  pretrained_model_path: null\n  eval_during_train: true\n  eval_with_no_grad: true\n  batch_size:\n    sup_validator: 128\n</code></pre>"},{"location":"zh/user_guide/#112","title":"1.1.2 \u547d\u4ee4\u884c\u65b9\u5f0f\u914d\u7f6e\u53c2\u6570","text":"<p>\u4ecd\u7136\u4ee5\u914d\u7f6e\u6587\u4ef6 <code>bracket.yaml</code> \u4e3a\u4f8b\uff0c\u5173\u4e8e\u5b66\u4e60\u7387\u90e8\u5206\u7684\u53c2\u6570\u914d\u7f6e\u5982\u4e0b\u6240\u793a\u3002</p> bracket.yaml<pre><code>...\nTRAIN:\n  epochs: 2000\n  iters_per_epoch: 1000\n  save_freq: 20\n  eval_during_train: true\n  eval_freq: 20\n  lr_scheduler:\n    epochs: ${TRAIN.epochs} # (1)\n    iters_per_epoch: ${TRAIN.iters_per_epoch}\n    learning_rate: 0.001\n    gamma: 0.95\n    decay_steps: 15000\n    by_epoch: false\n...\n</code></pre> <ol> <li><code>${...}$</code> \u662f omegaconf \u7684\u5f15\u7528\u8bed\u6cd5\uff0c\u53ef\u4ee5\u5f15\u7528\u914d\u7f6e\u6587\u4ef6\u4e2d\u5176\u4ed6\u4f4d\u7f6e\u4e0a\u7684\u53c2\u6570\uff0c\u907f\u514d\u540c\u65f6\u7ef4\u62a4\u591a\u4e2a\u76f8\u540c\u8bed\u4e49\u7684\u53c2\u6570\u526f\u672c\uff0c\u5176\u6548\u679c\u4e0e yaml \u7684 anchor \u8bed\u6cd5\u7c7b\u4f3c\u3002</li> </ol> <p>\u53ef\u4ee5\u770b\u5230\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u5b66\u4e60\u7387\u4e3a <code>0.001</code>\uff0c\u82e5\u9700\u4fee\u6539\u5b66\u4e60\u7387\u4e3a <code>0.002</code> \u4ee5\u8fd0\u884c\u65b0\u7684\u5b9e\u9a8c\uff0c\u5219\u6709\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\uff1a</p> <ul> <li>\u5c06\u4e0a\u8ff0\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 <code>learning_rate: 0.001</code> \u6539\u4e3a <code>learning_rate: 0.002</code>\uff0c\u7136\u540e\u518d\u8fd0\u884c\u7a0b\u5e8f\u3002\u8fd9\u79cd\u65b9\u5f0f\u867d\u7136\u7b80\u5355\uff0c\u4f46\u5728\u5b9e\u9a8c\u8f83\u591a\u65f6\u5bb9\u6613\u9020\u6210\u5b9e\u9a8c\u6df7\u4e71\uff0c\u56e0\u6b64\u4e0d\u63a8\u8350\u4f7f\u7528\u3002</li> <li> <p>\u901a\u8fc7\u547d\u4ee4\u884c\u53c2\u6570\u7684\u65b9\u5f0f\u8fdb\u884c\u4fee\u6539\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>python bracket.py TRAIN.lr_scheduler.learning_rate=0.002\n</code></pre> <p>\u8fd9\u79cd\u65b9\u5f0f\u901a\u8fc7\u547d\u4ee4\u884c\u53c2\u6570\u4e34\u65f6\u91cd\u8f7d\u8fd0\u884c\u914d\u7f6e\uff0c\u800c\u4e0d\u4f1a\u5bf9 <code>bracket.yaml</code> \u6587\u4ef6\u672c\u8eab\u8fdb\u884c\u4fee\u6539\uff0c\u80fd\u7075\u6d3b\u5730\u63a7\u5236\u8fd0\u884c\u65f6\u7684\u914d\u7f6e\uff0c\u4fdd\u8bc1\u4e0d\u540c\u5b9e\u9a8c\u4e4b\u95f4\u4e92\u4e0d\u5e72\u6270\u3002</p> </li> </ul> <p>\u8bbe\u7f6e\u542b\u8f6c\u4e49\u5b57\u7b26\u7684\u53c2\u6570\u503c</p> <p>\u4ee5\u547d\u4ee4\u884c\u65b9\u5f0f\u8bbe\u7f6e\u53c2\u6570\u65f6\uff0c\u82e5\u53c2\u6570\u503c\u4e2d\u542b\u6709\u5c5e\u4e8e omegaconf escaping characters \u7684\u8f6c\u4e49\u5b57\u7b26(<code>\\\\</code>, <code>[</code>, <code>]</code>, <code>{</code>, <code>}</code>, <code>(</code>, <code>)</code>, <code>:</code>, <code>=</code>, <code>\\</code>)\uff0c\u5219\u63a8\u8350\u4f7f\u7528 <code>\\'</code> \u5c06\u53c2\u6570\u503c\u5305\u56f4\u8d77\u6765\uff0c\u4fdd\u8bc1\u5185\u90e8\u7684\u5b57\u7b26\u4e0d\u88ab\u8f6c\u4e49\uff0c\u5426\u5219\u53ef\u80fd\u5728 hydra \u89e3\u6790\u53c2\u6570\u65f6\u5f15\u8d77\u62a5\u9519\uff0c\u6216\u4ee5\u4e0d\u6b63\u786e\u7684\u65b9\u5f0f\u8fd0\u884c\u7a0b\u5e8f\u3002\u5047\u8bbe\u6211\u4eec\u5728\u8fd0\u884c\u65f6\u9700\u8981\u6307\u5b9a <code>PATH</code> \u4e3a <code>/workspace/lr=0.1,s=[3]/best_model.pdparams</code>\uff0c\u8be5\u8def\u5f84\u542b\u6709\u8f6c\u4e49\u5b57\u7b26 <code>[</code>, <code>]</code> \u548c <code>=</code>\uff0c\u56e0\u6b64\u5219\u53ef\u4ee5\u6309\u7167\u5982\u4e0b\u65b9\u5f0f\u64b0\u5199\u53c2\u6570\u3002</p> <pre><code># \u6b63\u786e\u7684\u53c2\u6570\u6307\u5b9a\u65b9\u5f0f\u5982\u4e0b\npython example.py PATH=\\'/workspace/lr=0.1,s=[3]/best_model.pdparams\\'\n\n# \u9519\u8bef\u7684\u53c2\u6570\u6307\u5b9a\u65b9\u5f0f\u5982\u4e0b\n# python example.py PATH=/workspace/lr=0.1,s=[3]/best_model.pdparams\n# python example.py PATH='/workspace/lr=0.1,s=[3]/best_model.pdparams'\n# python example.py PATH=\"/workspace/lr=0.1,s=[3]/best_model.pdparams\"\n</code></pre>"},{"location":"zh/user_guide/#113","title":"1.1.3 \u81ea\u52a8\u5316\u8fd0\u884c\u5b9e\u9a8c","text":"<p>\u5982 1.1.2 \u547d\u4ee4\u884c\u65b9\u5f0f\u914d\u7f6e\u53c2\u6570 \u6240\u8ff0\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728\u7a0b\u5e8f\u6267\u884c\u547d\u4ee4\u7684\u672b\u5c3e\u52a0\u4e0a\u5408\u9002\u7684\u53c2\u6570\u6765\u63a7\u5236\u591a\u7ec4\u5b9e\u9a8c\u7684\u8fd0\u884c\u914d\u7f6e\uff0c\u63a5\u4e0b\u6765\u4ee5\u81ea\u52a8\u5316\u6267\u884c\u56db\u7ec4\u5b9e\u9a8c\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u5229\u7528 hydra \u7684 multirun \u529f\u80fd\uff0c\u5b9e\u73b0\u8be5\u76ee\u7684\u3002</p> <p>\u5047\u8bbe\u8fd9\u56db\u7ec4\u5b9e\u9a8c\u56f4\u7ed5\u968f\u673a\u79cd\u5b50 <code>seed</code> \u548c\u8bad\u7ec3\u8f6e\u6570 <code>epochs</code> \u8fdb\u884c\u914d\u7f6e\uff0c\u7ec4\u5408\u5982\u4e0b\uff1a</p> \u5b9e\u9a8c\u7f16\u53f7 seed epochs 1 42 10 2 42 20 3 1024 10 4 1024 20 <p>\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u5373\u53ef\u6309\u987a\u5e8f\u81ea\u52a8\u8fd0\u884c\u8fd9 4 \u7ec4\u5b9e\u9a8c\u3002</p> $ python bracket.py -m seed=42,1024 TRAIN.epochs=10,20<pre><code>[HYDRA] Launching 4 jobs locally\n[HYDRA]        #0 : seed=42 TRAIN.epochs=10\n...\n[HYDRA]        #1 : seed=42 TRAIN.epochs=20\n...\n[HYDRA]        #2 : seed=1024 TRAIN.epochs=10\n...\n[HYDRA]        #3 : seed=1024 TRAIN.epochs=20\n...\n</code></pre> <p>\u591a\u7ec4\u5b9e\u9a8c\u5404\u81ea\u7684\u53c2\u6570\u6587\u4ef6\u3001\u65e5\u5fd7\u6587\u4ef6\u5219\u4fdd\u5b58\u5728\u4ee5\u4e0d\u540c\u53c2\u6570\u7ec4\u5408\u4e3a\u540d\u79f0\u7684\u5b50\u6587\u4ef6\u5939\u4e2d\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> $ tree PaddleScience/examples/bracket/outputs_bracket/<pre><code>PaddleScience/examples/bracket/outputs_bracket/\n\u2514\u2500\u2500 2023-10-14 # (1)\n    \u2514\u2500\u2500 04-01-52 # (2)\n        \u251c\u2500\u2500 TRAIN.epochs=10,20,seed=42,1024 # multirun \u603b\u914d\u7f6e\u4fdd\u5b58\u76ee\u5f55\n        \u2502   \u2514\u2500\u2500 multirun.yaml # multirun \u914d\u7f6e\u6587\u4ef6 (3)\n        \u251c\u2500\u2500 TRAIN.epochs=10,seed=1024 # \u5b9e\u9a8c\u7f16\u53f73\u7684\u4fdd\u5b58\u76ee\u5f55\n        \u2502   \u251c\u2500\u2500 checkpoints\n        \u2502   \u2502   \u251c\u2500\u2500 latest.pdeqn\n        \u2502   \u2502   \u251c\u2500\u2500 latest.pdopt\n        \u2502   \u2502   \u251c\u2500\u2500 latest.pdparams\n        \u2502   \u2502   \u2514\u2500\u2500 latest.pdstates\n        \u2502   \u251c\u2500\u2500 train.log\n        \u2502   \u2514\u2500\u2500 visual\n        \u2502       \u2514\u2500\u2500 epoch_0\n        \u2502           \u2514\u2500\u2500 result_u_v_w_sigmas.vtu\n        \u251c\u2500\u2500 TRAIN.epochs=10,seed=42 # \u5b9e\u9a8c\u7f16\u53f71\u7684\u4fdd\u5b58\u76ee\u5f55\n        \u2502   \u251c\u2500\u2500 checkpoints\n        \u2502   \u2502   \u251c\u2500\u2500 latest.pdeqn\n        \u2502   \u2502   \u251c\u2500\u2500 latest.pdopt\n        \u2502   \u2502   \u251c\u2500\u2500 latest.pdparams\n        \u2502   \u2502   \u2514\u2500\u2500 latest.pdstates\n        \u2502   \u251c\u2500\u2500 train.log\n        \u2502   \u2514\u2500\u2500 visual\n        \u2502       \u2514\u2500\u2500 epoch_0\n        \u2502           \u2514\u2500\u2500 result_u_v_w_sigmas.vtu\n        \u251c\u2500\u2500 TRAIN.epochs=20,seed=1024 # \u5b9e\u9a8c\u7f16\u53f74\u7684\u4fdd\u5b58\u76ee\u5f55\n        \u2502   \u251c\u2500\u2500 checkpoints\n        \u2502   \u2502   \u251c\u2500\u2500 latest.pdeqn\n        \u2502   \u2502   \u251c\u2500\u2500 latest.pdopt\n        \u2502   \u2502   \u251c\u2500\u2500 latest.pdparams\n        \u2502   \u2502   \u2514\u2500\u2500 latest.pdstates\n        \u2502   \u251c\u2500\u2500 train.log\n        \u2502   \u2514\u2500\u2500 visual\n        \u2502       \u2514\u2500\u2500 epoch_0\n        \u2502           \u2514\u2500\u2500 result_u_v_w_sigmas.vtu\n        \u2514\u2500\u2500 TRAIN.epochs=20,seed=42 # \u5b9e\u9a8c\u7f16\u53f72\u7684\u4fdd\u5b58\u76ee\u5f55\n            \u251c\u2500\u2500 checkpoints\n            \u2502   \u251c\u2500\u2500 latest.pdeqn\n            \u2502   \u251c\u2500\u2500 latest.pdopt\n            \u2502   \u251c\u2500\u2500 latest.pdparams\n            \u2502   \u2514\u2500\u2500 latest.pdstates\n            \u251c\u2500\u2500 train.log\n            \u2514\u2500\u2500 visual\n                \u2514\u2500\u2500 epoch_0\n                    \u2514\u2500\u2500 result_u_v_w_sigmas.vtu\n</code></pre> <ol> <li>\u8be5\u6587\u4ef6\u5939\u662f\u7a0b\u5e8f\u8fd0\u884c\u65f6\u6839\u636e\u65e5\u671f\u81ea\u52a8\u521b\u5efa\u5f97\u5230\uff0c\u6b64\u5904\u8868\u793a2023\u5e7410\u670814\u65e5</li> <li>\u8be5\u6587\u4ef6\u5939\u662f\u7a0b\u5e8f\u8fd0\u884c\u65f6\u6839\u636e\u8fd0\u884c\u65f6\u523b(\u4e16\u754c\u6807\u51c6\u65f6\u95f4,UTC)\u81ea\u52a8\u521b\u5efa\u5f97\u5230\uff0c\u6b64\u5904\u8868\u793a04\u70b901\u520652\u79d2</li> <li>\u8be5\u6587\u4ef6\u5939\u662f multirun \u6a21\u5f0f\u4e0b\u989d\u5916\u4ea7\u751f\u4e00\u4e2a\u603b\u914d\u7f6e\u76ee\u5f55\uff0c\u4e3b\u8981\u7528\u4e8e\u4fdd\u5b58 multirun.yaml\uff0c\u5176\u5185\u7684 <code>hydra.overrides.task</code> \u5b57\u6bb5\u8bb0\u5f55\u4e86\u7528\u4e8e\u7ec4\u5408\u51fa\u4e0d\u540c\u8fd0\u884c\u53c2\u6570\u7684\u539f\u59cb\u914d\u7f6e\u3002</li> </ol> <p>\u8003\u8651\u5230\u7528\u6237\u7684\u9605\u8bfb\u548c\u5b66\u4e60\u6210\u672c\uff0c\u672c\u7ae0\u8282\u53ea\u4ecb\u7ecd\u4e86\u5e38\u7528\u7684\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u66f4\u591a\u8fdb\u9636\u7528\u6cd5\u8bf7\u53c2\u8003 hydra\u5b98\u65b9\u6559\u7a0b\u3002</p>"},{"location":"zh/user_guide/#12","title":"1.2 \u6a21\u578b\u5bfc\u51fa","text":""},{"location":"zh/user_guide/#121-paddle","title":"1.2.1 Paddle \u63a8\u7406\u6a21\u578b\u5bfc\u51fa","text":"<p>Warning</p> <p>\u5c11\u6570\u6848\u4f8b\u5c1a\u672a\u652f\u6301\u5bfc\u51fa\u529f\u80fd\uff0c\u56e0\u6b64\u5bf9\u5e94\u6587\u6863\u4e2d\u672a\u7ed9\u51fa\u5bfc\u51fa\u547d\u4ee4\u3002</p> <p>\u5728\u8bad\u7ec3\u5b8c\u6bd5\u540e\uff0c\u6211\u4eec\u901a\u5e38\u9700\u8981\u5c06\u6a21\u578b\u5bfc\u51fa\u4e3a <code>*.pdmodel</code>, <code>*.pdiparams</code>, <code>*.pdiparams.info</code> \u4e09\u4e2a\u6587\u4ef6\uff0c\u4ee5\u4fbf\u540e\u7eed\u63a8\u7406\u90e8\u7f72\u4f7f\u7528\u3002\u4ee5 Aneurysm \u6848\u4f8b\u4e3a\u4f8b\uff0c\u5bfc\u51fa\u6a21\u578b\u7684\u901a\u7528\u547d\u4ee4\u5982\u4e0b\u3002</p> <pre><code>python aneurysm.py mode=export \\\n    INFER.pretrained_model_path=\"https://paddle-org.bj.bcebos.com/paddlescience/models/aneurysm/aneurysm_pretrained.pdparams\"\n</code></pre> <p>Tip</p> <p>\u7531\u4e8e\u652f\u6301\u6a21\u578b\u5bfc\u51fa\u7684\u6848\u4f8b\u7684 YAML \u6587\u4ef6\u5df2\u7ecf\u5c06 <code>INFER.pretrained_model_path</code> \u7684\u9ed8\u8ba4\u503c\u8bbe\u7f6e\u4e3a\u5b98\u65b9\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5730\u5740\uff0c\u56e0\u6b64\u5bfc\u51fa\u5b98\u65b9\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u53ef\u4ee5\u5728\u547d\u4ee4\u884c\u4e2d\u7701\u7565 <code>INFER.pretrained_model_path=...</code> \u53c2\u6570\u3002</p> <p>\u6839\u636e\u7ec8\u7aef\u8f93\u51fa\u4fe1\u606f\uff0c\u5bfc\u51fa\u7684\u6a21\u578b\u4f1a\u88ab\u4fdd\u5b58\u5728\u6267\u884c\u5bfc\u51fa\u547d\u4ee4\u6240\u5728\u76ee\u5f55\u7684\u76f8\u5bf9\u8def\u5f84\uff1a<code>./inference/</code> \u6587\u4ef6\u5939\u4e0b\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>...\nppsci MESSAGE: Inference model has been exported to: ./inference/aneurysm, including *.pdmodel, *.pdiparams and *.pdiparams.info files.\n</code></pre> <pre><code>./inference/\n\u251c\u2500\u2500 aneurysm.pdiparams\n\u251c\u2500\u2500 aneurysm.pdiparams.info\n\u2514\u2500\u2500 aneurysm.pdmodel\n</code></pre>"},{"location":"zh/user_guide/#122-onnx","title":"1.2.2 ONNX \u63a8\u7406\u6a21\u578b\u5bfc\u51fa","text":"<p>\u5728\u5bfc\u51fa ONNX \u63a8\u7406\u6a21\u578b\u524d\uff0c\u9700\u8981\u5b8c\u6210 1.2.1 Paddle \u63a8\u7406\u6a21\u578b\u5bfc\u51fa \u7684\u6b65\u9aa4\uff0c\u5f97\u5230<code>inference/aneurysm.pdiparams</code>\u548c<code>inference/aneurysm.pdmodel</code>\u3002</p> <p>\u7136\u540e\u5b89\u88c5 paddle2onnx\u3002</p> <pre><code>pip install paddle2onnx\n</code></pre> <p>\u63a5\u4e0b\u6765\u4ecd\u7136\u4ee5 aneurysm \u6848\u4f8b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u547d\u4ee4\u884c\u76f4\u63a5\u5bfc\u51fa\u548c PaddleScience \u5bfc\u51fa\u4e24\u79cd\u65b9\u5f0f\u3002</p> \u547d\u4ee4\u884c\u5bfc\u51faPaddleScience \u5bfc\u51fa <pre><code>paddle2onnx \\\n    --model_dir=./inference/ \\\n    --model_filename=aneurysm.pdmodel \\\n    --params_filename=aneurysm.pdiparams \\\n    --save_file=./inference/aneurysm.onnx \\\n    --opset_version=13 \\\n    --enable_onnx_checker=True\n</code></pre> <p>\u82e5\u5bfc\u51fa\u6210\u529f\uff0c\u8f93\u51fa\u4fe1\u606f\u5982\u4e0b\u6240\u793a</p> <pre><code>[Paddle2ONNX] Start to parse PaddlePaddle model...\n[Paddle2ONNX] Model file path: ./inference/aneurysm.pdmodel\n[Paddle2ONNX] Paramters file path: ./inference/aneurysm.pdiparams\n[Paddle2ONNX] Start to parsing Paddle model...\n[Paddle2ONNX] Use opset_version = 13 for ONNX export.\n[Paddle2ONNX] PaddlePaddle model is exported as ONNX format now.\n2024-03-02 05:45:12 [INFO]      ===============Make PaddlePaddle Better!================\n2024-03-02 05:45:12 [INFO]      A little survey: https://iwenjuan.baidu.com/?code=r8hu2s\n</code></pre> <p>\u5728 aneurysm.py \u4e2d\u7684<code>export</code>\u51fd\u6570\u4e2d\uff0c\u5c06<code>with_onnx</code>\u53c2\u6570\u6539\u4e3a<code>True</code>\uff0c</p> <pre><code>def export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path, with_onnx=True)\n</code></pre> <p>\u7136\u540e\u6267\u884c\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u3002</p> <pre><code>python aneurysm.py mode=export\n</code></pre> <p>\u82e5\u5bfc\u51fa\u6210\u529f\uff0c\u8f93\u51fa\u4fe1\u606f\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>...\n[Paddle2ONNX] Start to parse PaddlePaddle model...\n[Paddle2ONNX] Model file path: ./inference/aneurysm.pdmodel\n[Paddle2ONNX] Paramters file path: ./inference/aneurysm.pdiparams\n[Paddle2ONNX] Start to parsing Paddle model...\n[Paddle2ONNX] Use opset_version = 13 for ONNX export.\n[Paddle2ONNX] PaddlePaddle model is exported as ONNX format now.\nppsci MESSAGE: ONNX model has been exported to: ./inference/aneurysm.onnx\n</code></pre>"},{"location":"zh/user_guide/#13","title":"1.3 \u6a21\u578b\u63a8\u7406\u9884\u6d4b","text":""},{"location":"zh/user_guide/#131","title":"1.3.1 \u52a8\u6001\u56fe\u63a8\u7406","text":"<p>\u82e5\u9700\u4f7f\u7528\u8bad\u7ec3\u5b8c\u6bd5\u4fdd\u5b58\u6216\u4e0b\u8f7d\u5f97\u5230\u7684\u6a21\u578b\u6587\u4ef6 <code>*.pdparams</code> \u76f4\u63a5\u8fdb\u884c\u63a8\u7406\uff08\u9884\u6d4b\uff09\uff0c\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u4ee3\u7801\u793a\u4f8b\u3002</p> <ol> <li> <p>\u52a0\u8f7d <code>*.pdparams</code> \u6587\u4ef6\u5185\u7684\u53c2\u6570\u5230\u6a21\u578b\u4e2d</p> <pre><code>import ppsci\nimport numpy as np\n\n# \u5b9e\u4f8b\u5316\u4e00\u4e2a\u8f93\u5165\u4e3a (x,y,z) \u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5750\u6807\uff0c\u8f93\u51fa\u4e3a (u,v,w) \u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u901f\u5ea6\u7684 model\nmodel = ppsci.arch.MLP((\"x\", \"y\", \"z\"), (\"u\", \"v\", \"w\"), 5, 64, \"tanh\")\n\n# \u7528\u8be5\u6a21\u578b\u53ca\u5176\u5bf9\u5e94\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8def\u5f84(\u6216\u4e0b\u8f7d\u5730\u5740 url)\u4e24\u4e2a\u53c2\u6570\u521d\u59cb\u5316 solver\nsolver = ppsci.solver.Solver(\n    model=model,\n    pretrained_model_path=\"/path/to/pretrained.pdparams\",\n)\n# \u5728 Solver(...) \u4e2d\u4f1a\u81ea\u52a8\u4ece\u7ed9\u5b9a\u7684 pretrained_model_path \u52a0\u8f7d(\u4e0b\u8f7d)\u53c2\u6570\u5e76\u8d4b\u503c\u7ed9 model \u7684\u5bf9\u5e94\u53c2\u6570\n</code></pre> </li> <li> <p>\u51c6\u5907\u597d\u7528\u4e8e\u9884\u6d4b\u7684\u8f93\u5165\u6570\u636e\uff0c\u5e76\u4ee5\u5b57\u5178 <code>dict</code> \u7684\u65b9\u5f0f\u4f20\u9012\u7ed9 <code>solver.predict</code>\u3002</p> <pre><code>N = 100 # \u5047\u8bbe\u8981\u9884\u6d4b100\u4e2a\u6837\u672c\u7684\u7ed3\u679c\nx = np.random.randn(N, 1) # \u8f93\u5165\u6570\u636ex\ny = np.random.randn(N, 1) # \u8f93\u5165\u6570\u636ey\nz = np.random.randn(N, 1) # \u8f93\u5165\u6570\u636ez\n\ninput_dict = {\n    \"x\": x,\n    \"y\": y,\n    \"z\": z,\n}\n\noutput_dict = solver.predict(\n    input_dict,\n    batch_size=32, # \u63a8\u7406\u65f6\u7684 batch_size\n    return_numpy=True, # \u8fd4\u56de\u7ed3\u679c\u662f\u5426\u8f6c\u6362\u4e3a numpy\n)\n\n# output_dict \u9884\u6d4b\u7ed3\u679c\u540c\u6837\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u4fdd\u5b58\u5728 output_dict \u4e2d\uff0c\u5176\u5177\u4f53\u5185\u5bb9\u5982\u4e0b\nfor k, v in output_dict.items():\n    print(f\"{k} {v.shape}\")\n# \"u\": (100, 1)\n# \"v\": (100, 1)\n# \"w\": (100, 1)\n</code></pre> </li> </ol>"},{"location":"zh/user_guide/#132-inference-python","title":"1.3.2 Inference \u63a8\u7406(python)","text":"<p>Paddle Inference \u662f\u98de\u6868\u7684\u539f\u751f\u63a8\u7406\u5e93\uff0c\u76f8\u6bd4 1.3.1 \u52a8\u6001\u56fe\u63a8\u7406 \u5177\u6709\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u9002\u5408\u4e0d\u540c\u5e73\u53f0\u4e0d\u540c\u5e94\u7528\u573a\u666f\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u8be6\u7ec6\u4fe1\u606f\u53ef\u53c2\u8003: Paddle Inference \u6587\u6863\u3002</p> <p>Warning</p> <p>\u5c11\u6570\u6848\u4f8b\u5c1a\u672a\u652f\u6301\u5bfc\u51fa\u3001\u63a8\u7406\u529f\u80fd\uff0c\u56e0\u6b64\u5bf9\u5e94\u6587\u6863\u4e2d\u672a\u7ed9\u51fa\u5bfc\u51fa\u3001\u63a8\u7406\u547d\u4ee4\u3002</p> <p>\u9996\u5148\u9700\u53c2\u8003 1.2 \u6a21\u578b\u5bfc\u51fa \u7ae0\u8282\uff0c\u4ece <code>*.pdparams</code> \u6587\u4ef6\u5bfc\u51fa <code>*.pdmodel</code>, <code>*.pdiparams</code> \u4e24\u4e2a\u6587\u4ef6\u3002</p> <p>\u4ee5 Aneurysm \u6848\u4f8b\u4e3a\u4f8b\uff0c\u5047\u8bbe\u5bfc\u51fa\u540e\u7684\u6a21\u578b\u6587\u4ef6\u4ee5 <code>./inference/aneurysm.*</code> \u7684\u5f62\u5f0f\u4fdd\u5b58\uff0c\u5219\u63a8\u7406\u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\u3002</p> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar -o aneurysm_dataset.tar\n# unzip it\ntar -xvf aneurysm_dataset.tar\npython aneurysm.py mode=infer\n</code></pre> <p>\u8f93\u51fa\u4fe1\u606f\u5982\u4e0b\uff1a</p> <pre><code>...\n...\nppsci INFO: Predicting batch 2880/2894\nppsci INFO: Predicting batch 2894/2894\nppsci MESSAGE: Visualization result is saved to: ./aneurysm_pred.vtu\n</code></pre>"},{"location":"zh/user_guide/#133","title":"1.3.3 \u4f7f\u7528\u4e0d\u540c\u7684\u63a8\u7406\u914d\u7f6e","text":"<p>PaddleScience \u63d0\u4f9b\u4e86\u591a\u79cd\u63a8\u7406\u914d\u7f6e\u7ec4\u5408\uff0c\u53ef\u901a\u8fc7\u547d\u4ee4\u884c\u8fdb\u884c\u7ec4\u5408\uff0c\u76ee\u524d\u652f\u6301\u7684\u63a8\u7406\u914d\u7f6e\u5982\u4e0b\uff1a</p> Native ONNX TensorRT MKLDNN CPU \u2705 \u2705 / \u2705 GPU \u2705 \u2705 \u2705 / XPU TODO / / / <p>\u63a5\u4e0b\u6765\u4ee5 aneurysm \u6848\u4f8b\u548c Linux x86_64 + TensorRT 8.6 GA + CUDA 11.6 \u8f6f\u786c\u4ef6\u73af\u5883\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528\u4e0d\u540c\u7684\u63a8\u7406\u914d\u7f6e\u3002</p> \u4f7f\u7528 Paddle \u539f\u751f\u63a8\u7406\u4f7f\u7528 TensorRT \u63a8\u7406\u4f7f\u7528 ONNX \u63a8\u7406\u4f7f\u7528 MKLDNN \u63a8\u7406 <p>Paddle \u63d0\u4f9b\u4e86\u539f\u751f\u63a8\u7406\u529f\u80fd\uff0c\u652f\u6301 CPU \u548c GPU\u3002</p> <p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u63a8\u7406\uff1a</p> <pre><code># CPU\npython aneurysm.py mode=infer \\\n    INFER.device=cpu \\\n    INFER.engine=native\n\n# GPU\npython aneurysm.py mode=infer \\\n    INFER.device=gpu \\\n    INFER.engine=native\n</code></pre> <p>TensorRT \u662f\u82f1\u4f1f\u8fbe\u63a8\u51fa\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u5f15\u64ce\uff0c\u9002\u7528\u4e8e GPU \u63a8\u7406\u52a0\u901f\uff0cPaddleScience \u652f\u6301\u4e86 TensorRT \u63a8\u7406\u529f\u80fd\u3002</p> <ol> <li> <p>\u6839\u636e\u4f60\u7684\u8f6f\u786c\u4ef6\u73af\u5883\uff0c\u4e0b\u8f7d\u5e76\u89e3\u538b\u5bf9\u5e94\u7684 TensorRT \u63a8\u7406\u5e93\u538b\u7f29\u5305(.tar \u6587\u4ef6)\uff1ahttps://developer.nvidia.com/tensorrt#\u3002 \u63a8\u8350\u4f7f\u7528 TensorRT 8.x\u30017.x \u7b49\u8f83\u65b0\u7684\u7248\u672c\u3002</p> </li> <li> <p>\u5728\u89e3\u538b\u5b8c\u6bd5\u7684\u6587\u4ef6\u4e2d\uff0c\u627e\u5230 <code>libnvinfer.so</code> \u6587\u4ef6\u6240\u5728\u7684\u76ee\u5f55\uff0c\u5c06\u5176\u52a0\u5165\u5230 <code>LD_LIBRARY_PATH</code> \u73af\u5883\u53d8\u91cf\u4e2d\u3002</p> <pre><code>TRT_PATH=/PATH/TO/TensorRT-8.6.1.6\nfind $TRT_PATH -name libnvinfer.so\n\n# /PATH/TO/TensorRT-8.6.1.6/targets/x86_64-linux-gnu/lib/libnvinfer.so   &lt;---- use this path\nexport LD_LIBRARY_PATH=/PATH/TO/TensorRT-8.6.1.6/targets/x86_64-linux-gnu/lib/:$LD_LIBRARY_PATH\n</code></pre> </li> <li> <p>\u8fd0\u884c <code>aneurysm.py</code> \u7684\u63a8\u7406\u529f\u80fd\uff0c\u540c\u65f6\u6307\u5b9a\u63a8\u7406\u5f15\u64ce\u4e3a TensorRT\u3002</p> <pre><code># \u8fd0\u884c\u524d\u9700\u8bbe\u7f6e\u6307\u5b9aGPU\uff0c\u5426\u5219\u53ef\u80fd\u65e0\u6cd5\u542f\u52a8 TensorRT\nexport CUDA_VISIBLE_DEVICES=0\n\npython aneurysm.py mode=infer \\\n    INFER.device=gpu \\\n    INFER.engine=tensorrt \\\n    INFER.min_subgraph_size=5\n</code></pre> </li> </ol> <p>ONNX \u662f\u5fae\u8f6f\u5f00\u6e90\u7684\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u6846\u67b6\uff0cPaddleScience \u652f\u6301\u4e86 ONNX \u63a8\u7406\u529f\u80fd\u3002</p> <p>\u9996\u5148\u6309\u7167 1.2.2 ONNX \u63a8\u7406\u6a21\u578b\u5bfc\u51fa \u7ae0\u8282\u5c06 <code>*.pdmodel</code> \u548c <code>*.pdiparams</code> \u8f6c\u6362\u4e3a <code>*.onnx</code> \u6587\u4ef6\uff0c \u7136\u540e\u6839\u636e\u786c\u4ef6\u73af\u5883\uff0c\u5b89\u88c5 CPU \u6216 GPU \u7248\u7684 onnxruntime\uff1a</p> <pre><code>pip install onnxruntime  # CPU\npip install onnxruntime-gpu  # GPU\n</code></pre> <p>\u6700\u540e\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u63a8\u7406\uff1a</p> <pre><code># CPU\npython aneurysm.py mode=infer \\\n    INFER.device=cpu \\\n    INFER.engine=onnx\n\n# GPU\npython aneurysm.py mode=infer \\\n    INFER.device=gpu \\\n    INFER.engine=onnx\n</code></pre> <p>MKLDNN \u662f\u82f1\u4f1f\u8fbe\u63a8\u51fa\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u5f15\u64ce\uff0c\u9002\u7528\u4e8e CPU \u63a8\u7406\u52a0\u901f\uff0cPaddleScience \u652f\u6301\u4e86 MKLDNN \u63a8\u7406\u529f\u80fd\u3002</p> <p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u63a8\u7406\uff1a</p> <pre><code>python aneurysm.py mode=infer \\\n    INFER.device=cpu \\\n    INFER.engine=mkldnn\n</code></pre> <p>\u5b8c\u6574\u63a8\u7406\u914d\u7f6e\u53c2\u6570</p> \u53c2\u6570 \u9ed8\u8ba4\u503c \u8bf4\u660e <code>INFER.device</code> <code>cpu</code> \u63a8\u7406\u8bbe\u5907\uff0c\u76ee\u524d\u652f\u6301 <code>cpu</code> \u548c <code>gpu</code> <code>INFER.engine</code> <code>native</code> \u63a8\u7406\u5f15\u64ce\uff0c\u76ee\u524d\u652f\u6301 <code>native</code>, <code>tensorrt</code>, <code>onnx</code> \u548c <code>mkldnn</code> <code>INFER.precision</code> <code>fp32</code> \u63a8\u7406\u7cbe\u5ea6\uff0c\u76ee\u524d\u652f\u6301 <code>fp32</code>, <code>fp16</code> <code>INFER.ir_optim</code> <code>True</code> \u662f\u5426\u542f\u7528 IR \u4f18\u5316 <code>INFER.min_subgraph_size</code> <code>30</code> TensorRT \u4e2d\u6700\u5c0f\u5b50\u56fe size\uff0c\u5f53\u5b50\u56fe\u7684 size \u5927\u4e8e\u8be5\u503c\u65f6\uff0c\u624d\u4f1a\u5c1d\u8bd5\u5bf9\u8be5\u5b50\u56fe\u4f7f\u7528 TensorRT \u8ba1\u7b97 <code>INFER.gpu_mem</code> <code>2000</code> \u521d\u59cb\u663e\u5b58\u5927\u5c0f <code>INFER.gpu_id</code> <code>0</code> GPU \u903b\u8f91\u8bbe\u5907\u53f7 <code>INFER.max_batch_size</code> <code>1024</code> \u63a8\u7406\u65f6\u7684\u6700\u5927 batch_size <code>INFER.num_cpu_threads</code> <code>10</code> MKLDNN \u548c ONNX \u5728 CPU \u63a8\u7406\u65f6\u7684\u7ebf\u7a0b\u6570 <code>INFER.batch_size</code> <code>256</code> \u63a8\u7406\u65f6\u7684 batch_size"},{"location":"zh/user_guide/#14","title":"1.4 \u65ad\u70b9\u7ee7\u7eed\u8bad\u7ec3","text":"<p>\u5728\u6a21\u578b\u7684\u65e5\u5e38\u8bad\u7ec3\u4e2d\uff0c\u53ef\u80fd\u5b58\u5728\u673a\u5668\u6545\u969c\u6216\u8005\u7528\u6237\u624b\u52a8\u64cd\u4f5c\u800c\u4e2d\u65ad\u8bad\u7ec3\u7684\u60c5\u51b5\uff0c\u9488\u5bf9\u8fd9\u79cd\u60c5\u51b5 PaddleScience \u63d0\u4f9b\u4e86\u65ad\u70b9\u7ee7\u7eed\u8bad\u7ec3\u7684\u529f\u80fd\uff0c\u5373\u5728\u8bad\u7ec3\u65f6\u9ed8\u8ba4\u4f1a\u4fdd\u5b58\u6700\u8fd1\u4e00\u4e2a\u8bad\u7ec3\u5b8c\u6bd5\u7684 epoch \u5bf9\u5e94\u7684\u5404\u79cd\u53c2\u6570\u5230\u4ee5\u4e0b 5 \u4e2a\u6587\u4ef6\u4e2d\uff1a</p> <ol> <li><code>latest.pdparams</code>\uff0c\u8be5\u6587\u4ef6\u4fdd\u5b58\u4e86\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u6240\u6709\u6743\u91cd\u53c2\u6570\u3002</li> <li><code>latest.pdopt</code>\uff0c\u8be5\u6587\u4ef6\u4fdd\u5b58\u4e86\u4f18\u5316\u5668\uff08\u5982 Adam \u7b49\u4e00\u4e9b\u5e26\u6709\u52a8\u91cf\u8bb0\u5f55\u529f\u80fd\u7684\u4f18\u5316\u5668\uff09\u7684\u6240\u6709\u53c2\u6570\u3002</li> <li><code>latest.pdeqn</code>\uff0c\u8be5\u6587\u4ef6\u4fdd\u5b58\u4e86\u6240\u6709\u65b9\u7a0b\u7684\u53c2\u6570\uff0c\u5728\u4e00\u4e9b\u9006\u95ee\u9898\u4e2d\u5982\u679c\u65b9\u7a0b\u672c\u8eab\u542b\u6709\u5f85\u4f30\u8ba1\uff08\u53ef\u5b66\u4e60\uff09\u7684\u53c2\u6570\uff0c\u90a3\u4e48\u8be5\u6587\u4ef6\u5c31\u4f1a\u4fdd\u5b58\u8fd9\u4e9b\u53c2\u6570\u3002</li> <li><code>latest.pdstates</code>\uff0c\u8be5\u6587\u4ef6\u4fdd\u5b58\u4e86 latest \u5bf9\u5e94 epoch \u7684\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4ee5\u53ca epoch \u6570\u3002</li> <li><code>latest.pdscaler</code>\uff08\u53ef\u9009\uff09\uff0c\u5728\u5f00\u542f\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff08AMP\uff09\u529f\u80fd\u65f6\uff0c\u8be5\u6587\u4ef6\u4fdd\u5b58\u4e86 <code>GradScaler</code> \u68af\u5ea6\u7f29\u653e\u5668\u5185\u90e8\u7684\u53c2\u6570\u3002</li> </ol> \u65b9\u5f0f1: \u547d\u4ee4\u884c\u6307\u5b9a[\u63a8\u8350]\u65b9\u5f0f2: \u4fee\u6539\u4ee3\u7801 <p>\u82e5\u5728\u6848\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4e3a <code>Solver</code> \u6784\u5efa\u65f6\u4f20\u9012\u4e86 <code>cfg</code> \u53c2\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a <code>TRAIN.checkpoint_path</code> \u4e3a <code>latest.*</code> \u7684\u6240\u5728\u8def\u5f84\uff08\u5efa\u8bae\u7528<code>\\'</code>\u5305\u88f9\uff09\uff0c\u518d\u6267\u884c\u8bad\u7ec3\u547d\u4ee4\u5373\u53ef\uff0c\u514d\u53bb\u4fee\u6539\u6848\u4f8b\u4ee3\u7801\u3002</p> <pre><code>python example.py TRAIN.checkpoint_path=\\'/path/to/latest\\'\n</code></pre> <p>\u56e0\u6b64\u6211\u4eec\u53ea\u9700\u8981\u5728 <code>Solver</code> \u65f6\u6307\u5b9a <code>checkpoint_path</code> \u53c2\u6570\u4e3a <code>latest.*</code> \u7684\u6240\u5728\u8def\u5f84\uff0c\u5373\u53ef\u81ea\u52a8\u8f7d\u5165\u4e0a\u8ff0\u7684\u51e0\u4e2a\u6587\u4ef6\uff0c\u5e76\u4ece <code>latest</code> \u4e2d\u8bb0\u5f55\u7684 epoch \u5f00\u59cb\u7ee7\u7eed\u8bad\u7ec3\u3002</p> <pre><code>import ppsci\n\n...\n\nsolver = ppsci.solver.Solver(\n    ...,\n    checkpoint_path=\"/path/to/latest\"\n)\n</code></pre> <p>\u8def\u5f84\u586b\u5199\u6ce8\u610f\u4e8b\u9879</p> <p>\u6b64\u5904\u53ea\u9700\u5c06\u8def\u5f84\u586b\u5199\u5230 \"latest\" \u4e3a\u6b62\u5373\u53ef\uff0c\u4e0d\u9700\u8981\u52a0\u4e0a\u5176\u540e\u7f00\uff0c\u7a0b\u5e8f\u4f1a\u6839\u636e \"/path/to/latest\"\uff0c\u81ea\u52a8\u8865\u5145\u4e0d\u540c\u6587\u4ef6\u5bf9\u5e94\u7684\u540e\u7f00\u540d\u6765\u52a0\u8f7d <code>latest.pdparams</code>\u3001<code>latest.pdopt</code> \u7b49\u6587\u4ef6\u3002</p>"},{"location":"zh/user_guide/#15","title":"1.5 \u8fc1\u79fb\u5b66\u4e60","text":"<p>\u8fc1\u79fb\u5b66\u4e60\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u3001\u4f4e\u6210\u672c\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u7684\u8bad\u7ec3\u65b9\u5f0f\u3002\u5728 PaddleScience \u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728 <code>model</code> \u5b9e\u4f8b\u5316\u5b8c\u6bd5\u4e4b\u540e\uff0c\u624b\u52a8\u4e3a\u5176\u8f7d\u5165\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u540e\u5f00\u59cb\u5fae\u8c03\u8bad\u7ec3\uff1b\u4e5f\u53ef\u4ee5\u8c03\u7528 <code>Solver.finetune</code> \u63a5\u53e3\u5e76\u6307\u5b9a <code>pretrained_model_path</code> \u53c2\u6570\uff0c\u81ea\u52a8\u8f7d\u5165\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u5e76\u5f00\u59cb\u5fae\u8c03\u8bad\u7ec3\u3002</p> \u65b9\u5f0f1: \u547d\u4ee4\u884c\u6307\u5b9a[\u63a8\u8350]\u65b9\u5f0f2: \u624b\u52a8\u8f7d\u5165\u9884\u8bad\u7ec3\u6a21\u578b\u65b9\u5f0f3: \u8c03\u7528 <code>Solver.finetune</code> \u63a5\u53e3 <p>\u82e5\u5728\u6848\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4e3a <code>Solver</code> \u6784\u5efa\u65f6\u4f20\u9012\u4e86 <code>cfg</code> \u53c2\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a <code>TRAIN.pretrained_model_path</code> \u4e3a\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u6240\u5728\u8def\u5f84\uff08\u5efa\u8bae\u7528<code>\\'</code>\u5305\u88f9\uff09\uff0c\u518d\u6267\u884c\u8bad\u7ec3\u547d\u4ee4\u5373\u53ef\uff0c\u514d\u53bb\u4fee\u6539\u6848\u4f8b\u4ee3\u7801\u3002</p> <pre><code>python example.py TRAIN.pretrained_model_path=\\'/path/to/pretrain\\'\n</code></pre> <pre><code>import ppsci\nfrom ppsci.utils import save_load\n\n...\n...\n\nmodel = ...\nsave_load.load_pretrain(model, \"/path/to/pretrain\")\nsolver = ppsci.solver.Solver(\n    ...,\n)\nsolver.train()\n</code></pre> <pre><code>import ppsci\n\n\n...\n...\n\nmodel = ...\nsolver = ppsci.solver.Solver(\n    ...,\n)\nsolver.finetune(pretrained_model_path=\"/path/to/pretrain\")\n</code></pre> <p>\u8fc1\u79fb\u5b66\u4e60\u5efa\u8bae</p> <p>\u5728\u8fc1\u79fb\u5b66\u4e60\u65f6\uff0c\u76f8\u5bf9\u4e8e\u5b8c\u5168\u968f\u673a\u521d\u59cb\u5316\u7684\u53c2\u6570\u800c\u8a00\uff0c\u8f7d\u5165\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u53c2\u6570\u662f\u4e00\u4e2a\u8f83\u597d\u7684\u521d\u59cb\u5316\u72b6\u6001\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u4f7f\u7528\u592a\u5927\u7684\u5b66\u4e60\u7387\uff0c\u800c\u53ef\u4ee5\u5c06\u5b66\u4e60\u7387\u9002\u5f53\u8c03\u5c0f 2~10 \u500d\u4ee5\u83b7\u5f97\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u8fc7\u7a0b\u548c\u66f4\u597d\u7684\u7cbe\u5ea6\u3002</p>"},{"location":"zh/user_guide/#16","title":"1.6 \u6a21\u578b\u8bc4\u4f30","text":"<p>\u5f53\u6a21\u578b\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u5982\u679c\u60f3\u624b\u52a8\u5bf9\u67d0\u4e00\u4e2a\u6a21\u578b\u6743\u91cd\u6587\u4ef6\uff0c\u8bc4\u4f30\u5176\u5728\u6570\u636e\u96c6\u4e0a\u7684\u7cbe\u5ea6\uff0c\u5219\u53ef\u4ee5\u9009\u62e9\u4e0b\u9762\u7684\u51e0\u79cd\u65b9\u5f0f\u4e4b\u4e00\u8fdb\u884c\u8bc4\u4f30\u3002</p> \u65b9\u5f0f1: \u547d\u4ee4\u884c\u6307\u5b9a[\u63a8\u8350]\u65b9\u5f0f2: \u4fee\u6539\u4ee3\u7801 <p>\u82e5\u5728\u6848\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4e3a <code>Solver</code> \u6784\u5efa\u65f6\u4f20\u9012\u4e86 <code>cfg</code> \u53c2\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a <code>EVAL.pretrained_model_path</code> \u4e3a\u5f85\u8bc4\u4f30\u6a21\u578b\u6743\u91cd\u7684\u6240\u5728\u8def\u5f84\uff08\u5efa\u8bae\u7528<code>\\'</code>\u5305\u88f9\uff09\uff0c\u5e76\u6307\u5b9a\u6a21\u5f0f\u4e3a<code>eval</code>\u540e\uff0c\u6267\u884c\u8bc4\u4f30\u547d\u4ee4\u5373\u53ef\uff0c\u514d\u53bb\u4fee\u6539\u6848\u4f8b\u4ee3\u7801\u3002</p> <pre><code>python example.py mode=eval EVAL.pretrained_model_path=\\'/path/to/pretrain\\'\n</code></pre> <p>\u5728 <code>Solver</code> \u5b9e\u4f8b\u5316\u65f6\u6307\u5b9a\u53c2\u6570 <code>pretrained_model_path</code> \u4e3a\u8be5\u6743\u91cd\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u7136\u540e\u8c03\u7528 <code>Solver.eval()</code> \u5373\u53ef\u3002</p> <pre><code>import ppsci\nimport ppsci.utils\n\n...\n...\n\nsolver = ppsci.solver.Solver(\n    ...,\n    ...,\n    pretrained_model_path=\"/path/to/model\"\n)\nsolver.eval()\n</code></pre>"},{"location":"zh/user_guide/#17","title":"1.7 \u5b9e\u9a8c\u8fc7\u7a0b\u53ef\u89c6\u5316","text":"TensorBoardXVisualDLWandB <p>TensorBoardX \u662f\u57fa\u4e8e TensorBoard \u7f16\u5199\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177\uff0c\u4ee5\u4e30\u5bcc\u7684\u56fe\u8868\u5448\u73b0\u8bad\u7ec3\u53c2\u6570\u53d8\u5316\u8d8b\u52bf\u3001\u6570\u636e\u6837\u672c\u3001\u6a21\u578b\u7ed3\u6784\u3001PR\u66f2\u7ebf\u3001ROC\u66f2\u7ebf\u3001\u9ad8\u7ef4\u6570\u636e\u5206\u5e03\u7b49\u3002\u5e2e\u52a9\u7528\u6237\u6e05\u6670\u76f4\u89c2\u5730\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u53ca\u6a21\u578b\u7ed3\u6784\uff0c\u8fdb\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u6a21\u578b\u8c03\u4f18\u3002</p> <p>PaddleScience \u652f\u6301\u4f7f\u7528 TensorBoardX \u8bb0\u5f55\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u57fa\u7840\u5b9e\u9a8c\u6570\u636e\uff0c\u5305\u62ec train/eval loss\uff0ceval metric\uff0clearning rate \u7b49\u57fa\u672c\u4fe1\u606f\uff0c\u53ef\u6309\u5982\u4e0b\u6b65\u9aa4\u4f7f\u7528\u8be5\u529f\u80fd\u3002</p> <ol> <li> <p>\u5b89\u88c5 Tensorboard \u548c TensorBoardX</p> <pre><code>pip install tensorboard tensorboardX\n</code></pre> </li> <li> <p>\u5728\u6848\u4f8b\u4e2d\u542f\u7528 tensorboardX</p> \u65b9\u5f0f1: \u547d\u4ee4\u884c\u6307\u5b9a[\u63a8\u8350]\u65b9\u5f0f2: \u4fee\u6539\u4ee3\u7801 <p>\u82e5\u5728\u6848\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4e3a <code>Solver</code> \u6784\u5efa\u65f6\u4f20\u9012\u4e86 <code>cfg</code> \u53c2\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a <code>use_tbd</code> \u518d\u6267\u884c\u8bad\u7ec3\u547d\u4ee4\u5373\u53ef\uff0c\u514d\u53bb\u4fee\u6539\u6848\u4f8b\u4ee3\u7801\u3002</p> <pre><code>python example.py use_tbd=True\n</code></pre> <pre><code>solver = ppsci.solver.Solver(\n    ...,\n    use_tbd=True,\n)\n</code></pre> </li> <li> <p>\u53ef\u89c6\u5316\u8bb0\u5f55\u6570\u636e</p> <p>\u6839\u636e\u4e0a\u8ff0\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u65f6 TensorBoardX \u4f1a\u81ea\u52a8\u8bb0\u5f55\u6570\u636e\u5e76\u4fdd\u5b58\u5230 <code>${solver.output_dir}/tensorboard</code> \u76ee\u5f55\u4e0b\uff0c\u5177\u4f53\u6240\u5728\u8def\u5f84\u5728\u5b9e\u4f8b\u5316 <code>Solver</code> \u65f6\uff0c\u4f1a\u81ea\u52a8\u6253\u5370\u5728\u7ec8\u7aef\u4e2d\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>ppsci MESSAGE: TensorboardX tool is enabled for logging, you can view it by running:\ntensorboard --logdir outputs_VIV/2024-01-01/08-00-00/tensorboard\n</code></pre> <p>Tip</p> <p>\u4e5f\u53ef\u4ee5\u8f93\u5165 <code>tensorboard --logdir ./outputs_VIV</code>\uff0c\u4e00\u6b21\u6027\u5728\u7f51\u9875\u4e0a\u5c55\u793a <code>outputs_VIV</code> \u76ee\u5f55\u4e0b\u6240\u6709\u8bad\u7ec3\u8bb0\u5f55\uff0c\u4fbf\u4e8e\u5bf9\u6bd4\u3002</p> <p>\u5728\u7ec8\u7aef\u91cc\u8f93\u5165\u4e0a\u8ff0\u53ef\u89c6\u5316\u547d\u4ee4\uff0c\u5e76\u7528\u6d4f\u89c8\u5668\u8fdb\u5165 TensorBoardX \u7ed9\u51fa\u7684\u53ef\u89c6\u5316\u5730\u5740\uff0c\u5373\u53ef\u5728\u6d4f\u89c8\u5668\u5185\u67e5\u770b\u8bb0\u5f55\u7684\u6570\u636e\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002</p> <p></p> </li> </ol> <p>VisualDL \u662f\u98de\u6868\u63a8\u51fa\u7684\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177\uff0c\u4ee5\u4e30\u5bcc\u7684\u56fe\u8868\u5448\u73b0\u8bad\u7ec3\u53c2\u6570\u53d8\u5316\u8d8b\u52bf\u3001\u6570\u636e\u6837\u672c\u3001\u6a21\u578b\u7ed3\u6784\u3001PR\u66f2\u7ebf\u3001ROC\u66f2\u7ebf\u3001\u9ad8\u7ef4\u6570\u636e\u5206\u5e03\u7b49\u3002\u5e2e\u52a9\u7528\u6237\u6e05\u6670\u76f4\u89c2\u5730\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u53ca\u6a21\u578b\u7ed3\u6784\uff0c\u8fdb\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u6a21\u578b\u8c03\u4f18\u3002</p> <p>PaddleScience \u652f\u6301\u4f7f\u7528 VisualDL \u8bb0\u5f55\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u57fa\u7840\u5b9e\u9a8c\u6570\u636e\uff0c\u5305\u62ec train/eval loss\uff0ceval metric\uff0clearning rate \u7b49\u57fa\u672c\u4fe1\u606f\uff0c\u53ef\u6309\u5982\u4e0b\u6b65\u9aa4\u4f7f\u7528\u8be5\u529f\u80fd\u3002</p> <ol> <li> <p>\u5b89\u88c5 VisualDL</p> <pre><code>pip install -U visualdl\n</code></pre> </li> <li> <p>\u5728\u6848\u4f8b\u4e2d\u542f\u7528 visualDL</p> \u65b9\u5f0f1: \u547d\u4ee4\u884c\u6307\u5b9a[\u63a8\u8350]\u65b9\u5f0f2: \u4fee\u6539\u4ee3\u7801 <p>\u82e5\u5728\u6848\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4e3a <code>Solver</code> \u6784\u5efa\u65f6\u4f20\u9012\u4e86 <code>cfg</code> \u53c2\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a <code>use_vdl</code> \u518d\u6267\u884c\u8bad\u7ec3\u547d\u4ee4\u5373\u53ef\uff0c\u514d\u53bb\u4fee\u6539\u6848\u4f8b\u4ee3\u7801\u3002</p> <pre><code>python example.py use_vdl=True\n</code></pre> <pre><code>solver = ppsci.solver.Solver(\n    ...,\n    use_vdl=True,\n)\n</code></pre> </li> <li> <p>\u53ef\u89c6\u5316\u8bb0\u5f55\u6570\u636e</p> <p>\u6839\u636e\u4e0a\u8ff0\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u65f6 VisualDL \u4f1a\u81ea\u52a8\u8bb0\u5f55\u6570\u636e\u5e76\u4fdd\u5b58\u5230 <code>${solver.output_dir}/vdl</code> \u76ee\u5f55\u4e0b\uff0c\u5177\u4f53\u6240\u5728\u8def\u5f84\u5728\u5b9e\u4f8b\u5316 <code>Solver</code> \u65f6\uff0c\u4f1a\u81ea\u52a8\u6253\u5370\u5728\u7ec8\u7aef\u4e2d\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.8, Runtime API Version: 11.6\ndevice: 0, cuDNN Version: 8.4.\nppsci INFO: VisualDL tool enabled for logging, you can view it by running:\nvisualdl --logdir outputs_darcy2d/2023-10-08/10-00-00/TRAIN.epochs=400/vdl --port 8080\n</code></pre> <p>\u5728\u7ec8\u7aef\u91cc\u8f93\u5165\u4e0a\u8ff0\u53ef\u89c6\u5316\u547d\u4ee4\uff0c\u5e76\u7528\u6d4f\u89c8\u5668\u8fdb\u5165 VisualDL \u7ed9\u51fa\u7684\u53ef\u89c6\u5316\u5730\u5740\uff0c\u5373\u53ef\u5728\u6d4f\u89c8\u5668\u5185\u67e5\u770b\u8bb0\u5f55\u7684\u6570\u636e\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002</p> <p></p> </li> </ol> <p>WandB \u662f\u4e00\u4e2a\u7b2c\u4e09\u65b9\u5b9e\u9a8c\u8bb0\u5f55\u5de5\u5177\uff0c\u80fd\u5728\u8bb0\u5f55\u5b9e\u9a8c\u6570\u636e\u7684\u540c\u65f6\u5c06\u6570\u636e\u4e0a\u4f20\u5230\u7528\u6237\u7684\u79c1\u4eba\u8d26\u6237\u4e0a\uff0c\u9632\u6b62\u5b9e\u9a8c\u8bb0\u5f55\u4e22\u5931\u3002</p> <p>PaddleScience \u652f\u6301\u4f7f\u7528 WandB \u8bb0\u5f55\u57fa\u672c\u7684\u5b9e\u9a8c\u6570\u636e\uff0c\u5305\u62ec train/eval loss\uff0ceval metric\uff0clearning rate \u7b49\u57fa\u672c\u4fe1\u606f\uff0c\u53ef\u6309\u5982\u4e0b\u6b65\u9aa4\u4f7f\u7528\u8be5\u529f\u80fd</p> <ol> <li> <p>\u5b89\u88c5 wandb</p> <pre><code>pip install wandb\n</code></pre> </li> <li> <p>\u6ce8\u518c wandb \u5e76\u5728\u7ec8\u7aef\u767b\u5f55</p> <pre><code># \u767b\u5f55 wandb \u83b7\u53d6 API key\nwandb login\n# \u6839\u636e login \u63d0\u793a\uff0c\u8f93\u5165 API key \u5e76\u56de\u8f66\u786e\u8ba4\n</code></pre> </li> <li> <p>\u5728\u6848\u4f8b\u4e2d\u542f\u7528 wandb</p> \u65b9\u5f0f1: \u547d\u4ee4\u884c\u6307\u5b9a[\u63a8\u8350]\u65b9\u5f0f2: \u4fee\u6539\u4ee3\u7801 <p>\u82e5\u5728\u6848\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4e3a <code>Solver</code> \u6784\u5efa\u65f6\u4f20\u9012\u4e86 <code>cfg</code> \u53c2\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a <code>use_wandb</code> \u518d\u6267\u884c\u8bad\u7ec3\u547d\u4ee4\u5373\u53ef\uff0c\u514d\u53bb\u4fee\u6539\u6848\u4f8b\u4ee3\u7801\u3002</p> <pre><code>python example.py use_wandb=True\n</code></pre> <pre><code>solver = ppsci.solver.Solver(\n    ...,\n    use_wandb=True,\n    wandb_config={\n        \"project\": \"PaddleScience\",\n        \"name\": \"Laplace2D\",\n        \"dir\": OUTPUT_DIR,\n    },\n    ...\n)\nsolver.train()\n</code></pre> <p>\u5982\u4e0a\u8ff0\u4ee3\u7801\u6240\u793a\uff0c\u6307\u5b9a <code>use_wandb=True</code>\uff0c\u5e76\u4e14\u8bbe\u7f6e <code>wandb_config</code> \u914d\u7f6e\u5b57\u5178\u4e2d\u7684 <code>project</code>\u3001<code>name</code>\u3001<code>dir</code> \u4e09\u4e2a\u5b57\u6bb5\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u5373\u53ef\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u5b9e\u65f6\u4e0a\u4f20\u8bb0\u5f55\u6570\u636e\u81f3 wandb \u670d\u52a1\u5668\uff0c\u8bad\u7ec3\u7ed3\u675f\u540e\u53ef\u4ee5\u8fdb\u5165\u7ec8\u7aef\u6253\u5370\u7684\u9884\u89c8\u5730\u5740\u5728\u7f51\u9875\u7aef\u67e5\u770b\u5b8c\u6574\u8bad\u7ec3\u8bb0\u5f55\u66f2\u7ebf\u3002</p> <p>\u6ce8\u610f</p> <p>\u7531\u4e8e\u6bcf\u6b21\u8c03\u7528 <code>wandb.log</code> \u4f1a\u4f7f\u5f97\u5176\u81ea\u5e26\u7684\u8ba1\u6570\u5668 <code>Step</code> \u81ea\u589e 1\uff0c\u56e0\u6b64\u5728 wandb \u7684\u7f51\u7ad9\u4e0a\u67e5\u770b\u8bad\u7ec3\u8bb0\u5f55\u65f6\uff0c\u9700\u8981\u624b\u52a8\u66f4\u6539 x \u8f74\u7684\u5355\u4f4d\u4e3a <code>step</code>(\u5168\u5c0f\u5199)\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <p>\u5426\u5219\u9ed8\u8ba4\u5355\u4f4d\u4e3a wandb \u81ea\u5e26\u7684 <code>Step</code> (S\u5927\u5199) \u5b57\u6bb5\uff0c\u4f1a\u5bfc\u81f4\u663e\u793a\u6b65\u6570\u6bd4\u5b9e\u9645\u6b65\u6570\u591a\u51e0\u500d\u3002 </p> </li> </ol>"},{"location":"zh/user_guide/#2","title":"2. \u8fdb\u9636\u529f\u80fd","text":""},{"location":"zh/user_guide/#21","title":"2.1 \u8d1d\u53f6\u65af\u8d85\u53c2\u641c\u7d22","text":"<p>hydra \u7684\u81ea\u52a8\u5316\u5b9e\u9a8c\u529f\u80fd\u53ef\u4ee5\u4e0e optuna \u8d85\u53c2\u6570\u8c03\u4f18\u5de5\u5177\u4e00\u8d77\u4f7f\u7528\u3002\u5728 yaml \u6587\u4ef6\u4e2d\u8bbe\u7f6e\u597d\u9700\u8981\u8c03\u6574\u7684\u53c2\u6570\u548c\u6700\u5927\u5b9e\u9a8c\u6b21\u6570\u540e\uff0c\u53ef\u4ee5\u8c03\u7528 Tree-structured Parzen Estimator(TPE) \u7b97\u6cd5\u8fdb\u884c\u81ea\u52a8\u5316\u8c03\u53c2\u5de5\u5177\uff0c\u6548\u7387\u9ad8\u4e8e\u7f51\u683c\u8c03\u53c2\u6cd5\u3002</p> <p>\u4e0b\u9762\u4ee5 viv \u6848\u4f8b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u5728 PaddleScience \u4e2d\u4f7f\u7528\u8be5\u65b9\u6cd5\u3002</p> <ol> <li> <p>\u5b89\u88c5 1.1.0 \u4ee5\u4e0a\u7248\u672c\u7684 <code>hydra-core</code> \u4ee5\u53ca <code>hydra-optuna</code> \u63d2\u4ef6</p> <pre><code>pip install 'hydra-core&gt;=1.1.0' hydra-optuna-sweeper\n</code></pre> </li> <li> <p>\u4fee\u6539 <code>viv.yaml</code> \u6587\u4ef6\uff0c\u5728 <code>defaults:</code> \u548c <code>hydra:</code> \u5b57\u6bb5\u4e0b\u5206\u522b\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e\uff08\u9ad8\u4eae\u90e8\u5206\u6240\u793a\uff09</p> viv.yaml<pre><code>defaults:\n  - ppsci_default\n  - TRAIN: train_default\n  - TRAIN/ema: ema_default\n  - TRAIN/swa: swa_default\n  - EVAL: eval_default\n  - INFER: infer_default\n  - override hydra/sweeper: optuna # (1)\n  - _self_\n\nhydra:\n  run:\n    # dynamic output directory according to running time and override name\n    dir: outputs_VIV/${now:%Y-%m-%d}/${now:%H-%M-%S}/${hydra.job.override_dirname}\n  job:\n    name: ${mode} # name of logfile\n    chdir: false # keep current working directory unchanged\n  callbacks:\n    init_callback:\n      _target_: ppsci.utils.callbacks.InitCallback\n  sweep:\n    # output directory for multirun\n    dir: ${hydra.run.dir}\n    subdir: ./\n\n  sweeper: # (2)\n    direction: minimize # (3)\n    study_name: viv_optuna # (4)\n    n_trials: 20 # (5)\n    n_jobs: 1 # (6)\n    params: # (7)\n      MODEL.num_layers: choice(2, 3, 4, 5, 6, 7) # (8)\n      TRAIN.lr_scheduler.learning_rate: interval(0.0001, 0.005) # (9)\n</code></pre> <ol> <li>\u6307\u5b9a\u4e86\u4f7f\u7528 <code>optuna</code> \u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\u3002</li> <li><code>sweeper:</code>\uff1a\u8fd9\u4e00\u884c\u6307\u5b9a\u4e86 Hydra \u4f7f\u7528\u7684 sweeper \u63d2\u4ef6\uff0c\u7528\u4e8e\u8fdb\u884c\u53c2\u6570\u626b\u63cf\u3002\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u5b83\u5c06\u4f7f\u7528 Optuna \u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\u3002</li> <li><code>direction: minimize</code>\uff1a\u8fd9\u6307\u5b9a\u4e86\u4f18\u5316\u7684\u76ee\u6807\u65b9\u5411\u3002minimize \u8868\u793a\u6211\u4eec\u5e0c\u671b\u6700\u5c0f\u5316\u76ee\u6807\u51fd\u6570\uff08\u4f8b\u5982\u6a21\u578b\u7684\u9a8c\u8bc1\u635f\u5931\uff09\u3002\u5982\u679c\u6211\u4eec\u5e0c\u671b\u6700\u5927\u5316\u67d0\u4e2a\u6307\u6807\uff08\u4f8b\u5982\u51c6\u786e\u7387\uff09\uff0c\u5219\u53ef\u4ee5\u8bbe\u7f6e\u4e3a maximize\u3002</li> <li><code>study_name: viv_optuna</code>\uff1a\u8fd9\u8bbe\u7f6e\u4e86 Optuna \u7814\u7a76\uff08Study\uff09\u7684\u540d\u79f0\u3002\u8fd9\u4e2a\u540d\u79f0\u7528\u4e8e\u6807\u8bc6\u548c\u5f15\u7528\u7279\u5b9a\u7684\u7814\u7a76\uff0c\u6709\u52a9\u4e8e\u5728\u4ee5\u540e\u7684\u5206\u6790\u6216\u7ee7\u7eed\u4f18\u5316\u65f6\u8ddf\u8e2a\u7ed3\u679c\u3002</li> <li><code>n_trials: 20</code>\uff1a\u8fd9\u6307\u5b9a\u4e86\u8981\u8fd0\u884c\u7684\u603b\u8bd5\u9a8c\u6b21\u6570\u3002\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0cOptuna \u5c06\u6267\u884c 20 \u6b21\u72ec\u7acb\u7684\u8bd5\u9a8c\u6765\u5bfb\u627e\u6700\u4f73\u7684\u8d85\u53c2\u6570\u7ec4\u5408\u3002</li> <li><code>n_jobs: 1</code>\uff1a\u8fd9\u8bbe\u7f6e\u4e86\u53ef\u4ee5\u5e76\u884c\u8fd0\u884c\u7684\u8bd5\u9a8c\u6570\u91cf\u3002\u503c\u4e3a 1 \u610f\u5473\u7740\u8bd5\u9a8c\u5c06\u4f9d\u6b21\u6267\u884c\uff0c\u800c\u4e0d\u662f\u5e76\u884c\u3002\u5982\u679c\u4f60\u7684\u7cfb\u7edf\u6709\u591a\u4e2a CPU \u6838\u5fc3\uff0c\u5e76\u4e14\u5e0c\u671b\u5e76\u884c\u5316\u4ee5\u52a0\u901f\u641c\u7d22\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u503c\u8bbe\u7f6e\u4e3a\u66f4\u9ad8\u7684\u6570\u5b57\u6216 -1\uff08\u8868\u793a\u4f7f\u7528\u6240\u6709\u53ef\u7528\u7684 CPU \u6838\u5fc3\uff09\u3002</li> <li><code>params:</code>\uff1a \u8fd9\u4e00\u8282\u5b9a\u4e49\u4e86\u8981\u4f18\u5316\u7684\u8d85\u53c2\u6570\u4ee5\u53ca\u5b83\u4eec\u7684\u641c\u7d22\u7a7a\u95f4\u3002</li> <li><code>MODEL.num_layers: choice(2, 3, 4, 5, 6, 7)</code>\uff1a\u8fd9\u6307\u5b9a\u4e86\u6a21\u578b\u5c42\u6570\u7684\u53ef\u9009\u503c\u3002choice \u51fd\u6570\u8868\u793a Optuna \u5728 2, 3, 4, 5, 6, \u548c 7 \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u503c\u3002</li> <li><code>TRAIN.lr_scheduler.learning_rate: interval(0.0001, 0.005)</code>\uff1a\u8fd9\u6307\u5b9a\u4e86\u5b66\u4e60\u7387\u7684\u641c\u7d22\u8303\u56f4\u3002interval \u8868\u793a\u5b66\u4e60\u7387\u7684\u503c\u5c06\u5728 0.0001 \u548c 0.005 \u4e4b\u95f4\u5747\u5300\u5206\u5e03\u5730\u9009\u62e9\u3002</li> </ol> <p>\u5982\u4e0a\u6240\u793a\uff0c\u5728 <code>hydra.sweeper</code> \u8282\u70b9\u4e0b\u6dfb\u52a0\u4e86 <code>optuna</code> \u63d2\u4ef6\u7684\u914d\u7f6e\uff0c\u5e76\u5728 <code>params</code> \u8282\u70b9\u4e0b\u6307\u5b9a\u4e86\u8981\u8c03\u4f18\u7684\u53c2\u6570\u53ca\u5176\u8303\u56f4\uff1a 1. \u6a21\u578b\u5c42\u6570 <code>MODEL.num_layers</code>\uff0c\u5728 [2, 3, 4, 5, 6, 7] \u5171 6 \u79cd\u5c42\u6570\u95f4\u8fdb\u884c\u8c03\u4f18\u3002 2. \u5b66\u4e60\u7387 <code>TRAIN.lr_scheduler.learning_rate</code>\uff0c\u5728 0.0001 ~ 0.005 \u4e4b\u95f4\u8fdb\u884c\u8c03\u4f18\u3002</p> <p>\u6ce8\u610f</p> <ol> <li>\u8c03\u4f18\u7684\u53c2\u6570\u9700\u8981\u4e0e yaml \u6587\u4ef6\u4e2d\u914d\u7f6e\u7684\u53c2\u6570\u540d\u4e00\u81f4\uff0c\u5982 <code>MODEL.num_layers</code>\u3001<code>TRAIN.lr_scheduler.learning_rate</code>\u3002</li> <li>\u8c03\u4f18\u7684\u53c2\u6570\u8303\u56f4\u6839\u636e\u4e0d\u540c\u8bed\u4e49\u8fdb\u884c\u6307\u5b9a\uff0c\u6bd4\u5982\u6a21\u578b\u5c42\u6570\u5fc5\u987b\u4e3a\u6574\u6570\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>choice(...)</code> \u8bbe\u7f6e\u6709\u9650\u8303\u56f4\uff0c\u800c\u5b66\u4e60\u7387\u4e00\u822c\u4e3a\u6d6e\u70b9\u6570\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>interval(...)</code> \u8bbe\u7f6e\u5176\u4e0a\u4e0b\u754c\u3002</li> </ol> </li> <li> <p>\u4fee\u6539 viv.py\uff0c\u4f7f\u5f97\u88ab <code>@hydra.main</code> \u88c5\u9970\u7684 <code>main</code> \u51fd\u6570\u8fd4\u56de\u5b9e\u9a8c\u6307\u6807\u7ed3\u679c\uff08\u9ad8\u4eae\u90e8\u5206\u6240\u793a\uff09</p> viv.py<pre><code>def train(cfg: DictConfig):\n    ...\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        equation=equation,\n        validator=validator,\n        visualizer=visualizer,\n        cfg=cfg,\n    )\n\n    # evaluate\n    l2_err_eval, _ = solver.eval()\n    return l2_err_eval\n\n...\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"viv.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        return train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n</code></pre> </li> <li> <p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5f00\u59cb\u81ea\u52a8\u5316\u8c03\u4f18</p> <pre><code>python viv.py --multirun\n</code></pre> </li> </ol> <p>\u5728 20 \u6b21\u8c03\u4f18\u5b9e\u9a8c\u8fd0\u884c\u5b8c\u6bd5\u540e\uff0c\u5728\u6a21\u578b\u4fdd\u5b58\u76ee\u5f55\u4e0b\uff0c\u4f1a\u751f\u6210 <code>optimization_results.yaml</code> \u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u6700\u4f73\u7684\u8c03\u4f18\u7ed3\u679c\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>name: optuna\nbest_params:\n  MODEL.num_layers: 7\n  TRAIN.lr_scheduler.learning_rate: 0.003982453338298202\nbest_value: 0.02460772916674614\n</code></pre> <p>\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u4ee5\u53ca\u591a\u76ee\u6807\u81ea\u52a8\u8c03\u4f18\u65b9\u6cd5\uff0c\u53ef\u53c2\u8003\uff1aOptuna Sweeper plugin \u548c Optuna\u3002</p>"},{"location":"zh/user_guide/#22","title":"2.2 \u5206\u5e03\u5f0f\u8bad\u7ec3","text":""},{"location":"zh/user_guide/#221","title":"2.2.1 \u6570\u636e\u5e76\u884c","text":"<p>\u63a5\u4e0b\u6765\u4ee5 <code>examples/pipe/poiseuille_flow.py</code> \u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u6b63\u786e\u4f7f\u7528 PaddleScience \u7684\u6570\u636e\u5e76\u884c\u529f\u80fd\u3002\u5206\u5e03\u5f0f\u8bad\u7ec3\u7ec6\u8282\u53ef\u4ee5\u53c2\u8003\uff1aPaddle-\u4f7f\u7528\u6307\u5357-\u5206\u5e03\u5f0f\u8bad\u7ec3-\u5feb\u901f\u5f00\u59cb-\u6570\u636e\u5e76\u884c\u3002</p> <ol> <li> <p>\u5728 constraint \u5b9e\u4f8b\u5316\u5b8c\u6bd5\u540e\uff0c\u5c06 <code>ITERS_PER_EPOCH</code> \u91cd\u65b0\u8d4b\u503c\u4e3a\u7ecf\u8fc7\u81ea\u52a8\u591a\u5361\u6570\u636e\u5207\u5206\u540e\u7684 <code>dataloader</code> \u7684\u957f\u5ea6\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\u5176\u957f\u5ea6\u7b49\u4e8e\u5355\u5361 dataloader \u7684\u957f\u5ea6\u9664\u4ee5\u5361\u6570\uff0c\u5411\u4e0a\u53d6\u6574\uff09\uff0c\u5982\u4ee3\u7801\u4e2d\u9ad8\u4eae\u884c\u6240\u793a\u3002</p> examples/pipe/poiseuille_flow.py<pre><code>ITERS_PER_EPOCH = int((N_x * N_y * N_p) / BATCH_SIZE)\n\npde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"NavierStokes\"].equations,\n    {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n    geom=interior_geom,\n    dataloader_cfg={\n        \"dataset\": \"NamedArrayDataset\",\n        \"num_workers\": 1,\n        \"batch_size\": BATCH_SIZE,\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n        },\n    },\n    loss=ppsci.loss.MSELoss(\"mean\"),\n    evenly=True,\n    name=\"EQ\",\n)\nITERS_PER_EPOCH = len(pde_constraint.data_loader) # re-assign to ITERS_PER_EPOCH\n\n# wrap constraints together\nconstraint = {pde_constraint.name: pde_constraint}\n\nEPOCHS = 3000 if not args.epochs else args.epochs\n</code></pre> </li> <li> <p>\u4f7f\u7528\u5206\u5e03\u5f0f\u8bad\u7ec3\u547d\u4ee4\u542f\u52a8\u8bad\u7ec3\uff0c\u4ee5 4 \u5361\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u4e3a\u4f8b</p> <pre><code># \u6307\u5b9a 0,1,2,3 \u5f20\u5361\u542f\u52a8\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\u8bad\u7ec3\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython -m paddle.distributed.launch --gpus=\"0,1,2,3\" poiseuille_flow.py\n</code></pre> </li> </ol>"},{"location":"zh/user_guide/#23","title":"2.3 \u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"<p>\u63a5\u4e0b\u6765\u4ecb\u7ecd\u5982\u4f55\u6b63\u786e\u4f7f\u7528 PaddleScience \u7684\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u529f\u80fd\u3002\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u7684\u539f\u7406\u53ef\u4ee5\u53c2\u8003\uff1aPaddle-\u4f7f\u7528\u6307\u5357-\u6027\u80fd\u8c03\u4f18-\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff08AMP\uff09\u3002</p> <p>\u82e5\u60f3\u5728\u8bad\u7ec3\u4e2d\u542f\u7528\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff0c\u5219\u53ef\u4ee5\u9009\u62e9\u4e0b\u9762\u7684\u51e0\u79cd\u65b9\u5f0f\u4e4b\u4e00\u3002<code>O1</code> \u4e3a\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff0c<code>O2</code> \u4e3a\u66f4\u6fc0\u8fdb\u7684\u7eaf fp16 \u8bad\u7ec3\u6a21\u5f0f\uff0c\u4e00\u822c\u63a8\u8350\u4f7f\u7528 <code>O1</code>\u3002</p> \u65b9\u5f0f1: \u547d\u4ee4\u884c\u6307\u5b9a[\u63a8\u8350]\u65b9\u5f0f2: \u4fee\u6539\u4ee3\u7801 <p>\u82e5\u5728\u6848\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4e3a <code>Solver</code> \u6784\u5efa\u65f6\u4f20\u9012\u4e86 <code>cfg</code> \u53c2\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a <code>use_amp</code>\u3001<code>amp_level</code> \u518d\u6267\u884c\u8bad\u7ec3/\u8bc4\u4f30\u547d\u4ee4\u5373\u53ef\uff0c\u514d\u53bb\u4fee\u6539\u6848\u4f8b\u4ee3\u7801\u3002</p> <pre><code>python example.py use_amp=True amp_level=O1\n</code></pre> <p>\u5b9e\u4f8b\u5316 <code>Solver</code> \u65f6\u52a0\u4e0a 2 \u4e2a\u53c2\u6570: <code>use_amp=True</code>, <code>amp_level=\"O1\"</code>(\u6216<code>amp_level=\"O2\"</code>)\u3002\u5982\u4ee3\u7801\u4e2d\u9ad8\u4eae\u884c\u6240\u793a\uff0c\u901a\u8fc7\u6307\u5b9a <code>use_amp=True</code>\uff0c\u5f00\u542f\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u529f\u80fd\uff0c\u63a5\u7740\u518d\u8bbe\u7f6e <code>amp_level=\"O1\"</code>\uff0c\u6307\u5b9a\u6df7\u5408\u7cbe\u5ea6\u6240\u7528\u7684\u6a21\u5f0f\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    ...,\n    ...,\n    use_amp=True,\n    amp_level=\"O1\", # or amp_level=\"O2\"\n)\n</code></pre>"},{"location":"zh/user_guide/#24","title":"2.4 \u68af\u5ea6\u7d2f\u52a0","text":"<p>\u63a5\u4e0b\u6765\u4ecb\u7ecd\u5982\u4f55\u6b63\u786e\u4f7f\u7528 PaddleScience \u7684\u68af\u5ea6\u7d2f\u52a0\u529f\u80fd\u3002\u68af\u5ea6\u7d2f\u52a0\u7684\u539f\u7406\u53ef\u4ee5\u53c2\u8003\uff1aPaddle-\u4f7f\u7528\u6307\u5357-\u6027\u80fd\u8c03\u4f18-\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff08AMP\uff09-\u52a8\u6001\u56fe\u4e0b\u4f7f\u7528\u68af\u5ea6\u7d2f\u52a0\u3002</p> <p>\u5b9e\u4f8b\u5316 <code>Solver</code> \u65f6\u6307\u5b9a <code>update_freq</code> \u53c2\u6570\u4e3a\u5927\u4e8e 1 \u7684\u6b63\u6574\u6570\u5373\u53ef\u3002\u5982\u4ee3\u7801\u4e2d\u9ad8\u4eae\u884c\u6240\u793a\uff0c<code>update_freq</code> \u53ef\u4ee5\u8bbe\u7f6e\u4e3a 2 \u6216\u8005\u66f4\u5927\u7684\u6574\u6570\uff0c\u63a8\u8350\u4f7f\u7528 2\u30014\u30018\uff0c\u6b64\u65f6\u5bf9\u4e8e\u8bad\u7ec3\u4efb\u52a1\u6765\u8bf4\uff0c\u5168\u5c40 <code>batch size</code> \u7b49\u4ef7\u4e8e <code>update_freq * batch size</code>\u3002\u68af\u5ea6\u7d2f\u52a0\u65b9\u6cd5\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u80fd\u591f\u8ba9\u95f4\u63a5\u5730\u6269\u5927\u6bcf\u4e2a batch \u5185\u7684\u6837\u672c\u6570\u91cf\uff0c\u4ece\u800c\u8ba9\u6bcf\u4e2a batch \u5206\u5e03\u66f4\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u5206\u5e03\uff0c\u63d0\u5347\u8bad\u7ec3\u4efb\u52a1\u7684\u6027\u80fd\u3002</p> \u65b9\u5f0f1: \u547d\u4ee4\u884c\u6307\u5b9a[\u63a8\u8350]\u65b9\u5f0f2: \u4fee\u6539\u4ee3\u7801 <p>\u82e5\u5728\u6848\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4e3a <code>Solver</code> \u6784\u5efa\u65f6\u4f20\u9012\u4e86 <code>cfg</code> \u53c2\u6570\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a <code>TRAIN.update_freq</code> \u518d\u6267\u884c\u8bad\u7ec3\u547d\u4ee4\u5373\u53ef\uff0c\u514d\u53bb\u4fee\u6539\u6848\u4f8b\u4ee3\u7801\u3002</p> <pre><code>python example.py TRAIN.update_freq=2\n</code></pre> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    ...,\n    ...,\n    update_freq=2, # or 4, 8\n)\n</code></pre>"},{"location":"zh/user_guide/#25","title":"2.5 \u591a\u4efb\u52a1\u5b66\u4e60","text":"<p>\u5728\u673a\u7406\u9a71\u52a8\u3001\u6570\u7406\u878d\u5408\u573a\u666f\u4e2d\uff0c\u5f80\u5f80\u4f1a\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u635f\u5931\u9879\uff0c\u5982\u63a7\u5236\u65b9\u7a0b\u6b8b\u5dee\u635f\u5931\u3001\uff08\u521d\uff09\u8fb9\u503c\u6761\u4ef6\u635f\u5931\u7b49\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fd9\u4e9b\u635f\u5931\u9879\u5bf9\u53c2\u6570\u7684\u68af\u5ea6\u65b9\u5411\u53ef\u80fd\u4f1a\u4e92\u76f8\u51b2\u7a81\uff0c\u963b\u788d\u8bad\u7ec3\u7cbe\u5ea6\u6536\u655b\uff0c\u800c\u8fd9\u6b63\u662f\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u80fd\u89e3\u51b3\u7684\u95ee\u9898\u3002\u56e0\u6b64 PaddleScience \u5728\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u5757\u4e2d\u5f15\u5165\u4e86\u51e0\u79cd\u5e38\u89c1\u7684\u7b97\u6cd5\uff0c\u5176\u4e3b\u8981\u901a\u8fc7\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u6743\u91cd\u6216\u4ea7\u751f\u7684\u68af\u5ea6\u8fdb\u884c\u8c03\u6574\uff0c\u4ece\u800c\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u6700\u7ec8\u63d0\u5347\u6a21\u578b\u6536\u655b\u7cbe\u5ea6\u3002\u4e0b\u9762\u4ee5 <code>Relobralo</code> \u7b97\u6cd5\u8fdb\u884c\u4e3e\u4f8b\uff0c\u4f7f\u7528\u65b9\u5f0f\u5982\u4e0b\uff1a</p> <ol> <li> <p>\u5b9e\u4f8b\u5316\u4e00\u4e2a\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u7684\u5bf9\u8c61</p> <pre><code>from ppsci.loss import mtl\nmodel = ...\nnum_losses = 2 # number of losses to be optimized\nloss_aggregator = mtl.Relobralo(num_losses)\n</code></pre> </li> <li> <p>\u5c06\u8be5\u5bf9\u8c61\u4f5c\u4e3a <code>Solver</code> \u7684\u5b9e\u4f8b\u5316\u53c2\u6570\u4e4b\u4e00\u4f20\u5165</p> <pre><code>solver = ppsci.solver.Solver(\n    ...,\n    ...,\n    loss_aggregator=loss_aggregator,\n)\n</code></pre> </li> <li> <p>\u542f\u52a8\u8bad\u7ec3\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d <code>loss_aggregator</code> \u4f1a\u81ea\u52a8\u5bf9\u83b7\u53d6\u5230\u7684\u591a\u4e2a\u635f\u5931\u9879\u5e94\u7528\u5bf9\u5e94\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316</p> <pre><code>solver.train()\n</code></pre> <p>\u5f71\u54cd\u8bf4\u660e</p> <p>\u4e2a\u522b\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff08\u5982weight based method\uff09\u53ef\u80fd\u4f1a\u6539\u53d8\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u635f\u5931\u51fd\u6570\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u4f46\u4ec5\u9650\u4e8e\u5f71\u54cd\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u6a21\u578b\u8bc4\u4f30\u8fc7\u7a0b\u7684\u635f\u5931\u8ba1\u7b97\u65b9\u5f0f\u4fdd\u6301\u4e0d\u53d8\u3002</p> </li> </ol>"},{"location":"zh/user_guide/#3-nsight","title":"3. \u4f7f\u7528 Nsight \u8fdb\u884c\u6027\u80fd\u5206\u6790","text":"<p>Nsight\u662fNVIDIA\u9762\u76f8\u5f00\u53d1\u8005\u63d0\u4f9b\u7684\u5f00\u53d1\u5de5\u5177\u5957\u4ef6\uff0c\u80fd\u63d0\u4f9b\u6df1\u5165\u7684\u8ddf\u8e2a\u3001\u8c03\u8bd5\u3001\u8bc4\u6d4b\u548c\u5206\u6790\uff0c\u4ee5\u4f18\u5316\u8de8 NVIDIA GPU\u548cCPU\u7684\u590d\u6742\u8ba1\u7b97\u5e94\u7528\u7a0b\u5e8f\u3002\u8be6\u7ec6\u6587\u6863\u53ef\u53c2\u8003\uff1aNsight Systems Document</p> <p>PaddleScience \u521d\u6b65\u652f\u6301\u4f7f\u7528 Nsight \u8fdb\u884c\u6027\u80fd\u5206\u6790\uff0c\u4ee5 linux \u5f00\u53d1\u73af\u5883 + laplace2d \u6848\u4f8b\u4e3a\u4f8b\uff0c\u6309\u7167\u5982\u4e0b\u6b65\u9aa4\u4f7f\u7528 nsight \u5de5\u5177\u751f\u6210\u6027\u80fd\u5206\u6790\u62a5\u544a\u5e76\u67e5\u770b\u5206\u6790\u7ed3\u679c\u3002</p> <ol> <li> <p>\u5b89\u88c5 nsight-system</p> <p>\u5f00\u53d1\u673a\u4e0a\u4e0b\u8f7d linux nsight-system \u8f6f\u4ef6\uff1ansight-systems/2023.4.1\uff0c\u5e76\u5c06 nsight \u6dfb\u52a0\u5230\u73af\u5883\u53d8\u91cf <code>PATH</code> \u4e2d\uff1a</p> <p>\u6267\u884c\uff1a<code>PATH=/path/to/nsight-systems/2023.4.1/bin:$PATH</code>\uff0c\u540c\u65f6\u5728 windows \u673a\u5668\u4e0a\u5b89\u88c5\u76f8\u540c\u7248\u672c\u7684 nsight-system \u8f6f\u4ef6\u3002</p> </li> <li> <p>\u7528 nsys \u547d\u4ee4\u8fd0\u884c\u7a0b\u5e8f\uff0c\u751f\u6210\u6027\u80fd\u5206\u6790\u6587\u4ef6</p> <pre><code>NVTX=1 nsys profile -t cuda,nvtx --stats=true -o laplace2d python laplace2d.py\n</code></pre> </li> <li> <p>\u67e5\u770b\u5206\u6790\u7ed3\u679c</p> <p>\u7a0b\u5e8f\u7ed3\u675f\u540e\uff0c\u5728\u7ec8\u7aef\u5185\u4f1a\u6253\u5370\u51fa\u6027\u80fd\u5206\u6790\u6570\u636e\uff08\u5982\u4e0b\u6240\u793a\uff09\uff0c\u540c\u65f6\u5728\u4e0a\u8ff0 <code>-o</code> \u53c2\u6570\u6307\u5b9a\u7684\u76f8\u5bf9\u6587\u4ef6\u8def\u5f84\u751f\u6210 <code>laplace2d.nsys-rep</code> \u548c <code>laplace2d.sqlite</code> \u4e24\u4e2a\u6587\u4ef6\u3002</p> <p>\u5728 windows \u4e0a\u4f7f\u7528 NVIDIA Nsight Systems \u8f6f\u4ef6\u6253\u5f00 <code>laplace2d.nsys-rep</code>\uff0c\u5373\u53ef\u5728\u56fe\u5f62\u5316\u7684\u754c\u9762\u4e0a\u67e5\u770b\u6027\u80fd\u5206\u6790\u6570\u636e\u3002</p> <pre><code>...\n...\nOnly run 25 steps when 'NVTX' is set in environment for nsight analysis. Exit now ......\n\nGenerating '/tmp/nsys-report-18e4.qdstrm'\n[1/7] [========================100%] laplace2d.nsys-rep\n[2/7] [========================100%] laplace2d.sqlite\n[3/7] Executing 'nvtx_sum' stats report\n\nTime (%)  Total Time (ns)  Instances    Avg (ns)       Med (ns)      Min (ns)     Max (ns)     StdDev (ns)    Style                  Range\n--------  ---------------  ---------  -------------  -------------  -----------  -----------  -------------  -------  ------------------------------------\n    15.1      794,212,341         25   31,768,493.6    5,446,410.0    5,328,471  661,841,104  131,265,333.9  PushPop  Loss computation\n    14.5      766,452,142         25   30,658,085.7    4,369,873.0    4,281,927  659,795,434  131,070,475.4  PushPop  Constraint EQ\n    13.0      687,324,359      1,300      528,711.0       32,567.5       21,218  641,625,892   17,794,532.4  PushPop  matmul dygraph\n    12.9      678,475,194          1  678,475,194.0  678,475,194.0  678,475,194  678,475,194            0.0  PushPop  Training iteration 1\n    12.8      673,614,062      1,300      518,164.7       19,802.5       14,499  641,525,121   17,792,027.2  PushPop  matmul compute\n    3.9      203,945,648         25    8,157,825.9    8,029,819.0    7,797,185    9,119,496      359,173.3  PushPop  Loss backward\n    ...\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/api/arch/","title":"ppsci.arch","text":""},{"location":"zh/api/arch/#arch","title":"Arch(\u7f51\u7edc\u6a21\u578b) \u6a21\u5757","text":""},{"location":"zh/api/arch/#ppsci.arch","title":"<code>ppsci.arch</code>","text":""},{"location":"zh/api/arch/#ppsci.arch.Arch","title":"<code>Arch</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Base class for Network.</p> Source code in <code>ppsci/arch/base.py</code> <pre><code>class Arch(nn.Layer):\n    \"\"\"Base class for Network.\"\"\"\n\n    input_keys: Tuple[str, ...]\n    output_keys: Tuple[str, ...]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._input_transform: Callable[\n            [Dict[str, paddle.Tensor]], Dict[str, paddle.Tensor]\n        ] = None\n\n        self._output_transform: Callable[\n            [Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]],\n            Dict[str, paddle.Tensor],\n        ] = None\n\n    def forward(self, *args, **kwargs):\n        raise NotImplementedError(\"Arch.forward is not implemented\")\n\n    @property\n    def num_params(self) -&gt; int:\n        \"\"\"Return number of parameters within network.\n\n        Returns:\n            int: Number of parameters.\n        \"\"\"\n        num = 0\n        for name, param in self.named_parameters():\n            if hasattr(param, \"shape\"):\n                num += np.prod(list(param.shape), dtype=\"int\")\n            else:\n                logger.warning(f\"{name} has no attribute 'shape'\")\n        return num\n\n    @staticmethod\n    def concat_to_tensor(\n        data_dict: Dict[str, paddle.Tensor], keys: Tuple[str, ...], axis=-1\n    ) -&gt; Tuple[paddle.Tensor, ...]:\n        \"\"\"Concatenate tensors from dict in the order of given keys.\n\n        Args:\n            data_dict (Dict[str, paddle.Tensor]): Dict contains tensor.\n            keys (Tuple[str, ...]): Keys tensor fetched from.\n            axis (int, optional): Axis concatenate at. Defaults to -1.\n\n        Returns:\n            Tuple[paddle.Tensor, ...]: Concatenated tensor.\n\n        Examples:\n            &gt;&gt;&gt; import paddle\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; model = ppsci.arch.Arch()\n            &gt;&gt;&gt; # fetch one tensor\n            &gt;&gt;&gt; out = model.concat_to_tensor({'x':paddle.rand([64, 64, 1])}, ('x',))\n            &gt;&gt;&gt; print(out.dtype, out.shape)\n            paddle.float32 [64, 64, 1]\n            &gt;&gt;&gt; # fetch more tensors\n            &gt;&gt;&gt; out = model.concat_to_tensor(\n            ...     {'x1':paddle.rand([64, 64, 1]), 'x2':paddle.rand([64, 64, 1])},\n            ...     ('x1', 'x2'),\n            ...     axis=2)\n            &gt;&gt;&gt; print(out.dtype, out.shape)\n            paddle.float32 [64, 64, 2]\n\n        \"\"\"\n        if len(keys) == 1:\n            return data_dict[keys[0]]\n        data = [data_dict[key] for key in keys]\n        return paddle.concat(data, axis)\n\n    @staticmethod\n    def split_to_dict(\n        data_tensor: paddle.Tensor, keys: Tuple[str, ...], axis=-1\n    ) -&gt; Dict[str, paddle.Tensor]:\n        \"\"\"Split tensor and wrap into a dict by given keys.\n\n        Args:\n            data_tensor (paddle.Tensor): Tensor to be split.\n            keys (Tuple[str, ...]): Keys tensor mapping to.\n            axis (int, optional): Axis split at. Defaults to -1.\n\n        Returns:\n            Dict[str, paddle.Tensor]: Dict contains tensor.\n\n        Examples:\n            &gt;&gt;&gt; import paddle\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; model = ppsci.arch.Arch()\n            &gt;&gt;&gt; # split one tensor\n            &gt;&gt;&gt; out = model.split_to_dict(paddle.rand([64, 64, 1]), ('x',))\n            &gt;&gt;&gt; for k, v in out.items():\n            ...     print(f\"{k} {v.dtype} {v.shape}\")\n            x paddle.float32 [64, 64, 1]\n            &gt;&gt;&gt; # split more tensors\n            &gt;&gt;&gt; out = model.split_to_dict(paddle.rand([64, 64, 2]), ('x1', 'x2'), axis=2)\n            &gt;&gt;&gt; for k, v in out.items():\n            ...     print(f\"{k} {v.dtype} {v.shape}\")\n            x1 paddle.float32 [64, 64, 1]\n            x2 paddle.float32 [64, 64, 1]\n\n        \"\"\"\n        if len(keys) == 1:\n            return {keys[0]: data_tensor}\n        data = paddle.split(data_tensor, len(keys), axis=axis)\n        return {key: data[i] for i, key in enumerate(keys)}\n\n    def register_input_transform(\n        self,\n        transform: Callable[[Dict[str, paddle.Tensor]], Dict[str, paddle.Tensor]],\n    ):\n        \"\"\"Register input transform.\n\n        Args:\n            transform (Callable[[Dict[str, paddle.Tensor]], Dict[str, paddle.Tensor]]):\n                Input transform of network, receive a single tensor dict and return a single tensor dict.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; def transform_in(in_):\n            ...     x = in_[\"x\"]\n            ...     # transform input\n            ...     x_ = 2.0 * x\n            ...     input_trans = {\"2x\": x_}\n            ...     return input_trans\n            &gt;&gt;&gt; # `MLP` inherits from `Arch`\n            &gt;&gt;&gt; model = ppsci.arch.MLP(\n            ...     input_keys=(\"2x\",),\n            ...     output_keys=(\"y\",),\n            ...     num_layers=5,\n            ...     hidden_size=32)\n            &gt;&gt;&gt; model.register_input_transform(transform_in)\n            &gt;&gt;&gt; out = model({\"x\":paddle.rand([64, 64, 1])})\n            &gt;&gt;&gt; for k, v in out.items():\n            ...     print(f\"{k} {v.dtype} {v.shape}\")\n            y paddle.float32 [64, 64, 1]\n\n        \"\"\"\n        self._input_transform = transform\n\n    def register_output_transform(\n        self,\n        transform: Callable[\n            [Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]],\n            Dict[str, paddle.Tensor],\n        ],\n    ):\n        \"\"\"Register output transform.\n\n        Args:\n            transform (Callable[[Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]], Dict[str, paddle.Tensor]]):\n                Output transform of network, receive two single tensor dict(raw input\n                and raw output) and return a single tensor dict(transformed output).\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; def transform_out(in_, out):\n            ...     x = in_[\"x\"]\n            ...     y = out[\"y\"]\n            ...     u = 2.0 * x * y\n            ...     output_trans = {\"u\": u}\n            ...     return output_trans\n            &gt;&gt;&gt; # `MLP` inherits from `Arch`\n            &gt;&gt;&gt; model = ppsci.arch.MLP(\n            ...     input_keys=(\"x\",),\n            ...     output_keys=(\"y\",),\n            ...     num_layers=5,\n            ...     hidden_size=32)\n            &gt;&gt;&gt; model.register_output_transform(transform_out)\n            &gt;&gt;&gt; out = model({\"x\":paddle.rand([64, 64, 1])})\n            &gt;&gt;&gt; for k, v in out.items():\n            ...     print(f\"{k} {v.dtype} {v.shape}\")\n            u paddle.float32 [64, 64, 1]\n\n        \"\"\"\n        self._output_transform = transform\n\n    def freeze(self):\n        \"\"\"Freeze all parameters.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; model = ppsci.arch.Arch()\n            &gt;&gt;&gt; # freeze all parameters and make model `eval`\n            &gt;&gt;&gt; model.freeze()\n            &gt;&gt;&gt; assert not model.training\n            &gt;&gt;&gt; for p in model.parameters():\n            ...     assert p.stop_gradient\n\n        \"\"\"\n        for param in self.parameters():\n            param.stop_gradient = True\n\n        self.eval()\n\n    def unfreeze(self):\n        \"\"\"Unfreeze all parameters.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; model = ppsci.arch.Arch()\n            &gt;&gt;&gt; # unfreeze all parameters and make model `train`\n            &gt;&gt;&gt; model.unfreeze()\n            &gt;&gt;&gt; assert model.training\n            &gt;&gt;&gt; for p in model.parameters():\n            ...     assert not p.stop_gradient\n\n        \"\"\"\n        for param in self.parameters():\n            param.stop_gradient = False\n\n        self.train()\n\n    def __str__(self):\n        num_fc = 0\n        num_conv = 0\n        num_bn = 0\n        for layer in self.sublayers(include_self=True):\n            if isinstance(layer, nn.Linear):\n                num_fc += 1\n            elif isinstance(layer, (nn.Conv2D, nn.Conv3D, nn.Conv1D)):\n                num_conv += 1\n            elif isinstance(layer, (nn.BatchNorm, nn.BatchNorm2D, nn.BatchNorm3D)):\n                num_bn += 1\n\n        return \", \".join(\n            [\n                self.__class__.__name__,\n                f\"input_keys = {self.input_keys}\",\n                f\"output_keys = {self.output_keys}\",\n                f\"num_fc = {num_fc}\",\n                f\"num_conv = {num_conv}\",\n                f\"num_bn = {num_bn}\",\n                f\"num_params = {self.num_params}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.Arch.num_params","title":"<code>num_params: int</code>  <code>property</code>","text":"<p>Return number of parameters within network.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of parameters.</p>"},{"location":"zh/api/arch/#ppsci.arch.Arch.concat_to_tensor","title":"<code>concat_to_tensor(data_dict, keys, axis=-1)</code>  <code>staticmethod</code>","text":"<p>Concatenate tensors from dict in the order of given keys.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict[str, Tensor]</code> <p>Dict contains tensor.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Keys tensor fetched from.</p> required <code>axis</code> <code>int</code> <p>Axis concatenate at. Defaults to -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, ...]</code> <p>Tuple[paddle.Tensor, ...]: Concatenated tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.Arch()\n&gt;&gt;&gt; # fetch one tensor\n&gt;&gt;&gt; out = model.concat_to_tensor({'x':paddle.rand([64, 64, 1])}, ('x',))\n&gt;&gt;&gt; print(out.dtype, out.shape)\npaddle.float32 [64, 64, 1]\n&gt;&gt;&gt; # fetch more tensors\n&gt;&gt;&gt; out = model.concat_to_tensor(\n...     {'x1':paddle.rand([64, 64, 1]), 'x2':paddle.rand([64, 64, 1])},\n...     ('x1', 'x2'),\n...     axis=2)\n&gt;&gt;&gt; print(out.dtype, out.shape)\npaddle.float32 [64, 64, 2]\n</code></pre> Source code in <code>ppsci/arch/base.py</code> <pre><code>@staticmethod\ndef concat_to_tensor(\n    data_dict: Dict[str, paddle.Tensor], keys: Tuple[str, ...], axis=-1\n) -&gt; Tuple[paddle.Tensor, ...]:\n    \"\"\"Concatenate tensors from dict in the order of given keys.\n\n    Args:\n        data_dict (Dict[str, paddle.Tensor]): Dict contains tensor.\n        keys (Tuple[str, ...]): Keys tensor fetched from.\n        axis (int, optional): Axis concatenate at. Defaults to -1.\n\n    Returns:\n        Tuple[paddle.Tensor, ...]: Concatenated tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.Arch()\n        &gt;&gt;&gt; # fetch one tensor\n        &gt;&gt;&gt; out = model.concat_to_tensor({'x':paddle.rand([64, 64, 1])}, ('x',))\n        &gt;&gt;&gt; print(out.dtype, out.shape)\n        paddle.float32 [64, 64, 1]\n        &gt;&gt;&gt; # fetch more tensors\n        &gt;&gt;&gt; out = model.concat_to_tensor(\n        ...     {'x1':paddle.rand([64, 64, 1]), 'x2':paddle.rand([64, 64, 1])},\n        ...     ('x1', 'x2'),\n        ...     axis=2)\n        &gt;&gt;&gt; print(out.dtype, out.shape)\n        paddle.float32 [64, 64, 2]\n\n    \"\"\"\n    if len(keys) == 1:\n        return data_dict[keys[0]]\n    data = [data_dict[key] for key in keys]\n    return paddle.concat(data, axis)\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.Arch.freeze","title":"<code>freeze()</code>","text":"<p>Freeze all parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.Arch()\n&gt;&gt;&gt; # freeze all parameters and make model `eval`\n&gt;&gt;&gt; model.freeze()\n&gt;&gt;&gt; assert not model.training\n&gt;&gt;&gt; for p in model.parameters():\n...     assert p.stop_gradient\n</code></pre> Source code in <code>ppsci/arch/base.py</code> <pre><code>def freeze(self):\n    \"\"\"Freeze all parameters.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.Arch()\n        &gt;&gt;&gt; # freeze all parameters and make model `eval`\n        &gt;&gt;&gt; model.freeze()\n        &gt;&gt;&gt; assert not model.training\n        &gt;&gt;&gt; for p in model.parameters():\n        ...     assert p.stop_gradient\n\n    \"\"\"\n    for param in self.parameters():\n        param.stop_gradient = True\n\n    self.eval()\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.Arch.register_input_transform","title":"<code>register_input_transform(transform)</code>","text":"<p>Register input transform.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Dict[str, Tensor]], Dict[str, Tensor]]</code> <p>Input transform of network, receive a single tensor dict and return a single tensor dict.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; def transform_in(in_):\n...     x = in_[\"x\"]\n...     # transform input\n...     x_ = 2.0 * x\n...     input_trans = {\"2x\": x_}\n...     return input_trans\n&gt;&gt;&gt; # `MLP` inherits from `Arch`\n&gt;&gt;&gt; model = ppsci.arch.MLP(\n...     input_keys=(\"2x\",),\n...     output_keys=(\"y\",),\n...     num_layers=5,\n...     hidden_size=32)\n&gt;&gt;&gt; model.register_input_transform(transform_in)\n&gt;&gt;&gt; out = model({\"x\":paddle.rand([64, 64, 1])})\n&gt;&gt;&gt; for k, v in out.items():\n...     print(f\"{k} {v.dtype} {v.shape}\")\ny paddle.float32 [64, 64, 1]\n</code></pre> Source code in <code>ppsci/arch/base.py</code> <pre><code>def register_input_transform(\n    self,\n    transform: Callable[[Dict[str, paddle.Tensor]], Dict[str, paddle.Tensor]],\n):\n    \"\"\"Register input transform.\n\n    Args:\n        transform (Callable[[Dict[str, paddle.Tensor]], Dict[str, paddle.Tensor]]):\n            Input transform of network, receive a single tensor dict and return a single tensor dict.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; def transform_in(in_):\n        ...     x = in_[\"x\"]\n        ...     # transform input\n        ...     x_ = 2.0 * x\n        ...     input_trans = {\"2x\": x_}\n        ...     return input_trans\n        &gt;&gt;&gt; # `MLP` inherits from `Arch`\n        &gt;&gt;&gt; model = ppsci.arch.MLP(\n        ...     input_keys=(\"2x\",),\n        ...     output_keys=(\"y\",),\n        ...     num_layers=5,\n        ...     hidden_size=32)\n        &gt;&gt;&gt; model.register_input_transform(transform_in)\n        &gt;&gt;&gt; out = model({\"x\":paddle.rand([64, 64, 1])})\n        &gt;&gt;&gt; for k, v in out.items():\n        ...     print(f\"{k} {v.dtype} {v.shape}\")\n        y paddle.float32 [64, 64, 1]\n\n    \"\"\"\n    self._input_transform = transform\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.Arch.register_output_transform","title":"<code>register_output_transform(transform)</code>","text":"<p>Register output transform.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Dict[str, Tensor], Dict[str, Tensor]], Dict[str, Tensor]]</code> <p>Output transform of network, receive two single tensor dict(raw input and raw output) and return a single tensor dict(transformed output).</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; def transform_out(in_, out):\n...     x = in_[\"x\"]\n...     y = out[\"y\"]\n...     u = 2.0 * x * y\n...     output_trans = {\"u\": u}\n...     return output_trans\n&gt;&gt;&gt; # `MLP` inherits from `Arch`\n&gt;&gt;&gt; model = ppsci.arch.MLP(\n...     input_keys=(\"x\",),\n...     output_keys=(\"y\",),\n...     num_layers=5,\n...     hidden_size=32)\n&gt;&gt;&gt; model.register_output_transform(transform_out)\n&gt;&gt;&gt; out = model({\"x\":paddle.rand([64, 64, 1])})\n&gt;&gt;&gt; for k, v in out.items():\n...     print(f\"{k} {v.dtype} {v.shape}\")\nu paddle.float32 [64, 64, 1]\n</code></pre> Source code in <code>ppsci/arch/base.py</code> <pre><code>def register_output_transform(\n    self,\n    transform: Callable[\n        [Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]],\n        Dict[str, paddle.Tensor],\n    ],\n):\n    \"\"\"Register output transform.\n\n    Args:\n        transform (Callable[[Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]], Dict[str, paddle.Tensor]]):\n            Output transform of network, receive two single tensor dict(raw input\n            and raw output) and return a single tensor dict(transformed output).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; def transform_out(in_, out):\n        ...     x = in_[\"x\"]\n        ...     y = out[\"y\"]\n        ...     u = 2.0 * x * y\n        ...     output_trans = {\"u\": u}\n        ...     return output_trans\n        &gt;&gt;&gt; # `MLP` inherits from `Arch`\n        &gt;&gt;&gt; model = ppsci.arch.MLP(\n        ...     input_keys=(\"x\",),\n        ...     output_keys=(\"y\",),\n        ...     num_layers=5,\n        ...     hidden_size=32)\n        &gt;&gt;&gt; model.register_output_transform(transform_out)\n        &gt;&gt;&gt; out = model({\"x\":paddle.rand([64, 64, 1])})\n        &gt;&gt;&gt; for k, v in out.items():\n        ...     print(f\"{k} {v.dtype} {v.shape}\")\n        u paddle.float32 [64, 64, 1]\n\n    \"\"\"\n    self._output_transform = transform\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.Arch.split_to_dict","title":"<code>split_to_dict(data_tensor, keys, axis=-1)</code>  <code>staticmethod</code>","text":"<p>Split tensor and wrap into a dict by given keys.</p> <p>Parameters:</p> Name Type Description Default <code>data_tensor</code> <code>Tensor</code> <p>Tensor to be split.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Keys tensor mapping to.</p> required <code>axis</code> <code>int</code> <p>Axis split at. Defaults to -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict[str, paddle.Tensor]: Dict contains tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.Arch()\n&gt;&gt;&gt; # split one tensor\n&gt;&gt;&gt; out = model.split_to_dict(paddle.rand([64, 64, 1]), ('x',))\n&gt;&gt;&gt; for k, v in out.items():\n...     print(f\"{k} {v.dtype} {v.shape}\")\nx paddle.float32 [64, 64, 1]\n&gt;&gt;&gt; # split more tensors\n&gt;&gt;&gt; out = model.split_to_dict(paddle.rand([64, 64, 2]), ('x1', 'x2'), axis=2)\n&gt;&gt;&gt; for k, v in out.items():\n...     print(f\"{k} {v.dtype} {v.shape}\")\nx1 paddle.float32 [64, 64, 1]\nx2 paddle.float32 [64, 64, 1]\n</code></pre> Source code in <code>ppsci/arch/base.py</code> <pre><code>@staticmethod\ndef split_to_dict(\n    data_tensor: paddle.Tensor, keys: Tuple[str, ...], axis=-1\n) -&gt; Dict[str, paddle.Tensor]:\n    \"\"\"Split tensor and wrap into a dict by given keys.\n\n    Args:\n        data_tensor (paddle.Tensor): Tensor to be split.\n        keys (Tuple[str, ...]): Keys tensor mapping to.\n        axis (int, optional): Axis split at. Defaults to -1.\n\n    Returns:\n        Dict[str, paddle.Tensor]: Dict contains tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.Arch()\n        &gt;&gt;&gt; # split one tensor\n        &gt;&gt;&gt; out = model.split_to_dict(paddle.rand([64, 64, 1]), ('x',))\n        &gt;&gt;&gt; for k, v in out.items():\n        ...     print(f\"{k} {v.dtype} {v.shape}\")\n        x paddle.float32 [64, 64, 1]\n        &gt;&gt;&gt; # split more tensors\n        &gt;&gt;&gt; out = model.split_to_dict(paddle.rand([64, 64, 2]), ('x1', 'x2'), axis=2)\n        &gt;&gt;&gt; for k, v in out.items():\n        ...     print(f\"{k} {v.dtype} {v.shape}\")\n        x1 paddle.float32 [64, 64, 1]\n        x2 paddle.float32 [64, 64, 1]\n\n    \"\"\"\n    if len(keys) == 1:\n        return {keys[0]: data_tensor}\n    data = paddle.split(data_tensor, len(keys), axis=axis)\n    return {key: data[i] for i, key in enumerate(keys)}\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.Arch.unfreeze","title":"<code>unfreeze()</code>","text":"<p>Unfreeze all parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.Arch()\n&gt;&gt;&gt; # unfreeze all parameters and make model `train`\n&gt;&gt;&gt; model.unfreeze()\n&gt;&gt;&gt; assert model.training\n&gt;&gt;&gt; for p in model.parameters():\n...     assert not p.stop_gradient\n</code></pre> Source code in <code>ppsci/arch/base.py</code> <pre><code>def unfreeze(self):\n    \"\"\"Unfreeze all parameters.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.Arch()\n        &gt;&gt;&gt; # unfreeze all parameters and make model `train`\n        &gt;&gt;&gt; model.unfreeze()\n        &gt;&gt;&gt; assert model.training\n        &gt;&gt;&gt; for p in model.parameters():\n        ...     assert not p.stop_gradient\n\n    \"\"\"\n    for param in self.parameters():\n        param.stop_gradient = False\n\n    self.train()\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.AMGNet","title":"<code>AMGNet</code>","text":"<p>               Bases: <code>Layer</code></p> <p>A Multi-scale Graph neural Network model based on Encoder-Process-Decoder structure for flow field prediction.</p> <p>https://doi.org/10.1080/09540091.2022.2131737</p> <p>Code reference: https://github.com/baoshiaijhin/amgnet</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\", ).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"pred\", ).</p> required <code>input_dim</code> <code>int</code> <p>Number of input dimension.</p> required <code>output_dim</code> <code>int</code> <p>Number of output dimension.</p> required <code>latent_dim</code> <code>int</code> <p>Number of hidden(feature) dimension.</p> required <code>num_layers</code> <code>int</code> <p>Number of layer(s).</p> required <code>message_passing_aggregator</code> <code>Literal['sum']</code> <p>Message aggregator method in graph. Only \"sum\" available now.</p> required <code>message_passing_steps</code> <code>int</code> <p>Message passing steps in graph.</p> required <code>speed</code> <code>str</code> <p>Whether use vanilla method or fast method for graph_connectivity computation.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.AMGNet(\n...     (\"input\", ), (\"pred\", ), 5, 3, 64, 2, \"sum\", 6, \"norm\",\n... )\n</code></pre> Source code in <code>ppsci/arch/amgnet.py</code> <pre><code>class AMGNet(nn.Layer):\n    \"\"\"A Multi-scale Graph neural Network model\n    based on Encoder-Process-Decoder structure for flow field prediction.\n\n    https://doi.org/10.1080/09540091.2022.2131737\n\n    Code reference: https://github.com/baoshiaijhin/amgnet\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\", ).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"pred\", ).\n        input_dim (int): Number of input dimension.\n        output_dim (int): Number of output dimension.\n        latent_dim (int): Number of hidden(feature) dimension.\n        num_layers (int): Number of layer(s).\n        message_passing_aggregator (Literal[\"sum\"]): Message aggregator method in graph.\n            Only \"sum\" available now.\n        message_passing_steps (int): Message passing steps in graph.\n        speed (str): Whether use vanilla method or fast method for graph_connectivity\n            computation.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.AMGNet(\n        ...     (\"input\", ), (\"pred\", ), 5, 3, 64, 2, \"sum\", 6, \"norm\",\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        input_dim: int,\n        output_dim: int,\n        latent_dim: int,\n        num_layers: int,\n        message_passing_aggregator: Literal[\"sum\"],\n        message_passing_steps: int,\n        speed: Literal[\"norm\", \"fast\"],\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self._latent_dim = latent_dim\n        self.speed = speed\n        self._output_dim = output_dim\n        self._num_layers = num_layers\n\n        self.encoder = Encoder(input_dim, self._make_mlp, latent_dim=self._latent_dim)\n        self.processor = Processor(\n            make_mlp=self._make_mlp,\n            output_dim=self._latent_dim,\n            message_passing_steps=message_passing_steps,\n            message_passing_aggregator=message_passing_aggregator,\n            use_stochastic_message_passing=False,\n        )\n        self.post_processor = self._make_mlp(self._latent_dim, 128)\n        self.decoder = Decoder(\n            make_mlp=functools.partial(self._make_mlp, layer_norm=False),\n            output_dim=self._output_dim,\n        )\n\n    def forward(self, x: Dict[str, \"pgl.Graph\"]) -&gt; Dict[str, paddle.Tensor]:\n        graphs = x[self.input_keys[0]]\n        latent_graph = self.encoder(graphs)\n        x, p = self.processor(latent_graph, speed=self.speed)\n        node_features = self._spa_compute(x, p)\n        pred_field = self.decoder(node_features)\n        return {self.output_keys[0]: pred_field}\n\n    def _make_mlp(self, output_dim: int, input_dim: int = 5, layer_norm: bool = True):\n        widths = (self._latent_dim,) * self._num_layers + (output_dim,)\n        network = FullyConnectedLayer(input_dim, widths)\n        if layer_norm:\n            network = nn.Sequential(network, nn.LayerNorm(normalized_shape=widths[-1]))\n        return network\n\n    def _spa_compute(self, x: List[\"pgl.Graph\"], p):\n        j = len(x) - 1\n        node_features = x[j].x\n\n        for k in range(1, j + 1):\n            pos = p[-k]\n            fine_nodes = x[-(k + 1)].pos\n            feature = _knn_interpolate(node_features, pos, fine_nodes)\n            node_features = x[-(k + 1)].x + feature\n            node_features = self.post_processor(node_features)\n\n        return node_features\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Multi layer perceptron network.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"x\", \"y\", \"z\").</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"u\", \"v\", \"w\").</p> required <code>num_layers</code> <code>int</code> <p>Number of hidden layers.</p> required <code>hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of hidden size. An integer for all layers, or list of integer specify each layer's size.</p> required <code>activation</code> <code>str</code> <p>Name of activation function. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>skip_connection</code> <code>bool</code> <p>Whether to use skip connection. Defaults to False.</p> <code>False</code> <code>weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s). Defaults to False.</p> <code>False</code> <code>input_dim</code> <code>Optional[int]</code> <p>Number of input's dimension. Defaults to None.</p> <code>None</code> <code>output_dim</code> <code>Optional[int]</code> <p>Number of output's dimension. Defaults to None.</p> <code>None</code> <code>periods</code> <code>Optional[Dict[int, Tuple[float, bool]]]</code> <p>Period of each input key, input in given channel will be period embeded if specified, each tuple of periods list is [period, trainable]. Defaults to None.</p> <code>None</code> <code>fourier</code> <code>Optional[Dict[str, Union[float, int]]]</code> <p>Random fourier feature embedding, e.g. {'dim': 256, 'scale': 1.0}. Defaults to None.</p> <code>None</code> <code>random_weight</code> <code>Optional[Dict[str, float]]</code> <p>Mean and std of random weight factorization layer, e.g. {\"mean\": 0.5, \"std: 0.1\"}. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP(\n...     input_keys=(\"x\", \"y\"),\n...     output_keys=(\"u\", \"v\"),\n...     num_layers=5,\n...     hidden_size=128\n... )\n&gt;&gt;&gt; input_dict = {\"x\": paddle.rand([64, 1]),\n...               \"y\": paddle.rand([64, 1])}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict[\"u\"].shape)\n[64, 1]\n&gt;&gt;&gt; print(output_dict[\"v\"].shape)\n[64, 1]\n</code></pre> Source code in <code>ppsci/arch/mlp.py</code> <pre><code>class MLP(base.Arch):\n    \"\"\"Multi layer perceptron network.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"x\", \"y\", \"z\").\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"u\", \"v\", \"w\").\n        num_layers (int): Number of hidden layers.\n        hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size.\n            An integer for all layers, or list of integer specify each layer's size.\n        activation (str, optional): Name of activation function. Defaults to \"tanh\".\n        skip_connection (bool, optional): Whether to use skip connection. Defaults to False.\n        weight_norm (bool, optional): Whether to apply weight norm on parameter(s). Defaults to False.\n        input_dim (Optional[int]): Number of input's dimension. Defaults to None.\n        output_dim (Optional[int]): Number of output's dimension. Defaults to None.\n        periods (Optional[Dict[int, Tuple[float, bool]]]): Period of each input key,\n            input in given channel will be period embeded if specified, each tuple of\n            periods list is [period, trainable]. Defaults to None.\n        fourier (Optional[Dict[str, Union[float, int]]]): Random fourier feature embedding,\n            e.g. {'dim': 256, 'scale': 1.0}. Defaults to None.\n        random_weight (Optional[Dict[str, float]]): Mean and std of random weight\n            factorization layer, e.g. {\"mean\": 0.5, \"std: 0.1\"}. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP(\n        ...     input_keys=(\"x\", \"y\"),\n        ...     output_keys=(\"u\", \"v\"),\n        ...     num_layers=5,\n        ...     hidden_size=128\n        ... )\n        &gt;&gt;&gt; input_dict = {\"x\": paddle.rand([64, 1]),\n        ...               \"y\": paddle.rand([64, 1])}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict[\"u\"].shape)\n        [64, 1]\n        &gt;&gt;&gt; print(output_dict[\"v\"].shape)\n        [64, 1]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        num_layers: int,\n        hidden_size: Union[int, Tuple[int, ...]],\n        activation: str = \"tanh\",\n        skip_connection: bool = False,\n        weight_norm: bool = False,\n        input_dim: Optional[int] = None,\n        output_dim: Optional[int] = None,\n        periods: Optional[Dict[int, Tuple[float, bool]]] = None,\n        fourier: Optional[Dict[str, Union[float, int]]] = None,\n        random_weight: Optional[Dict[str, float]] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.linears = []\n        self.acts = []\n        self.periods = periods\n        self.fourier = fourier\n        if periods:\n            self.period_emb = PeriodEmbedding(periods)\n\n        if isinstance(hidden_size, (tuple, list)):\n            if num_layers is not None:\n                raise ValueError(\n                    \"num_layers should be None when hidden_size is specified\"\n                )\n        elif isinstance(hidden_size, int):\n            if not isinstance(num_layers, int):\n                raise ValueError(\n                    \"num_layers should be an int when hidden_size is an int\"\n                )\n            hidden_size = [hidden_size] * num_layers\n        else:\n            raise ValueError(\n                f\"hidden_size should be list of int or int, but got {type(hidden_size)}\"\n            )\n\n        # initialize FC layer(s)\n        cur_size = len(self.input_keys) if input_dim is None else input_dim\n        if input_dim is None and periods:\n            # period embeded channel(s) will be doubled automatically\n            # if input_dim is not specified\n            cur_size += len(periods)\n\n        if fourier:\n            self.fourier_emb = FourierEmbedding(\n                cur_size, fourier[\"dim\"], fourier[\"scale\"]\n            )\n            cur_size = fourier[\"dim\"]\n\n        for i, _size in enumerate(hidden_size):\n            if weight_norm:\n                self.linears.append(WeightNormLinear(cur_size, _size))\n            elif random_weight:\n                self.linears.append(\n                    RandomWeightFactorization(\n                        cur_size,\n                        _size,\n                        mean=random_weight[\"mean\"],\n                        std=random_weight[\"std\"],\n                    )\n                )\n            else:\n                self.linears.append(nn.Linear(cur_size, _size))\n\n            # initialize activation function\n            self.acts.append(\n                act_mod.get_activation(activation)\n                if activation != \"stan\"\n                else act_mod.get_activation(activation)(_size)\n            )\n            # special initialization for certain activation\n            # TODO: Adapt code below to a more elegant style\n            if activation == \"siren\":\n                if i == 0:\n                    act_mod.Siren.init_for_first_layer(self.linears[-1])\n                else:\n                    act_mod.Siren.init_for_hidden_layer(self.linears[-1])\n\n            cur_size = _size\n\n        self.linears = nn.LayerList(self.linears)\n        self.acts = nn.LayerList(self.acts)\n        if random_weight:\n            self.last_fc = RandomWeightFactorization(\n                cur_size,\n                len(self.output_keys) if output_dim is None else output_dim,\n                mean=random_weight[\"mean\"],\n                std=random_weight[\"std\"],\n            )\n        else:\n            self.last_fc = nn.Linear(\n                cur_size,\n                len(self.output_keys) if output_dim is None else output_dim,\n            )\n\n        self.skip_connection = skip_connection\n\n    def forward_tensor(self, x):\n        y = x\n        skip = None\n        for i, linear in enumerate(self.linears):\n            y = linear(y)\n            if self.skip_connection and i % 2 == 0:\n                if skip is not None:\n                    skip = y\n                    y = y + skip\n                else:\n                    skip = y\n            y = self.acts[i](y)\n\n        y = self.last_fc(y)\n\n        return y\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        if self.periods:\n            x = self.period_emb(x)\n\n        y = self.concat_to_tensor(x, self.input_keys, axis=-1)\n\n        if self.fourier:\n            y = self.fourier_emb(y)\n\n        y = self.forward_tensor(y)\n        y = self.split_to_dict(y, self.output_keys, axis=-1)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.ModifiedMLP","title":"<code>ModifiedMLP</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Modified Multi layer perceptron network.</p> <p>Understanding and mitigating gradient pathologies in physics-informed neural networks. https://arxiv.org/pdf/2001.04536.pdf.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"x\", \"y\", \"z\").</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"u\", \"v\", \"w\").</p> required <code>num_layers</code> <code>int</code> <p>Number of hidden layers.</p> required <code>hidden_size</code> <code>int</code> <p>Number of hidden size, an integer for all layers.</p> required <code>activation</code> <code>str</code> <p>Name of activation function. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>skip_connection</code> <code>bool</code> <p>Whether to use skip connection. Defaults to False.</p> <code>False</code> <code>weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s). Defaults to False.</p> <code>False</code> <code>input_dim</code> <code>Optional[int]</code> <p>Number of input's dimension. Defaults to None.</p> <code>None</code> <code>output_dim</code> <code>Optional[int]</code> <p>Number of output's dimension. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.ModifiedMLP(\n...     input_keys=(\"x\", \"y\"),\n...     output_keys=(\"u\", \"v\"),\n...     num_layers=5,\n...     hidden_size=128\n... )\n&gt;&gt;&gt; input_dict = {\"x\": paddle.rand([64, 1]),\n...               \"y\": paddle.rand([64, 1])}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict[\"u\"].shape)\n[64, 1]\n&gt;&gt;&gt; print(output_dict[\"v\"].shape)\n[64, 1]\n</code></pre> Source code in <code>ppsci/arch/mlp.py</code> <pre><code>class ModifiedMLP(base.Arch):\n    \"\"\"Modified Multi layer perceptron network.\n\n    Understanding and mitigating gradient pathologies in physics-informed\n    neural networks. https://arxiv.org/pdf/2001.04536.pdf.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"x\", \"y\", \"z\").\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"u\", \"v\", \"w\").\n        num_layers (int): Number of hidden layers.\n        hidden_size (int): Number of hidden size, an integer for all layers.\n        activation (str, optional): Name of activation function. Defaults to \"tanh\".\n        skip_connection (bool, optional): Whether to use skip connection. Defaults to False.\n        weight_norm (bool, optional): Whether to apply weight norm on parameter(s). Defaults to False.\n        input_dim (Optional[int]): Number of input's dimension. Defaults to None.\n        output_dim (Optional[int]): Number of output's dimension. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.ModifiedMLP(\n        ...     input_keys=(\"x\", \"y\"),\n        ...     output_keys=(\"u\", \"v\"),\n        ...     num_layers=5,\n        ...     hidden_size=128\n        ... )\n        &gt;&gt;&gt; input_dict = {\"x\": paddle.rand([64, 1]),\n        ...               \"y\": paddle.rand([64, 1])}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict[\"u\"].shape)\n        [64, 1]\n        &gt;&gt;&gt; print(output_dict[\"v\"].shape)\n        [64, 1]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        num_layers: int,\n        hidden_size: int,\n        activation: str = \"tanh\",\n        skip_connection: bool = False,\n        weight_norm: bool = False,\n        input_dim: Optional[int] = None,\n        output_dim: Optional[int] = None,\n        periods: Optional[Dict[int, Tuple[float, bool]]] = None,\n        fourier: Optional[Dict[str, Union[float, int]]] = None,\n        random_weight: Optional[Dict[str, float]] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.linears = []\n        self.acts = []\n        self.periods = periods\n        self.fourier = fourier\n        if periods:\n            self.period_emb = PeriodEmbedding(periods)\n        if isinstance(hidden_size, int):\n            if not isinstance(num_layers, int):\n                raise ValueError(\"num_layers should be an int\")\n            hidden_size = [hidden_size] * num_layers\n        else:\n            raise ValueError(f\"hidden_size should be int, but got {type(hidden_size)}\")\n\n        # initialize FC layer(s)\n        cur_size = len(self.input_keys) if input_dim is None else input_dim\n        if input_dim is None and periods:\n            # period embeded channel(s) will be doubled automatically\n            # if input_dim is not specified\n            cur_size += len(periods)\n\n        if fourier:\n            self.fourier_emb = FourierEmbedding(\n                cur_size, fourier[\"dim\"], fourier[\"scale\"]\n            )\n            cur_size = fourier[\"dim\"]\n\n        self.embed_u = nn.Sequential(\n            (\n                WeightNormLinear(cur_size, hidden_size[0])\n                if weight_norm\n                else (\n                    nn.Linear(cur_size, hidden_size[0])\n                    if random_weight is None\n                    else RandomWeightFactorization(\n                        cur_size,\n                        hidden_size[0],\n                        mean=random_weight[\"mean\"],\n                        std=random_weight[\"std\"],\n                    )\n                )\n            ),\n            (\n                act_mod.get_activation(activation)\n                if activation != \"stan\"\n                else act_mod.get_activation(activation)(hidden_size[0])\n            ),\n        )\n        self.embed_v = nn.Sequential(\n            (\n                WeightNormLinear(cur_size, hidden_size[0])\n                if weight_norm\n                else (\n                    nn.Linear(cur_size, hidden_size[0])\n                    if random_weight is None\n                    else RandomWeightFactorization(\n                        cur_size,\n                        hidden_size[0],\n                        mean=random_weight[\"mean\"],\n                        std=random_weight[\"std\"],\n                    )\n                )\n            ),\n            (\n                act_mod.get_activation(activation)\n                if activation != \"stan\"\n                else act_mod.get_activation(activation)(hidden_size[0])\n            ),\n        )\n\n        for i, _size in enumerate(hidden_size):\n            if weight_norm:\n                self.linears.append(WeightNormLinear(cur_size, _size))\n            elif random_weight:\n                self.linears.append(\n                    RandomWeightFactorization(\n                        cur_size,\n                        _size,\n                        mean=random_weight[\"mean\"],\n                        std=random_weight[\"std\"],\n                    )\n                )\n            else:\n                self.linears.append(nn.Linear(cur_size, _size))\n\n            # initialize activation function\n            self.acts.append(\n                act_mod.get_activation(activation)\n                if activation != \"stan\"\n                else act_mod.get_activation(activation)(_size)\n            )\n            # special initialization for certain activation\n            # TODO: Adapt code below to a more elegant style\n            if activation == \"siren\":\n                if i == 0:\n                    act_mod.Siren.init_for_first_layer(self.linears[-1])\n                else:\n                    act_mod.Siren.init_for_hidden_layer(self.linears[-1])\n\n            cur_size = _size\n\n        self.linears = nn.LayerList(self.linears)\n        self.acts = nn.LayerList(self.acts)\n        if random_weight:\n            self.last_fc = RandomWeightFactorization(\n                cur_size,\n                len(self.output_keys) if output_dim is None else output_dim,\n                mean=random_weight[\"mean\"],\n                std=random_weight[\"std\"],\n            )\n        else:\n            self.last_fc = nn.Linear(\n                cur_size,\n                len(self.output_keys) if output_dim is None else output_dim,\n            )\n\n        self.skip_connection = skip_connection\n\n    def forward_tensor(self, x):\n        u = self.embed_u(x)\n        v = self.embed_v(x)\n\n        y = x\n        skip = None\n        for i, linear in enumerate(self.linears):\n            y = linear(y)\n            y = self.acts[i](y)\n            y = y * u + (1 - y) * v\n            if self.skip_connection and i % 2 == 0:\n                if skip is not None:\n                    skip = y\n                    y = y + skip\n                else:\n                    skip = y\n\n        y = self.last_fc(y)\n\n        return y\n\n    def forward(self, x):\n        x_identity = x\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        if self.periods:\n            x = self.period_emb(x)\n\n        y = self.concat_to_tensor(x, self.input_keys, axis=-1)\n\n        if self.fourier:\n            y = self.fourier_emb(y)\n\n        y = self.forward_tensor(y)\n        y = self.split_to_dict(y, self.output_keys, axis=-1)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x_identity, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.PirateNet","title":"<code>PirateNet</code>","text":"<p>               Bases: <code>Arch</code></p> <p>PirateNet.</p> <p>PIRATENETS: PHYSICS-INFORMED DEEP LEARNING WITHRESIDUAL ADAPTIVE NETWORKS</p> \\[ \\begin{align*}     \\Phi(\\mathbf{x}) &amp;= \\left[\\begin{array}{l}     \\cos (\\mathbf{B} \\mathbf{x}) \\\\     \\sin (\\mathbf{B} \\mathbf{x})     \\end{array}\\right] \\\\     \\mathbf{f}^{(l)} &amp;= \\sigma\\left(\\mathbf{W}_1^{(l)} \\mathbf{x}^{(l)}+\\mathbf{b}_1^{(l)}\\right) \\\\     \\mathbf{z}_1^{(l)} &amp;= \\mathbf{f}^{(l)} \\odot \\mathbf{U}+\\left(1-\\mathbf{f}^{(l)}\\right) \\odot \\mathbf{V} \\\\     \\mathbf{g}^{(l)} &amp;= \\sigma\\left(\\mathbf{W}_2^{(l)} \\mathbf{z}_1^{(l)}+\\mathbf{b}_2^{(l)}\\right) \\\\     \\mathbf{z}_2^{(l)} &amp;= \\mathbf{g}^{(l)} \\odot \\mathbf{U}+\\left(1-\\mathbf{g}^{(l)}\\right) \\odot \\mathbf{V} \\\\     \\mathbf{h}^{(l)} &amp;= \\sigma\\left(\\mathbf{W}_3^{(l)} \\mathbf{z}_2^{(l)}+\\mathbf{b}_3^{(l)}\\right) \\\\     \\mathbf{x}^{(l+1)} &amp;= \\text{PirateBlock}^{(l)}\\left(\\mathbf{x}^{(l)}\\right), l=1...L-1\\\\     \\mathbf{u}_\\theta &amp;= \\mathbf{W}^{(L+1)} \\mathbf{x}^{(L)} \\end{align*} \\] <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"x\", \"y\", \"z\").</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"u\", \"v\", \"w\").</p> required <code>num_blocks</code> <code>int</code> <p>Number of PirateBlocks.</p> required <code>hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of hidden size. An integer for all layers, or list of integer specify each layer's size.</p> required <code>activation</code> <code>str</code> <p>Name of activation function. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s). Defaults to False.</p> <code>False</code> <code>input_dim</code> <code>Optional[int]</code> <p>Number of input's dimension. Defaults to None.</p> <code>None</code> <code>output_dim</code> <code>Optional[int]</code> <p>Number of output's dimension. Defaults to None.</p> <code>None</code> <code>periods</code> <code>Optional[Dict[int, Tuple[float, bool]]]</code> <p>Period of each input key, input in given channel will be period embeded if specified, each tuple of periods list is [period, trainable]. Defaults to None.</p> <code>None</code> <code>fourier</code> <code>Optional[Dict[str, Union[float, int]]]</code> <p>Random fourier feature embedding, e.g. {'dim': 256, 'scale': 1.0}. Defaults to None.</p> <code>None</code> <code>random_weight</code> <code>Optional[Dict[str, float]]</code> <p>Mean and std of random weight factorization layer, e.g. {\"mean\": 0.5, \"std: 0.1\"}. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.PirateNet(\n...     input_keys=(\"x\", \"y\"),\n...     output_keys=(\"u\", \"v\"),\n...     num_blocks=3,\n...     hidden_size=256,\n...     fourier={'dim': 256, 'scale': 1.0},\n... )\n&gt;&gt;&gt; input_dict = {\"x\": paddle.rand([64, 1]),\n...               \"y\": paddle.rand([64, 1])}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict[\"u\"].shape)\n[64, 1]\n&gt;&gt;&gt; print(output_dict[\"v\"].shape)\n[64, 1]\n</code></pre> Source code in <code>ppsci/arch/mlp.py</code> <pre><code>class PirateNet(base.Arch):\n    r\"\"\"PirateNet.\n\n    [PIRATENETS: PHYSICS-INFORMED DEEP LEARNING WITHRESIDUAL ADAPTIVE NETWORKS](https://arxiv.org/pdf/2402.00326.pdf)\n\n    $$\n    \\begin{align*}\n        \\Phi(\\mathbf{x}) &amp;= \\left[\\begin{array}{l}\n        \\cos (\\mathbf{B} \\mathbf{x}) \\\\\n        \\sin (\\mathbf{B} \\mathbf{x})\n        \\end{array}\\right] \\\\\n        \\mathbf{f}^{(l)} &amp;= \\sigma\\left(\\mathbf{W}_1^{(l)} \\mathbf{x}^{(l)}+\\mathbf{b}_1^{(l)}\\right) \\\\\n        \\mathbf{z}_1^{(l)} &amp;= \\mathbf{f}^{(l)} \\odot \\mathbf{U}+\\left(1-\\mathbf{f}^{(l)}\\right) \\odot \\mathbf{V} \\\\\n        \\mathbf{g}^{(l)} &amp;= \\sigma\\left(\\mathbf{W}_2^{(l)} \\mathbf{z}_1^{(l)}+\\mathbf{b}_2^{(l)}\\right) \\\\\n        \\mathbf{z}_2^{(l)} &amp;= \\mathbf{g}^{(l)} \\odot \\mathbf{U}+\\left(1-\\mathbf{g}^{(l)}\\right) \\odot \\mathbf{V} \\\\\n        \\mathbf{h}^{(l)} &amp;= \\sigma\\left(\\mathbf{W}_3^{(l)} \\mathbf{z}_2^{(l)}+\\mathbf{b}_3^{(l)}\\right) \\\\\n        \\mathbf{x}^{(l+1)} &amp;= \\text{PirateBlock}^{(l)}\\left(\\mathbf{x}^{(l)}\\right), l=1...L-1\\\\\n        \\mathbf{u}_\\theta &amp;= \\mathbf{W}^{(L+1)} \\mathbf{x}^{(L)}\n    \\end{align*}\n    $$\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"x\", \"y\", \"z\").\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"u\", \"v\", \"w\").\n        num_blocks (int): Number of PirateBlocks.\n        hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size.\n            An integer for all layers, or list of integer specify each layer's size.\n        activation (str, optional): Name of activation function. Defaults to \"tanh\".\n        weight_norm (bool, optional): Whether to apply weight norm on parameter(s). Defaults to False.\n        input_dim (Optional[int]): Number of input's dimension. Defaults to None.\n        output_dim (Optional[int]): Number of output's dimension. Defaults to None.\n        periods (Optional[Dict[int, Tuple[float, bool]]]): Period of each input key,\n            input in given channel will be period embeded if specified, each tuple of\n            periods list is [period, trainable]. Defaults to None.\n        fourier (Optional[Dict[str, Union[float, int]]]): Random fourier feature embedding,\n            e.g. {'dim': 256, 'scale': 1.0}. Defaults to None.\n        random_weight (Optional[Dict[str, float]]): Mean and std of random weight\n            factorization layer, e.g. {\"mean\": 0.5, \"std: 0.1\"}. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.PirateNet(\n        ...     input_keys=(\"x\", \"y\"),\n        ...     output_keys=(\"u\", \"v\"),\n        ...     num_blocks=3,\n        ...     hidden_size=256,\n        ...     fourier={'dim': 256, 'scale': 1.0},\n        ... )\n        &gt;&gt;&gt; input_dict = {\"x\": paddle.rand([64, 1]),\n        ...               \"y\": paddle.rand([64, 1])}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict[\"u\"].shape)\n        [64, 1]\n        &gt;&gt;&gt; print(output_dict[\"v\"].shape)\n        [64, 1]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        num_blocks: int,\n        hidden_size: int,\n        activation: str = \"tanh\",\n        weight_norm: bool = False,\n        input_dim: Optional[int] = None,\n        output_dim: Optional[int] = None,\n        periods: Optional[Dict[int, Tuple[float, bool]]] = None,\n        fourier: Optional[Dict[str, Union[float, int]]] = None,\n        random_weight: Optional[Dict[str, float]] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.blocks = []\n        self.periods = periods\n        self.fourier = fourier\n        if periods:\n            self.period_emb = PeriodEmbedding(periods)\n\n        if isinstance(hidden_size, int):\n            if not isinstance(num_blocks, int):\n                raise ValueError(\"num_blocks should be an int\")\n            hidden_size = [hidden_size] * num_blocks\n        else:\n            raise ValueError(f\"hidden_size should be int, but got {type(hidden_size)}\")\n\n        # initialize FC layer(s)\n        cur_size = len(self.input_keys) if input_dim is None else input_dim\n        if input_dim is None and periods:\n            # period embeded channel(s) will be doubled automatically\n            # if input_dim is not specified\n            cur_size += len(periods)\n\n        if fourier:\n            self.fourier_emb = FourierEmbedding(\n                cur_size, fourier[\"dim\"], fourier[\"scale\"]\n            )\n            cur_size = fourier[\"dim\"]\n\n        self.embed_u = nn.Sequential(\n            (\n                WeightNormLinear(cur_size, hidden_size[0])\n                if weight_norm\n                else (\n                    nn.Linear(cur_size, hidden_size[0])\n                    if random_weight is None\n                    else RandomWeightFactorization(\n                        cur_size,\n                        hidden_size[0],\n                        mean=random_weight[\"mean\"],\n                        std=random_weight[\"std\"],\n                    )\n                )\n            ),\n            (\n                act_mod.get_activation(activation)\n                if activation != \"stan\"\n                else act_mod.get_activation(activation)(hidden_size[0])\n            ),\n        )\n        self.embed_v = nn.Sequential(\n            (\n                WeightNormLinear(cur_size, hidden_size[0])\n                if weight_norm\n                else (\n                    nn.Linear(cur_size, hidden_size[0])\n                    if random_weight is None\n                    else RandomWeightFactorization(\n                        cur_size,\n                        hidden_size[0],\n                        mean=random_weight[\"mean\"],\n                        std=random_weight[\"std\"],\n                    )\n                )\n            ),\n            (\n                act_mod.get_activation(activation)\n                if activation != \"stan\"\n                else act_mod.get_activation(activation)(hidden_size[0])\n            ),\n        )\n\n        for i, _size in enumerate(hidden_size):\n            self.blocks.append(\n                PirateNetBlock(\n                    cur_size,\n                    activation=activation,\n                    random_weight=random_weight,\n                )\n            )\n            cur_size = _size\n\n        self.blocks = nn.LayerList(self.blocks)\n        if random_weight:\n            self.last_fc = RandomWeightFactorization(\n                cur_size,\n                len(self.output_keys) if output_dim is None else output_dim,\n                mean=random_weight[\"mean\"],\n                std=random_weight[\"std\"],\n            )\n        else:\n            self.last_fc = nn.Linear(\n                cur_size,\n                len(self.output_keys) if output_dim is None else output_dim,\n            )\n\n    def forward_tensor(self, x):\n        u = self.embed_u(x)\n        v = self.embed_v(x)\n\n        y = x\n        for i, block in enumerate(self.blocks):\n            y = block(y, u, v)\n\n        y = self.last_fc(y)\n        return y\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        if self.periods:\n            x = self.period_emb(x)\n\n        y = self.concat_to_tensor(x, self.input_keys, axis=-1)\n\n        if self.fourier:\n            y = self.fourier_emb(y)\n\n        y = self.forward_tensor(y)\n        y = self.split_to_dict(y, self.output_keys, axis=-1)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.DeepONet","title":"<code>DeepONet</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Deep operator network.</p> <p>Lu et al. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nat Mach Intell, 2021.</p> <p>Parameters:</p> Name Type Description Default <code>u_key</code> <code>str</code> <p>Name of function data for input function u(x).</p> required <code>y_key</code> <code>str</code> <p>Name of location data for input function G(u).</p> required <code>G_key</code> <code>str</code> <p>Output name of predicted G(u)(y).</p> required <code>num_loc</code> <code>int</code> <p>Number of sampled u(x), i.e. <code>m</code> in paper.</p> required <code>num_features</code> <code>int</code> <p>Number of features extracted from u(x), same for y.</p> required <code>branch_num_layers</code> <code>int</code> <p>Number of hidden layers of branch net.</p> required <code>trunk_num_layers</code> <code>int</code> <p>Number of hidden layers of trunk net.</p> required <code>branch_hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of hidden size of branch net. An integer for all layers, or list of integer specify each layer's size.</p> required <code>trunk_hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of hidden size of trunk net. An integer for all layers, or list of integer specify each layer's size.</p> required <code>branch_skip_connection</code> <code>bool</code> <p>Whether to use skip connection for branch net. Defaults to False.</p> <code>False</code> <code>trunk_skip_connection</code> <code>bool</code> <p>Whether to use skip connection for trunk net. Defaults to False.</p> <code>False</code> <code>branch_activation</code> <code>str</code> <p>Name of activation function. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>trunk_activation</code> <code>str</code> <p>Name of activation function. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>branch_weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s) for branch net. Defaults to False.</p> <code>False</code> <code>trunk_weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s) for trunk net. Defaults to False.</p> <code>False</code> <code>use_bias</code> <code>bool</code> <p>Whether to add bias on predicted G(u)(y). Defaults to True.</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.DeepONet(\n...     \"u\", \"y\", \"G\",\n...     100, 40,\n...     1, 1,\n...     40, 40,\n...     branch_activation=\"relu\", trunk_activation=\"relu\",\n...     use_bias=True,\n... )\n&gt;&gt;&gt; input_dict = {\"u\": paddle.rand([200, 100]),\n...               \"y\": paddle.rand([200, 1])}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict[\"G\"].shape)\n[200, 1]\n</code></pre> Source code in <code>ppsci/arch/deeponet.py</code> <pre><code>class DeepONet(base.Arch):\n    \"\"\"Deep operator network.\n\n    [Lu et al. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nat Mach Intell, 2021.](https://doi.org/10.1038/s42256-021-00302-5)\n\n    Args:\n        u_key (str): Name of function data for input function u(x).\n        y_key (str): Name of location data for input function G(u).\n        G_key (str): Output name of predicted G(u)(y).\n        num_loc (int): Number of sampled u(x), i.e. `m` in paper.\n        num_features (int): Number of features extracted from u(x), same for y.\n        branch_num_layers (int): Number of hidden layers of branch net.\n        trunk_num_layers (int): Number of hidden layers of trunk net.\n        branch_hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size of branch net.\n            An integer for all layers, or list of integer specify each layer's size.\n        trunk_hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size of trunk net.\n            An integer for all layers, or list of integer specify each layer's size.\n        branch_skip_connection (bool, optional): Whether to use skip connection for branch net. Defaults to False.\n        trunk_skip_connection (bool, optional): Whether to use skip connection for trunk net. Defaults to False.\n        branch_activation (str, optional): Name of activation function. Defaults to \"tanh\".\n        trunk_activation (str, optional): Name of activation function. Defaults to \"tanh\".\n        branch_weight_norm (bool, optional): Whether to apply weight norm on parameter(s) for branch net. Defaults to False.\n        trunk_weight_norm (bool, optional): Whether to apply weight norm on parameter(s) for trunk net. Defaults to False.\n        use_bias (bool, optional): Whether to add bias on predicted G(u)(y). Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.DeepONet(\n        ...     \"u\", \"y\", \"G\",\n        ...     100, 40,\n        ...     1, 1,\n        ...     40, 40,\n        ...     branch_activation=\"relu\", trunk_activation=\"relu\",\n        ...     use_bias=True,\n        ... )\n        &gt;&gt;&gt; input_dict = {\"u\": paddle.rand([200, 100]),\n        ...               \"y\": paddle.rand([200, 1])}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict[\"G\"].shape)\n        [200, 1]\n    \"\"\"\n\n    def __init__(\n        self,\n        u_key: str,\n        y_key: str,\n        G_key: str,\n        num_loc: int,\n        num_features: int,\n        branch_num_layers: int,\n        trunk_num_layers: int,\n        branch_hidden_size: Union[int, Tuple[int, ...]],\n        trunk_hidden_size: Union[int, Tuple[int, ...]],\n        branch_skip_connection: bool = False,\n        trunk_skip_connection: bool = False,\n        branch_activation: str = \"tanh\",\n        trunk_activation: str = \"tanh\",\n        branch_weight_norm: bool = False,\n        trunk_weight_norm: bool = False,\n        use_bias: bool = True,\n    ):\n        super().__init__()\n        self.u_key = u_key\n        self.y_key = y_key\n        self.input_keys = (u_key, y_key)\n        self.output_keys = (G_key,)\n\n        self.branch_net = mlp.MLP(\n            (self.u_key,),\n            (\"b\",),\n            branch_num_layers,\n            branch_hidden_size,\n            branch_activation,\n            branch_skip_connection,\n            branch_weight_norm,\n            input_dim=num_loc,\n            output_dim=num_features,\n        )\n\n        self.trunk_net = mlp.MLP(\n            (self.y_key,),\n            (\"t\",),\n            trunk_num_layers,\n            trunk_hidden_size,\n            trunk_activation,\n            trunk_skip_connection,\n            trunk_weight_norm,\n            input_dim=1,\n            output_dim=num_features,\n        )\n        self.trunk_act = act_mod.get_activation(trunk_activation)\n\n        self.use_bias = use_bias\n        if use_bias:\n            # register bias to parameter for updating in optimizer and storage\n            self.b = self.create_parameter(\n                shape=(1,),\n                attr=nn.initializer.Constant(0.0),\n            )\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        # Branch net to encode the input function\n        u_features = self.branch_net(x)[self.branch_net.output_keys[0]]\n\n        # Trunk net to encode the domain of the output function\n        y_features = self.trunk_net(x)\n        y_features = self.trunk_act(y_features[self.trunk_net.output_keys[0]])\n\n        # Dot product\n        G_u = paddle.einsum(\"bi,bi-&gt;b\", u_features, y_features)  # [batch_size, ]\n        G_u = paddle.reshape(G_u, [-1, 1])  # reshape [batch_size, ] to [batch_size, 1]\n\n        # Add bias\n        if self.use_bias:\n            G_u += self.b\n\n        result_dict = {\n            self.output_keys[0]: G_u,\n        }\n        if self._output_transform is not None:\n            result_dict = self._output_transform(x, result_dict)\n\n        return result_dict\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.DeepPhyLSTM","title":"<code>DeepPhyLSTM</code>","text":"<p>               Bases: <code>Arch</code></p> <p>DeepPhyLSTM init function.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>The input size.</p> required <code>output_size</code> <code>int</code> <p>The output size.</p> required <code>hidden_size</code> <code>int</code> <p>The hidden size. Defaults to 100.</p> <code>100</code> <code>model_type</code> <code>int</code> <p>The model type, value is 2 or 3, 2 indicates having two sub-models, 3 indicates having three submodels. Defaults to 2.</p> <code>2</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; # model_type is `2`\n&gt;&gt;&gt; model = ppsci.arch.DeepPhyLSTM(\n...     input_size=16,\n...     output_size=1,\n...     hidden_size=100,\n...     model_type=2)\n&gt;&gt;&gt; out = model(\n...     {\"ag\":paddle.rand([64, 16, 16]),\n...     \"ag_c\":paddle.rand([64, 16, 16]),\n...     \"phi\":paddle.rand([1, 16, 16])})\n&gt;&gt;&gt; for k, v in out.items():\n...     print(f\"{k} {v.dtype} {v.shape}\")\neta_pred paddle.float32 [64, 16, 1]\neta_dot_pred paddle.float32 [64, 16, 1]\ng_pred paddle.float32 [64, 16, 1]\neta_t_pred_c paddle.float32 [64, 16, 1]\neta_dot_pred_c paddle.float32 [64, 16, 1]\nlift_pred_c paddle.float32 [64, 16, 1]\n&gt;&gt;&gt; # model_type is `3`\n&gt;&gt;&gt; model = ppsci.arch.DeepPhyLSTM(\n...     input_size=16,\n...     output_size=1,\n...     hidden_size=100,\n...     model_type=3)\n&gt;&gt;&gt; out = model(\n...     {\"ag\":paddle.rand([64, 16, 1]),\n...     \"ag_c\":paddle.rand([64, 16, 1]),\n...     \"phi\":paddle.rand([1, 16, 16])})\n&gt;&gt;&gt; for k, v in out.items():\n...     print(f\"{k} {v.dtype} {v.shape}\")\neta_pred paddle.float32 [64, 16, 1]\neta_dot_pred paddle.float32 [64, 16, 1]\ng_pred paddle.float32 [64, 16, 1]\neta_t_pred_c paddle.float32 [64, 16, 1]\neta_dot_pred_c paddle.float32 [64, 16, 1]\nlift_pred_c paddle.float32 [64, 16, 1]\ng_t_pred_c paddle.float32 [64, 16, 1]\ng_dot_pred_c paddle.float32 [64, 16, 1]\n</code></pre> Source code in <code>ppsci/arch/phylstm.py</code> <pre><code>class DeepPhyLSTM(base.Arch):\n    \"\"\"DeepPhyLSTM init function.\n\n    Args:\n        input_size (int): The input size.\n        output_size (int): The output size.\n        hidden_size (int, optional): The hidden size. Defaults to 100.\n        model_type (int, optional): The model type, value is 2 or 3, 2 indicates having two sub-models, 3 indicates having three submodels. Defaults to 2.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; # model_type is `2`\n        &gt;&gt;&gt; model = ppsci.arch.DeepPhyLSTM(\n        ...     input_size=16,\n        ...     output_size=1,\n        ...     hidden_size=100,\n        ...     model_type=2)\n        &gt;&gt;&gt; out = model(\n        ...     {\"ag\":paddle.rand([64, 16, 16]),\n        ...     \"ag_c\":paddle.rand([64, 16, 16]),\n        ...     \"phi\":paddle.rand([1, 16, 16])})\n        &gt;&gt;&gt; for k, v in out.items():\n        ...     print(f\"{k} {v.dtype} {v.shape}\")\n        eta_pred paddle.float32 [64, 16, 1]\n        eta_dot_pred paddle.float32 [64, 16, 1]\n        g_pred paddle.float32 [64, 16, 1]\n        eta_t_pred_c paddle.float32 [64, 16, 1]\n        eta_dot_pred_c paddle.float32 [64, 16, 1]\n        lift_pred_c paddle.float32 [64, 16, 1]\n        &gt;&gt;&gt; # model_type is `3`\n        &gt;&gt;&gt; model = ppsci.arch.DeepPhyLSTM(\n        ...     input_size=16,\n        ...     output_size=1,\n        ...     hidden_size=100,\n        ...     model_type=3)\n        &gt;&gt;&gt; out = model(\n        ...     {\"ag\":paddle.rand([64, 16, 1]),\n        ...     \"ag_c\":paddle.rand([64, 16, 1]),\n        ...     \"phi\":paddle.rand([1, 16, 16])})\n        &gt;&gt;&gt; for k, v in out.items():\n        ...     print(f\"{k} {v.dtype} {v.shape}\")\n        eta_pred paddle.float32 [64, 16, 1]\n        eta_dot_pred paddle.float32 [64, 16, 1]\n        g_pred paddle.float32 [64, 16, 1]\n        eta_t_pred_c paddle.float32 [64, 16, 1]\n        eta_dot_pred_c paddle.float32 [64, 16, 1]\n        lift_pred_c paddle.float32 [64, 16, 1]\n        g_t_pred_c paddle.float32 [64, 16, 1]\n        g_dot_pred_c paddle.float32 [64, 16, 1]\n    \"\"\"\n\n    def __init__(self, input_size, output_size, hidden_size=100, model_type=2):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.model_type = model_type\n\n        if self.model_type == 2:\n            self.lstm_model = nn.Sequential(\n                nn.LSTM(input_size, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.Linear(hidden_size, hidden_size),\n                nn.Linear(hidden_size, 3 * output_size),\n            )\n\n            self.lstm_model_f = nn.Sequential(\n                nn.LSTM(3 * output_size, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.Linear(hidden_size, hidden_size),\n                nn.Linear(hidden_size, output_size),\n            )\n        elif self.model_type == 3:\n            self.lstm_model = nn.Sequential(\n                nn.LSTM(1, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.Linear(hidden_size, 3 * output_size),\n            )\n\n            self.lstm_model_f = nn.Sequential(\n                nn.LSTM(3 * output_size, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.Linear(hidden_size, output_size),\n            )\n\n            self.lstm_model_g = nn.Sequential(\n                nn.LSTM(2 * output_size, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.LSTM(hidden_size, hidden_size),\n                nn.ReLU(),\n                nn.Linear(hidden_size, output_size),\n            )\n        else:\n            raise ValueError(f\"model_type should be 2 or 3, but got {model_type}\")\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        if self.model_type == 2:\n            result_dict = self._forward_type_2(x)\n        elif self.model_type == 3:\n            result_dict = self._forward_type_3(x)\n        if self._output_transform is not None:\n            result_dict = self._output_transform(x, result_dict)\n        return result_dict\n\n    def _forward_type_2(self, x):\n        output = self.lstm_model(x[\"ag\"])\n        eta_pred = output[:, :, 0 : self.output_size]\n        eta_dot_pred = output[:, :, self.output_size : 2 * self.output_size]\n        g_pred = output[:, :, 2 * self.output_size :]\n\n        # for ag_c\n        output_c = self.lstm_model(x[\"ag_c\"])\n        eta_pred_c = output_c[:, :, 0 : self.output_size]\n        eta_dot_pred_c = output_c[:, :, self.output_size : 2 * self.output_size]\n        g_pred_c = output_c[:, :, 2 * self.output_size :]\n        eta_t_pred_c = paddle.matmul(x[\"phi\"], eta_pred_c)\n        eta_tt_pred_c = paddle.matmul(x[\"phi\"], eta_dot_pred_c)\n        eta_dot1_pred_c = eta_dot_pred_c[:, :, 0:1]\n        tmp = paddle.concat([eta_pred_c, eta_dot1_pred_c, g_pred_c], 2)\n        f = self.lstm_model_f(tmp)\n        lift_pred_c = eta_tt_pred_c + f\n\n        return {\n            \"eta_pred\": eta_pred,\n            \"eta_dot_pred\": eta_dot_pred,\n            \"g_pred\": g_pred,\n            \"eta_t_pred_c\": eta_t_pred_c,\n            \"eta_dot_pred_c\": eta_dot_pred_c,\n            \"lift_pred_c\": lift_pred_c,\n        }\n\n    def _forward_type_3(self, x):\n        # physics informed neural networks\n        output = self.lstm_model(x[\"ag\"])\n        eta_pred = output[:, :, 0 : self.output_size]\n        eta_dot_pred = output[:, :, self.output_size : 2 * self.output_size]\n        g_pred = output[:, :, 2 * self.output_size :]\n\n        output_c = self.lstm_model(x[\"ag_c\"])\n        eta_pred_c = output_c[:, :, 0 : self.output_size]\n        eta_dot_pred_c = output_c[:, :, self.output_size : 2 * self.output_size]\n        g_pred_c = output_c[:, :, 2 * self.output_size :]\n\n        eta_t_pred_c = paddle.matmul(x[\"phi\"], eta_pred_c)\n        eta_tt_pred_c = paddle.matmul(x[\"phi\"], eta_dot_pred_c)\n        g_t_pred_c = paddle.matmul(x[\"phi\"], g_pred_c)\n\n        f = self.lstm_model_f(paddle.concat([eta_pred_c, eta_dot_pred_c, g_pred_c], 2))\n        lift_pred_c = eta_tt_pred_c + f\n\n        eta_dot1_pred_c = eta_dot_pred_c[:, :, 0:1]\n        g_dot_pred_c = self.lstm_model_g(paddle.concat([eta_dot1_pred_c, g_pred_c], 2))\n\n        return {\n            \"eta_pred\": eta_pred,\n            \"eta_dot_pred\": eta_dot_pred,\n            \"g_pred\": g_pred,\n            \"eta_t_pred_c\": eta_t_pred_c,\n            \"eta_dot_pred_c\": eta_dot_pred_c,\n            \"lift_pred_c\": lift_pred_c,\n            \"g_t_pred_c\": g_t_pred_c,\n            \"g_dot_pred_c\": g_dot_pred_c,\n        }\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.LorenzEmbedding","title":"<code>LorenzEmbedding</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Embedding Koopman model for the Lorenz ODE system.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"states\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"pred_states\", \"recover_states\").</p> required <code>mean</code> <code>Optional[Tuple[float, ...]]</code> <p>Mean of training dataset. Defaults to None.</p> <code>None</code> <code>std</code> <code>Optional[Tuple[float, ...]]</code> <p>Standard Deviation of training dataset. Defaults to None.</p> <code>None</code> <code>input_size</code> <code>int</code> <p>Size of input data. Defaults to 3.</p> <code>3</code> <code>hidden_size</code> <code>int</code> <p>Number of hidden size. Defaults to 500.</p> <code>500</code> <code>embed_size</code> <code>int</code> <p>Number of embedding size. Defaults to 32.</p> <code>32</code> <code>drop</code> <code>float</code> <p>Probability of dropout the units. Defaults to 0.0.</p> <code>0.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.LorenzEmbedding(\n...     input_keys=(\"x\", \"y\"),\n...     output_keys=(\"u\", \"v\"),\n...     input_size=3,\n...     hidden_size=500,\n...     embed_size=32,\n...     drop=0.0,\n...     mean=None,\n...     std=None,\n... )\n&gt;&gt;&gt; x_shape = [8, 3, 2]\n&gt;&gt;&gt; y_shape = [8, 3, 1]\n&gt;&gt;&gt; input_dict = {\"x\": paddle.rand(x_shape),\n...               \"y\": paddle.rand(y_shape)}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict[\"u\"].shape)\n[8, 2, 3]\n&gt;&gt;&gt; print(output_dict[\"v\"].shape)\n[8, 3, 3]\n</code></pre> Source code in <code>ppsci/arch/embedding_koopman.py</code> <pre><code>class LorenzEmbedding(base.Arch):\n    \"\"\"Embedding Koopman model for the Lorenz ODE system.\n\n    Args:\n        input_keys (Tuple[str, ...]): Input keys, such as (\"states\",).\n        output_keys (Tuple[str, ...]): Output keys, such as (\"pred_states\", \"recover_states\").\n        mean (Optional[Tuple[float, ...]]): Mean of training dataset. Defaults to None.\n        std (Optional[Tuple[float, ...]]): Standard Deviation of training dataset. Defaults to None.\n        input_size (int, optional): Size of input data. Defaults to 3.\n        hidden_size (int, optional): Number of hidden size. Defaults to 500.\n        embed_size (int, optional): Number of embedding size. Defaults to 32.\n        drop (float, optional):  Probability of dropout the units. Defaults to 0.0.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.LorenzEmbedding(\n        ...     input_keys=(\"x\", \"y\"),\n        ...     output_keys=(\"u\", \"v\"),\n        ...     input_size=3,\n        ...     hidden_size=500,\n        ...     embed_size=32,\n        ...     drop=0.0,\n        ...     mean=None,\n        ...     std=None,\n        ... )\n        &gt;&gt;&gt; x_shape = [8, 3, 2]\n        &gt;&gt;&gt; y_shape = [8, 3, 1]\n        &gt;&gt;&gt; input_dict = {\"x\": paddle.rand(x_shape),\n        ...               \"y\": paddle.rand(y_shape)}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict[\"u\"].shape)\n        [8, 2, 3]\n        &gt;&gt;&gt; print(output_dict[\"v\"].shape)\n        [8, 3, 3]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        mean: Optional[Tuple[float, ...]] = None,\n        std: Optional[Tuple[float, ...]] = None,\n        input_size: int = 3,\n        hidden_size: int = 500,\n        embed_size: int = 32,\n        drop: float = 0.0,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.embed_size = embed_size\n\n        # build observable network\n        self.encoder_net = self.build_encoder(input_size, hidden_size, embed_size, drop)\n        # build koopman operator\n        self.k_diag, self.k_ut = self.build_koopman_operator(embed_size)\n        # build recovery network\n        self.decoder_net = self.build_decoder(input_size, hidden_size, embed_size)\n\n        mean = [0.0, 0.0, 0.0] if mean is None else mean\n        std = [1.0, 1.0, 1.0] if std is None else std\n        self.register_buffer(\"mean\", paddle.to_tensor(mean).reshape([1, 3]))\n        self.register_buffer(\"std\", paddle.to_tensor(std).reshape([1, 3]))\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m: nn.Layer):\n        if isinstance(m, nn.Linear):\n            k = 1 / m.weight.shape[0]\n            uniform = Uniform(-(k**0.5), k**0.5)\n            uniform(m.weight)\n            if m.bias is not None:\n                uniform(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n\n    def build_encoder(\n        self, input_size: int, hidden_size: int, embed_size: int, drop: float = 0.0\n    ):\n        net = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, embed_size),\n            nn.LayerNorm(embed_size),\n            nn.Dropout(drop),\n        )\n        return net\n\n    def build_decoder(self, input_size: int, hidden_size: int, embed_size: int):\n        net = nn.Sequential(\n            nn.Linear(embed_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, input_size),\n        )\n        return net\n\n    def build_koopman_operator(self, embed_size: int):\n        # Learned Koopman operator\n        data = paddle.linspace(1, 0, embed_size)\n        k_diag = paddle.create_parameter(\n            shape=data.shape,\n            dtype=paddle.get_default_dtype(),\n            default_initializer=nn.initializer.Assign(data),\n        )\n\n        data = 0.1 * paddle.rand([2 * embed_size - 3])\n        k_ut = paddle.create_parameter(\n            shape=data.shape,\n            dtype=paddle.get_default_dtype(),\n            default_initializer=nn.initializer.Assign(data),\n        )\n        return k_diag, k_ut\n\n    def encoder(self, x: paddle.Tensor):\n        x = self._normalize(x)\n        g = self.encoder_net(x)\n        return g\n\n    def decoder(self, g: paddle.Tensor):\n        out = self.decoder_net(g)\n        x = self._unnormalize(out)\n        return x\n\n    def koopman_operation(self, embed_data: paddle.Tensor, k_matrix: paddle.Tensor):\n        # Apply Koopman operation\n        embed_pred_data = paddle.bmm(\n            k_matrix.expand(\n                [embed_data.shape[0], k_matrix.shape[0], k_matrix.shape[1]]\n            ),\n            embed_data.transpose([0, 2, 1]),\n        ).transpose([0, 2, 1])\n        return embed_pred_data\n\n    def _normalize(self, x: paddle.Tensor):\n        return (x - self.mean) / self.std\n\n    def _unnormalize(self, x: paddle.Tensor):\n        return self.std * x + self.mean\n\n    def get_koopman_matrix(self):\n        # # Koopman operator\n        k_ut_tensor = self.k_ut * 1\n        k_ut_tensor = paddle.diag(\n            k_ut_tensor[0 : self.embed_size - 1], offset=1\n        ) + paddle.diag(k_ut_tensor[self.embed_size - 1 :], offset=2)\n        k_matrix = k_ut_tensor + (-1) * k_ut_tensor.t()\n        k_matrix = k_matrix + paddle.diag(self.k_diag)\n        return k_matrix\n\n    def forward_tensor(self, x):\n        k_matrix = self.get_koopman_matrix()\n        embed_data = self.encoder(x)\n        recover_data = self.decoder(embed_data)\n\n        embed_pred_data = self.koopman_operation(embed_data, k_matrix)\n        pred_data = self.decoder(embed_pred_data)\n\n        return (pred_data[:, :-1, :], recover_data, k_matrix)\n\n    @staticmethod\n    def split_to_dict(data_tensors: Tuple[paddle.Tensor, ...], keys: Tuple[str, ...]):\n        return {key: data_tensors[i] for i, key in enumerate(keys)}\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        x_tensor = self.concat_to_tensor(x, self.input_keys, axis=-1)\n        y = self.forward_tensor(x_tensor)\n        y = self.split_to_dict(y, self.output_keys)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.RosslerEmbedding","title":"<code>RosslerEmbedding</code>","text":"<p>               Bases: <code>LorenzEmbedding</code></p> <p>Embedding Koopman model for the Rossler ODE system.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"states\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"pred_states\", \"recover_states\").</p> required <code>mean</code> <code>Optional[Tuple[float, ...]]</code> <p>Mean of training dataset. Defaults to None.</p> <code>None</code> <code>std</code> <code>Optional[Tuple[float, ...]]</code> <p>Standard Deviation of training dataset. Defaults to None.</p> <code>None</code> <code>input_size</code> <code>int</code> <p>Size of input data. Defaults to 3.</p> <code>3</code> <code>hidden_size</code> <code>int</code> <p>Number of hidden size. Defaults to 500.</p> <code>500</code> <code>embed_size</code> <code>int</code> <p>Number of embedding size. Defaults to 32.</p> <code>32</code> <code>drop</code> <code>float</code> <p>Probability of dropout the units. Defaults to 0.0.</p> <code>0.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.RosslerEmbedding(\n...     input_keys=(\"x\", \"y\"),\n...     output_keys=(\"u\", \"v\"),\n...     input_size=3,\n...     hidden_size=500,\n...     embed_size=32,\n...     drop=0.0,\n...     mean=None,\n...     std=None,\n... )\n&gt;&gt;&gt; x_shape = [8, 3, 2]\n&gt;&gt;&gt; y_shape = [8, 3, 1]\n&gt;&gt;&gt; input_dict = {\"x\": paddle.rand(x_shape),\n...               \"y\": paddle.rand(y_shape)}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict[\"u\"].shape)\n[8, 2, 3]\n&gt;&gt;&gt; print(output_dict[\"v\"].shape)\n[8, 3, 3]\n</code></pre> Source code in <code>ppsci/arch/embedding_koopman.py</code> <pre><code>class RosslerEmbedding(LorenzEmbedding):\n    \"\"\"Embedding Koopman model for the Rossler ODE system.\n\n    Args:\n        input_keys (Tuple[str, ...]): Input keys, such as (\"states\",).\n        output_keys (Tuple[str, ...]): Output keys, such as (\"pred_states\", \"recover_states\").\n        mean (Optional[Tuple[float, ...]]): Mean of training dataset. Defaults to None.\n        std (Optional[Tuple[float, ...]]): Standard Deviation of training dataset. Defaults to None.\n        input_size (int, optional): Size of input data. Defaults to 3.\n        hidden_size (int, optional): Number of hidden size. Defaults to 500.\n        embed_size (int, optional): Number of embedding size. Defaults to 32.\n        drop (float, optional):  Probability of dropout the units. Defaults to 0.0.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.RosslerEmbedding(\n        ...     input_keys=(\"x\", \"y\"),\n        ...     output_keys=(\"u\", \"v\"),\n        ...     input_size=3,\n        ...     hidden_size=500,\n        ...     embed_size=32,\n        ...     drop=0.0,\n        ...     mean=None,\n        ...     std=None,\n        ... )\n        &gt;&gt;&gt; x_shape = [8, 3, 2]\n        &gt;&gt;&gt; y_shape = [8, 3, 1]\n        &gt;&gt;&gt; input_dict = {\"x\": paddle.rand(x_shape),\n        ...               \"y\": paddle.rand(y_shape)}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict[\"u\"].shape)\n        [8, 2, 3]\n        &gt;&gt;&gt; print(output_dict[\"v\"].shape)\n        [8, 3, 3]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        mean: Optional[Tuple[float, ...]] = None,\n        std: Optional[Tuple[float, ...]] = None,\n        input_size: int = 3,\n        hidden_size: int = 500,\n        embed_size: int = 32,\n        drop: float = 0.0,\n    ):\n        super().__init__(\n            input_keys,\n            output_keys,\n            mean,\n            std,\n            input_size,\n            hidden_size,\n            embed_size,\n            drop,\n        )\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.CylinderEmbedding","title":"<code>CylinderEmbedding</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Embedding Koopman model for the Cylinder system.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"states\", \"visc\").</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"pred_states\", \"recover_states\").</p> required <code>mean</code> <code>Optional[Tuple[float, ...]]</code> <p>Mean of training dataset. Defaults to None.</p> <code>None</code> <code>std</code> <code>Optional[Tuple[float, ...]]</code> <p>Standard Deviation of training dataset. Defaults to None.</p> <code>None</code> <code>embed_size</code> <code>int</code> <p>Number of embedding size. Defaults to 128.</p> <code>128</code> <code>encoder_channels</code> <code>Optional[Tuple[int, ...]]</code> <p>Number of channels in encoder network. Defaults to None.</p> <code>None</code> <code>decoder_channels</code> <code>Optional[Tuple[int, ...]]</code> <p>Number of channels in decoder network. Defaults to None.</p> <code>None</code> <code>drop</code> <code>float</code> <p>Probability of dropout the units. Defaults to 0.0.</p> <code>0.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.CylinderEmbedding((\"states\", \"visc\"), (\"pred_states\", \"recover_states\"))\n&gt;&gt;&gt; states_shape = [32, 10, 3, 64, 128]\n&gt;&gt;&gt; visc_shape = [32, 1]\n&gt;&gt;&gt; input_dict = {\"states\" : paddle.rand(states_shape),\n...               \"visc\" : paddle.rand(visc_shape)}\n&gt;&gt;&gt; out_dict = model(input_dict)\n&gt;&gt;&gt; print(out_dict[\"pred_states\"].shape)\n[32, 9, 3, 64, 128]\n&gt;&gt;&gt; print(out_dict[\"recover_states\"].shape)\n[32, 10, 3, 64, 128]\n</code></pre> Source code in <code>ppsci/arch/embedding_koopman.py</code> <pre><code>class CylinderEmbedding(base.Arch):\n    \"\"\"Embedding Koopman model for the Cylinder system.\n\n    Args:\n        input_keys (Tuple[str, ...]): Input keys, such as (\"states\", \"visc\").\n        output_keys (Tuple[str, ...]): Output keys, such as (\"pred_states\", \"recover_states\").\n        mean (Optional[Tuple[float, ...]]): Mean of training dataset. Defaults to None.\n        std (Optional[Tuple[float, ...]]): Standard Deviation of training dataset. Defaults to None.\n        embed_size (int, optional): Number of embedding size. Defaults to 128.\n        encoder_channels (Optional[Tuple[int, ...]]): Number of channels in encoder network. Defaults to None.\n        decoder_channels (Optional[Tuple[int, ...]]): Number of channels in decoder network. Defaults to None.\n        drop (float, optional):  Probability of dropout the units. Defaults to 0.0.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.CylinderEmbedding((\"states\", \"visc\"), (\"pred_states\", \"recover_states\"))\n        &gt;&gt;&gt; states_shape = [32, 10, 3, 64, 128]\n        &gt;&gt;&gt; visc_shape = [32, 1]\n        &gt;&gt;&gt; input_dict = {\"states\" : paddle.rand(states_shape),\n        ...               \"visc\" : paddle.rand(visc_shape)}\n        &gt;&gt;&gt; out_dict = model(input_dict)\n        &gt;&gt;&gt; print(out_dict[\"pred_states\"].shape)\n        [32, 9, 3, 64, 128]\n        &gt;&gt;&gt; print(out_dict[\"recover_states\"].shape)\n        [32, 10, 3, 64, 128]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        mean: Optional[Tuple[float, ...]] = None,\n        std: Optional[Tuple[float, ...]] = None,\n        embed_size: int = 128,\n        encoder_channels: Optional[Tuple[int, ...]] = None,\n        decoder_channels: Optional[Tuple[int, ...]] = None,\n        drop: float = 0.0,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.embed_size = embed_size\n\n        X, Y = np.meshgrid(np.linspace(-2, 14, 128), np.linspace(-4, 4, 64))\n        self.mask = paddle.to_tensor(np.sqrt(X**2 + Y**2)).unsqueeze(0).unsqueeze(0)\n\n        encoder_channels = (\n            [4, 16, 32, 64, 128] if encoder_channels is None else encoder_channels\n        )\n        decoder_channels = (\n            [embed_size // 32, 128, 64, 32, 16]\n            if decoder_channels is None\n            else decoder_channels\n        )\n        self.encoder_net = self.build_encoder(embed_size, encoder_channels, drop)\n        self.k_diag_net, self.k_ut_net, self.k_lt_net = self.build_koopman_operator(\n            embed_size\n        )\n        self.decoder_net = self.build_decoder(decoder_channels)\n\n        xidx = []\n        yidx = []\n        for i in range(1, 5):\n            yidx.append(np.arange(i, embed_size))\n            xidx.append(np.arange(0, embed_size - i))\n        self.xidx = paddle.to_tensor(np.concatenate(xidx), dtype=\"int64\")\n        self.yidx = paddle.to_tensor(np.concatenate(yidx), dtype=\"int64\")\n\n        mean = [0.0, 0.0, 0.0, 0.0] if mean is None else mean\n        std = [1.0, 1.0, 1.0, 1.0] if std is None else std\n        self.register_buffer(\"mean\", paddle.to_tensor(mean).reshape([1, 4, 1, 1]))\n        self.register_buffer(\"std\", paddle.to_tensor(std).reshape([1, 4, 1, 1]))\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            k = 1 / m.weight.shape[0]\n            uniform = Uniform(-(k**0.5), k**0.5)\n            uniform(m.weight)\n            if m.bias is not None:\n                uniform(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n        elif isinstance(m, nn.Conv2D):\n            k = 1 / (m.weight.shape[1] * m.weight.shape[2] * m.weight.shape[3])\n            uniform = Uniform(-(k**0.5), k**0.5)\n            uniform(m.weight)\n            if m.bias is not None:\n                uniform(m.bias)\n\n    def _build_conv_relu_list(\n        self, in_channels: Tuple[int, ...], out_channels: Tuple[int, ...]\n    ):\n        net_list = [\n            nn.Conv2D(\n                in_channels,\n                out_channels,\n                kernel_size=(3, 3),\n                stride=2,\n                padding=1,\n                padding_mode=\"replicate\",\n            ),\n            nn.ReLU(),\n        ]\n        return net_list\n\n    def build_encoder(\n        self, embed_size: int, channels: Tuple[int, ...], drop: float = 0.0\n    ):\n        net = []\n        for i in range(1, len(channels)):\n            net.extend(self._build_conv_relu_list(channels[i - 1], channels[i]))\n        net.append(\n            nn.Conv2D(\n                channels[-1],\n                embed_size // 32,\n                kernel_size=(3, 3),\n                padding=1,\n                padding_mode=\"replicate\",\n            )\n        )\n        net.append(\n            nn.LayerNorm(\n                (4, 4, 8),\n            )\n        )\n        net.append(nn.Dropout(drop))\n        net = nn.Sequential(*net)\n        return net\n\n    def _build_upsample_conv_relu(\n        self, in_channels: Tuple[int, ...], out_channels: Tuple[int, ...]\n    ):\n        net_list = [\n            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2D(\n                in_channels,\n                out_channels,\n                kernel_size=(3, 3),\n                stride=1,\n                padding=1,\n                padding_mode=\"replicate\",\n            ),\n            nn.ReLU(),\n        ]\n        return net_list\n\n    def build_decoder(self, channels: Tuple[int, ...]):\n        net = []\n        for i in range(1, len(channels)):\n            net.extend(self._build_upsample_conv_relu(channels[i - 1], channels[i]))\n        net.append(\n            nn.Conv2D(\n                channels[-1],\n                3,\n                kernel_size=(3, 3),\n                stride=1,\n                padding=1,\n                padding_mode=\"replicate\",\n            ),\n        )\n        net = nn.Sequential(*net)\n        return net\n\n    def build_koopman_operator(self, embed_size: int):\n        # Learned Koopman operator parameters\n        k_diag_net = nn.Sequential(\n            nn.Linear(1, 50), nn.ReLU(), nn.Linear(50, embed_size)\n        )\n\n        k_ut_net = nn.Sequential(\n            nn.Linear(1, 50), nn.ReLU(), nn.Linear(50, 4 * embed_size - 10)\n        )\n        k_lt_net = nn.Sequential(\n            nn.Linear(1, 50), nn.ReLU(), nn.Linear(50, 4 * embed_size - 10)\n        )\n        return k_diag_net, k_ut_net, k_lt_net\n\n    def encoder(self, x: paddle.Tensor, viscosity: paddle.Tensor):\n        B, T, C, H, W = x.shape\n        x = x.reshape((B * T, C, H, W))\n        viscosity = viscosity.repeat_interleave(T, axis=1).reshape((B * T, 1))\n        x = paddle.concat(\n            [x, viscosity.unsqueeze(-1).unsqueeze(-1) * paddle.ones_like(x[:, :1])],\n            axis=1,\n        )\n        x = self._normalize(x)\n        g = self.encoder_net(x)\n        g = g.reshape([B, T, -1])\n        return g\n\n    def decoder(self, g: paddle.Tensor):\n        B, T, _ = g.shape\n        x = self.decoder_net(g.reshape([-1, self.embed_size // 32, 4, 8]))\n        x = self._unnormalize(x)\n        mask0 = (\n            self.mask.repeat_interleave(x.shape[1], axis=1).repeat_interleave(\n                x.shape[0], axis=0\n            )\n            &lt; 1\n        )\n        x[mask0] = 0\n        _, C, H, W = x.shape\n        x = x.reshape([B, T, C, H, W])\n        return x\n\n    def get_koopman_matrix(self, g: paddle.Tensor, visc: paddle.Tensor):\n        # # Koopman operator\n        kMatrix = paddle.zeros([g.shape[0], self.embed_size, self.embed_size])\n        kMatrix.stop_gradient = False\n        # Populate the off diagonal terms\n        kMatrixUT_data = self.k_ut_net(100 * visc)\n        kMatrixLT_data = self.k_lt_net(100 * visc)\n\n        kMatrix = kMatrix.transpose([1, 2, 0])\n        kMatrixUT_data_t = kMatrixUT_data.transpose([1, 0])\n        kMatrixLT_data_t = kMatrixLT_data.transpose([1, 0])\n        kMatrix[self.xidx, self.yidx] = kMatrixUT_data_t\n        kMatrix[self.yidx, self.xidx] = kMatrixLT_data_t\n\n        # Populate the diagonal\n        ind = np.diag_indices(kMatrix.shape[1])\n        ind = paddle.to_tensor(ind, dtype=\"int64\")\n\n        kMatrixDiag = self.k_diag_net(100 * visc)\n        kMatrixDiag_t = kMatrixDiag.transpose([1, 0])\n        kMatrix[ind[0], ind[1]] = kMatrixDiag_t\n        return kMatrix.transpose([2, 0, 1])\n\n    def koopman_operation(self, embed_data: paddle.Tensor, k_matrix: paddle.Tensor):\n        embed_pred_data = paddle.bmm(\n            k_matrix, embed_data.transpose([0, 2, 1])\n        ).transpose([0, 2, 1])\n        return embed_pred_data\n\n    def _normalize(self, x: paddle.Tensor):\n        x = (x - self.mean) / self.std\n        return x\n\n    def _unnormalize(self, x: paddle.Tensor):\n        return self.std[:, :3] * x + self.mean[:, :3]\n\n    def forward_tensor(self, states, visc):\n        # states.shape=(B, T, C, H, W)\n        embed_data = self.encoder(states, visc)\n        recover_data = self.decoder(embed_data)\n\n        k_matrix = self.get_koopman_matrix(embed_data, visc)\n        embed_pred_data = self.koopman_operation(embed_data, k_matrix)\n        pred_data = self.decoder(embed_pred_data)\n\n        return (pred_data[:, :-1], recover_data, k_matrix)\n\n    @staticmethod\n    def split_to_dict(data_tensors: Tuple[paddle.Tensor, ...], keys: Tuple[str, ...]):\n        return {key: data_tensors[i] for i, key in enumerate(keys)}\n\n    def forward(self, x):\n\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        y = self.forward_tensor(**x)\n        y = self.split_to_dict(y, self.output_keys)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.Generator","title":"<code>Generator</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Generator Net of GAN. Attention, the net using a kind of variant of ResBlock which is     unique to \"tempoGAN\" example but not an open source network.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input1\", \"input2\").</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output1\", \"output2\").</p> required <code>in_channel</code> <code>int</code> <p>Number of input channels of the first conv layer.</p> required <code>out_channels_tuple</code> <code>Tuple[Tuple[int, ...], ...]</code> <p>Number of output channels of all conv layers, such as [[out_res0_conv0, out_res0_conv1], [out_res1_conv0, out_res1_conv1]]</p> required <code>kernel_sizes_tuple</code> <code>Tuple[Tuple[int, ...], ...]</code> <p>Number of kernel_size of all conv layers, such as [[kernel_size_res0_conv0, kernel_size_res0_conv1], [kernel_size_res1_conv0, kernel_size_res1_conv1]]</p> required <code>strides_tuple</code> <code>Tuple[Tuple[int, ...], ...]</code> <p>Number of stride of all conv layers, such as [[stride_res0_conv0, stride_res0_conv1], [stride_res1_conv0, stride_res1_conv1]]</p> required <code>use_bns_tuple</code> <code>Tuple[Tuple[bool, ...], ...]</code> <p>Whether to use the batch_norm layer after each conv layer.</p> required <code>acts_tuple</code> <code>Tuple[Tuple[str, ...], ...]</code> <p>Whether to use the activation layer after each conv layer. If so, witch activation to use, such as [[act_res0_conv0, act_res0_conv1], [act_res1_conv0, act_res1_conv1]]</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; in_channel = 1\n&gt;&gt;&gt; rb_channel0 = (2, 8, 8)\n&gt;&gt;&gt; rb_channel1 = (128, 128, 128)\n&gt;&gt;&gt; rb_channel2 = (32, 8, 8)\n&gt;&gt;&gt; rb_channel3 = (2, 1, 1)\n&gt;&gt;&gt; out_channels_tuple = (rb_channel0, rb_channel1, rb_channel2, rb_channel3)\n&gt;&gt;&gt; kernel_sizes_tuple = (((5, 5), ) * 2 + ((1, 1), ), ) * 4\n&gt;&gt;&gt; strides_tuple = ((1, 1, 1), ) * 4\n&gt;&gt;&gt; use_bns_tuple = ((True, True, True), ) * 3 + ((False, False, False), )\n&gt;&gt;&gt; acts_tuple = ((\"relu\", None, None), ) * 4\n&gt;&gt;&gt; model = ppsci.arch.Generator((\"in\",), (\"out\",), in_channel, out_channels_tuple, kernel_sizes_tuple, strides_tuple, use_bns_tuple, acts_tuple)\n&gt;&gt;&gt; batch_size = 4\n&gt;&gt;&gt; height = 64\n&gt;&gt;&gt; width = 64\n&gt;&gt;&gt; input_data = paddle.randn([batch_size, in_channel, height, width])\n&gt;&gt;&gt; input_dict = {'in': input_data}\n&gt;&gt;&gt; output_data = model(input_dict)\n&gt;&gt;&gt; print(output_data['out'].shape)\n[4, 1, 64, 64]\n</code></pre> Source code in <code>ppsci/arch/gan.py</code> <pre><code>class Generator(base.Arch):\n    \"\"\"Generator Net of GAN. Attention, the net using a kind of variant of ResBlock which is\n        unique to \"tempoGAN\" example but not an open source network.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input1\", \"input2\").\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output1\", \"output2\").\n        in_channel (int): Number of input channels of the first conv layer.\n        out_channels_tuple (Tuple[Tuple[int, ...], ...]): Number of output channels of all conv layers,\n            such as [[out_res0_conv0, out_res0_conv1], [out_res1_conv0, out_res1_conv1]]\n        kernel_sizes_tuple (Tuple[Tuple[int, ...], ...]): Number of kernel_size of all conv layers,\n            such as [[kernel_size_res0_conv0, kernel_size_res0_conv1], [kernel_size_res1_conv0, kernel_size_res1_conv1]]\n        strides_tuple (Tuple[Tuple[int, ...], ...]): Number of stride of all conv layers,\n            such as [[stride_res0_conv0, stride_res0_conv1], [stride_res1_conv0, stride_res1_conv1]]\n        use_bns_tuple (Tuple[Tuple[bool, ...], ...]): Whether to use the batch_norm layer after each conv layer.\n        acts_tuple (Tuple[Tuple[str, ...], ...]): Whether to use the activation layer after each conv layer. If so, witch activation to use,\n            such as [[act_res0_conv0, act_res0_conv1], [act_res1_conv0, act_res1_conv1]]\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; in_channel = 1\n        &gt;&gt;&gt; rb_channel0 = (2, 8, 8)\n        &gt;&gt;&gt; rb_channel1 = (128, 128, 128)\n        &gt;&gt;&gt; rb_channel2 = (32, 8, 8)\n        &gt;&gt;&gt; rb_channel3 = (2, 1, 1)\n        &gt;&gt;&gt; out_channels_tuple = (rb_channel0, rb_channel1, rb_channel2, rb_channel3)\n        &gt;&gt;&gt; kernel_sizes_tuple = (((5, 5), ) * 2 + ((1, 1), ), ) * 4\n        &gt;&gt;&gt; strides_tuple = ((1, 1, 1), ) * 4\n        &gt;&gt;&gt; use_bns_tuple = ((True, True, True), ) * 3 + ((False, False, False), )\n        &gt;&gt;&gt; acts_tuple = ((\"relu\", None, None), ) * 4\n        &gt;&gt;&gt; model = ppsci.arch.Generator((\"in\",), (\"out\",), in_channel, out_channels_tuple, kernel_sizes_tuple, strides_tuple, use_bns_tuple, acts_tuple)\n        &gt;&gt;&gt; batch_size = 4\n        &gt;&gt;&gt; height = 64\n        &gt;&gt;&gt; width = 64\n        &gt;&gt;&gt; input_data = paddle.randn([batch_size, in_channel, height, width])\n        &gt;&gt;&gt; input_dict = {'in': input_data}\n        &gt;&gt;&gt; output_data = model(input_dict)\n        &gt;&gt;&gt; print(output_data['out'].shape)\n        [4, 1, 64, 64]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        in_channel: int,\n        out_channels_tuple: Tuple[Tuple[int, ...], ...],\n        kernel_sizes_tuple: Tuple[Tuple[int, ...], ...],\n        strides_tuple: Tuple[Tuple[int, ...], ...],\n        use_bns_tuple: Tuple[Tuple[bool, ...], ...],\n        acts_tuple: Tuple[Tuple[str, ...], ...],\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.in_channel = in_channel\n        self.out_channels_tuple = out_channels_tuple\n        self.kernel_sizes_tuple = kernel_sizes_tuple\n        self.strides_tuple = strides_tuple\n        self.use_bns_tuple = use_bns_tuple\n        self.acts_tuple = acts_tuple\n\n        self.init_blocks()\n\n    def init_blocks(self):\n        blocks_list = []\n        for i in range(len(self.out_channels_tuple)):\n            in_channel = (\n                self.in_channel if i == 0 else self.out_channels_tuple[i - 1][-1]\n            )\n            blocks_list.append(\n                VariantResBlock(\n                    in_channel=in_channel,\n                    out_channels=self.out_channels_tuple[i],\n                    kernel_sizes=self.kernel_sizes_tuple[i],\n                    strides=self.strides_tuple[i],\n                    use_bns=self.use_bns_tuple[i],\n                    acts=self.acts_tuple[i],\n                    mean=0.0,\n                    std=0.04,\n                    value=0.1,\n                )\n            )\n        self.blocks = nn.LayerList(blocks_list)\n\n    def forward_tensor(self, x):\n        y = x\n        for block in self.blocks:\n            y = block(y)\n        return y\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        y = self.concat_to_tensor(x, self.input_keys, axis=-1)\n        y = self.forward_tensor(y)\n        y = self.split_to_dict(y, self.output_keys, axis=-1)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.Discriminator","title":"<code>Discriminator</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Discriminator Net of GAN.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input1\", \"input2\").</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output1\", \"output2\").</p> required <code>in_channel</code> <code>int</code> <p>Number of input channels of the first conv layer.</p> required <code>out_channels</code> <code>Tuple[int, ...]</code> <p>Number of output channels of all conv layers, such as (out_conv0, out_conv1, out_conv2).</p> required <code>fc_channel</code> <code>int</code> <p>Number of input features of linear layer. Number of output features of the layer is set to 1 in this Net to construct a fully_connected layer.</p> required <code>kernel_sizes</code> <code>Tuple[int, ...]</code> <p>Number of kernel_size of all conv layers, such as (kernel_size_conv0, kernel_size_conv1, kernel_size_conv2).</p> required <code>strides</code> <code>Tuple[int, ...]</code> <p>Number of stride of all conv layers, such as (stride_conv0, stride_conv1, stride_conv2).</p> required <code>use_bns</code> <code>Tuple[bool, ...]</code> <p>Whether to use the batch_norm layer after each conv layer.</p> required <code>acts</code> <code>Tuple[str, ...]</code> <p>Whether to use the activation layer after each conv layer. If so, witch activation to use, such as (act_conv0, act_conv1, act_conv2).</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; in_channel = 2\n&gt;&gt;&gt; in_channel_tempo = 3\n&gt;&gt;&gt; out_channels = (32, 64, 128, 256)\n&gt;&gt;&gt; fc_channel = 65536\n&gt;&gt;&gt; kernel_sizes = ((4, 4), (4, 4), (4, 4), (4, 4))\n&gt;&gt;&gt; strides = (2, 2, 2, 1)\n&gt;&gt;&gt; use_bns = (False, True, True, True)\n&gt;&gt;&gt; acts = (\"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", None)\n&gt;&gt;&gt; output_keys_disc = (\"out_1\", \"out_2\", \"out_3\", \"out_4\", \"out_5\", \"out_6\", \"out_7\", \"out_8\", \"out_9\", \"out_10\")\n&gt;&gt;&gt; model = ppsci.arch.Discriminator((\"in_1\",\"in_2\"), output_keys_disc, in_channel, out_channels, fc_channel, kernel_sizes, strides, use_bns, acts)\n&gt;&gt;&gt; input_data = [paddle.to_tensor(paddle.randn([1, in_channel, 128, 128])),paddle.to_tensor(paddle.randn([1, in_channel, 128, 128]))]\n&gt;&gt;&gt; input_dict = {\"in_1\": input_data[0],\"in_2\": input_data[1]}\n&gt;&gt;&gt; out_dict = model(input_dict)\n&gt;&gt;&gt; for k, v in out_dict.items():\n...     print(k, v.shape)\nout_1 [1, 32, 64, 64]\nout_2 [1, 64, 32, 32]\nout_3 [1, 128, 16, 16]\nout_4 [1, 256, 16, 16]\nout_5 [1, 1]\nout_6 [1, 32, 64, 64]\nout_7 [1, 64, 32, 32]\nout_8 [1, 128, 16, 16]\nout_9 [1, 256, 16, 16]\nout_10 [1, 1]\n</code></pre> Source code in <code>ppsci/arch/gan.py</code> <pre><code>class Discriminator(base.Arch):\n    \"\"\"Discriminator Net of GAN.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input1\", \"input2\").\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output1\", \"output2\").\n        in_channel (int):  Number of input channels of the first conv layer.\n        out_channels (Tuple[int, ...]): Number of output channels of all conv layers,\n            such as (out_conv0, out_conv1, out_conv2).\n        fc_channel (int):  Number of input features of linear layer. Number of output features of the layer\n            is set to 1 in this Net to construct a fully_connected layer.\n        kernel_sizes (Tuple[int, ...]): Number of kernel_size of all conv layers,\n            such as (kernel_size_conv0, kernel_size_conv1, kernel_size_conv2).\n        strides (Tuple[int, ...]): Number of stride of all conv layers,\n            such as (stride_conv0, stride_conv1, stride_conv2).\n        use_bns (Tuple[bool, ...]): Whether to use the batch_norm layer after each conv layer.\n        acts (Tuple[str, ...]): Whether to use the activation layer after each conv layer. If so, witch activation to use,\n            such as (act_conv0, act_conv1, act_conv2).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; in_channel = 2\n        &gt;&gt;&gt; in_channel_tempo = 3\n        &gt;&gt;&gt; out_channels = (32, 64, 128, 256)\n        &gt;&gt;&gt; fc_channel = 65536\n        &gt;&gt;&gt; kernel_sizes = ((4, 4), (4, 4), (4, 4), (4, 4))\n        &gt;&gt;&gt; strides = (2, 2, 2, 1)\n        &gt;&gt;&gt; use_bns = (False, True, True, True)\n        &gt;&gt;&gt; acts = (\"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", None)\n        &gt;&gt;&gt; output_keys_disc = (\"out_1\", \"out_2\", \"out_3\", \"out_4\", \"out_5\", \"out_6\", \"out_7\", \"out_8\", \"out_9\", \"out_10\")\n        &gt;&gt;&gt; model = ppsci.arch.Discriminator((\"in_1\",\"in_2\"), output_keys_disc, in_channel, out_channels, fc_channel, kernel_sizes, strides, use_bns, acts)\n        &gt;&gt;&gt; input_data = [paddle.to_tensor(paddle.randn([1, in_channel, 128, 128])),paddle.to_tensor(paddle.randn([1, in_channel, 128, 128]))]\n        &gt;&gt;&gt; input_dict = {\"in_1\": input_data[0],\"in_2\": input_data[1]}\n        &gt;&gt;&gt; out_dict = model(input_dict)\n        &gt;&gt;&gt; for k, v in out_dict.items():\n        ...     print(k, v.shape)\n        out_1 [1, 32, 64, 64]\n        out_2 [1, 64, 32, 32]\n        out_3 [1, 128, 16, 16]\n        out_4 [1, 256, 16, 16]\n        out_5 [1, 1]\n        out_6 [1, 32, 64, 64]\n        out_7 [1, 64, 32, 32]\n        out_8 [1, 128, 16, 16]\n        out_9 [1, 256, 16, 16]\n        out_10 [1, 1]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        in_channel: int,\n        out_channels: Tuple[int, ...],\n        fc_channel: int,\n        kernel_sizes: Tuple[int, ...],\n        strides: Tuple[int, ...],\n        use_bns: Tuple[bool, ...],\n        acts: Tuple[str, ...],\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.in_channel = in_channel\n        self.out_channels = out_channels\n        self.fc_channel = fc_channel\n        self.kernel_sizes = kernel_sizes\n        self.strides = strides\n        self.use_bns = use_bns\n        self.acts = acts\n\n        self.init_layers()\n\n    def init_layers(self):\n        layers_list = []\n        for i in range(len(self.out_channels)):\n            in_channel = self.in_channel if i == 0 else self.out_channels[i - 1]\n            layers_list.append(\n                Conv2DBlock(\n                    in_channel=in_channel,\n                    out_channel=self.out_channels[i],\n                    kernel_size=self.kernel_sizes[i],\n                    stride=self.strides[i],\n                    use_bn=self.use_bns[i],\n                    act=self.acts[i],\n                    mean=0.0,\n                    std=0.04,\n                    value=0.1,\n                )\n            )\n\n        layers_list.append(\n            FCBlock(self.fc_channel, self.acts[4], mean=0.0, std=0.04, value=0.1)\n        )\n        self.layers = nn.LayerList(layers_list)\n\n    def forward_tensor(self, x):\n        y = x\n        y_list = []\n        for layer in self.layers:\n            y = layer(y)\n            y_list.append(y)\n        return y_list  # y_conv1, y_conv2, y_conv3, y_conv4, y_fc(y_out)\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        y_list = []\n        # y1_conv1, y1_conv2, y1_conv3, y1_conv4, y1_fc, y2_conv1, y2_conv2, y2_conv3, y2_conv4, y2_fc\n        for k in x:\n            y_list.extend(self.forward_tensor(x[k]))\n\n        y = self.split_to_dict(y_list, self.output_keys)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n\n        return y\n\n    @staticmethod\n    def split_to_dict(\n        data_list: List[paddle.Tensor], keys: Tuple[str, ...]\n    ) -&gt; Dict[str, paddle.Tensor]:\n        \"\"\"Overwrite of split_to_dict() method belongs to Class base.Arch.\n\n        Reason for overwriting is there is no concat_to_tensor() method called in \"tempoGAN\" example.\n        That is because input in \"tempoGAN\" example is not in a regular format, but a format like:\n        {\n            \"input1\": paddle.concat([in1, in2], axis=1),\n            \"input2\": paddle.concat([in1, in3], axis=1),\n        }\n\n        Args:\n            data_list (List[paddle.Tensor]): The data to be split. It should be a list of tensor(s), but not a paddle.Tensor.\n            keys (Tuple[str, ...]): Keys of outputs.\n\n        Returns:\n            Dict[str, paddle.Tensor]: Dict with split data.\n        \"\"\"\n        if len(keys) == 1:\n            return {keys[0]: data_list[0]}\n        return {key: data_list[i] for i, key in enumerate(keys)}\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.Discriminator.split_to_dict","title":"<code>split_to_dict(data_list, keys)</code>  <code>staticmethod</code>","text":"<p>Overwrite of split_to_dict() method belongs to Class base.Arch.</p> <p>Reason for overwriting is there is no concat_to_tensor() method called in \"tempoGAN\" example. That is because input in \"tempoGAN\" example is not in a regular format, but a format like: {     \"input1\": paddle.concat([in1, in2], axis=1),     \"input2\": paddle.concat([in1, in3], axis=1), }</p> <p>Parameters:</p> Name Type Description Default <code>data_list</code> <code>List[Tensor]</code> <p>The data to be split. It should be a list of tensor(s), but not a paddle.Tensor.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Keys of outputs.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict[str, paddle.Tensor]: Dict with split data.</p> Source code in <code>ppsci/arch/gan.py</code> <pre><code>@staticmethod\ndef split_to_dict(\n    data_list: List[paddle.Tensor], keys: Tuple[str, ...]\n) -&gt; Dict[str, paddle.Tensor]:\n    \"\"\"Overwrite of split_to_dict() method belongs to Class base.Arch.\n\n    Reason for overwriting is there is no concat_to_tensor() method called in \"tempoGAN\" example.\n    That is because input in \"tempoGAN\" example is not in a regular format, but a format like:\n    {\n        \"input1\": paddle.concat([in1, in2], axis=1),\n        \"input2\": paddle.concat([in1, in3], axis=1),\n    }\n\n    Args:\n        data_list (List[paddle.Tensor]): The data to be split. It should be a list of tensor(s), but not a paddle.Tensor.\n        keys (Tuple[str, ...]): Keys of outputs.\n\n    Returns:\n        Dict[str, paddle.Tensor]: Dict with split data.\n    \"\"\"\n    if len(keys) == 1:\n        return {keys[0]: data_list[0]}\n    return {key: data_list[i] for i, key in enumerate(keys)}\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.PhysformerGPT2","title":"<code>PhysformerGPT2</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Transformer decoder model for modeling physics.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"embeds\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"pred_embeds\",).</p> required <code>num_layers</code> <code>int</code> <p>Number of transformer layers.</p> required <code>num_ctx</code> <code>int</code> <p>Context length of block.</p> required <code>embed_size</code> <code>int</code> <p>The number of embedding size.</p> required <code>num_heads</code> <code>int</code> <p>The number of heads in multi-head attention.</p> required <code>embd_pdrop</code> <code>float</code> <p>The dropout probability used on embedding features. Defaults to 0.0.</p> <code>0.0</code> <code>attn_pdrop</code> <code>float</code> <p>The dropout probability used on attention weights. Defaults to 0.0.</p> <code>0.0</code> <code>resid_pdrop</code> <code>float</code> <p>The dropout probability used on block outputs. Defaults to 0.0.</p> <code>0.0</code> <code>initializer_range</code> <code>float</code> <p>Initializer range of linear layer. Defaults to 0.05.</p> <code>0.05</code> <code>embedding_model</code> <code>Optional[Arch]</code> <p>Embedding model, If this parameter is set, the embedding model will map the input data to the embedding space and the output data to the physical space. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.PhysformerGPT2((\"embeds\", ), (\"pred_embeds\", ), 6, 16, 128, 4)\n&gt;&gt;&gt; data = paddle.to_tensor(paddle.randn([10, 16, 128]))\n&gt;&gt;&gt; inputs = {\"embeds\": data}\n&gt;&gt;&gt; outputs = model(inputs)\n&gt;&gt;&gt; print(outputs[\"pred_embeds\"].shape)\n[10, 16, 128]\n</code></pre> Source code in <code>ppsci/arch/physx_transformer.py</code> <pre><code>class PhysformerGPT2(base.Arch):\n    \"\"\"Transformer decoder model for modeling physics.\n\n    Args:\n        input_keys (Tuple[str, ...]): Input keys, such as (\"embeds\",).\n        output_keys (Tuple[str, ...]): Output keys, such as (\"pred_embeds\",).\n        num_layers (int): Number of transformer layers.\n        num_ctx (int): Context length of block.\n        embed_size (int): The number of embedding size.\n        num_heads (int): The number of heads in multi-head attention.\n        embd_pdrop (float, optional): The dropout probability used on embedding features. Defaults to 0.0.\n        attn_pdrop (float, optional): The dropout probability used on attention weights. Defaults to 0.0.\n        resid_pdrop (float, optional): The dropout probability used on block outputs. Defaults to 0.0.\n        initializer_range (float, optional): Initializer range of linear layer. Defaults to 0.05.\n        embedding_model (Optional[base.Arch]): Embedding model, If this parameter is set,\n            the embedding model will map the input data to the embedding space and the\n            output data to the physical space. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.PhysformerGPT2((\"embeds\", ), (\"pred_embeds\", ), 6, 16, 128, 4)\n        &gt;&gt;&gt; data = paddle.to_tensor(paddle.randn([10, 16, 128]))\n        &gt;&gt;&gt; inputs = {\"embeds\": data}\n        &gt;&gt;&gt; outputs = model(inputs)\n        &gt;&gt;&gt; print(outputs[\"pred_embeds\"].shape)\n        [10, 16, 128]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        num_layers: int,\n        num_ctx: int,\n        embed_size: int,\n        num_heads: int,\n        embd_pdrop: float = 0.0,\n        attn_pdrop: float = 0.0,\n        resid_pdrop: float = 0.0,\n        initializer_range: float = 0.05,\n        embedding_model: Optional[base.Arch] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n\n        self.num_layers = num_layers\n        self.num_ctx = num_ctx\n        self.embed_size = embed_size\n        self.num_heads = num_heads\n        self.embd_pdrop = embd_pdrop\n        self.attn_pdrop = attn_pdrop\n        self.resid_pdrop = resid_pdrop\n        self.initializer_range = initializer_range\n\n        self.drop = nn.Dropout(embd_pdrop)\n        self.blocks = nn.LayerList(\n            [\n                Block(\n                    num_ctx, embed_size, num_heads, attn_pdrop, resid_pdrop, scale=True\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        self.ln = nn.LayerNorm(embed_size)\n        self.linear = nn.Linear(embed_size, embed_size)\n\n        self.apply(self._init_weights)\n        self.embedding_model = embedding_model\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            normal_ = Normal(mean=0.0, std=self.initializer_range)\n            normal_(module.weight)\n            if module.bias is not None:\n                zeros_(module.bias)\n        elif isinstance(module, nn.LayerNorm):\n            zeros_(module.bias)\n            ones_(module.weight)\n\n    def get_position_embed(self, x):\n        B, N, _ = x.shape\n        position_ids = paddle.arange(0, N, dtype=paddle.get_default_dtype()).reshape(\n            [1, N, 1]\n        )\n        position_ids = position_ids.repeat_interleave(B, axis=0)\n\n        position_embeds = paddle.zeros_like(x)\n        i = paddle.arange(0, self.embed_size // 2).unsqueeze(0).unsqueeze(0)\n        position_embeds[:, :, ::2] = paddle.sin(\n            position_ids / 10000 ** (2 * i / self.embed_size)\n        )\n        position_embeds[:, :, 1::2] = paddle.cos(\n            position_ids / 10000 ** (2 * i / self.embed_size)\n        )\n        return position_embeds\n\n    def _generate_time_series(self, x, max_length):\n        cur_len = x.shape[1]\n        if cur_len &gt;= max_length:\n            raise ValueError(\n                f\"max_length({max_length}) should be larger than \"\n                f\"the length of input context({cur_len})\"\n            )\n\n        while cur_len &lt; max_length:\n            model_inputs = x[:, -1:]\n            outputs = self.forward_tensor(model_inputs)\n            next_output = outputs[0][:, -1:]\n            x = paddle.concat([x, next_output], axis=1)\n            cur_len = cur_len + 1\n        return x\n\n    @paddle.no_grad()\n    def generate(self, x, max_length=256):\n        if max_length &lt;= 0:\n            raise ValueError(\n                f\"max_length({max_length}) should be a strictly positive integer.\"\n            )\n        outputs = self._generate_time_series(x, max_length)\n        return outputs\n\n    def forward_tensor(self, x):\n        position_embeds = self.get_position_embed(x)\n        # Combine input embedding, position embedding\n        hidden_states = x + position_embeds\n        hidden_states = self.drop(hidden_states)\n\n        # Loop through transformer self-attention layers\n        for block in self.blocks:\n            block_outputs = block(hidden_states)\n            hidden_states = block_outputs[0]\n        outputs = self.linear(self.ln(hidden_states))\n        return (outputs,)\n\n    def forward_eval(self, x):\n        input_embeds = x[:, :1]\n        outputs = self.generate(input_embeds)\n        return (outputs[:, 1:],)\n\n    @staticmethod\n    def split_to_dict(data_tensors, keys):\n        return {key: data_tensors[i] for i, key in enumerate(keys)}\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n        x_tensor = self.concat_to_tensor(x, self.input_keys, axis=-1)\n        if self.embedding_model is not None:\n            if isinstance(self.embedding_model, CylinderEmbedding):\n                x_tensor = self.embedding_model.encoder(x_tensor, x[\"visc\"])\n            else:\n                x_tensor = self.embedding_model.encoder(x_tensor)\n\n        if self.training:\n            y = self.forward_tensor(x_tensor)\n        else:\n            y = self.forward_eval(x_tensor)\n\n        if self.embedding_model is not None:\n            y = (self.embedding_model.decoder(y[0]),)\n\n        y = self.split_to_dict(y, self.output_keys)\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.ModelList","title":"<code>ModelList</code>","text":"<p>               Bases: <code>Arch</code></p> <p>ModelList layer which wrap more than one model that shares inputs.</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>Tuple[Arch, ...]</code> <p>Model(s) nested in tuple.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model1 = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\"), 10, 128)\n&gt;&gt;&gt; model2 = ppsci.arch.MLP((\"x\", \"y\"), (\"w\", \"p\"), 5, 128)\n&gt;&gt;&gt; model = ppsci.arch.ModelList((model1, model2))\n&gt;&gt;&gt; input_dict = {\"x\": paddle.rand([64, 64, 1]),\"y\": paddle.rand([64, 64, 1])}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; for k, v in output_dict.items():\n...     print(k, v.shape)\nu [64, 64, 1]\nv [64, 64, 1]\nw [64, 64, 1]\np [64, 64, 1]\n</code></pre> Source code in <code>ppsci/arch/model_list.py</code> <pre><code>class ModelList(base.Arch):\n    \"\"\"ModelList layer which wrap more than one model that shares inputs.\n\n    Args:\n        model_list (Tuple[base.Arch, ...]): Model(s) nested in tuple.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model1 = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\"), 10, 128)\n        &gt;&gt;&gt; model2 = ppsci.arch.MLP((\"x\", \"y\"), (\"w\", \"p\"), 5, 128)\n        &gt;&gt;&gt; model = ppsci.arch.ModelList((model1, model2))\n        &gt;&gt;&gt; input_dict = {\"x\": paddle.rand([64, 64, 1]),\"y\": paddle.rand([64, 64, 1])}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; for k, v in output_dict.items():\n        ...     print(k, v.shape)\n        u [64, 64, 1]\n        v [64, 64, 1]\n        w [64, 64, 1]\n        p [64, 64, 1]\n    \"\"\"\n\n    def __init__(\n        self,\n        model_list: Tuple[base.Arch, ...],\n    ):\n        super().__init__()\n        self.input_keys = sum([model.input_keys for model in model_list], ())\n        self.input_keys = set(self.input_keys)\n\n        output_keys_set = set()\n        for model in model_list:\n            if len(output_keys_set &amp; set(model.output_keys)):\n                raise ValueError(\n                    \"output_keys of model from model_list should be unique,\"\n                    f\"but got duplicate keys: {output_keys_set &amp; set(model.output_keys)}\"\n                )\n            output_keys_set = output_keys_set | set(model.output_keys)\n        self.output_keys = tuple(output_keys_set)\n\n        self.model_list = nn.LayerList(model_list)\n\n    def forward(self, x):\n        y_all = {}\n        for model in self.model_list:\n            y = model(x)\n            y_all.update(y)\n\n        return y_all\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.AFNONet","title":"<code>AFNONet</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Adaptive Fourier Neural Network.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>img_size</code> <code>Tuple[int, ...]</code> <p>Image size. Defaults to (720, 1440).</p> <code>(720, 1440)</code> <code>patch_size</code> <code>Tuple[int, ...]</code> <p>Path. Defaults to (8, 8).</p> <code>(8, 8)</code> <code>in_channels</code> <code>int</code> <p>The input tensor channels. Defaults to 20.</p> <code>20</code> <code>out_channels</code> <code>int</code> <p>The output tensor channels. Defaults to 20.</p> <code>20</code> <code>embed_dim</code> <code>int</code> <p>The embedding dimension for PatchEmbed. Defaults to 768.</p> <code>768</code> <code>depth</code> <code>int</code> <p>Number of transformer depth. Defaults to 12.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Number of ratio used in MLP. Defaults to 4.0.</p> <code>4.0</code> <code>drop_rate</code> <code>float</code> <p>The drop ratio used in MLP. Defaults to 0.0.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>The drop ratio used in DropPath. Defaults to 0.0.</p> <code>0.0</code> <code>num_blocks</code> <code>int</code> <p>Number of blocks. Defaults to 8.</p> <code>8</code> <code>sparsity_threshold</code> <code>float</code> <p>The value of threshold for softshrink. Defaults to 0.01.</p> <code>0.01</code> <code>hard_thresholding_fraction</code> <code>float</code> <p>The value of threshold for keep mode. Defaults to 1.0.</p> <code>1.0</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamp. Defaults to 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.AFNONet((\"input\", ), (\"output\", ))\n&gt;&gt;&gt; input_data = {\"input\": paddle.randn([1, 20, 720, 1440])}\n&gt;&gt;&gt; output_data = model(input_data)\n&gt;&gt;&gt; for k, v in output_data.items():\n...     print(k, v.shape)\noutput [1, 20, 720, 1440]\n</code></pre> Source code in <code>ppsci/arch/afno.py</code> <pre><code>class AFNONet(base.Arch):\n    \"\"\"Adaptive Fourier Neural Network.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n        img_size (Tuple[int, ...], optional): Image size. Defaults to (720, 1440).\n        patch_size (Tuple[int, ...], optional): Path. Defaults to (8, 8).\n        in_channels (int, optional): The input tensor channels. Defaults to 20.\n        out_channels (int, optional): The output tensor channels. Defaults to 20.\n        embed_dim (int, optional): The embedding dimension for PatchEmbed. Defaults to 768.\n        depth (int, optional): Number of transformer depth. Defaults to 12.\n        mlp_ratio (float, optional): Number of ratio used in MLP. Defaults to 4.0.\n        drop_rate (float, optional): The drop ratio used in MLP. Defaults to 0.0.\n        drop_path_rate (float, optional): The drop ratio used in DropPath. Defaults to 0.0.\n        num_blocks (int, optional): Number of blocks. Defaults to 8.\n        sparsity_threshold (float, optional): The value of threshold for softshrink. Defaults to 0.01.\n        hard_thresholding_fraction (float, optional): The value of threshold for keep mode. Defaults to 1.0.\n        num_timestamps (int, optional): Number of timestamp. Defaults to 1.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.AFNONet((\"input\", ), (\"output\", ))\n        &gt;&gt;&gt; input_data = {\"input\": paddle.randn([1, 20, 720, 1440])}\n        &gt;&gt;&gt; output_data = model(input_data)\n        &gt;&gt;&gt; for k, v in output_data.items():\n        ...     print(k, v.shape)\n        output [1, 20, 720, 1440]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        img_size: Tuple[int, ...] = (720, 1440),\n        patch_size: Tuple[int, ...] = (8, 8),\n        in_channels: int = 20,\n        out_channels: int = 20,\n        embed_dim: int = 768,\n        depth: int = 12,\n        mlp_ratio: float = 4.0,\n        drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        num_blocks: int = 8,\n        sparsity_threshold: float = 0.01,\n        hard_thresholding_fraction: float = 1.0,\n        num_timestamps: int = 1,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.embed_dim = embed_dim\n        self.num_blocks = num_blocks\n        self.num_timestamps = num_timestamps\n        norm_layer = partial(nn.LayerNorm, epsilon=1e-6)\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=self.patch_size,\n            in_channels=self.in_channels,\n            embed_dim=embed_dim,\n        )\n        num_patches = self.patch_embed.num_patches\n\n        data = paddle.zeros((1, num_patches, embed_dim))\n        data = initializer.trunc_normal_(data, std=0.02)\n        self.pos_embed = paddle.create_parameter(\n            shape=data.shape,\n            dtype=data.dtype,\n            default_initializer=nn.initializer.Assign(data),\n        )\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in paddle.linspace(0, drop_path_rate, depth)]\n\n        self.h = img_size[0] // self.patch_size[0]\n        self.w = img_size[1] // self.patch_size[1]\n\n        self.blocks = nn.LayerList(\n            [\n                Block(\n                    dim=embed_dim,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    num_blocks=self.num_blocks,\n                    sparsity_threshold=sparsity_threshold,\n                    hard_thresholding_fraction=hard_thresholding_fraction,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.norm = norm_layer(embed_dim)\n        self.head = nn.Linear(\n            embed_dim,\n            self.out_channels * self.patch_size[0] * self.patch_size[1],\n            bias_attr=False,\n        )\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            initializer.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                initializer.zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            initializer.ones_(m.weight)\n            initializer.zeros_(m.bias)\n        elif isinstance(m, nn.Conv2D):\n            initializer.conv_init_(m)\n\n    def forward_tensor(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        x = x.reshape((B, self.h, self.w, self.embed_dim))\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.head(x)\n\n        b = x.shape[0]\n        p1 = self.patch_size[0]\n        p2 = self.patch_size[1]\n        h = self.img_size[0] // self.patch_size[0]\n        w = self.img_size[1] // self.patch_size[1]\n        c_out = x.shape[3] // (p1 * p2)\n        x = x.reshape((b, h, w, p1, p2, c_out))\n        x = x.transpose((0, 5, 1, 3, 2, 4))\n        x = x.reshape((b, c_out, h * p1, w * p2))\n\n        return x\n\n    @staticmethod\n    def split_to_dict(data_tensors: Tuple[paddle.Tensor, ...], keys: Tuple[str, ...]):\n        return {key: data_tensors[i] for i, key in enumerate(keys)}\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        x_tensor = self.concat_to_tensor(x, self.input_keys)\n\n        y = []\n        input = x_tensor\n        for _ in range(self.num_timestamps):\n            out = self.forward_tensor(input)\n            y.append(out)\n            input = out\n        y = self.split_to_dict(y, self.output_keys)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.PrecipNet","title":"<code>PrecipNet</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Precipitation Network.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>wind_model</code> <code>Arch</code> <p>Wind model.</p> required <code>img_size</code> <code>Tuple[int, ...]</code> <p>Image size. Defaults to (720, 1440).</p> <code>(720, 1440)</code> <code>patch_size</code> <code>Tuple[int, ...]</code> <p>Path. Defaults to (8, 8).</p> <code>(8, 8)</code> <code>in_channels</code> <code>int</code> <p>The input tensor channels. Defaults to 20.</p> <code>20</code> <code>out_channels</code> <code>int</code> <p>The output tensor channels. Defaults to 1.</p> <code>1</code> <code>embed_dim</code> <code>int</code> <p>The embedding dimension for PatchEmbed. Defaults to 768.</p> <code>768</code> <code>depth</code> <code>int</code> <p>Number of transformer depth. Defaults to 12.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Number of ratio used in MLP. Defaults to 4.0.</p> <code>4.0</code> <code>drop_rate</code> <code>float</code> <p>The drop ratio used in MLP. Defaults to 0.0.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>The drop ratio used in DropPath. Defaults to 0.0.</p> <code>0.0</code> <code>num_blocks</code> <code>int</code> <p>Number of blocks. Defaults to 8.</p> <code>8</code> <code>sparsity_threshold</code> <code>float</code> <p>The value of threshold for softshrink. Defaults to 0.01.</p> <code>0.01</code> <code>hard_thresholding_fraction</code> <code>float</code> <p>The value of threshold for keep mode. Defaults to 1.0.</p> <code>1.0</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamp. Defaults to 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; wind_model = ppsci.arch.AFNONet((\"input\", ), (\"output\", ))\n&gt;&gt;&gt; model = ppsci.arch.PrecipNet((\"input\", ), (\"output\", ), wind_model)\n&gt;&gt;&gt; data = paddle.randn([1, 20, 720, 1440])\n&gt;&gt;&gt; data_dict = {\"input\": data}\n&gt;&gt;&gt; output = model.forward(data_dict)\n&gt;&gt;&gt; print(output['output'].shape)\n[1, 1, 720, 1440]\n</code></pre> Source code in <code>ppsci/arch/afno.py</code> <pre><code>class PrecipNet(base.Arch):\n    \"\"\"Precipitation Network.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n        wind_model (base.Arch): Wind model.\n        img_size (Tuple[int, ...], optional): Image size. Defaults to (720, 1440).\n        patch_size (Tuple[int, ...], optional): Path. Defaults to (8, 8).\n        in_channels (int, optional): The input tensor channels. Defaults to 20.\n        out_channels (int, optional): The output tensor channels. Defaults to 1.\n        embed_dim (int, optional): The embedding dimension for PatchEmbed. Defaults to 768.\n        depth (int, optional): Number of transformer depth. Defaults to 12.\n        mlp_ratio (float, optional): Number of ratio used in MLP. Defaults to 4.0.\n        drop_rate (float, optional): The drop ratio used in MLP. Defaults to 0.0.\n        drop_path_rate (float, optional): The drop ratio used in DropPath. Defaults to 0.0.\n        num_blocks (int, optional): Number of blocks. Defaults to 8.\n        sparsity_threshold (float, optional): The value of threshold for softshrink. Defaults to 0.01.\n        hard_thresholding_fraction (float, optional): The value of threshold for keep mode. Defaults to 1.0.\n        num_timestamps (int, optional): Number of timestamp. Defaults to 1.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; wind_model = ppsci.arch.AFNONet((\"input\", ), (\"output\", ))\n        &gt;&gt;&gt; model = ppsci.arch.PrecipNet((\"input\", ), (\"output\", ), wind_model)\n        &gt;&gt;&gt; data = paddle.randn([1, 20, 720, 1440])\n        &gt;&gt;&gt; data_dict = {\"input\": data}\n        &gt;&gt;&gt; output = model.forward(data_dict)\n        &gt;&gt;&gt; print(output['output'].shape)\n        [1, 1, 720, 1440]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        wind_model: base.Arch,\n        img_size: Tuple[int, ...] = (720, 1440),\n        patch_size: Tuple[int, ...] = (8, 8),\n        in_channels: int = 20,\n        out_channels: int = 1,\n        embed_dim: int = 768,\n        depth: int = 12,\n        mlp_ratio: float = 4.0,\n        drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        num_blocks: int = 8,\n        sparsity_threshold: float = 0.01,\n        hard_thresholding_fraction: float = 1.0,\n        num_timestamps=1,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.embed_dim = embed_dim\n        self.num_blocks = num_blocks\n        self.num_timestamps = num_timestamps\n        self.backbone = AFNONet(\n            (\"input\",),\n            (\"output\",),\n            img_size=img_size,\n            patch_size=patch_size,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            embed_dim=embed_dim,\n            depth=depth,\n            mlp_ratio=mlp_ratio,\n            drop_rate=drop_rate,\n            drop_path_rate=drop_path_rate,\n            num_blocks=num_blocks,\n            sparsity_threshold=sparsity_threshold,\n            hard_thresholding_fraction=hard_thresholding_fraction,\n        )\n        self.ppad = PeriodicPad2d(1)\n        self.conv = nn.Conv2D(\n            self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=0\n        )\n        self.act = nn.ReLU()\n        self.apply(self._init_weights)\n        self.wind_model = wind_model\n        self.wind_model.eval()\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            initializer.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                initializer.zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            initializer.ones_(m.weight)\n            initializer.zeros_(m.bias)\n        elif isinstance(m, nn.Conv2D):\n            initializer.conv_init_(m)\n\n    def forward_tensor(self, x):\n        x = self.backbone.forward_tensor(x)\n        x = self.ppad(x)\n        x = self.conv(x)\n        x = self.act(x)\n        return x\n\n    @staticmethod\n    def split_to_dict(data_tensors: Tuple[paddle.Tensor, ...], keys: Tuple[str, ...]):\n        return {key: data_tensors[i] for i, key in enumerate(keys)}\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        x_tensor = self.concat_to_tensor(x, self.input_keys)\n\n        input_wind = x_tensor\n        y = []\n        for _ in range(self.num_timestamps):\n            with paddle.no_grad():\n                out_wind = self.wind_model.forward_tensor(input_wind)\n            out = self.forward_tensor(out_wind)\n            y.append(out)\n            input_wind = out_wind\n        y = self.split_to_dict(y, self.output_keys)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.PhyCRNet","title":"<code>PhyCRNet</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Physics-informed convolutional-recurrent neural networks.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>The input channels.</p> required <code>hidden_channels</code> <code>Tuple[int, ...]</code> <p>The hidden channels.</p> required <code>input_kernel_size</code> <code>Tuple[int, ...]</code> <p>The input kernel size(s).</p> required <code>input_stride</code> <code>Tuple[int, ...]</code> <p>The input stride(s).</p> required <code>input_padding</code> <code>Tuple[int, ...]</code> <p>The input padding(s).</p> required <code>dt</code> <code>float</code> <p>The dt parameter.</p> required <code>num_layers</code> <code>Tuple[int, ...]</code> <p>The number of layers.</p> required <code>upscale_factor</code> <code>int</code> <p>The upscale factor.</p> required <code>step</code> <code>int</code> <p>The step(s). Defaults to 1.</p> <code>1</code> <code>effective_step</code> <code>Tuple[int, ...]</code> <p>The effective step. Defaults to (1, ).</p> <code>(1)</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.PhyCRNet(\n...     input_channels=2,\n...     hidden_channels=[8, 32, 128, 128],\n...     input_kernel_size=[4, 4, 4, 3],\n...     input_stride=[2, 2, 2, 1],\n...     input_padding=[1, 1, 1, 1],\n...     dt=0.002,\n...     num_layers=[3, 1],\n...     upscale_factor=8\n... )\n</code></pre> Source code in <code>ppsci/arch/phycrnet.py</code> <pre><code>class PhyCRNet(base.Arch):\n    \"\"\"Physics-informed convolutional-recurrent neural networks.\n\n    Args:\n        input_channels (int): The input channels.\n        hidden_channels (Tuple[int, ...]): The hidden channels.\n        input_kernel_size (Tuple[int, ...]):  The input kernel size(s).\n        input_stride (Tuple[int, ...]): The input stride(s).\n        input_padding (Tuple[int, ...]): The input padding(s).\n        dt (float): The dt parameter.\n        num_layers (Tuple[int, ...]): The number of layers.\n        upscale_factor (int): The upscale factor.\n        step (int, optional): The step(s). Defaults to 1.\n        effective_step (Tuple[int, ...], optional): The effective step. Defaults to (1, ).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.PhyCRNet(\n        ...     input_channels=2,\n        ...     hidden_channels=[8, 32, 128, 128],\n        ...     input_kernel_size=[4, 4, 4, 3],\n        ...     input_stride=[2, 2, 2, 1],\n        ...     input_padding=[1, 1, 1, 1],\n        ...     dt=0.002,\n        ...     num_layers=[3, 1],\n        ...     upscale_factor=8\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_channels: int,\n        hidden_channels: Tuple[int, ...],\n        input_kernel_size: Tuple[int, ...],\n        input_stride: Tuple[int, ...],\n        input_padding: Tuple[int, ...],\n        dt: float,\n        num_layers: Tuple[int, ...],\n        upscale_factor: int,\n        step: int = 1,\n        effective_step: Tuple[int, ...] = (1,),\n    ):\n        super(PhyCRNet, self).__init__()\n\n        # input channels of layer includes input_channels and hidden_channels of cells\n        self.input_channels = [input_channels] + hidden_channels\n        self.hidden_channels = hidden_channels\n        self.input_kernel_size = input_kernel_size\n        self.input_stride = input_stride\n        self.input_padding = input_padding\n        self.step = step\n        self.effective_step = effective_step\n        self._all_layers = []\n        self.dt = dt\n        self.upscale_factor = upscale_factor\n\n        # number of layers\n        self.num_encoder = num_layers[0]\n        self.num_convlstm = num_layers[1]\n\n        # encoder - downsampling\n        self.encoder = paddle.nn.LayerList(\n            [\n                encoder_block(\n                    input_channels=self.input_channels[i],\n                    hidden_channels=self.hidden_channels[i],\n                    input_kernel_size=self.input_kernel_size[i],\n                    input_stride=self.input_stride[i],\n                    input_padding=self.input_padding[i],\n                )\n                for i in range(self.num_encoder)\n            ]\n        )\n\n        # ConvLSTM\n        self.convlstm = paddle.nn.LayerList(\n            [\n                ConvLSTMCell(\n                    input_channels=self.input_channels[i],\n                    hidden_channels=self.hidden_channels[i],\n                    input_kernel_size=self.input_kernel_size[i],\n                    input_stride=self.input_stride[i],\n                    input_padding=self.input_padding[i],\n                )\n                for i in range(self.num_encoder, self.num_encoder + self.num_convlstm)\n            ]\n        )\n\n        # output layer\n        self.output_layer = nn.Conv2D(\n            2, 2, kernel_size=5, stride=1, padding=2, padding_mode=\"circular\"\n        )\n\n        # pixelshuffle - upscale\n        self.pixelshuffle = nn.PixelShuffle(self.upscale_factor)\n\n        # initialize weights\n        self.apply(_initialize_weights)\n        initializer_0 = paddle.nn.initializer.Constant(0.0)\n        initializer_0(self.output_layer.bias)\n        self.enable_transform = True\n\n    def forward(self, x):\n        if self.enable_transform:\n            if self._input_transform is not None:\n                x = self._input_transform(x)\n        output_x = x\n\n        self.initial_state = x[\"initial_state\"]\n        x = x[\"input\"]\n        internal_state = []\n        outputs = []\n        second_last_state = []\n\n        for step in range(self.step):\n            xt = x\n\n            # encoder\n            for encoder in self.encoder:\n                x = encoder(x)\n\n            # convlstm\n            for i, lstm in enumerate(self.convlstm, self.num_encoder):\n                if step == 0:\n                    (h, c) = lstm.init_hidden_tensor(\n                        prev_state=self.initial_state[i - self.num_encoder]\n                    )\n                    internal_state.append((h, c))\n\n                # one-step forward\n                (h, c) = internal_state[i - self.num_encoder]\n                x, new_c = lstm(x, h, c)\n                internal_state[i - self.num_encoder] = (x, new_c)\n\n            # output\n            x = self.pixelshuffle(x)\n            x = self.output_layer(x)\n\n            # residual connection\n            x = xt + self.dt * x\n\n            if step == (self.step - 2):\n                second_last_state = internal_state.copy()\n\n            if step in self.effective_step:\n                outputs.append(x)\n\n        result_dict = {\"outputs\": outputs, \"second_last_state\": second_last_state}\n        if self.enable_transform:\n            if self._output_transform is not None:\n                result_dict = self._output_transform(output_x, result_dict)\n        return result_dict\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.UNetEx","title":"<code>UNetEx</code>","text":"<p>               Bases: <code>Arch</code></p> <p>U-Net Extension for CFD.</p> <p>Reference: Ribeiro M D, Rehman A, Ahmed S, et al. DeepCFD: Efficient steady-state laminar flow approximation with deep convolutional neural networks[J]. arXiv preprint arXiv:2004.08826, 2020.</p> <p>Parameters:</p> Name Type Description Default <code>input_key</code> <code>str</code> <p>Name of function data for input.</p> required <code>output_key</code> <code>str</code> <p>Name of function data for output.</p> required <code>in_channel</code> <code>int</code> <p>Number of channels of input.</p> required <code>out_channel</code> <code>int</code> <p>Number of channels of output.</p> required <code>kernel_size</code> <code>int</code> <p>Size of kernel of convolution layer. Defaults to 3.</p> <code>3</code> <code>filters</code> <code>Tuple[int, ...]</code> <p>Number of filters. Defaults to (16, 32, 64).</p> <code>(16, 32, 64)</code> <code>layers</code> <code>int</code> <p>Number of encoders or decoders. Defaults to 3.</p> <code>3</code> <code>weight_norm</code> <code>bool</code> <p>Whether use weight normalization layer. Defaults to True.</p> <code>True</code> <code>batch_norm</code> <code>bool</code> <p>Whether add batch normalization layer. Defaults to True.</p> <code>True</code> <code>activation</code> <code>Type[Layer]</code> <p>Name of activation function. Defaults to nn.ReLU.</p> <code>ReLU</code> <code>final_activation</code> <code>Optional[Type[Layer]]</code> <p>Name of final activation function. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.UNetEx(\n...     input_key=\"input\",\n...     output_key=\"output\",\n...     in_channel=3,\n...     out_channel=3,\n...     kernel_size=5,\n...     filters=(4, 4, 4, 4),\n...     layers=3,\n...     weight_norm=False,\n...     batch_norm=False,\n...     activation=None,\n...     final_activation=None,\n... )\n&gt;&gt;&gt; input_dict = {'input': paddle.rand([4, 3, 4, 4])}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict['output'])\n&gt;&gt;&gt; print(output_dict['output'].shape)\n[4, 3, 4, 4]\n</code></pre> Source code in <code>ppsci/arch/unetex.py</code> <pre><code>class UNetEx(base.Arch):\n    \"\"\"U-Net Extension for CFD.\n\n    Reference: [Ribeiro M D, Rehman A, Ahmed S, et al. DeepCFD: Efficient steady-state laminar flow approximation with deep convolutional neural networks[J]. arXiv preprint arXiv:2004.08826, 2020.](https://arxiv.org/abs/2004.08826)\n\n    Args:\n        input_key (str): Name of function data for input.\n        output_key (str): Name of function data for output.\n        in_channel (int): Number of channels of input.\n        out_channel (int): Number of channels of output.\n        kernel_size (int, optional): Size of kernel of convolution layer. Defaults to 3.\n        filters (Tuple[int, ...], optional): Number of filters. Defaults to (16, 32, 64).\n        layers (int, optional): Number of encoders or decoders. Defaults to 3.\n        weight_norm (bool, optional): Whether use weight normalization layer. Defaults to True.\n        batch_norm (bool, optional): Whether add batch normalization layer. Defaults to True.\n        activation (Type[nn.Layer], optional): Name of activation function. Defaults to nn.ReLU.\n        final_activation (Optional[Type[nn.Layer]]): Name of final activation function. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.UNetEx(\n        ...     input_key=\"input\",\n        ...     output_key=\"output\",\n        ...     in_channel=3,\n        ...     out_channel=3,\n        ...     kernel_size=5,\n        ...     filters=(4, 4, 4, 4),\n        ...     layers=3,\n        ...     weight_norm=False,\n        ...     batch_norm=False,\n        ...     activation=None,\n        ...     final_activation=None,\n        ... )\n        &gt;&gt;&gt; input_dict = {'input': paddle.rand([4, 3, 4, 4])}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict['output']) # doctest: +SKIP\n        &gt;&gt;&gt; print(output_dict['output'].shape)\n        [4, 3, 4, 4]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_key: str,\n        output_key: str,\n        in_channel: int,\n        out_channel: int,\n        kernel_size: int = 3,\n        filters: Tuple[int, ...] = (16, 32, 64),\n        layers: int = 3,\n        weight_norm: bool = True,\n        batch_norm: bool = True,\n        activation: Type[nn.Layer] = nn.ReLU,\n        final_activation: Optional[Type[nn.Layer]] = None,\n    ):\n        if len(filters) == 0:\n            raise ValueError(\"The filters shouldn't be empty \")\n\n        super().__init__()\n        self.input_keys = (input_key,)\n        self.output_keys = (output_key,)\n        self.final_activation = final_activation\n        self.encoder = create_encoder(\n            in_channel,\n            filters,\n            kernel_size,\n            weight_norm,\n            batch_norm,\n            activation,\n            layers,\n        )\n        decoders = [\n            create_decoder(\n                1, filters, kernel_size, weight_norm, batch_norm, activation, layers\n            )\n            for i in range(out_channel)\n        ]\n        self.decoders = nn.Sequential(*decoders)\n\n    def encode(self, x):\n        tensors = []\n        indices = []\n        sizes = []\n        for encoder in self.encoder:\n            x = encoder(x)\n            sizes.append(x.shape)\n            tensors.append(x)\n            x, ind = nn.functional.max_pool2d(x, 2, 2, return_mask=True)\n            indices.append(ind)\n        return x, tensors, indices, sizes\n\n    def decode(self, x, tensors, indices, sizes):\n        y = []\n        for _decoder in self.decoders:\n            _x = x\n            _tensors = tensors[:]\n            _indices = indices[:]\n            _sizes = sizes[:]\n            for decoder in _decoder:\n                tensor = _tensors.pop()\n                size = _sizes.pop()\n                indice = _indices.pop()\n                # upsample operations\n                _x = nn.functional.max_unpool2d(_x, indice, 2, 2, output_size=size)\n                _x = paddle.concat([tensor, _x], axis=1)\n                _x = decoder(_x)\n            y.append(_x)\n        return paddle.concat(y, axis=1)\n\n    def forward(self, x):\n        x = x[self.input_keys[0]]\n        x, tensors, indices, sizes = self.encode(x)\n        x = self.decode(x, tensors, indices, sizes)\n        if self.final_activation is not None:\n            x = self.final_activation(x)\n        return {self.output_keys[0]: x}\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.USCNN","title":"<code>USCNN</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Physics-informed convolutional neural networks.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"coords\").</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"outputV\").</p> required <code>hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>The hidden channel for convolutional layers</p> required <code>h</code> <code>float</code> <p>The spatial step</p> required <code>nx</code> <code>int</code> <p>the number of grids along x-axis</p> required <code>ny</code> <code>int</code> <p>The number of grids along y-axis</p> required <code>nvar_in</code> <code>int</code> <p>input channel. Defaults to 1.</p> <code>1</code> <code>nvar_out</code> <code>int</code> <p>Output channel. Defaults to 1.</p> <code>1</code> <code>pad_singleside</code> <code>int</code> <p>Pad for hard boundary constraint. Defaults to 1.</p> <code>1</code> <code>k</code> <code>int</code> <p>Kernel_size. Defaults to 5.</p> <code>5</code> <code>s</code> <code>int</code> <p>Stride. Defaults to 1.</p> <code>1</code> <code>p</code> <code>int</code> <p>Padding. Defaults to 2.</p> <code>2</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.USCNN(\n...     [\"coords\"],\n...     [\"outputV\"],\n...     [16, 32, 16],\n...     h=0.01,\n...     ny=19,\n...     nx=84,\n...     nvar_in=2,\n...     nvar_out=1,\n...     pad_singleside=1,\n... )\n</code></pre> Source code in <code>ppsci/arch/uscnn.py</code> <pre><code>class USCNN(base.Arch):\n    \"\"\"Physics-informed convolutional neural networks.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"coords\").\n        output_keys (Tuple[str, ...]):Name of output keys, such as (\"outputV\").\n        hidden_size (Union[int, Tuple[int, ...]]): The hidden channel for convolutional layers\n        h (float): The spatial step\n        nx (int):  the number of grids along x-axis\n        ny (int): The number of grids along y-axis\n        nvar_in (int, optional):  input channel. Defaults to 1.\n        nvar_out (int, optional): Output channel. Defaults to 1.\n        pad_singleside (int, optional): Pad for hard boundary constraint. Defaults to 1.\n        k (int, optional): Kernel_size. Defaults to 5.\n        s (int, optional): Stride. Defaults to 1.\n        p (int, optional): Padding. Defaults to 2.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.USCNN(\n        ...     [\"coords\"],\n        ...     [\"outputV\"],\n        ...     [16, 32, 16],\n        ...     h=0.01,\n        ...     ny=19,\n        ...     nx=84,\n        ...     nvar_in=2,\n        ...     nvar_out=1,\n        ...     pad_singleside=1,\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        hidden_size: Union[int, Tuple[int, ...]],\n        h: float,\n        nx: int,\n        ny: int,\n        nvar_in: int = 1,\n        nvar_out: int = 1,\n        pad_singleside: int = 1,\n        k: int = 5,\n        s: int = 1,\n        p: int = 2,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.nvar_in = nvar_in\n        self.nvar_out = nvar_out\n        self.k = k\n        self.s = s\n        self.p = p\n        self.deltaX = h\n        self.nx = nx\n        self.ny = ny\n        self.pad_singleside = pad_singleside\n        self.relu = nn.ReLU()\n        self.US = nn.Upsample(size=[self.ny - 2, self.nx - 2], mode=\"bicubic\")\n        self.conv1 = nn.Conv2D(\n            self.nvar_in, hidden_size[0], kernel_size=k, stride=s, padding=p\n        )\n        self.conv2 = nn.Conv2D(\n            hidden_size[0], hidden_size[1], kernel_size=k, stride=s, padding=p\n        )\n        self.conv3 = nn.Conv2D(\n            hidden_size[1], hidden_size[2], kernel_size=k, stride=s, padding=p\n        )\n        self.conv4 = nn.Conv2D(\n            hidden_size[2], self.nvar_out, kernel_size=k, stride=s, padding=p\n        )\n        self.pixel_shuffle = nn.PixelShuffle(1)\n        self.apply(self.init_weights)\n        self.udfpad = nn.Pad2D(\n            [pad_singleside, pad_singleside, pad_singleside, pad_singleside], value=0\n        )\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Conv2D):\n            bound = 1 / np.sqrt(np.prod(m.weight.shape[1:]))\n            ppsci.utils.initializer.uniform_(m.weight, -bound, bound)\n            if m.bias is not None:\n                ppsci.utils.initializer.uniform_(m.bias, -bound, bound)\n\n    def forward(self, x):\n        y = self.concat_to_tensor(x, self.input_keys, axis=-1)\n        y = self.US(y)\n        y = self.relu(self.conv1(y))\n        y = self.relu(self.conv2(y))\n        y = self.relu(self.conv3(y))\n        y = self.pixel_shuffle(self.conv4(y))\n\n        y = self.udfpad(y)\n        y = y[:, 0, :, :].reshape([y.shape[0], 1, y.shape[2], y.shape[3]])\n        y = self.split_to_dict(y, self.output_keys)\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.NowcastNet","title":"<code>NowcastNet</code>","text":"<p>               Bases: <code>Arch</code></p> <p>The NowcastNet model.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>input_length</code> <code>int</code> <p>Input length. Defaults to 9.</p> <code>9</code> <code>total_length</code> <code>int</code> <p>Total length. Defaults to 29.</p> <code>29</code> <code>image_height</code> <code>int</code> <p>Image height. Defaults to 512.</p> <code>512</code> <code>image_width</code> <code>int</code> <p>Image width. Defaults to 512.</p> <code>512</code> <code>image_ch</code> <code>int</code> <p>Image channel. Defaults to 2.</p> <code>2</code> <code>ngf</code> <code>int</code> <p>Noise Projector input length. Defaults to 32.</p> <code>32</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.NowcastNet((\"input\", ), (\"output\", ))\n&gt;&gt;&gt; input_data = paddle.rand([1, 9, 512, 512, 2])\n&gt;&gt;&gt; input_dict = {\"input\": input_data}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict[\"output\"].shape)\n[1, 20, 512, 512, 1]\n</code></pre> Source code in <code>ppsci/arch/nowcastnet.py</code> <pre><code>class NowcastNet(base.Arch):\n    \"\"\"The NowcastNet model.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n        input_length (int, optional): Input length. Defaults to 9.\n        total_length (int, optional): Total length. Defaults to 29.\n        image_height (int, optional): Image height. Defaults to 512.\n        image_width (int, optional): Image width. Defaults to 512.\n        image_ch (int, optional): Image channel. Defaults to 2.\n        ngf (int, optional): Noise Projector input length. Defaults to 32.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.NowcastNet((\"input\", ), (\"output\", ))\n        &gt;&gt;&gt; input_data = paddle.rand([1, 9, 512, 512, 2])\n        &gt;&gt;&gt; input_dict = {\"input\": input_data}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict[\"output\"].shape)\n        [1, 20, 512, 512, 1]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        input_length: int = 9,\n        total_length: int = 29,\n        image_height: int = 512,\n        image_width: int = 512,\n        image_ch: int = 2,\n        ngf: int = 32,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n\n        self.input_length = input_length\n        self.total_length = total_length\n        self.image_height = image_height\n        self.image_width = image_width\n        self.image_ch = image_ch\n        self.ngf = ngf\n\n        configs = collections.namedtuple(\n            \"Object\", [\"ngf\", \"evo_ic\", \"gen_oc\", \"ic_feature\"]\n        )\n        configs.ngf = self.ngf\n        configs.evo_ic = self.total_length - self.input_length\n        configs.gen_oc = self.total_length - self.input_length\n        configs.ic_feature = self.ngf * 10\n\n        self.pred_length = self.total_length - self.input_length\n        self.evo_net = Evolution_Network(self.input_length, self.pred_length, base_c=32)\n        self.gen_enc = Generative_Encoder(self.total_length, base_c=self.ngf)\n        self.gen_dec = Generative_Decoder(configs)\n        self.proj = Noise_Projector(self.ngf)\n        sample_tensor = paddle.zeros(shape=[1, 1, self.image_height, self.image_width])\n        self.grid = make_grid(sample_tensor)\n\n    @staticmethod\n    def split_to_dict(data_tensors: Tuple[paddle.Tensor, ...], keys: Tuple[str, ...]):\n        return {key: data_tensors[i] for i, key in enumerate(keys)}\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        x_tensor = self.concat_to_tensor(x, self.input_keys)\n\n        y = []\n        out = self.forward_tensor(x_tensor)\n        y.append(out)\n        y = self.split_to_dict(y, self.output_keys)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n\n    def forward_tensor(self, x):\n        all_frames = x[:, :, :, :, :1]\n        frames = all_frames.transpose(perm=[0, 1, 4, 2, 3])\n        batch = frames.shape[0]\n        height = frames.shape[3]\n        width = frames.shape[4]\n        # Input Frames\n        input_frames = frames[:, : self.input_length]\n        input_frames = input_frames.reshape((batch, self.input_length, height, width))\n        # Evolution Network\n        intensity, motion = self.evo_net(input_frames)\n        motion_ = motion.reshape((batch, self.pred_length, 2, height, width))\n        intensity_ = intensity.reshape((batch, self.pred_length, 1, height, width))\n        series = []\n        last_frames = all_frames[:, self.input_length - 1 : self.input_length, :, :, 0]\n        grid = self.grid.tile((batch, 1, 1, 1))\n        for i in range(self.pred_length):\n            last_frames = warp(\n                last_frames, motion_[:, i], grid, mode=\"nearest\", padding_mode=\"border\"\n            )\n            last_frames = last_frames + intensity_[:, i]\n            series.append(last_frames)\n        evo_result = paddle.concat(x=series, axis=1)\n        evo_result = evo_result / 128\n        # Generative Network\n        evo_feature = self.gen_enc(paddle.concat(x=[input_frames, evo_result], axis=1))\n        noise = paddle.randn(shape=[batch, self.ngf, height // 32, width // 32])\n        noise = self.proj(noise)\n        ngf = noise.shape[1]\n        noise_feature = (\n            noise.reshape((batch, -1, 4, 4, 8, 8))\n            .transpose(perm=[0, 1, 4, 5, 2, 3])\n            .reshape((batch, ngf // 16, height // 8, width // 8))\n        )\n        feature = paddle.concat(x=[evo_feature, noise_feature], axis=1)\n        gen_result = self.gen_dec(feature, evo_result)\n        return gen_result.unsqueeze(axis=-1)\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.HEDeepONets","title":"<code>HEDeepONets</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Physical information deep operator networks.</p> <p>Parameters:</p> Name Type Description Default <code>heat_input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input data for heat boundary.</p> required <code>cold_input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input data for cold boundary.</p> required <code>trunk_input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input data for trunk net.</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Output name of predicted temperature.</p> required <code>heat_num_loc</code> <code>int</code> <p>Number of sampled input data for heat boundary.</p> required <code>cold_num_loc</code> <code>int</code> <p>Number of sampled input data for cold boundary.</p> required <code>num_features</code> <code>int</code> <p>Number of features extracted from heat boundary, same for cold boundary and trunk net.</p> required <code>branch_num_layers</code> <code>int</code> <p>Number of hidden layers of branch net.</p> required <code>trunk_num_layers</code> <code>int</code> <p>Number of hidden layers of trunk net.</p> required <code>branch_hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of hidden size of branch net. An integer for all layers, or list of integer specify each layer's size.</p> required <code>trunk_hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of hidden size of trunk net. An integer for all layers, or list of integer specify each layer's size.</p> required <code>branch_skip_connection</code> <code>bool</code> <p>Whether to use skip connection for branch net. Defaults to False.</p> <code>False</code> <code>trunk_skip_connection</code> <code>bool</code> <p>Whether to use skip connection for trunk net. Defaults to False.</p> <code>False</code> <code>branch_activation</code> <code>str</code> <p>Name of activation function for branch net. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>trunk_activation</code> <code>str</code> <p>Name of activation function for trunk net. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>branch_weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s) for branch net. Defaults to False.</p> <code>False</code> <code>trunk_weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s) for trunk net. Defaults to False.</p> <code>False</code> <code>use_bias</code> <code>bool</code> <p>Whether to add bias on predicted G(u)(y). Defaults to True.</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.HEDeepONets(\n...     ('qm_h',),\n...     ('qm_c',),\n...     (\"x\",'t'),\n...     (\"T_h\",'T_c','T_w'),\n...     1,\n...     1,\n...     100,\n...     9,\n...     6,\n...     256,\n...     128,\n...     branch_activation=\"swish\",\n...     trunk_activation=\"swish\",\n...     use_bias=True,\n... )\n</code></pre> Source code in <code>ppsci/arch/he_deeponets.py</code> <pre><code>class HEDeepONets(base.Arch):\n    \"\"\"Physical information deep operator networks.\n\n    Args:\n        heat_input_keys (Tuple[str, ...]): Name of input data for heat boundary.\n        cold_input_keys (Tuple[str, ...]): Name of input data for cold boundary.\n        trunk_input_keys (Tuple[str, ...]): Name of input data for trunk net.\n        output_keys (Tuple[str, ...]): Output name of predicted temperature.\n        heat_num_loc (int): Number of sampled input data for heat boundary.\n        cold_num_loc (int): Number of sampled input data for cold boundary.\n        num_features (int): Number of features extracted from heat boundary, same for cold boundary and trunk net.\n        branch_num_layers (int): Number of hidden layers of branch net.\n        trunk_num_layers (int): Number of hidden layers of trunk net.\n        branch_hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size of branch net.\n            An integer for all layers, or list of integer specify each layer's size.\n        trunk_hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size of trunk net.\n            An integer for all layers, or list of integer specify each layer's size.\n        branch_skip_connection (bool, optional): Whether to use skip connection for branch net. Defaults to False.\n        trunk_skip_connection (bool, optional): Whether to use skip connection for trunk net. Defaults to False.\n        branch_activation (str, optional): Name of activation function for branch net. Defaults to \"tanh\".\n        trunk_activation (str, optional): Name of activation function for trunk net. Defaults to \"tanh\".\n        branch_weight_norm (bool, optional): Whether to apply weight norm on parameter(s) for branch net. Defaults to False.\n        trunk_weight_norm (bool, optional): Whether to apply weight norm on parameter(s) for trunk net. Defaults to False.\n        use_bias (bool, optional): Whether to add bias on predicted G(u)(y). Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.HEDeepONets(\n        ...     ('qm_h',),\n        ...     ('qm_c',),\n        ...     (\"x\",'t'),\n        ...     (\"T_h\",'T_c','T_w'),\n        ...     1,\n        ...     1,\n        ...     100,\n        ...     9,\n        ...     6,\n        ...     256,\n        ...     128,\n        ...     branch_activation=\"swish\",\n        ...     trunk_activation=\"swish\",\n        ...     use_bias=True,\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        heat_input_keys: Tuple[str, ...],\n        cold_input_keys: Tuple[str, ...],\n        trunk_input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        heat_num_loc: int,\n        cold_num_loc: int,\n        num_features: int,\n        branch_num_layers: int,\n        trunk_num_layers: int,\n        branch_hidden_size: Union[int, Tuple[int, ...]],\n        trunk_hidden_size: Union[int, Tuple[int, ...]],\n        branch_skip_connection: bool = False,\n        trunk_skip_connection: bool = False,\n        branch_activation: str = \"tanh\",\n        trunk_activation: str = \"tanh\",\n        branch_weight_norm: bool = False,\n        trunk_weight_norm: bool = False,\n        use_bias: bool = True,\n    ):\n        super().__init__()\n        self.trunk_input_keys = trunk_input_keys\n        self.heat_input_keys = heat_input_keys\n        self.cold_input_keys = cold_input_keys\n        self.input_keys = (\n            self.trunk_input_keys + self.heat_input_keys + self.cold_input_keys\n        )\n        self.output_keys = output_keys\n        self.num_features = num_features\n\n        self.heat_net = mlp.MLP(\n            self.heat_input_keys,\n            (\"h\",),\n            branch_num_layers,\n            branch_hidden_size,\n            branch_activation,\n            branch_skip_connection,\n            branch_weight_norm,\n            input_dim=heat_num_loc,\n            output_dim=num_features * len(self.output_keys),\n        )\n\n        self.cold_net = mlp.MLP(\n            self.cold_input_keys,\n            (\"c\",),\n            branch_num_layers,\n            branch_hidden_size,\n            branch_activation,\n            branch_skip_connection,\n            branch_weight_norm,\n            input_dim=cold_num_loc,\n            output_dim=num_features * len(self.output_keys),\n        )\n\n        self.trunk_net = mlp.MLP(\n            self.trunk_input_keys,\n            (\"t\",),\n            trunk_num_layers,\n            trunk_hidden_size,\n            trunk_activation,\n            trunk_skip_connection,\n            trunk_weight_norm,\n            input_dim=len(self.trunk_input_keys),\n            output_dim=num_features * len(self.output_keys),\n        )\n        self.trunk_act = act_mod.get_activation(trunk_activation)\n        self.heat_act = act_mod.get_activation(branch_activation)\n        self.cold_act = act_mod.get_activation(branch_activation)\n\n        self.use_bias = use_bias\n        if use_bias:\n            # register bias to parameter for updating in optimizer and storage\n            self.b = self.create_parameter(\n                shape=(len(self.output_keys),),\n                attr=nn.initializer.Constant(0.0),\n            )\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        # Branch net to encode the input function\n        heat_features = self.heat_net(x)[self.heat_net.output_keys[0]]\n        cold_features = self.cold_net(x)[self.cold_net.output_keys[0]]\n        # Trunk net to encode the domain of the output function\n        y_features = self.trunk_net(x)[self.trunk_net.output_keys[0]]\n        y_features = self.trunk_act(y_features)\n        # Dot product\n        G_u_h = paddle.sum(\n            heat_features[:, : self.num_features]\n            * y_features[:, : self.num_features]\n            * cold_features[:, : self.num_features],\n            axis=1,\n            keepdim=True,\n        )\n        G_u_c = paddle.sum(\n            heat_features[:, self.num_features : 2 * self.num_features]\n            * y_features[:, self.num_features : 2 * self.num_features]\n            * cold_features[:, self.num_features : 2 * self.num_features],\n            axis=1,\n            keepdim=True,\n        )\n        G_u_w = paddle.sum(\n            heat_features[:, 2 * self.num_features :]\n            * y_features[:, 2 * self.num_features :]\n            * cold_features[:, 2 * self.num_features :],\n            axis=1,\n            keepdim=True,\n        )\n        # Add bias\n        if self.use_bias:\n            G_u_h += self.b[0]\n            G_u_c += self.b[1]\n            G_u_w += self.b[2]\n\n        result_dict = {\n            self.output_keys[0]: G_u_h,\n            self.output_keys[1]: G_u_c,\n            self.output_keys[2]: G_u_w,\n        }\n        if self._output_transform is not None:\n            result_dict = self._output_transform(x, result_dict)\n\n        return result_dict\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.DGMR","title":"<code>DGMR</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Deep Generative Model of Radar.     Nowcasting GAN is an attempt to recreate DeepMind's Skillful Nowcasting GAN from https://arxiv.org/abs/2104.00954.     but slightly modified for multiple satellite channels</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>forecast_steps</code> <code>int</code> <p>Number of steps to predict in the future</p> <code>18</code> <code>input_channels</code> <code>int</code> <p>Number of input channels per image</p> <code>1</code> <code>gen_lr</code> <code>float</code> <p>Learning rate for the generator</p> <code>5e-05</code> <code>disc_lr</code> <code>float</code> <p>Learning rate for the discriminators, shared for both temporal and spatial discriminator</p> <code>0.0002</code> <code>conv_type</code> <code>str</code> <p>Type of 2d convolution to use, see satflow/models/utils.py for options</p> <code>'standard'</code> <code>beta1</code> <code>float</code> <p>Beta1 for Adam optimizer</p> <code>0.0</code> <code>beta2</code> <code>float</code> <p>Beta2 for Adam optimizer</p> <code>0.999</code> <code>num_samples</code> <code>int</code> <p>Number of samples of the latent space to sample for training/validation</p> <code>6</code> <code>grid_lambda</code> <code>float</code> <p>Lambda for the grid regularization loss</p> <code>20.0</code> <code>output_shape</code> <code>int</code> <p>Shape of the output predictions, generally should be same as the input shape</p> <code>256</code> <code>generation_steps</code> <code>int</code> <p>Number of generation steps to use in forward pass, in paper is 6 and the best is chosen for the loss this results in huge amounts of GPU memory though, so less might work better for training.</p> <code>6</code> <code>context_channels</code> <code>int</code> <p>Number of output channels for the lowest block of conditioning stack</p> <code>384</code> <code>latent_channels</code> <code>int</code> <p>Number of channels that the latent space should be reshaped to, input dimension into ConvGRU, also affects the number of channels for other linked inputs/outputs</p> <code>768</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; model = ppsci.arch.DGMR((\"input\", ), (\"output\", ))\n&gt;&gt;&gt; input_dict = {\"input\": paddle.randn((1, 4, 1, 256, 256))}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict[\"output\"].shape)\n[1, 18, 1, 256, 256]\n</code></pre> Source code in <code>ppsci/arch/dgmr.py</code> <pre><code>class DGMR(base.Arch):\n    \"\"\"Deep Generative Model of Radar.\n        Nowcasting GAN is an attempt to recreate DeepMind's Skillful Nowcasting GAN from https://arxiv.org/abs/2104.00954.\n        but slightly modified for multiple satellite channels\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n        forecast_steps (int, optional): Number of steps to predict in the future\n        input_channels (int, optional): Number of input channels per image\n        gen_lr (float, optional): Learning rate for the generator\n        disc_lr (float, optional): Learning rate for the discriminators, shared for both temporal and spatial discriminator\n        conv_type (str, optional): Type of 2d convolution to use, see satflow/models/utils.py for options\n        beta1 (float, optional): Beta1 for Adam optimizer\n        beta2 (float, optional): Beta2 for Adam optimizer\n        num_samples (int, optional): Number of samples of the latent space to sample for training/validation\n        grid_lambda (float, optional): Lambda for the grid regularization loss\n        output_shape (int, optional): Shape of the output predictions, generally should be same as the input shape\n        generation_steps (int, optional): Number of generation steps to use in forward pass, in paper is 6 and the best is chosen for the loss\n            this results in huge amounts of GPU memory though, so less might work better for training.\n        context_channels (int, optional): Number of output channels for the lowest block of conditioning stack\n        latent_channels (int, optional): Number of channels that the latent space should be reshaped to,\n            input dimension into ConvGRU, also affects the number of channels for other linked inputs/outputs\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; model = ppsci.arch.DGMR((\"input\", ), (\"output\", ))\n        &gt;&gt;&gt; input_dict = {\"input\": paddle.randn((1, 4, 1, 256, 256))}\n        &gt;&gt;&gt; output_dict = model(input_dict) # doctest: +SKIP\n        &gt;&gt;&gt; print(output_dict[\"output\"].shape) # doctest: +SKIP\n        [1, 18, 1, 256, 256]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        forecast_steps: int = 18,\n        input_channels: int = 1,\n        output_shape: int = 256,\n        gen_lr: float = 5e-05,\n        disc_lr: float = 0.0002,\n        conv_type: str = \"standard\",\n        num_samples: int = 6,\n        grid_lambda: float = 20.0,\n        beta1: float = 0.0,\n        beta2: float = 0.999,\n        latent_channels: int = 768,\n        context_channels: int = 384,\n        generation_steps: int = 6,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.gen_lr = gen_lr\n        self.disc_lr = disc_lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.grid_lambda = grid_lambda\n        self.num_samples = num_samples\n        self.latent_channels = latent_channels\n        self.context_channels = context_channels\n        self.input_channels = input_channels\n        self.generation_steps = generation_steps\n        self.conditioning_stack = ContextConditioningStack(\n            input_channels=input_channels,\n            conv_type=conv_type,\n            output_channels=self.context_channels,\n        )\n        self.latent_stack = LatentConditioningStack(\n            shape=(8 * self.input_channels, output_shape // 32, output_shape // 32),\n            output_channels=self.latent_channels,\n        )\n        self.sampler = Sampler(\n            forecast_steps=forecast_steps,\n            latent_channels=self.latent_channels,\n            context_channels=self.context_channels,\n        )\n        self.generator = Generator(\n            self.conditioning_stack, self.latent_stack, self.sampler\n        )\n        self.discriminator = Discriminator(input_channels)\n        self.global_iteration = 0\n        self.automatic_optimization = False\n\n    def split_to_dict(\n        self, data_tensors: Tuple[paddle.Tensor, ...], keys: Tuple[str, ...]\n    ):\n        return {key: data_tensors[i] for i, key in enumerate(keys)}\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n        x_tensor = self.concat_to_tensor(x, self.input_keys)\n        y = [self.generator(x_tensor)]\n        y = self.split_to_dict(y, self.output_keys)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.ChipDeepONets","title":"<code>ChipDeepONets</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Multi-branch physics-informed deep operator neural network. The network consists of three branch networks: random heat source, boundary function, and boundary type, as well as a trunk network.</p> <p>Parameters:</p> Name Type Description Default <code>branch_input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input data for internal heat source on branch nets.</p> required <code>BCtype_input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input data for boundary types on branch nets.</p> required <code>BC_input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input data for boundary on branch nets.</p> required <code>trunk_input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input data for trunk net.</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Output name of predicted temperature.</p> required <code>num_loc</code> <code>int</code> <p>Number of sampled input data for internal heat source.</p> required <code>bctype_loc</code> <code>int</code> <p>Number of sampled input data for boundary types.</p> required <code>BC_num_loc</code> <code>int</code> <p>Number of sampled input data for boundary.</p> required <code>num_features</code> <code>int</code> <p>Number of features extracted from trunk net, same for all branch nets.</p> required <code>branch_num_layers</code> <code>int</code> <p>Number of hidden layers of internal heat source on branch nets.</p> required <code>BC_num_layers</code> <code>int</code> <p>Number of hidden layers of boundary on branch nets.</p> required <code>trunk_num_layers</code> <code>int</code> <p>Number of hidden layers of trunk net.</p> required <code>branch_hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of hidden size of internal heat source on branch nets. An integer for all layers, or list of integer specify each layer's size.</p> required <code>BC_hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of hidden size of boundary on branch nets. An integer for all layers, or list of integer specify each layer's size.</p> required <code>trunk_hidden_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of hidden size of trunk net. An integer for all layers, or list of integer specify each layer's size.</p> required <code>branch_skip_connection</code> <code>bool</code> <p>Whether to use skip connection for internal heat source on branch net. Defaults to False.</p> <code>False</code> <code>BC_skip_connection</code> <code>bool</code> <p>Whether to use skip connection for boundary on branch net. Defaults to False.</p> <code>False</code> <code>trunk_skip_connection</code> <code>bool</code> <p>Whether to use skip connection for trunk net. Defaults to False.</p> <code>False</code> <code>branch_activation</code> <code>str</code> <p>Name of activation function for internal heat source on branch net. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>BC_activation</code> <code>str</code> <p>Name of activation function for boundary on branch net. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>trunk_activation</code> <code>str</code> <p>Name of activation function for trunk net. Defaults to \"tanh\".</p> <code>'tanh'</code> <code>branch_weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s) for internal heat source on branch net. Defaults to False.</p> <code>False</code> <code>BC_weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s) for boundary on branch net. Defaults to False.</p> <code>False</code> <code>trunk_weight_norm</code> <code>bool</code> <p>Whether to apply weight norm on parameter(s) for trunk net. Defaults to False.</p> <code>False</code> <code>use_bias</code> <code>bool</code> <p>Whether to add bias on predicted G(u)(y). Defaults to True.</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.ChipDeepONets(\n...     ('u',),\n...     ('bc',),\n...     ('bc_data',),\n...     (\"x\",'y'),\n...     (\"T\",),\n...     324,\n...     1,\n...     76,\n...     400,\n...     9,\n...     9,\n...     6,\n...     256,\n...     256,\n...     128,\n...     branch_activation=\"swish\",\n...     BC_activation=\"swish\",\n...     trunk_activation=\"swish\",\n...     use_bias=True,\n... )\n</code></pre> Source code in <code>ppsci/arch/chip_deeponets.py</code> <pre><code>class ChipDeepONets(base.Arch):\n    \"\"\"Multi-branch physics-informed deep operator neural network. The network consists of three branch networks: random heat source, boundary function, and boundary type, as well as a trunk network.\n\n    Args:\n        branch_input_keys (Tuple[str, ...]): Name of input data for internal heat source on branch nets.\n        BCtype_input_keys (Tuple[str, ...]): Name of input data for boundary types on branch nets.\n        BC_input_keys (Tuple[str, ...]): Name of input data for boundary on branch nets.\n        trunk_input_keys (Tuple[str, ...]): Name of input data for trunk net.\n        output_keys (Tuple[str, ...]): Output name of predicted temperature.\n        num_loc (int): Number of sampled input data for internal heat source.\n        bctype_loc (int): Number of sampled input data for boundary types.\n        BC_num_loc (int): Number of sampled input data for boundary.\n        num_features (int): Number of features extracted from trunk net, same for all branch nets.\n        branch_num_layers (int): Number of hidden layers of internal heat source on branch nets.\n        BC_num_layers (int): Number of hidden layers of boundary on branch nets.\n        trunk_num_layers (int): Number of hidden layers of trunk net.\n        branch_hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size of internal heat source on branch nets.\n            An integer for all layers, or list of integer specify each layer's size.\n        BC_hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size of boundary on branch nets.\n            An integer for all layers, or list of integer specify each layer's size.\n        trunk_hidden_size (Union[int, Tuple[int, ...]]): Number of hidden size of trunk net.\n            An integer for all layers, or list of integer specify each layer's size.\n        branch_skip_connection (bool, optional): Whether to use skip connection for internal heat source on branch net. Defaults to False.\n        BC_skip_connection (bool, optional): Whether to use skip connection for boundary on branch net. Defaults to False.\n        trunk_skip_connection (bool, optional): Whether to use skip connection for trunk net. Defaults to False.\n        branch_activation (str, optional): Name of activation function for internal heat source on branch net. Defaults to \"tanh\".\n        BC_activation (str, optional): Name of activation function for boundary on branch net. Defaults to \"tanh\".\n        trunk_activation (str, optional): Name of activation function for trunk net. Defaults to \"tanh\".\n        branch_weight_norm (bool, optional): Whether to apply weight norm on parameter(s) for internal heat source on branch net. Defaults to False.\n        BC_weight_norm (bool, optional): Whether to apply weight norm on parameter(s) for boundary on branch net. Defaults to False.\n        trunk_weight_norm (bool, optional): Whether to apply weight norm on parameter(s) for trunk net. Defaults to False.\n        use_bias (bool, optional): Whether to add bias on predicted G(u)(y). Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.ChipDeepONets(\n        ...     ('u',),\n        ...     ('bc',),\n        ...     ('bc_data',),\n        ...     (\"x\",'y'),\n        ...     (\"T\",),\n        ...     324,\n        ...     1,\n        ...     76,\n        ...     400,\n        ...     9,\n        ...     9,\n        ...     6,\n        ...     256,\n        ...     256,\n        ...     128,\n        ...     branch_activation=\"swish\",\n        ...     BC_activation=\"swish\",\n        ...     trunk_activation=\"swish\",\n        ...     use_bias=True,\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        branch_input_keys: Tuple[str, ...],\n        BCtype_input_keys: Tuple[str, ...],\n        BC_input_keys: Tuple[str, ...],\n        trunk_input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        num_loc: int,\n        bctype_loc: int,\n        BC_num_loc: int,\n        num_features: int,\n        branch_num_layers: int,\n        BC_num_layers: int,\n        trunk_num_layers: int,\n        branch_hidden_size: Union[int, Tuple[int, ...]],\n        BC_hidden_size: Union[int, Tuple[int, ...]],\n        trunk_hidden_size: Union[int, Tuple[int, ...]],\n        branch_skip_connection: bool = False,\n        BC_skip_connection: bool = False,\n        trunk_skip_connection: bool = False,\n        branch_activation: str = \"tanh\",\n        BC_activation: str = \"tanh\",\n        trunk_activation: str = \"tanh\",\n        branch_weight_norm: bool = False,\n        BC_weight_norm: bool = False,\n        trunk_weight_norm: bool = False,\n        use_bias: bool = True,\n    ):\n        super().__init__()\n        self.trunk_input_keys = trunk_input_keys\n        self.branch_input_keys = branch_input_keys\n        self.BCtype_input_keys = BCtype_input_keys\n        self.BC_input_keys = BC_input_keys\n        self.input_keys = (\n            self.trunk_input_keys\n            + self.branch_input_keys\n            + self.BC_input_keys\n            + self.BCtype_input_keys\n        )\n        self.output_keys = output_keys\n\n        self.branch_net = mlp.MLP(\n            self.branch_input_keys,\n            (\"b\",),\n            branch_num_layers,\n            branch_hidden_size,\n            branch_activation,\n            branch_skip_connection,\n            branch_weight_norm,\n            input_dim=num_loc,\n            output_dim=num_features,\n        )\n\n        self.BCtype_net = mlp.MLP(\n            self.BCtype_input_keys,\n            (\"bctype\",),\n            BC_num_layers,\n            BC_hidden_size,\n            BC_activation,\n            BC_skip_connection,\n            BC_weight_norm,\n            input_dim=bctype_loc,\n            output_dim=num_features,\n        )\n\n        self.BC_net = mlp.MLP(\n            self.BC_input_keys,\n            (\"bc\",),\n            BC_num_layers,\n            BC_hidden_size,\n            BC_activation,\n            BC_skip_connection,\n            BC_weight_norm,\n            input_dim=BC_num_loc,\n            output_dim=num_features,\n        )\n\n        self.trunk_net = mlp.MLP(\n            self.trunk_input_keys,\n            (\"t\",),\n            trunk_num_layers,\n            trunk_hidden_size,\n            trunk_activation,\n            trunk_skip_connection,\n            trunk_weight_norm,\n            input_dim=len(self.trunk_input_keys),\n            output_dim=num_features,\n        )\n        self.trunk_act = act_mod.get_activation(trunk_activation)\n        self.bc_act = act_mod.get_activation(BC_activation)\n        self.branch_act = act_mod.get_activation(branch_activation)\n\n        self.use_bias = use_bias\n        if use_bias:\n            # register bias to parameter for updating in optimizer and storage\n            self.b = self.create_parameter(\n                shape=(1,),\n                attr=nn.initializer.Constant(0.0),\n            )\n\n    def forward(self, x):\n\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        # Branch net to encode the input function\n        u_features = self.branch_net(x)[self.branch_net.output_keys[0]]\n        bc_features = self.BC_net(x)[self.BC_net.output_keys[0]]\n        bctype_features = self.BCtype_net(x)[self.BCtype_net.output_keys[0]]\n        # Trunk net to encode the domain of the output function\n        y_features = self.trunk_net(x)[self.trunk_net.output_keys[0]]\n        y_features = self.trunk_act(y_features)\n        # Dot product\n        G_u = paddle.sum(\n            u_features * y_features * bc_features * bctype_features,\n            axis=1,\n            keepdim=True,\n        )\n        # Add bias\n        if self.use_bias:\n            G_u += self.b\n\n        result_dict = {\n            self.output_keys[0]: G_u,\n        }\n        if self._output_transform is not None:\n            result_dict = self._output_transform(x, result_dict)\n\n        return result_dict\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.AutoEncoder","title":"<code>AutoEncoder</code>","text":"<p>               Bases: <code>Arch</code></p> <p>AutoEncoder is a class that represents an autoencoder neural network model.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>A tuple of input keys.</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>A tuple of output keys.</p> required <code>input_dim</code> <code>int</code> <p>The dimension of the input data.</p> required <code>latent_dim</code> <code>int</code> <p>The dimension of the latent space.</p> required <code>hidden_dim</code> <code>int</code> <p>The dimension of the hidden layer.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.AutoEncoder(\n...    input_keys=(\"input1\",),\n...    output_keys=(\"mu\", \"log_sigma\", \"decoder_z\",),\n...    input_dim=100,\n...    latent_dim=50,\n...    hidden_dim=200\n... )\n&gt;&gt;&gt; input_dict = {\"input1\": paddle.rand([200, 100]),}\n&gt;&gt;&gt; output_dict = model(input_dict)\n&gt;&gt;&gt; print(output_dict[\"mu\"].shape)\n[200, 50]\n&gt;&gt;&gt; print(output_dict[\"log_sigma\"].shape)\n[200, 50]\n&gt;&gt;&gt; print(output_dict[\"decoder_z\"].shape)\n[200, 100]\n</code></pre> Source code in <code>ppsci/arch/vae.py</code> <pre><code>class AutoEncoder(base.Arch):\n    \"\"\"\n    AutoEncoder is a class that represents an autoencoder neural network model.\n\n    Args:\n        input_keys (Tuple[str, ...]): A tuple of input keys.\n        output_keys (Tuple[str, ...]): A tuple of output keys.\n        input_dim (int): The dimension of the input data.\n        latent_dim (int): The dimension of the latent space.\n        hidden_dim (int): The dimension of the hidden layer.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.AutoEncoder(\n        ...    input_keys=(\"input1\",),\n        ...    output_keys=(\"mu\", \"log_sigma\", \"decoder_z\",),\n        ...    input_dim=100,\n        ...    latent_dim=50,\n        ...    hidden_dim=200\n        ... )\n        &gt;&gt;&gt; input_dict = {\"input1\": paddle.rand([200, 100]),}\n        &gt;&gt;&gt; output_dict = model(input_dict)\n        &gt;&gt;&gt; print(output_dict[\"mu\"].shape)\n        [200, 50]\n        &gt;&gt;&gt; print(output_dict[\"log_sigma\"].shape)\n        [200, 50]\n        &gt;&gt;&gt; print(output_dict[\"decoder_z\"].shape)\n        [200, 100]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        input_dim: int,\n        latent_dim: int,\n        hidden_dim: int,\n    ):\n        super(AutoEncoder, self).__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        # encoder\n        self._encoder_linear = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n        )\n        self._encoder_mu = nn.Linear(hidden_dim, latent_dim)\n        self._encoder_log_sigma = nn.Linear(hidden_dim, latent_dim)\n\n        self._decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, input_dim),\n        )\n\n    def encoder(self, x):\n        h = self._encoder_linear(x)\n        mu = self._encoder_mu(h)\n        log_sigma = self._encoder_log_sigma(h)\n        return mu, log_sigma\n\n    def decoder(self, x):\n        return self._decoder(x)\n\n    def forward_tensor(self, x):\n        mu, log_sigma = self.encoder(x)\n        z = mu + paddle.randn(mu.shape) * paddle.exp(log_sigma)\n        return mu, log_sigma, self.decoder(z)\n\n    def forward(self, x):\n        x = self.concat_to_tensor(x, self.input_keys, axis=-1)\n        mu, log_sigma, decoder_z = self.forward_tensor(x)\n        result_dict = {\n            self.output_keys[0]: mu,\n            self.output_keys[1]: log_sigma,\n            self.output_keys[2]: decoder_z,\n        }\n        return result_dict\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.CuboidTransformer","title":"<code>CuboidTransformer</code>","text":"<p>               Bases: <code>Arch</code></p> <p>Cuboid Transformer for spatiotemporal forecasting</p> <p>We adopt the Non-autoregressive encoder-decoder architecture. The decoder takes the multi-scale memory output from the encoder.</p> <p>The initial downsampling / upsampling layers will be Downsampling: [K x Conv2D --&gt; PatchMerge] Upsampling: [Nearest Interpolation-based Upsample --&gt; K x Conv2D]</p> <p>x --&gt; downsample (optional) ---&gt; (+pos_embed) ---&gt; enc --&gt; mem_l         initial_z (+pos_embed) ---&gt; FC                                                  |            |                                                  |------------|                                                        |                                                        |          y &lt;--- upsample (optional) &lt;--- dec &lt;----------</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>input_shape</code> <code>Tuple[int, ...]</code> <p>The shape of the input data.</p> required <code>target_shape</code> <code>Tuple[int, ...]</code> <p>The shape of the target data.</p> required <code>base_units</code> <code>int</code> <p>The base units. Defaults to 128.</p> <code>128</code> <code>block_units</code> <code>int</code> <p>The block units. Defaults to None.</p> <code>None</code> <code>scale_alpha</code> <code>float</code> <p>We scale up the channels based on the formula: - round_to(base_units * max(downsample_scale) ** units_alpha, 4). Defaults to 1.0.</p> <code>1.0</code> <code>num_heads</code> <code>int</code> <p>The number of heads. Defaults to 4.</p> <code>4</code> <code>attn_drop</code> <code>float</code> <p>The attention dropout. Defaults to 0.0.</p> <code>0.0</code> <code>proj_drop</code> <code>float</code> <p>The projection dropout. Defaults to 0.0.</p> <code>0.0</code> <code>ffn_drop</code> <code>float</code> <p>The ffn dropout. Defaults to 0.0.</p> <code>0.0</code> <code>downsample</code> <code>int</code> <p>The rate of downsample. Defaults to 2.</p> <code>2</code> <code>downsample_type</code> <code>str</code> <p>The type of downsample. Defaults to \"patch_merge\".</p> <code>'patch_merge'</code> <code>upsample_type</code> <code>str</code> <p>The rate of upsample. Defaults to \"upsample\".</p> <code>'upsample'</code> <code>upsample_kernel_size</code> <code>int</code> <p>The kernel size of upsample. Defaults to 3.</p> <code>3</code> <code>enc_depth</code> <code>list</code> <p>The depth of encoder. Defaults to [4, 4, 4].</p> <code>[4, 4, 4]</code> <code>enc_attn_patterns</code> <code>str</code> <p>The pattern of encoder attention. Defaults to None.</p> <code>None</code> <code>enc_cuboid_size</code> <code>list</code> <p>The cuboid size of encoder. Defaults to [(4, 4, 4), (4, 4, 4)].</p> <code>[(4, 4, 4), (4, 4, 4)]</code> <code>enc_cuboid_strategy</code> <code>list</code> <p>The cuboid strategy of encoder. Defaults to [(\"l\", \"l\", \"l\"), (\"d\", \"d\", \"d\")].</p> <code>[('l', 'l', 'l'), ('d', 'd', 'd')]</code> <code>enc_shift_size</code> <code>list</code> <p>The shift size of encoder. Defaults to [(0, 0, 0), (0, 0, 0)].</p> <code>[(0, 0, 0), (0, 0, 0)]</code> <code>enc_use_inter_ffn</code> <code>bool</code> <p>Whether to use intermediate FFN for encoder. Defaults to True.</p> <code>True</code> <code>dec_depth</code> <code>list</code> <p>The depth of decoder. Defaults to [2, 2].</p> <code>[2, 2]</code> <code>dec_cross_start</code> <code>int</code> <p>The cross start of decoder. Defaults to 0.</p> <code>0</code> <code>dec_self_attn_patterns</code> <code>str</code> <p>The partterns of decoder. Defaults to None.</p> <code>None</code> <code>dec_self_cuboid_size</code> <code>list</code> <p>The cuboid size of decoder. Defaults to [(4, 4, 4), (4, 4, 4)].</p> <code>[(4, 4, 4), (4, 4, 4)]</code> <code>dec_self_cuboid_strategy</code> <code>list</code> <p>The strategy of decoder. Defaults to [(\"l\", \"l\", \"l\"), (\"d\", \"d\", \"d\")].</p> <code>[('l', 'l', 'l'), ('d', 'd', 'd')]</code> <code>dec_self_shift_size</code> <code>list</code> <p>The shift size of decoder. Defaults to [(1, 1, 1), (0, 0, 0)].</p> <code>[(1, 1, 1), (0, 0, 0)]</code> <code>dec_cross_attn_patterns</code> <code>_type_</code> <p>The cross attention patterns of decoder. Defaults to None.</p> <code>None</code> <code>dec_cross_cuboid_hw</code> <code>list</code> <p>The cuboid_hw of decoder. Defaults to [(4, 4), (4, 4)].</p> <code>[(4, 4), (4, 4)]</code> <code>dec_cross_cuboid_strategy</code> <code>list</code> <p>The cuboid strategy of decoder. Defaults to [(\"l\", \"l\", \"l\"), (\"d\", \"l\", \"l\")].</p> <code>[('l', 'l', 'l'), ('d', 'l', 'l')]</code> <code>dec_cross_shift_hw</code> <code>list</code> <p>The shift_hw of decoder. Defaults to [(0, 0), (0, 0)].</p> <code>[(0, 0), (0, 0)]</code> <code>dec_cross_n_temporal</code> <code>list</code> <p>The cross_n_temporal of decoder. Defaults to [1, 2].</p> <code>[1, 2]</code> <code>dec_cross_last_n_frames</code> <code>int</code> <p>The cross_last_n_frames of decoder. Defaults to None.</p> <code>None</code> <code>dec_use_inter_ffn</code> <code>bool</code> <p>Whether to use intermediate FFN for decoder. Defaults to True.</p> <code>True</code> <code>dec_hierarchical_pos_embed</code> <code>bool</code> <p>Whether to use hierarchical pos_embed for decoder. Defaults to False.</p> <code>False</code> <code>num_global_vectors</code> <code>int</code> <p>The num of global vectors. Defaults to 4.</p> <code>4</code> <code>use_dec_self_global</code> <code>bool</code> <p>Whether to use global vector for decoder. Defaults to True.</p> <code>True</code> <code>dec_self_update_global</code> <code>bool</code> <p>Whether to update global vector for decoder. Defaults to True.</p> <code>True</code> <code>use_dec_cross_global</code> <code>bool</code> <p>Whether to use cross global vector for decoder. Defaults to True.</p> <code>True</code> <code>use_global_vector_ffn</code> <code>bool</code> <p>Whether to use global vector FFN. Defaults to True.</p> <code>True</code> <code>use_global_self_attn</code> <code>bool</code> <p>Whether to use global attentions. Defaults to False.</p> <code>False</code> <code>separate_global_qkv</code> <code>bool</code> <p>Whether to separate global qkv. Defaults to False.</p> <code>False</code> <code>global_dim_ratio</code> <code>int</code> <p>The ratio of global dim. Defaults to 1.</p> <code>1</code> <code>self_pattern</code> <code>str</code> <p>The pattern. Defaults to \"axial\".</p> <code>'axial'</code> <code>cross_self_pattern</code> <code>str</code> <p>The self cross pattern. Defaults to \"axial\".</p> <code>'axial'</code> <code>cross_pattern</code> <code>str</code> <p>The cross pattern. Defaults to \"cross_1x1\".</p> <code>'cross_1x1'</code> <code>z_init_method</code> <code>str</code> <p>How the initial input to the decoder is initialized. Defaults to \"nearest_interp\".</p> <code>'nearest_interp'</code> <code>initial_downsample_type</code> <code>str</code> <p>The downsample type of initial. Defaults to \"conv\".</p> <code>'conv'</code> <code>initial_downsample_activation</code> <code>str</code> <p>The downsample activation of initial. Defaults to \"leaky\".</p> <code>'leaky'</code> <code>initial_downsample_scale</code> <code>int</code> <p>The downsample scale of initial. Defaults to 1.</p> <code>1</code> <code>initial_downsample_conv_layers</code> <code>int</code> <p>The conv layer of downsample of initial. Defaults to 2.</p> <code>2</code> <code>final_upsample_conv_layers</code> <code>int</code> <p>The conv layer of final upsample. Defaults to 2.</p> <code>2</code> <code>initial_downsample_stack_conv_num_layers</code> <code>int</code> <p>The num of stack conv layer of initial downsample. Defaults to 1.</p> <code>1</code> <code>initial_downsample_stack_conv_dim_list</code> <code>list</code> <p>The dim list of stack conv of initial downsample. Defaults to None.</p> <code>None</code> <code>initial_downsample_stack_conv_downscale_list</code> <code>list</code> <p>The downscale list of stack conv of initial downsample. Defaults to [1].</p> <code>[1]</code> <code>initial_downsample_stack_conv_num_conv_list</code> <code>list</code> <p>The num of stack conv list of initial downsample. Defaults to [2].</p> <code>[2]</code> <code>ffn_activation</code> <code>str</code> <p>The activation of FFN. Defaults to \"leaky\".</p> <code>'leaky'</code> <code>gated_ffn</code> <code>bool</code> <p>Whether to use gate FFN. Defaults to False.</p> <code>False</code> <code>norm_layer</code> <code>str</code> <p>The type of normilize. Defaults to \"layer_norm\".</p> <code>'layer_norm'</code> <code>padding_type</code> <code>str</code> <p>The type of padding. Defaults to \"ignore\".</p> <code>'ignore'</code> <code>pos_embed_type</code> <code>str</code> <p>The type of pos embeding. Defaults to \"t+hw\".</p> <code>'t+hw'</code> <code>checkpoint_level</code> <code>bool</code> <p>Whether to use checkpoint. Defaults to True.</p> <code>True</code> <code>use_relative_pos</code> <code>bool</code> <p>Whether to use relative pose. Defaults to True.</p> <code>True</code> <code>self_attn_use_final_proj</code> <code>bool</code> <p>Whether to use final projection. Defaults to True.</p> <code>True</code> <code>dec_use_first_self_attn</code> <code>bool</code> <p>Whether to use first self attention for decoder. Defaults to False.</p> <code>False</code> <code>attn_linear_init_mode</code> <code>str</code> <p>The mode of attention linear init. Defaults to \"0\".</p> <code>'0'</code> <code>ffn_linear_init_mode</code> <code>str</code> <p>The mode of FFN linear init. Defaults to \"0\".</p> <code>'0'</code> <code>conv_init_mode</code> <code>str</code> <p>The mode of conv init. Defaults to \"0\".</p> <code>'0'</code> <code>down_up_linear_init_mode</code> <code>str</code> <p>The mode of downsample and upsample linear init. Defaults to \"0\".</p> <code>'0'</code> <code>norm_init_mode</code> <code>str</code> <p>The mode of normalization init. Defaults to \"0\".</p> <code>'0'</code> Source code in <code>ppsci/arch/cuboid_transformer.py</code> <pre><code>class CuboidTransformer(base.Arch):\n    \"\"\"Cuboid Transformer for spatiotemporal forecasting\n\n    We adopt the Non-autoregressive encoder-decoder architecture.\n    The decoder takes the multi-scale memory output from the encoder.\n\n    The initial downsampling / upsampling layers will be\n    Downsampling: [K x Conv2D --&gt; PatchMerge]\n    Upsampling: [Nearest Interpolation-based Upsample --&gt; K x Conv2D]\n\n    x --&gt; downsample (optional) ---&gt; (+pos_embed) ---&gt; enc --&gt; mem_l         initial_z (+pos_embed) ---&gt; FC\n                                                     |            |\n                                                     |------------|\n                                                           |\n                                                           |\n             y &lt;--- upsample (optional) &lt;--- dec &lt;----------\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n        input_shape (Tuple[int, ...]): The shape of the input data.\n        target_shape (Tuple[int, ...]): The shape of the target data.\n        base_units (int, optional): The base units. Defaults to 128.\n        block_units (int, optional): The block units. Defaults to None.\n        scale_alpha (float, optional): We scale up the channels based on the formula:\n            - round_to(base_units * max(downsample_scale) ** units_alpha, 4). Defaults to 1.0.\n        num_heads (int, optional): The number of heads. Defaults to 4.\n        attn_drop (float, optional): The attention dropout. Defaults to 0.0.\n        proj_drop (float, optional): The projection dropout. Defaults to 0.0.\n        ffn_drop (float, optional): The ffn dropout. Defaults to 0.0.\n        downsample (int, optional): The rate of downsample. Defaults to 2.\n        downsample_type (str, optional): The type of downsample. Defaults to \"patch_merge\".\n        upsample_type (str, optional): The rate of upsample. Defaults to \"upsample\".\n        upsample_kernel_size (int, optional): The kernel size of upsample. Defaults to 3.\n        enc_depth (list, optional): The depth of encoder. Defaults to [4, 4, 4].\n        enc_attn_patterns (str, optional): The pattern of encoder attention. Defaults to None.\n        enc_cuboid_size (list, optional): The cuboid size of encoder. Defaults to [(4, 4, 4), (4, 4, 4)].\n        enc_cuboid_strategy (list, optional): The cuboid strategy of encoder. Defaults to [(\"l\", \"l\", \"l\"), (\"d\", \"d\", \"d\")].\n        enc_shift_size (list, optional): The shift size of encoder. Defaults to [(0, 0, 0), (0, 0, 0)].\n        enc_use_inter_ffn (bool, optional): Whether to use intermediate FFN for encoder. Defaults to True.\n        dec_depth (list, optional): The depth of decoder. Defaults to [2, 2].\n        dec_cross_start (int, optional): The cross start of decoder. Defaults to 0.\n        dec_self_attn_patterns (str, optional): The partterns of decoder. Defaults to None.\n        dec_self_cuboid_size (list, optional): The cuboid size of decoder. Defaults to [(4, 4, 4), (4, 4, 4)].\n        dec_self_cuboid_strategy (list, optional): The strategy of decoder. Defaults to [(\"l\", \"l\", \"l\"), (\"d\", \"d\", \"d\")].\n        dec_self_shift_size (list, optional): The shift size of decoder. Defaults to [(1, 1, 1), (0, 0, 0)].\n        dec_cross_attn_patterns (_type_, optional): The cross attention patterns of decoder. Defaults to None.\n        dec_cross_cuboid_hw (list, optional): The cuboid_hw of decoder. Defaults to [(4, 4), (4, 4)].\n        dec_cross_cuboid_strategy (list, optional): The cuboid strategy of decoder. Defaults to [(\"l\", \"l\", \"l\"), (\"d\", \"l\", \"l\")].\n        dec_cross_shift_hw (list, optional): The shift_hw of decoder. Defaults to [(0, 0), (0, 0)].\n        dec_cross_n_temporal (list, optional): The cross_n_temporal of decoder. Defaults to [1, 2].\n        dec_cross_last_n_frames (int, optional): The cross_last_n_frames of decoder. Defaults to None.\n        dec_use_inter_ffn (bool, optional): Whether to use intermediate FFN for decoder. Defaults to True.\n        dec_hierarchical_pos_embed (bool, optional): Whether to use hierarchical pos_embed for decoder. Defaults to False.\n        num_global_vectors (int, optional): The num of global vectors. Defaults to 4.\n        use_dec_self_global (bool, optional): Whether to use global vector for decoder. Defaults to True.\n        dec_self_update_global (bool, optional): Whether to update global vector for decoder. Defaults to True.\n        use_dec_cross_global (bool, optional): Whether to use cross global vector for decoder. Defaults to True.\n        use_global_vector_ffn (bool, optional): Whether to use global vector FFN. Defaults to True.\n        use_global_self_attn (bool, optional): Whether to use global attentions. Defaults to False.\n        separate_global_qkv (bool, optional): Whether to separate global qkv. Defaults to False.\n        global_dim_ratio (int, optional): The ratio of global dim. Defaults to 1.\n        self_pattern (str, optional): The pattern. Defaults to \"axial\".\n        cross_self_pattern (str, optional): The self cross pattern. Defaults to \"axial\".\n        cross_pattern (str, optional): The cross pattern. Defaults to \"cross_1x1\".\n        z_init_method (str, optional): How the initial input to the decoder is initialized. Defaults to \"nearest_interp\".\n        initial_downsample_type (str, optional): The downsample type of initial. Defaults to \"conv\".\n        initial_downsample_activation (str, optional): The downsample activation of initial. Defaults to \"leaky\".\n        initial_downsample_scale (int, optional): The downsample scale of initial. Defaults to 1.\n        initial_downsample_conv_layers (int, optional): The conv layer of downsample of initial. Defaults to 2.\n        final_upsample_conv_layers (int, optional): The conv layer of final upsample. Defaults to 2.\n        initial_downsample_stack_conv_num_layers (int, optional): The num of stack conv layer of initial downsample. Defaults to 1.\n        initial_downsample_stack_conv_dim_list (list, optional): The dim list of stack conv of initial downsample. Defaults to None.\n        initial_downsample_stack_conv_downscale_list (list, optional): The downscale list of stack conv of initial downsample. Defaults to [1].\n        initial_downsample_stack_conv_num_conv_list (list, optional): The num of stack conv list of initial downsample. Defaults to [2].\n        ffn_activation (str, optional): The activation of FFN. Defaults to \"leaky\".\n        gated_ffn (bool, optional): Whether to use gate FFN. Defaults to False.\n        norm_layer (str, optional): The type of normilize. Defaults to \"layer_norm\".\n        padding_type (str, optional): The type of padding. Defaults to \"ignore\".\n        pos_embed_type (str, optional): The type of pos embeding. Defaults to \"t+hw\".\n        checkpoint_level (bool, optional): Whether to use checkpoint. Defaults to True.\n        use_relative_pos (bool, optional): Whether to use relative pose. Defaults to True.\n        self_attn_use_final_proj (bool, optional): Whether to use final projection. Defaults to True.\n        dec_use_first_self_attn (bool, optional): Whether to use first self attention for decoder. Defaults to False.\n        attn_linear_init_mode (str, optional): The mode of attention linear init. Defaults to \"0\".\n        ffn_linear_init_mode (str, optional): The mode of FFN linear init. Defaults to \"0\".\n        conv_init_mode (str, optional): The mode of conv init. Defaults to \"0\".\n        down_up_linear_init_mode (str, optional): The mode of downsample and upsample linear init. Defaults to \"0\".\n        norm_init_mode (str, optional): The mode of normalization init. Defaults to \"0\".\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        input_shape: Tuple[int, ...],\n        target_shape: Tuple[int, ...],\n        base_units: int = 128,\n        block_units: int = None,\n        scale_alpha: float = 1.0,\n        num_heads: int = 4,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        ffn_drop: float = 0.0,\n        downsample: int = 2,\n        downsample_type: str = \"patch_merge\",\n        upsample_type: str = \"upsample\",\n        upsample_kernel_size: int = 3,\n        enc_depth: Tuple[int, ...] = [4, 4, 4],\n        enc_attn_patterns: str = None,\n        enc_cuboid_size: Tuple[Tuple[int, ...], ...] = [(4, 4, 4), (4, 4, 4)],\n        enc_cuboid_strategy: Tuple[Tuple[str, ...], ...] = [\n            (\"l\", \"l\", \"l\"),\n            (\"d\", \"d\", \"d\"),\n        ],\n        enc_shift_size: Tuple[Tuple[int, ...], ...] = [(0, 0, 0), (0, 0, 0)],\n        enc_use_inter_ffn: bool = True,\n        dec_depth: Tuple[int, ...] = [2, 2],\n        dec_cross_start: int = 0,\n        dec_self_attn_patterns: str = None,\n        dec_self_cuboid_size: Tuple[Tuple[int, ...], ...] = [(4, 4, 4), (4, 4, 4)],\n        dec_self_cuboid_strategy: Tuple[Tuple[str, ...], ...] = [\n            (\"l\", \"l\", \"l\"),\n            (\"d\", \"d\", \"d\"),\n        ],\n        dec_self_shift_size: Tuple[Tuple[int, ...], ...] = [(1, 1, 1), (0, 0, 0)],\n        dec_cross_attn_patterns: str = None,\n        dec_cross_cuboid_hw: Tuple[Tuple[int, ...], ...] = [(4, 4), (4, 4)],\n        dec_cross_cuboid_strategy: Tuple[Tuple[str, ...], ...] = [\n            (\"l\", \"l\", \"l\"),\n            (\"d\", \"l\", \"l\"),\n        ],\n        dec_cross_shift_hw: Tuple[Tuple[int, ...], ...] = [(0, 0), (0, 0)],\n        dec_cross_n_temporal: Tuple[int, ...] = [1, 2],\n        dec_cross_last_n_frames: int = None,\n        dec_use_inter_ffn: bool = True,\n        dec_hierarchical_pos_embed: bool = False,\n        num_global_vectors: int = 4,\n        use_dec_self_global: bool = True,\n        dec_self_update_global: bool = True,\n        use_dec_cross_global: bool = True,\n        use_global_vector_ffn: bool = True,\n        use_global_self_attn: bool = False,\n        separate_global_qkv: bool = False,\n        global_dim_ratio: int = 1,\n        self_pattern: str = \"axial\",\n        cross_self_pattern: str = \"axial\",\n        cross_pattern: str = \"cross_1x1\",\n        z_init_method: str = \"nearest_interp\",\n        initial_downsample_type: str = \"conv\",\n        initial_downsample_activation: str = \"leaky\",\n        initial_downsample_scale: int = 1,\n        initial_downsample_conv_layers: int = 2,\n        final_upsample_conv_layers: int = 2,\n        initial_downsample_stack_conv_num_layers: int = 1,\n        initial_downsample_stack_conv_dim_list: Tuple[int, ...] = None,\n        initial_downsample_stack_conv_downscale_list: Tuple[int, ...] = [1],\n        initial_downsample_stack_conv_num_conv_list: Tuple[int, ...] = [2],\n        ffn_activation: str = \"leaky\",\n        gated_ffn: bool = False,\n        norm_layer: str = \"layer_norm\",\n        padding_type: str = \"ignore\",\n        pos_embed_type: str = \"t+hw\",\n        checkpoint_level: bool = True,\n        use_relative_pos: bool = True,\n        self_attn_use_final_proj: bool = True,\n        dec_use_first_self_attn: bool = False,\n        attn_linear_init_mode: str = \"0\",\n        ffn_linear_init_mode: str = \"0\",\n        conv_init_mode: str = \"0\",\n        down_up_linear_init_mode: str = \"0\",\n        norm_init_mode: str = \"0\",\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.attn_linear_init_mode = attn_linear_init_mode\n        self.ffn_linear_init_mode = ffn_linear_init_mode\n        self.conv_init_mode = conv_init_mode\n        self.down_up_linear_init_mode = down_up_linear_init_mode\n        self.norm_init_mode = norm_init_mode\n        assert len(enc_depth) == len(dec_depth)\n        self.base_units = base_units\n        self.num_global_vectors = num_global_vectors\n\n        num_blocks = len(enc_depth)\n        if isinstance(self_pattern, str):\n            enc_attn_patterns = [self_pattern] * num_blocks\n\n        if isinstance(cross_self_pattern, str):\n            dec_self_attn_patterns = [cross_self_pattern] * num_blocks\n\n        if isinstance(cross_pattern, str):\n            dec_cross_attn_patterns = [cross_pattern] * num_blocks\n\n        if global_dim_ratio != 1:\n            assert (\n                separate_global_qkv is True\n            ), \"Setting global_dim_ratio != 1 requires separate_global_qkv == True.\"\n        self.global_dim_ratio = global_dim_ratio\n        self.z_init_method = z_init_method\n        assert self.z_init_method in [\"zeros\", \"nearest_interp\", \"last\", \"mean\"]\n        self.input_shape = input_shape\n        self.target_shape = target_shape\n        T_in, H_in, W_in, C_in = input_shape\n        T_out, H_out, W_out, C_out = target_shape\n        assert H_in == H_out and W_in == W_out\n        if self.num_global_vectors &gt; 0:\n            init_data = paddle.zeros(\n                (self.num_global_vectors, global_dim_ratio * base_units)\n            )\n            self.init_global_vectors = paddle.create_parameter(\n                shape=init_data.shape,\n                dtype=init_data.dtype,\n                default_initializer=nn.initializer.Constant(0.0),\n            )\n\n            self.init_global_vectors.stop_gradient = not True\n        new_input_shape = self.get_initial_encoder_final_decoder(\n            initial_downsample_scale=initial_downsample_scale,\n            initial_downsample_type=initial_downsample_type,\n            activation=initial_downsample_activation,\n            initial_downsample_conv_layers=initial_downsample_conv_layers,\n            final_upsample_conv_layers=final_upsample_conv_layers,\n            padding_type=padding_type,\n            initial_downsample_stack_conv_num_layers=initial_downsample_stack_conv_num_layers,\n            initial_downsample_stack_conv_dim_list=initial_downsample_stack_conv_dim_list,\n            initial_downsample_stack_conv_downscale_list=initial_downsample_stack_conv_downscale_list,\n            initial_downsample_stack_conv_num_conv_list=initial_downsample_stack_conv_num_conv_list,\n        )\n        T_in, H_in, W_in, _ = new_input_shape\n        self.encoder = cuboid_encoder.CuboidTransformerEncoder(\n            input_shape=(T_in, H_in, W_in, base_units),\n            base_units=base_units,\n            block_units=block_units,\n            scale_alpha=scale_alpha,\n            depth=enc_depth,\n            downsample=downsample,\n            downsample_type=downsample_type,\n            block_attn_patterns=enc_attn_patterns,\n            block_cuboid_size=enc_cuboid_size,\n            block_strategy=enc_cuboid_strategy,\n            block_shift_size=enc_shift_size,\n            num_heads=num_heads,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            ffn_drop=ffn_drop,\n            gated_ffn=gated_ffn,\n            ffn_activation=ffn_activation,\n            norm_layer=norm_layer,\n            use_inter_ffn=enc_use_inter_ffn,\n            padding_type=padding_type,\n            use_global_vector=num_global_vectors &gt; 0,\n            use_global_vector_ffn=use_global_vector_ffn,\n            use_global_self_attn=use_global_self_attn,\n            separate_global_qkv=separate_global_qkv,\n            global_dim_ratio=global_dim_ratio,\n            checkpoint_level=checkpoint_level,\n            use_relative_pos=use_relative_pos,\n            self_attn_use_final_proj=self_attn_use_final_proj,\n            attn_linear_init_mode=attn_linear_init_mode,\n            ffn_linear_init_mode=ffn_linear_init_mode,\n            conv_init_mode=conv_init_mode,\n            down_linear_init_mode=down_up_linear_init_mode,\n            norm_init_mode=norm_init_mode,\n        )\n        self.enc_pos_embed = cuboid_decoder.PosEmbed(\n            embed_dim=base_units, typ=pos_embed_type, maxH=H_in, maxW=W_in, maxT=T_in\n        )\n        mem_shapes = self.encoder.get_mem_shapes()\n        self.z_proj = paddle.nn.Linear(\n            in_features=mem_shapes[-1][-1], out_features=mem_shapes[-1][-1]\n        )\n        self.dec_pos_embed = cuboid_decoder.PosEmbed(\n            embed_dim=mem_shapes[-1][-1],\n            typ=pos_embed_type,\n            maxT=T_out,\n            maxH=mem_shapes[-1][1],\n            maxW=mem_shapes[-1][2],\n        )\n        self.decoder = cuboid_decoder.CuboidTransformerDecoder(\n            target_temporal_length=T_out,\n            mem_shapes=mem_shapes,\n            cross_start=dec_cross_start,\n            depth=dec_depth,\n            upsample_type=upsample_type,\n            block_self_attn_patterns=dec_self_attn_patterns,\n            block_self_cuboid_size=dec_self_cuboid_size,\n            block_self_shift_size=dec_self_shift_size,\n            block_self_cuboid_strategy=dec_self_cuboid_strategy,\n            block_cross_attn_patterns=dec_cross_attn_patterns,\n            block_cross_cuboid_hw=dec_cross_cuboid_hw,\n            block_cross_shift_hw=dec_cross_shift_hw,\n            block_cross_cuboid_strategy=dec_cross_cuboid_strategy,\n            block_cross_n_temporal=dec_cross_n_temporal,\n            cross_last_n_frames=dec_cross_last_n_frames,\n            num_heads=num_heads,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            ffn_drop=ffn_drop,\n            upsample_kernel_size=upsample_kernel_size,\n            ffn_activation=ffn_activation,\n            gated_ffn=gated_ffn,\n            norm_layer=norm_layer,\n            use_inter_ffn=dec_use_inter_ffn,\n            max_temporal_relative=T_in + T_out,\n            padding_type=padding_type,\n            hierarchical_pos_embed=dec_hierarchical_pos_embed,\n            pos_embed_type=pos_embed_type,\n            use_self_global=num_global_vectors &gt; 0 and use_dec_self_global,\n            self_update_global=dec_self_update_global,\n            use_cross_global=num_global_vectors &gt; 0 and use_dec_cross_global,\n            use_global_vector_ffn=use_global_vector_ffn,\n            use_global_self_attn=use_global_self_attn,\n            separate_global_qkv=separate_global_qkv,\n            global_dim_ratio=global_dim_ratio,\n            checkpoint_level=checkpoint_level,\n            use_relative_pos=use_relative_pos,\n            self_attn_use_final_proj=self_attn_use_final_proj,\n            use_first_self_attn=dec_use_first_self_attn,\n            attn_linear_init_mode=attn_linear_init_mode,\n            ffn_linear_init_mode=ffn_linear_init_mode,\n            conv_init_mode=conv_init_mode,\n            up_linear_init_mode=down_up_linear_init_mode,\n            norm_init_mode=norm_init_mode,\n        )\n        self.reset_parameters()\n\n    def get_initial_encoder_final_decoder(\n        self,\n        initial_downsample_type,\n        activation,\n        initial_downsample_scale,\n        initial_downsample_conv_layers,\n        final_upsample_conv_layers,\n        padding_type,\n        initial_downsample_stack_conv_num_layers,\n        initial_downsample_stack_conv_dim_list,\n        initial_downsample_stack_conv_downscale_list,\n        initial_downsample_stack_conv_num_conv_list,\n    ):\n        T_in, H_in, W_in, C_in = self.input_shape\n        T_out, H_out, W_out, C_out = self.target_shape\n        self.initial_downsample_type = initial_downsample_type\n        if self.initial_downsample_type == \"conv\":\n            if isinstance(initial_downsample_scale, int):\n                initial_downsample_scale = (\n                    1,\n                    initial_downsample_scale,\n                    initial_downsample_scale,\n                )\n            elif len(initial_downsample_scale) == 2:\n                initial_downsample_scale = 1, *initial_downsample_scale\n            elif len(initial_downsample_scale) == 3:\n                initial_downsample_scale = tuple(initial_downsample_scale)\n            else:\n                raise NotImplementedError(\n                    f\"initial_downsample_scale {initial_downsample_scale} format not supported!\"\n                )\n            self.initial_encoder = InitialEncoder(\n                dim=C_in,\n                out_dim=self.base_units,\n                downsample_scale=initial_downsample_scale,\n                num_conv_layers=initial_downsample_conv_layers,\n                padding_type=padding_type,\n                activation=activation,\n                conv_init_mode=self.conv_init_mode,\n                linear_init_mode=self.down_up_linear_init_mode,\n                norm_init_mode=self.norm_init_mode,\n            )\n\n            self.final_decoder = FinalDecoder(\n                dim=self.base_units,\n                target_thw=(T_out, H_out, W_out),\n                num_conv_layers=final_upsample_conv_layers,\n                activation=activation,\n                conv_init_mode=self.conv_init_mode,\n                linear_init_mode=self.down_up_linear_init_mode,\n                norm_init_mode=self.norm_init_mode,\n            )\n            new_input_shape = self.initial_encoder.patch_merge.get_out_shape(\n                self.input_shape\n            )\n            self.dec_final_proj = paddle.nn.Linear(\n                in_features=self.base_units, out_features=C_out\n            )\n        elif self.initial_downsample_type == \"stack_conv\":\n            if initial_downsample_stack_conv_dim_list is None:\n                initial_downsample_stack_conv_dim_list = [\n                    self.base_units\n                ] * initial_downsample_stack_conv_num_layers\n            self.initial_encoder = InitialStackPatchMergingEncoder(\n                num_merge=initial_downsample_stack_conv_num_layers,\n                in_dim=C_in,\n                out_dim_list=initial_downsample_stack_conv_dim_list,\n                downsample_scale_list=initial_downsample_stack_conv_downscale_list,\n                num_conv_per_merge_list=initial_downsample_stack_conv_num_conv_list,\n                padding_type=padding_type,\n                activation=activation,\n                conv_init_mode=self.conv_init_mode,\n                linear_init_mode=self.down_up_linear_init_mode,\n                norm_init_mode=self.norm_init_mode,\n            )\n            initial_encoder_out_shape_list = self.initial_encoder.get_out_shape_list(\n                self.target_shape\n            )\n            (\n                dec_target_shape_list,\n                dec_in_dim,\n            ) = FinalStackUpsamplingDecoder.get_init_params(\n                enc_input_shape=self.target_shape,\n                enc_out_shape_list=initial_encoder_out_shape_list,\n                large_channel=True,\n            )\n            self.final_decoder = FinalStackUpsamplingDecoder(\n                target_shape_list=dec_target_shape_list,\n                in_dim=dec_in_dim,\n                num_conv_per_up_list=initial_downsample_stack_conv_num_conv_list[::-1],\n                activation=activation,\n                conv_init_mode=self.conv_init_mode,\n                linear_init_mode=self.down_up_linear_init_mode,\n                norm_init_mode=self.norm_init_mode,\n            )\n            self.dec_final_proj = paddle.nn.Linear(\n                in_features=dec_target_shape_list[-1][-1], out_features=C_out\n            )\n            new_input_shape = self.initial_encoder.get_out_shape_list(self.input_shape)[\n                -1\n            ]\n        else:\n            raise NotImplementedError(f\"{self.initial_downsample_type} is invalid.\")\n        self.input_shape_after_initial_downsample = new_input_shape\n        T_in, H_in, W_in, _ = new_input_shape\n        return new_input_shape\n\n    def reset_parameters(self):\n        if self.num_global_vectors &gt; 0:\n            self.init_global_vectors = initializer.trunc_normal_(\n                self.init_global_vectors, std=0.02\n            )\n        if hasattr(self.initial_encoder, \"reset_parameters\"):\n            self.initial_encoder.reset_parameters()\n        else:\n            cuboid_utils.apply_initialization(\n                self.initial_encoder,\n                conv_mode=self.conv_init_mode,\n                linear_mode=self.down_up_linear_init_mode,\n                norm_mode=self.norm_init_mode,\n            )\n        if hasattr(self.final_decoder, \"reset_parameters\"):\n            self.final_decoder.reset_parameters()\n        else:\n            cuboid_utils.apply_initialization(\n                self.final_decoder,\n                conv_mode=self.conv_init_mode,\n                linear_mode=self.down_up_linear_init_mode,\n                norm_mode=self.norm_init_mode,\n            )\n        cuboid_utils.apply_initialization(\n            self.dec_final_proj, linear_mode=self.down_up_linear_init_mode\n        )\n        self.encoder.reset_parameters()\n        self.enc_pos_embed.reset_parameters()\n        self.decoder.reset_parameters()\n        self.dec_pos_embed.reset_parameters()\n        cuboid_utils.apply_initialization(self.z_proj, linear_mode=\"0\")\n\n    def get_initial_z(self, final_mem, T_out):\n        B = final_mem.shape[0]\n        if self.z_init_method == \"zeros\":\n            z_shape = list((1, T_out)) + final_mem.shape[2:]\n            initial_z = paddle.zeros(shape=z_shape, dtype=final_mem.dtype)\n            initial_z = self.z_proj(self.dec_pos_embed(initial_z)).expand(\n                shape=[B, -1, -1, -1, -1]\n            )\n        elif self.z_init_method == \"nearest_interp\":\n            initial_z = paddle.nn.functional.interpolate(\n                x=final_mem.transpose(perm=[0, 4, 1, 2, 3]),\n                size=(T_out, final_mem.shape[2], final_mem.shape[3]),\n            ).transpose(perm=[0, 2, 3, 4, 1])\n            initial_z = self.z_proj(initial_z)\n        elif self.z_init_method == \"last\":\n            initial_z = paddle.broadcast_to(\n                x=final_mem[:, -1:, :, :, :], shape=(B, T_out) + final_mem.shape[2:]\n            )\n            initial_z = self.z_proj(initial_z)\n        elif self.z_init_method == \"mean\":\n            initial_z = paddle.broadcast_to(\n                x=final_mem.mean(axis=1, keepdims=True),\n                shape=(B, T_out) + final_mem.shape[2:],\n            )\n            initial_z = self.z_proj(initial_z)\n        else:\n            raise NotImplementedError\n        return initial_z\n\n    def forward(self, x: \"paddle.Tensor\", verbose: bool = False) -&gt; \"paddle.Tensor\":\n        \"\"\"\n        Args:\n            x (paddle.Tensor): Tensor with shape (B, T, H, W, C).\n            verbose (bool): If True, print intermediate shapes.\n\n        Returns:\n            out (paddle.Tensor): The output Shape (B, T_out, H, W, C_out)\n        \"\"\"\n\n        x = self.concat_to_tensor(x, self.input_keys)\n        flag_ndim = x.ndim\n        if flag_ndim == 6:\n            x = x.reshape([-1, *x.shape[2:]])\n        B, _, _, _, _ = x.shape\n\n        T_out = self.target_shape[0]\n        x = self.initial_encoder(x)\n        x = self.enc_pos_embed(x)\n\n        if self.num_global_vectors &gt; 0:\n            init_global_vectors = self.init_global_vectors.expand(\n                shape=[\n                    B,\n                    self.num_global_vectors,\n                    self.global_dim_ratio * self.base_units,\n                ]\n            )\n            mem_l, mem_global_vector_l = self.encoder(x, init_global_vectors)\n        else:\n            mem_l = self.encoder(x)\n\n        if verbose:\n            for i, mem in enumerate(mem_l):\n                print(f\"mem[{i}].shape = {mem.shape}\")\n        initial_z = self.get_initial_z(final_mem=mem_l[-1], T_out=T_out)\n\n        if self.num_global_vectors &gt; 0:\n            dec_out = self.decoder(initial_z, mem_l, mem_global_vector_l)\n        else:\n            dec_out = self.decoder(initial_z, mem_l)\n\n        dec_out = self.final_decoder(dec_out)\n\n        out = self.dec_final_proj(dec_out)\n        if flag_ndim == 6:\n            out = out.reshape([-1, *out.shape])\n        return {key: out for key in self.output_keys}\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.CuboidTransformer.forward","title":"<code>forward(x, verbose=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor with shape (B, T, H, W, C).</p> required <code>verbose</code> <code>bool</code> <p>If True, print intermediate shapes.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tensor</code> <p>The output Shape (B, T_out, H, W, C_out)</p> Source code in <code>ppsci/arch/cuboid_transformer.py</code> <pre><code>def forward(self, x: \"paddle.Tensor\", verbose: bool = False) -&gt; \"paddle.Tensor\":\n    \"\"\"\n    Args:\n        x (paddle.Tensor): Tensor with shape (B, T, H, W, C).\n        verbose (bool): If True, print intermediate shapes.\n\n    Returns:\n        out (paddle.Tensor): The output Shape (B, T_out, H, W, C_out)\n    \"\"\"\n\n    x = self.concat_to_tensor(x, self.input_keys)\n    flag_ndim = x.ndim\n    if flag_ndim == 6:\n        x = x.reshape([-1, *x.shape[2:]])\n    B, _, _, _, _ = x.shape\n\n    T_out = self.target_shape[0]\n    x = self.initial_encoder(x)\n    x = self.enc_pos_embed(x)\n\n    if self.num_global_vectors &gt; 0:\n        init_global_vectors = self.init_global_vectors.expand(\n            shape=[\n                B,\n                self.num_global_vectors,\n                self.global_dim_ratio * self.base_units,\n            ]\n        )\n        mem_l, mem_global_vector_l = self.encoder(x, init_global_vectors)\n    else:\n        mem_l = self.encoder(x)\n\n    if verbose:\n        for i, mem in enumerate(mem_l):\n            print(f\"mem[{i}].shape = {mem.shape}\")\n    initial_z = self.get_initial_z(final_mem=mem_l[-1], T_out=T_out)\n\n    if self.num_global_vectors &gt; 0:\n        dec_out = self.decoder(initial_z, mem_l, mem_global_vector_l)\n    else:\n        dec_out = self.decoder(initial_z, mem_l)\n\n    dec_out = self.final_decoder(dec_out)\n\n    out = self.dec_final_proj(dec_out)\n    if flag_ndim == 6:\n        out = out.reshape([-1, *out.shape])\n    return {key: out for key in self.output_keys}\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.SFNONet","title":"<code>SFNONet</code>","text":"<p>               Bases: <code>Arch</code></p> <p>N-Dimensional Tensorized Fourier Neural Operator.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>n_modes</code> <code>Tuple[int, ...]</code> <p>Number of modes to keep in Fourier Layer, along each dimension The dimensionality of the SFNO is inferred from <code>`len(n_modes)</code></p> required <code>hidden_channels</code> <code>int</code> <p>Width of the FNO (i.e. number of channels)</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>3</code> <code>out_channels</code> <code>int</code> <p>Number of output channels. Defaults to 1.</p> <code>1</code> <code>lifting_channels</code> <code>int</code> <p>Number of hidden channels of the lifting block of the FNO. Defaults to 256.</p> <code>256</code> <code>projection_channels</code> <code>int</code> <p>Number of hidden channels of the projection block of the FNO. Defaults to 256.</p> <code>256</code> <code>n_layers</code> <code>int</code> <p>Number of Fourier Layers. Defaults to 4.</p> <code>4</code> <code>use_mlp</code> <code>bool</code> <p>Whether to use an MLP layer after each FNO block. Defaults to False.</p> <code>False</code> <code>mlp</code> <code>Dict[str, float]</code> <p>Parameters of the MLP. {'expansion': float, 'dropout': float}. Defaults to None.</p> <code>None</code> <code>non_linearity</code> <code>functional</code> <p>Non-Linearity module to use. Defaults to F.gelu.</p> <code>gelu</code> <code>norm</code> <code>str</code> <p>Normalization layer to use. Defaults to None.</p> <code>None</code> <code>ada_in_features</code> <code>(int, optional)</code> <p>The input channles of the adaptive normalization.Defaults to None.</p> <code>None</code> <code>preactivation</code> <code>bool</code> <p>Whether to use resnet-style preactivation. Defaults to False.</p> <code>False</code> <code>fno_skip</code> <code>str</code> <p>Type of skip connection to use,{'linear', 'identity', 'soft-gating'}. Defaults to \"soft-gating\".</p> <code>'linear'</code> <code>separable</code> <code>bool</code> <p>Whether to use a depthwise separable spectral convolution. Defaults to  False.</p> <code>False</code> <code>factorization</code> <code>str</code> <p>Tensor factorization of the parameters weight to use. * If None, a dense tensor parametrizes the Spectral convolutions. * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".</p> <code>None</code> <code>rank</code> <code>float</code> <p>Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.</p> <code>1.0</code> <code>joint_factorization</code> <code>bool</code> <p>Whether all the Fourier Layers should be parametrized by a single tensor (vs one per layer). Defaults to False.</p> <code>False</code> <code>implementation</code> <code>str</code> <p>{'factorized', 'reconstructed'}, optional. Defaults to \"factorized\". If factorization is not None, forward mode to use:: * <code>reconstructed</code> : the full weight tensor is reconstructed from the factorization and used for the forward pass. * <code>factorized</code> : the input is directly contracted with the factors of the decomposition.</p> <code>'factorized'</code> <code>domain_padding</code> <code>Optional[list]</code> <p>Whether to use percentage of padding. Defaults to None.</p> <code>None</code> <code>domain_padding_mode</code> <code>str</code> <p>{'symmetric', 'one-sided'}, optional How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".</p> <code>'one-sided'</code> <code>fft_norm</code> <code>str</code> <p>The normalization mode for the FFT. Defaults to \"forward\".</p> <code>'forward'</code> <code>patching_levels</code> <code>int</code> <p>Number of patching levels to use. Defaults to 0.</p> <code>0</code> Source code in <code>ppsci/arch/sfnonet.py</code> <pre><code>class SFNONet(base.Arch):\n    \"\"\"N-Dimensional Tensorized Fourier Neural Operator.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n        n_modes (Tuple[int, ...]): Number of modes to keep in Fourier Layer, along each dimension\n            The dimensionality of the SFNO is inferred from ``len(n_modes)`\n        hidden_channels (int): Width of the FNO (i.e. number of channels)\n        in_channels (int, optional): Number of input channels. Defaults to 3.\n        out_channels (int, optional): Number of output channels. Defaults to 1.\n        lifting_channels (int, optional): Number of hidden channels of the lifting block of the FNO.\n            Defaults to 256.\n        projection_channels (int, optional): Number of hidden channels of the projection block of the FNO.\n            Defaults to 256.\n        n_layers (int, optional): Number of Fourier Layers. Defaults to 4.\n        use_mlp (bool, optional): Whether to use an MLP layer after each FNO block. Defaults to False.\n        mlp (Dict[str, float], optional): Parameters of the MLP. {'expansion': float, 'dropout': float}.\n            Defaults to None.\n        non_linearity (nn.functional, optional): Non-Linearity module to use. Defaults to F.gelu.\n        norm (str, optional): Normalization layer to use. Defaults to None.\n        ada_in_features (int,optional): The input channles of the adaptive normalization.Defaults to None.\n        preactivation (bool, optional): Whether to use resnet-style preactivation. Defaults to False.\n        fno_skip (str, optional): Type of skip connection to use,{'linear', 'identity', 'soft-gating'}.\n            Defaults to \"soft-gating\".\n        separable (bool, optional): Whether to use a depthwise separable spectral convolution.\n            Defaults to  False.\n        factorization (str, optional): Tensor factorization of the parameters weight to use.\n            * If None, a dense tensor parametrizes the Spectral convolutions.\n            * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".\n        rank (float, optional): Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.\n        joint_factorization (bool, optional): Whether all the Fourier Layers should be parametrized by a\n            single tensor (vs one per layer). Defaults to False.\n        implementation (str, optional): {'factorized', 'reconstructed'}, optional. Defaults to \"factorized\".\n            If factorization is not None, forward mode to use::\n            * `reconstructed` : the full weight tensor is reconstructed from the factorization and used for the forward pass.\n            * `factorized` : the input is directly contracted with the factors of the decomposition.\n        domain_padding (Optional[list], optional): Whether to use percentage of padding. Defaults to None.\n        domain_padding_mode (str, optional): {'symmetric', 'one-sided'}, optional\n            How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".\n        fft_norm (str, optional): The normalization mode for the FFT. Defaults to \"forward\".\n        patching_levels (int, optional): Number of patching levels to use. Defaults to 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        n_modes: Tuple[int, ...],\n        hidden_channels: int,\n        in_channels: int = 3,\n        out_channels: int = 1,\n        lifting_channels: int = 256,\n        projection_channels: int = 256,\n        n_layers: int = 4,\n        use_mlp: bool = False,\n        mlp: Optional[Dict[str, float]] = None,\n        max_n_modes: int = None,\n        non_linearity: nn.functional = F.gelu,\n        stabilizer: str = None,\n        norm: str = None,\n        ada_in_features: Optional[int] = None,\n        preactivation: bool = False,\n        fno_skip: str = \"linear\",\n        mlp_skip: str = \"soft-gating\",\n        separable: bool = False,\n        factorization: str = None,\n        rank: float = 1.0,\n        joint_factorization: bool = False,\n        implementation: str = \"factorized\",\n        domain_padding: Optional[list] = None,\n        domain_padding_mode: str = \"one-sided\",\n        fft_norm: str = \"forward\",\n        patching_levels: int = 0,\n        **kwargs,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n\n        self.n_dim = len(n_modes)\n        self.n_modes = n_modes\n        self.hidden_channels = hidden_channels\n        self.lifting_channels = lifting_channels\n        self.projection_channels = projection_channels\n        self.in_channels = in_channels\n        if patching_levels:\n            self.in_channels = self.in_channels * patching_levels + 1\n        self.out_channels = out_channels\n        self.n_layers = n_layers\n        self.joint_factorization = joint_factorization\n        self.non_linearity = non_linearity\n        self.rank = rank\n        self.factorization = factorization\n        self.fno_skip = (fno_skip,)\n        self.mlp_skip = (mlp_skip,)\n        self.fft_norm = fft_norm\n        self.implementation = implementation\n        self.separable = separable\n        self.preactivation = preactivation\n        self.stabilizer = stabilizer\n        if domain_padding is not None and (\n            (isinstance(domain_padding, list) and sum(domain_padding) &gt; 0)\n            or (isinstance(domain_padding, (float, int)) and domain_padding &gt; 0)\n        ):\n            self.domain_padding = fno_block.DomainPadding(\n                domain_padding=domain_padding, padding_mode=domain_padding_mode\n            )\n        else:\n            self.domain_padding = None\n        self.domain_padding_mode = domain_padding_mode\n\n        self.fno_blocks = fno_block.FNOBlocks(\n            in_channels=hidden_channels,\n            out_channels=hidden_channels,\n            n_modes=self.n_modes,\n            n_layers=n_layers,\n            max_n_modes=max_n_modes,\n            use_mlp=use_mlp,\n            mlp=mlp,\n            non_linearity=non_linearity,\n            stabilizer=stabilizer,\n            norm=norm,\n            ada_in_features=ada_in_features,\n            preactivation=preactivation,\n            fno_skip=fno_skip,\n            mlp_skip=mlp_skip,\n            separable=separable,\n            factorization=factorization,\n            rank=rank,\n            SpectralConv=SphericalConv,\n            joint_factorization=joint_factorization,\n            implementation=implementation,\n            fft_norm=fft_norm,\n        )\n        # if lifting_channels is passed, make lifting an MLP\n        # with a hidden layer of size lifting_channels\n        if self.lifting_channels:\n            self.lifting = fno_block.MLP(\n                in_channels=in_channels,\n                out_channels=self.hidden_channels,\n                hidden_channels=self.lifting_channels,\n                n_layers=2,\n                n_dim=self.n_dim,\n            )\n        # otherwise, make it a linear layer\n        else:\n            self.lifting = fno_block.MLP(\n                in_channels=in_channels,\n                out_channels=self.hidden_channels,\n                hidden_channels=self.hidden_channels,\n                n_layers=1,\n                n_dim=self.n_dim,\n            )\n        self.projection = fno_block.MLP(\n            in_channels=self.hidden_channels,\n            out_channels=out_channels,\n            hidden_channels=self.projection_channels,\n            n_layers=2,\n            n_dim=self.n_dim,\n            non_linearity=non_linearity,\n        )\n\n    def forward(self, x):\n        \"\"\"SFNO's forward pass\"\"\"\n        x = self.concat_to_tensor(x, self.input_keys)\n\n        x = self.lifting(x)\n        if self.domain_padding is not None:\n            x = self.domain_padding.pad(x)\n        # x is 0.4 * [1, 32, 16, 16], passed\n        for index in range(self.n_layers):\n            x = self.fno_blocks(x, index)\n\n        if self.domain_padding is not None:\n            x = self.domain_padding.unpad(x)\n        out = self.projection(x)\n\n        return {self.output_keys[0]: out}\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.SFNONet.forward","title":"<code>forward(x)</code>","text":"<p>SFNO's forward pass</p> Source code in <code>ppsci/arch/sfnonet.py</code> <pre><code>def forward(self, x):\n    \"\"\"SFNO's forward pass\"\"\"\n    x = self.concat_to_tensor(x, self.input_keys)\n\n    x = self.lifting(x)\n    if self.domain_padding is not None:\n        x = self.domain_padding.pad(x)\n    # x is 0.4 * [1, 32, 16, 16], passed\n    for index in range(self.n_layers):\n        x = self.fno_blocks(x, index)\n\n    if self.domain_padding is not None:\n        x = self.domain_padding.unpad(x)\n    out = self.projection(x)\n\n    return {self.output_keys[0]: out}\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.UNONet","title":"<code>UNONet</code>","text":"<p>               Bases: <code>Arch</code></p> <p>N-Dimensional U-Shaped Neural Operator.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>hidden_channels</code> <code>int</code> <p>Width of the FNO (i.e. number of channels).</p> required <code>lifting_channels</code> <code>int</code> <p>Number of hidden channels of the lifting block of the FNO. Defaults to 256.</p> <code>256</code> <code>projection_channels</code> <code>int</code> <p>Number of hidden channels of the projection block of the FNO. Defaults to 256.</p> <code>256</code> <code>n_layers</code> <code>int</code> <p>Number of Fourier Layers. Defaults to 4.</p> <code>4</code> <code>uno_out_channels</code> <code>Tuple[int, ...]</code> <p>Number of output channel of each Fourier Layers. Eaxmple: For a Five layer UNO uno_out_channels can be [32,64,64,64,32].c</p> <code>None</code> <code>uno_n_modes</code> <code>Tuple[Tuple[int, ...], ...]</code> <p>Number of Fourier Modes to use in integral operation of each Fourier Layers (along each dimension). Example: For a five layer UNO with 2D input the uno_n_modes can be: [[5,5],[5,5],[5,5],[5,5],[5,5]]. Defaults to None.</p> <code>None</code> <code>uno_scalings</code> <code>Tuple[Tuple[int, ...], ...]</code> <p>Scaling Factors for each Fourier Layers. Example: For a five layer UNO with 2D input, the uno_scalings can be : [[1.0,1.0],[0.5,0.5],[1,1],[1,1],[2,2]].Defaults to None.</p> <code>None</code> <code>horizontal_skips_map</code> <code>Dict</code> <p>A map {...., b: a, ....} denoting horizontal skip connection from a-th layer to b-th layer. If None default skip connection is applied. Example: For a 5 layer UNO architecture, the skip connections can be horizontal_skips_map ={4:0,3:1}.Defaults to None.</p> <code>None</code> <code>incremental_n_modes</code> <code>(tuple[int], optional)</code> <p>Incremental number of modes to use in Fourier domain. * If not None, this allows to incrementally increase the number of modes in Fourier domain during training. Has to verify n &lt;= N for (n, m) in zip(incremental_n_modes, n_modes). * If None, all the n_modes are used. This can be updated dynamically during training.Defaults to None.</p> <code>None</code> <code>use_mlp</code> <code>bool</code> <p>Whether to use an MLP layer after each FNO block. Defaults to False.</p> <code>False</code> <code>mlp</code> <code>Dict[str, float]</code> <p>Parameters of the MLP. {'expansion': float, 'dropout': float}. Defaults to None.</p> <code>None</code> <code>non_linearity</code> <code>functional</code> <p>Non-Linearity module to use. Defaults to F.gelu.</p> <code>gelu</code> <code>norm</code> <code>str</code> <p>Normalization layer to use. Defaults to None.</p> <code>None</code> <code>ada_in_features</code> <code>(Optional[int], optional)</code> <p>The input channles of the adaptive normalization.Defaults to None.</p> <code>None</code> <code>preactivation</code> <code>bool</code> <p>Whether to use resnet-style preactivation. Defaults to False.</p> <code>False</code> <code>fno_skip</code> <code>str</code> <p>Type of skip connection to use for fno_block. Defaults to \"linear\".</p> <code>'linear'</code> <code>horizontal_skip</code> <code>str</code> <p>Type of skip connection to use for horizontal skip. Defaults to \"linear\".</p> <code>'linear'</code> <code>mlp_skip</code> <code>str</code> <p>Type of skip connection to use for mlp. Defaults to \"soft-gating\".</p> <code>'soft-gating'</code> <code>separable</code> <code>bool</code> <p>Whether to use a depthwise separable spectral convolution. Defaults to  False.</p> <code>False</code> <code>factorization</code> <code>str</code> <p>Tensor factorization of the parameters weight to use. * If None, a dense tensor parametrizes the Spectral convolutions. * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".</p> <code>None</code> <code>rank</code> <code>float</code> <p>Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.</p> <code>1.0</code> <code>joint_factorization</code> <code>bool</code> <p>Whether all the Fourier Layers should be parametrized by a single tensor (vs one per layer). Defaults to False.</p> <code>False</code> <code>implementation</code> <code>str</code> <p>{'factorized', 'reconstructed'}, optional. Defaults to \"factorized\". If factorization is not None, forward mode to use:: * <code>reconstructed</code> : the full weight tensor is reconstructed from the factorization and used for the forward pass. * <code>factorized</code> : the input is directly contracted with the factors of the decomposition.</p> <code>'factorized'</code> <code>domain_padding</code> <code>Optional[Union[list, float, int]]</code> <p>Whether to use percentage of padding. Defaults to None.</p> <code>None</code> <code>domain_padding_mode</code> <code>str</code> <p>{'symmetric', 'one-sided'}, optional How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".</p> <code>'one-sided'</code> <code>fft_norm</code> <code>str</code> <p>The normalization mode for the FFT. Defaults to \"forward\".</p> <code>'forward'</code> <code>patching_levels</code> <code>int</code> <p>Number of patching levels to use. Defaults to 0.</p> <code>0</code> Source code in <code>ppsci/arch/unonet.py</code> <pre><code>class UNONet(base.Arch):\n    \"\"\"N-Dimensional U-Shaped Neural Operator.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n        in_channels (int, optional): Number of input channels.\n        out_channels (int, optional): Number of output channels.\n        hidden_channels (int): Width of the FNO (i.e. number of channels).\n        lifting_channels (int, optional): Number of hidden channels of the lifting block of the FNO.\n            Defaults to 256.\n        projection_channels (int, optional): Number of hidden channels of the projection block of the FNO.\n            Defaults to 256.\n        n_layers (int, optional): Number of Fourier Layers. Defaults to 4.\n        uno_out_channels (Tuple[int, ...], optional): Number of output channel of each Fourier Layers.\n            Eaxmple: For a Five layer UNO uno_out_channels can be [32,64,64,64,32].c\n        uno_n_modes (Tuple[Tuple[int, ...], ...]): Number of Fourier Modes to use in integral operation of each\n            Fourier Layers (along each dimension).\n            Example: For a five layer UNO with 2D input the uno_n_modes can be: [[5,5],[5,5],[5,5],[5,5],[5,5]]. Defaults to None.\n        uno_scalings (Tuple[Tuple[int, ...], ...]): Scaling Factors for each Fourier Layers.\n            Example: For a five layer UNO with 2D input, the uno_scalings can be : [[1.0,1.0],[0.5,0.5],[1,1],[1,1],[2,2]].Defaults to None.\n        horizontal_skips_map (Dict, optional): A map {...., b: a, ....} denoting horizontal skip connection\n            from a-th layer to b-th layer. If None default skip connection is applied.\n            Example: For a 5 layer UNO architecture, the skip connections can be horizontal_skips_map ={4:0,3:1}.Defaults to None.\n        incremental_n_modes (tuple[int],optional): Incremental number of modes to use in Fourier domain.\n            * If not None, this allows to incrementally increase the number of modes in Fourier domain\n            during training. Has to verify n &lt;= N for (n, m) in zip(incremental_n_modes, n_modes).\n            * If None, all the n_modes are used.\n            This can be updated dynamically during training.Defaults to None.\n        use_mlp (bool, optional): Whether to use an MLP layer after each FNO block. Defaults to False.\n        mlp (Dict[str, float], optional): Parameters of the MLP. {'expansion': float, 'dropout': float}.\n            Defaults to None.\n        non_linearity (nn.functional, optional): Non-Linearity module to use. Defaults to F.gelu.\n        norm (str, optional): Normalization layer to use. Defaults to None.\n        ada_in_features (Optional[int],optional): The input channles of the adaptive normalization.Defaults to\n            None.\n        preactivation (bool, optional): Whether to use resnet-style preactivation. Defaults to False.\n        fno_skip (str, optional): Type of skip connection to use for fno_block. Defaults to \"linear\".\n        horizontal_skip (str, optional): Type of skip connection to use for horizontal skip. Defaults to\n            \"linear\".\n        mlp_skip (str, optional): Type of skip connection to use for mlp. Defaults to \"soft-gating\".\n        separable (bool, optional): Whether to use a depthwise separable spectral convolution.\n            Defaults to  False.\n        factorization (str, optional): Tensor factorization of the parameters weight to use.\n            * If None, a dense tensor parametrizes the Spectral convolutions.\n            * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".\n        rank (float, optional): Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.\n        joint_factorization (bool, optional): Whether all the Fourier Layers should be parametrized by a\n            single tensor (vs one per layer). Defaults to False.\n        implementation (str, optional): {'factorized', 'reconstructed'}, optional. Defaults to \"factorized\".\n            If factorization is not None, forward mode to use::\n            * `reconstructed` : the full weight tensor is reconstructed from the factorization and used for the forward pass.\n            * `factorized` : the input is directly contracted with the factors of the decomposition.\n        domain_padding (Optional[Union[list, float, int]], optional): Whether to use percentage of padding.\n            Defaults to None.\n        domain_padding_mode (str, optional): {'symmetric', 'one-sided'}, optional\n            How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".\n        fft_norm (str, optional): The normalization mode for the FFT. Defaults to \"forward\".\n        patching_levels (int, optional): Number of patching levels to use. Defaults to 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        in_channels: int,\n        out_channels: int,\n        hidden_channels: int,\n        lifting_channels: int = 256,\n        projection_channels: int = 256,\n        n_layers: int = 4,\n        uno_out_channels: Tuple[int, ...] = None,\n        uno_n_modes: Tuple[Tuple[int, ...], ...] = None,\n        uno_scalings: Tuple[Tuple[int, ...], ...] = None,\n        horizontal_skips_map: Dict = None,\n        incremental_n_modes: Tuple[int, ...] = None,\n        use_mlp: bool = False,\n        mlp: Optional[Dict[str, float]] = None,\n        non_linearity: nn.functional = F.gelu,\n        norm: str = None,\n        ada_in_features: Optional[int] = None,\n        preactivation: bool = False,\n        fno_skip: str = \"linear\",\n        horizontal_skip: str = \"linear\",\n        mlp_skip: str = \"soft-gating\",\n        separable: bool = False,\n        factorization: str = None,\n        rank: float = 1.0,\n        joint_factorization: bool = False,\n        implementation: str = \"factorized\",\n        domain_padding: Optional[Union[list, float, int]] = None,\n        domain_padding_mode: str = \"one-sided\",\n        fft_norm: str = \"forward\",\n        patching_levels: int = 0,\n        **kwargs,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        if uno_out_channels is None:\n            raise ValueError(\"uno_out_channels can not be None\")\n        if uno_n_modes is None:\n            raise ValueError(\"uno_n_modes can not be None\")\n        if uno_scalings is None:\n            raise ValueError(\"uno_scalings can not be None\")\n\n        if len(uno_out_channels) != n_layers:\n            raise ValueError(\"Output channels for all layers are not given\")\n\n        if len(uno_n_modes) != n_layers:\n            raise ValueError(\"Number of modes for all layers are not given\")\n\n        if len(uno_scalings) != n_layers:\n            raise ValueError(\"Scaling factor for all layers are not given\")\n\n        self.n_dim = len(uno_n_modes[0])\n        self.uno_out_channels = uno_out_channels\n        self.uno_n_modes = uno_n_modes\n        self.uno_scalings = uno_scalings\n\n        self.hidden_channels = hidden_channels\n        self.lifting_channels = lifting_channels\n        self.projection_channels = projection_channels\n        self.in_channels = in_channels\n        if patching_levels:\n            self.in_channels = self.in_channels * patching_levels + 1\n        self.out_channels = out_channels\n        self.n_layers = n_layers\n        self.horizontal_skips_map = horizontal_skips_map\n        self.joint_factorization = joint_factorization\n        self.non_linearity = non_linearity\n        self.rank = rank\n        self.factorization = factorization\n        self.fno_skip = (fno_skip,)\n        self.mlp_skip = (mlp_skip,)\n        self.fft_norm = fft_norm\n        self.implementation = implementation\n        self.separable = separable\n        self.preactivation = preactivation\n        self._incremental_n_modes = incremental_n_modes\n        self.mlp = mlp\n        # constructing default skip maps\n        if self.horizontal_skips_map is None:\n            self.horizontal_skips_map = {}\n            for i in range(\n                0,\n                n_layers // 2,\n            ):\n                # example, if n_layers = 5, then 4:0, 3:1\n                self.horizontal_skips_map[n_layers - i - 1] = i\n        # self.uno_scalings may be a 1d list specifying uniform scaling factor at each layer\n        # or a 2d list, where each row specifies scaling factors along each dimention.\n        # To get the final (end to end) scaling factors we need to multiply\n        # the scaling factors (a list) of all layer.\n\n        self.end_to_end_scaling_factor = [1] * len(self.uno_scalings[0])\n        # multiplying scaling factors\n        for k in self.uno_scalings:\n            self.end_to_end_scaling_factor = [\n                i * j for (i, j) in zip(self.end_to_end_scaling_factor, k)\n            ]\n\n        # list with a single element is replaced by the scaler.\n        if len(self.end_to_end_scaling_factor) == 1:\n            self.end_to_end_scaling_factor = self.end_to_end_scaling_factor[0]\n\n        if isinstance(self.end_to_end_scaling_factor, (float, int)):\n            self.end_to_end_scaling_factor = [\n                self.end_to_end_scaling_factor\n            ] * self.n_dim\n\n        if domain_padding is not None and (\n            (isinstance(domain_padding, list) and sum(domain_padding) &gt; 0)\n            or (isinstance(domain_padding, (float, int)) and domain_padding &gt; 0)\n        ):\n            self.domain_padding = fno_block.DomainPadding(\n                domain_padding=domain_padding, padding_mode=domain_padding_mode\n            )\n        else:\n            self.domain_padding = None\n        self.domain_padding_mode = domain_padding_mode\n\n        self.lifting = fno_block.MLP(\n            in_channels=in_channels,\n            out_channels=self.hidden_channels,\n            hidden_channels=self.lifting_channels,\n            n_layers=2,\n            n_dim=self.n_dim,\n        )\n\n        self.fno_blocks = nn.LayerList([])\n        self.horizontal_skips = nn.LayerDict({})\n        prev_out = self.hidden_channels\n        for i in range(self.n_layers):\n            if i in self.horizontal_skips_map.keys():\n                prev_out = (\n                    prev_out + self.uno_out_channels[self.horizontal_skips_map[i]]\n                )\n            self.fno_blocks.append(\n                fno_block.FNOBlocks(\n                    in_channels=prev_out,\n                    out_channels=self.uno_out_channels[i],\n                    n_modes=self.uno_n_modes[i],\n                    use_mlp=use_mlp,\n                    mlp=mlp,\n                    output_scaling_factor=[self.uno_scalings[i]],\n                    non_linearity=non_linearity,\n                    norm=norm,\n                    ada_in_features=ada_in_features,\n                    preactivation=preactivation,\n                    fno_skip=fno_skip,\n                    mlp_skip=mlp_skip,\n                    separable=separable,\n                    incremental_n_modes=incremental_n_modes,\n                    factorization=factorization,\n                    rank=rank,\n                    SpectralConv=fno_block.FactorizedSpectralConv,\n                    joint_factorization=joint_factorization,\n                    implementation=implementation,\n                    fft_norm=fft_norm,\n                )\n            )\n\n            if i in self.horizontal_skips_map.values():\n                self.horizontal_skips[str(i)] = fno_block.skip_connection(\n                    self.uno_out_channels[i],\n                    self.uno_out_channels[i],\n                    type=horizontal_skip,\n                    n_dim=self.n_dim,\n                )\n            prev_out = self.uno_out_channels[i]\n\n        self.projection = fno_block.MLP(\n            in_channels=prev_out,\n            out_channels=out_channels,\n            hidden_channels=self.projection_channels,\n            n_layers=2,\n            n_dim=self.n_dim,\n            non_linearity=non_linearity,\n        )\n\n    def forward(self, x, **kwargs):\n        x = self.concat_to_tensor(x, self.input_keys)\n        x = self.lifting(x)\n        if self.domain_padding is not None:\n            x = self.domain_padding.pad(x)\n        output_shape = [\n            int(round(i * j))\n            for (i, j) in zip(x.shape[-self.n_dim :], self.end_to_end_scaling_factor)\n        ]\n\n        skip_outputs = {}\n        cur_output = None\n        for layer_idx in range(self.n_layers):\n            if layer_idx in self.horizontal_skips_map.keys():\n                skip_val = skip_outputs[self.horizontal_skips_map[layer_idx]]\n                output_scaling_factors = [\n                    m / n for (m, n) in zip(x.shape, skip_val.shape)\n                ]\n                output_scaling_factors = output_scaling_factors[-1 * self.n_dim :]\n                t = fno_block.resample(\n                    skip_val, output_scaling_factors, list(range(-self.n_dim, 0))\n                )\n                x = paddle.concat([x, t], axis=1)\n\n            if layer_idx == self.n_layers - 1:\n                cur_output = output_shape\n            x = self.fno_blocks[layer_idx](x, output_shape=cur_output)\n            if layer_idx in self.horizontal_skips_map.values():\n                skip_outputs[layer_idx] = self.horizontal_skips[str(layer_idx)](x)\n\n        if self.domain_padding is not None:\n            x = self.domain_padding.unpad(x)\n\n        out = self.projection(x)\n        return {self.output_keys[0]: out}\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.TFNO1dNet","title":"<code>TFNO1dNet</code>","text":"<p>               Bases: <code>FNONet</code></p> <p>1D Fourier Neural Operator.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>n_modes_height</code> <code>Tuple[int, ...]</code> <p>Number of Fourier modes to keep along the height, along each dimension.</p> required <code>hidden_channels</code> <code>int</code> <p>Width of the FNO (i.e. number of channels).</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>3</code> <code>out_channels</code> <code>int</code> <p>Number of output channels. Defaults to 1.</p> <code>1</code> <code>lifting_channels</code> <code>int</code> <p>Number of hidden channels of the lifting block of the FNO. Defaults to 256.</p> <code>256</code> <code>projection_channels</code> <code>int</code> <p>Number of hidden channels of the projection block of the FNO. Defaults to 256.</p> <code>256</code> <code>n_layers</code> <code>int</code> <p>Number of Fourier Layers. Defaults to 4.</p> <code>4</code> <code>use_mlp</code> <code>bool</code> <p>Whether to use an MLP layer after each FNO block. Defaults to False.</p> <code>False</code> <code>mlp</code> <code>dict[str, float]</code> <p>Parameters of the MLP. {'expansion': float, 'dropout': float}. Defaults to None.</p> <code>None</code> <code>non_linearity</code> <code>functional</code> <p>Non-Linearity module to use. Defaults to F.gelu.</p> <code>gelu</code> <code>norm</code> <code>module</code> <p>Normalization layer to use. Defaults to None.</p> <code>None</code> <code>preactivation</code> <code>bool</code> <p>Whether to use resnet-style preactivation. Defaults to False.</p> <code>False</code> <code>skip</code> <code>str</code> <p>Type of skip connection to use,{'linear', 'identity', 'soft-gating'}. Defaults to \"soft-gating\".</p> <code>'soft-gating'</code> <code>separable</code> <code>bool</code> <p>Whether to use a depthwise separable spectral convolution. Defaults to  False.</p> <code>False</code> <code>factorization</code> <code>str</code> <p>Tensor factorization of the parameters weight to use. * If None, a dense tensor parametrizes the Spectral convolutions. * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".</p> <code>'Tucker'</code> <code>rank</code> <code>float</code> <p>Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.</p> <code>1.0</code> <code>joint_factorization</code> <code>bool</code> <p>Whether all the Fourier Layers should be parametrized by a single tensor (vs one per layer). Defaults to False.</p> <code>False</code> <code>implementation</code> <code>str</code> <p>{'factorized', 'reconstructed'}, optional. Defaults to \"factorized\". If factorization is not None, forward mode to use:: * <code>reconstructed</code> : the full weight tensor is reconstructed from the factorization and used for the forward pass. * <code>factorized</code> : the input is directly contracted with the factors of the decomposition.</p> <code>'factorized'</code> <code>domain_padding</code> <code>Optional[Union[list, float, int]]</code> <p>Whether to use percentage of padding. Defaults to None.</p> <code>None</code> <code>domain_padding_mode</code> <code>str</code> <p>{'symmetric', 'one-sided'}, optional How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".</p> <code>'one-sided'</code> <code>fft_norm</code> <code>str</code> <p>The normalization mode for the FFT. Defaults to \"forward\".</p> <code>'forward'</code> <code>patching_levels</code> <code>int</code> <p>Number of patching levels to use. Defaults to 0.</p> <code>0</code> <code>SpectralConv</code> <code>layer</code> <p>Spectral convolution layer to use. Defaults to fno_block.FactorizedSpectralConv.</p> <code>FactorizedSpectralConv</code> Source code in <code>ppsci/arch/tfnonet.py</code> <pre><code>class TFNO1dNet(FNONet):\n    \"\"\"1D Fourier Neural Operator.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n        n_modes_height (Tuple[int, ...]): Number of Fourier modes to keep along the height, along each\n            dimension.\n        hidden_channels (int): Width of the FNO (i.e. number of channels).\n        in_channels (int, optional): Number of input channels. Defaults to 3.\n        out_channels (int, optional): Number of output channels. Defaults to 1.\n        lifting_channels (int, optional): Number of hidden channels of the lifting block of the FNO.\n            Defaults to 256.\n        projection_channels (int, optional): Number of hidden channels of the projection block of the FNO.\n            Defaults to 256.\n        n_layers (int, optional): Number of Fourier Layers. Defaults to 4.\n        use_mlp (bool, optional): Whether to use an MLP layer after each FNO block. Defaults to False.\n        mlp (dict[str, float], optional): Parameters of the MLP. {'expansion': float, 'dropout': float}.\n            Defaults to None.\n        non_linearity (nn.functional, optional): Non-Linearity module to use. Defaults to F.gelu.\n        norm (F.module, optional): Normalization layer to use. Defaults to None.\n        preactivation (bool, optional): Whether to use resnet-style preactivation. Defaults to False.\n        skip (str, optional): Type of skip connection to use,{'linear', 'identity', 'soft-gating'}.\n            Defaults to \"soft-gating\".\n        separable (bool, optional): Whether to use a depthwise separable spectral convolution.\n            Defaults to  False.\n        factorization (str, optional): Tensor factorization of the parameters weight to use.\n            * If None, a dense tensor parametrizes the Spectral convolutions.\n            * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".\n        rank (float, optional): Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.\n        joint_factorization (bool, optional): Whether all the Fourier Layers should be parametrized by a\n            single tensor (vs one per layer). Defaults to False.\n        implementation (str, optional): {'factorized', 'reconstructed'}, optional. Defaults to \"factorized\".\n            If factorization is not None, forward mode to use::\n            * `reconstructed` : the full weight tensor is reconstructed from the factorization and used for the forward pass.\n            * `factorized` : the input is directly contracted with the factors of the decomposition.\n        domain_padding (Optional[Union[list, float, int]], optional): Whether to use percentage of padding.\n            Defaults to None.\n        domain_padding_mode (str, optional): {'symmetric', 'one-sided'}, optional\n            How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".\n        fft_norm (str, optional): The normalization mode for the FFT. Defaults to \"forward\".\n        patching_levels (int, optional): Number of patching levels to use. Defaults to 0.\n        SpectralConv (nn.layer, optional): Spectral convolution layer to use.\n            Defaults to fno_block.FactorizedSpectralConv.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        n_modes_height: Tuple[int, ...],\n        hidden_channels: int,\n        in_channels: int = 3,\n        out_channels: int = 1,\n        lifting_channels: int = 256,\n        projection_channels: int = 256,\n        n_layers: int = 4,\n        non_linearity: nn.functional = F.gelu,\n        use_mlp: bool = False,\n        mlp: Optional[Dict[str, float]] = None,\n        norm: str = None,\n        skip: str = \"soft-gating\",\n        separable: bool = False,\n        preactivation: bool = False,\n        factorization: str = \"Tucker\",\n        rank: float = 1.0,\n        joint_factorization: bool = False,\n        implementation: str = \"factorized\",\n        domain_padding: Optional[Union[list, float, int]] = None,\n        domain_padding_mode: str = \"one-sided\",\n        fft_norm: str = \"forward\",\n        patching_levels: int = 0,\n        SpectralConv: nn.Layer = fno_block.FactorizedSpectralConv,\n        **kwargs,\n    ):\n        super().__init__(\n            input_keys=input_keys,\n            output_keys=output_keys,\n            n_modes=(n_modes_height,),\n            hidden_channels=hidden_channels,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            lifting_channels=lifting_channels,\n            projection_channels=projection_channels,\n            n_layers=n_layers,\n            non_linearity=non_linearity,\n            use_mlp=use_mlp,\n            mlp=mlp,\n            norm=norm,\n            skip=skip,\n            separable=separable,\n            preactivation=preactivation,\n            factorization=factorization,\n            rank=rank,\n            joint_factorization=joint_factorization,\n            implementation=implementation,\n            domain_padding=domain_padding,\n            domain_padding_mode=domain_padding_mode,\n            fft_norm=fft_norm,\n            patching_levels=patching_levels,\n            SpectralConv=SpectralConv,\n        )\n        self.n_modes_height = n_modes_height\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.TFNO2dNet","title":"<code>TFNO2dNet</code>","text":"<p>               Bases: <code>FNONet</code></p> <p>2D Fourier Neural Operator.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>n_modes_height</code> <code>int</code> <p>Number of Fourier modes to keep along the height.</p> required <code>n_modes_width</code> <code>int</code> <p>Number of modes to keep in Fourier Layer, along the width.</p> required <code>hidden_channels</code> <code>int</code> <p>Width of the FNO (i.e. number of channels).</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>3</code> <code>out_channels</code> <code>int</code> <p>Number of output channels. Defaults to 1.</p> <code>1</code> <code>lifting_channels</code> <code>int</code> <p>Number of hidden channels of the lifting block of the FNO.  Defaults to 256.</p> <code>256</code> <code>projection_channels</code> <code>int</code> <p>Number of hidden channels of the projection block of the FNO.  Defaults to 256.</p> <code>256</code> <code>n_layers</code> <code>int</code> <p>Number of Fourier Layers. Defaults to 4.</p> <code>4</code> <code>use_mlp</code> <code>bool</code> <p>Whether to use an MLP layer after each FNO block. Defaults to False.</p> <code>False</code> <code>mlp</code> <code>Dict[str, float]</code> <p>Parameters of the MLP. {'expansion': float, 'dropout': float}.  Defaults to None.</p> <code>None</code> <code>non_linearity</code> <code>Layer</code> <p>Non-Linearity module to use. Defaults to F.gelu.</p> <code>gelu</code> <code>norm</code> <code>module</code> <p>Normalization layer to use. Defaults to None.</p> <code>None</code> <code>preactivation</code> <code>bool</code> <p>Whether to use resnet-style preactivation. Defaults to False.</p> <code>False</code> <code>skip</code> <code>str</code> <p>Type of skip connection to use,{'linear', 'identity', 'soft-gating'}.  Defaults to \"soft-gating\".</p> <code>'soft-gating'</code> <code>separable</code> <code>bool</code> <p>Whether to use a depthwise separable spectral convolution.  Defaults to  False.</p> <code>False</code> <code>factorization</code> <code>str</code> <p>Tensor factorization of the parameters weight to use.  * If None, a dense tensor parametrizes the Spectral convolutions.  * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".</p> <code>'Tucker'</code> <code>rank</code> <code>float</code> <p>Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.</p> <code>1.0</code> <code>joint_factorization</code> <code>bool</code> <p>Whether all the Fourier Layers should be parametrized by a  single tensor (vs one per layer). Defaults to False.</p> <code>False</code> <code>implementation</code> <code>str</code> <p>{'factorized', 'reconstructed'}, optional. Defaults to \"factorized\".  If factorization is not None, forward mode to use::  * <code>reconstructed</code> : the full weight tensor is reconstructed from the factorization and used for the forward pass.  * <code>factorized</code> : the input is directly contracted with the factors of the decomposition.</p> <code>'factorized'</code> <code>domain_padding</code> <code>Union[list, float, int]</code> <p>Whether to use percentage of padding. Defaults to   None.</p> <code>None</code> <code>domain_padding_mode</code> <code>str</code> <p>{'symmetric', 'one-sided'}, optional  How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".</p> <code>'one-sided'</code> <code>fft_norm</code> <code>str</code> <p>The normalization mode for the FFT. Defaults to \"forward\".</p> <code>'forward'</code> <code>patching_levels</code> <code>int</code> <p>Number of patching levels to use. Defaults to 0.</p> <code>0</code> <code>SpectralConv</code> <code>layer</code> <p>Spectral convolution layer to use.   Defaults to fno_block.FactorizedSpectralConv.</p> <code>FactorizedSpectralConv</code> Source code in <code>ppsci/arch/tfnonet.py</code> <pre><code>class TFNO2dNet(FNONet):\n    \"\"\"2D Fourier Neural Operator.\n\n    Args:\n       input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n       output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n       n_modes_height (int): Number of Fourier modes to keep along the height.\n       n_modes_width (int): Number of modes to keep in Fourier Layer, along the width.\n       hidden_channels (int): Width of the FNO (i.e. number of channels).\n       in_channels (int, optional): Number of input channels. Defaults to 3.\n       out_channels (int, optional): Number of output channels. Defaults to 1.\n       lifting_channels (int, optional): Number of hidden channels of the lifting block of the FNO.\n           Defaults to 256.\n       projection_channels (int, optional): Number of hidden channels of the projection block of the FNO.\n           Defaults to 256.\n       n_layers (int, optional): Number of Fourier Layers. Defaults to 4.\n       use_mlp (bool, optional): Whether to use an MLP layer after each FNO block. Defaults to False.\n       mlp (Dict[str, float], optional): Parameters of the MLP. {'expansion': float, 'dropout': float}.\n           Defaults to None.\n       non_linearity (nn.Layer, optional): Non-Linearity module to use. Defaults to F.gelu.\n       norm (F.module, optional): Normalization layer to use. Defaults to None.\n       preactivation (bool, optional): Whether to use resnet-style preactivation. Defaults to False.\n       skip (str, optional): Type of skip connection to use,{'linear', 'identity', 'soft-gating'}.\n           Defaults to \"soft-gating\".\n       separable (bool, optional): Whether to use a depthwise separable spectral convolution.\n           Defaults to  False.\n       factorization (str, optional): Tensor factorization of the parameters weight to use.\n           * If None, a dense tensor parametrizes the Spectral convolutions.\n           * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".\n       rank (float, optional): Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.\n       joint_factorization (bool, optional): Whether all the Fourier Layers should be parametrized by a\n           single tensor (vs one per layer). Defaults to False.\n       implementation (str, optional): {'factorized', 'reconstructed'}, optional. Defaults to \"factorized\".\n           If factorization is not None, forward mode to use::\n           * `reconstructed` : the full weight tensor is reconstructed from the factorization and used for the forward pass.\n           * `factorized` : the input is directly contracted with the factors of the decomposition.\n       domain_padding (Union[list,float,int], optional): Whether to use percentage of padding. Defaults to\n            None.\n       domain_padding_mode (str, optional): {'symmetric', 'one-sided'}, optional\n           How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".\n       fft_norm (str, optional): The normalization mode for the FFT. Defaults to \"forward\".\n       patching_levels (int, optional): Number of patching levels to use. Defaults to 0.\n       SpectralConv (nn.layer, optional): Spectral convolution layer to use.\n            Defaults to fno_block.FactorizedSpectralConv.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        n_modes_height: int,\n        n_modes_width: int,\n        hidden_channels: int,\n        in_channels: int = 3,\n        out_channels: int = 1,\n        lifting_channels: int = 256,\n        projection_channels: int = 256,\n        n_layers: int = 4,\n        non_linearity: nn.functional = F.gelu,\n        use_mlp: bool = False,\n        mlp: Optional[Dict[str, float]] = None,\n        norm: str = None,\n        skip: str = \"soft-gating\",\n        separable: bool = False,\n        preactivation: bool = False,\n        factorization: str = \"Tucker\",\n        rank: float = 1.0,\n        joint_factorization: bool = False,\n        implementation: str = \"factorized\",\n        domain_padding: Optional[Union[list, float, int]] = None,\n        domain_padding_mode: str = \"one-sided\",\n        fft_norm: str = \"forward\",\n        patching_levels: int = 0,\n        SpectralConv: nn.layer = fno_block.FactorizedSpectralConv,\n        **kwargs,\n    ):\n        super().__init__(\n            input_keys=input_keys,\n            output_keys=output_keys,\n            n_modes=(n_modes_height, n_modes_width),\n            hidden_channels=hidden_channels,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            lifting_channels=lifting_channels,\n            projection_channels=projection_channels,\n            n_layers=n_layers,\n            non_linearity=non_linearity,\n            use_mlp=use_mlp,\n            mlp=mlp,\n            norm=norm,\n            skip=skip,\n            separable=separable,\n            preactivation=preactivation,\n            factorization=factorization,\n            rank=rank,\n            joint_factorization=joint_factorization,\n            implementation=implementation,\n            domain_padding=domain_padding,\n            domain_padding_mode=domain_padding_mode,\n            fft_norm=fft_norm,\n            patching_levels=patching_levels,\n            SpectralConv=SpectralConv,\n        )\n        self.n_modes_height = n_modes_height\n        self.n_modes_width = n_modes_width\n</code></pre>"},{"location":"zh/api/arch/#ppsci.arch.TFNO3dNet","title":"<code>TFNO3dNet</code>","text":"<p>               Bases: <code>FNONet</code></p> <p>3D Fourier Neural Operator.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input keys, such as (\"input\",).</p> required <code>output_keys</code> <code>Tuple[str, ...]</code> <p>Name of output keys, such as (\"output\",).</p> required <code>n_modes_height</code> <code>int</code> <p>Number of Fourier modes to keep along the height.</p> required <code>n_modes_width</code> <code>int</code> <p>Number of modes to keep in Fourier Layer, along the width.</p> required <code>n_modes_depth</code> <code>int</code> <p>Number of Fourier modes to keep along the depth.</p> required <code>hidden_channels</code> <code>int</code> <p>Width of the FNO (i.e. number of channels).</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>3</code> <code>out_channels</code> <code>int</code> <p>Number of output channels. Defaults to 1.</p> <code>1</code> <code>lifting_channels</code> <code>int</code> <p>Number of hidden channels of the lifting block of the FNO. Defaults to 256.</p> <code>256</code> <code>projection_channels</code> <code>int</code> <p>Number of hidden channels of the projection block of the FNO. Defaults to 256.</p> <code>256</code> <code>n_layers</code> <code>int</code> <p>Number of Fourier Layers. Defaults to 4.</p> <code>4</code> <code>use_mlp</code> <code>bool</code> <p>Whether to use an MLP layer after each FNO block. Defaults to False.</p> <code>False</code> <code>mlp</code> <code>Dict[str, float]</code> <p>Parameters of the MLP. {'expansion': float, 'dropout': float}. Defaults to None.</p> <code>None</code> <code>non_linearity</code> <code>Layer</code> <p>Non-Linearity module to use. Defaults to F.gelu.</p> <code>gelu</code> <code>norm</code> <code>module</code> <p>Normalization layer to use. Defaults to None.</p> <code>None</code> <code>preactivation</code> <code>bool</code> <p>Whether to use resnet-style preactivation. Defaults to False.</p> <code>False</code> <code>skip</code> <code>str</code> <p>Type of skip connection to use,{'linear', 'identity', 'soft-gating'}. Defaults to \"soft-gating\".</p> <code>'soft-gating'</code> <code>separable</code> <code>bool</code> <p>Whether to use a depthwise separable spectral convolution. Defaults to  False.</p> <code>False</code> <code>factorization</code> <code>str</code> <p>Tensor factorization of the parameters weight to use. * If None, a dense tensor parametrizes the Spectral convolutions. * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".</p> <code>'Tucker'</code> <code>rank</code> <code>float</code> <p>Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.</p> <code>1.0</code> <code>joint_factorization</code> <code>bool</code> <p>Whether all the Fourier Layers should be parametrized by a single tensor (vs one per layer). Defaults to False.</p> <code>False</code> <code>implementation</code> <code>str</code> <p>{'factorized', 'reconstructed'}, optional. Defaults to \"factorized\". If factorization is not None, forward mode to use:: * <code>reconstructed</code> : the full weight tensor is reconstructed from the factorization and used for the forward pass. * <code>factorized</code> : the input is directly contracted with the factors of the decomposition.</p> <code>'factorized'</code> <code>domain_padding</code> <code>str</code> <p>Whether to use percentage of padding. Defaults to None.</p> <code>None</code> <code>domain_padding_mode</code> <code>str</code> <p>{'symmetric', 'one-sided'}, optional How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".</p> <code>'one-sided'</code> <code>fft_norm</code> <code>str</code> <p>The normalization mode for the FFT. Defaults to \"forward\".</p> <code>'forward'</code> <code>patching_levels</code> <code>int</code> <p>Number of patching levels to use. Defaults to 0.</p> <code>0</code> <code>SpectralConv</code> <code>layer</code> <p>Spectral convolution layer to use. Defaults to fno_block. FactorizedSpectralConv.</p> <code>FactorizedSpectralConv</code> Source code in <code>ppsci/arch/tfnonet.py</code> <pre><code>class TFNO3dNet(FNONet):\n    \"\"\"3D Fourier Neural Operator.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input\",).\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output\",).\n        n_modes_height (int): Number of Fourier modes to keep along the height.\n        n_modes_width (int): Number of modes to keep in Fourier Layer, along the width.\n        n_modes_depth (int): Number of Fourier modes to keep along the depth.\n        hidden_channels (int): Width of the FNO (i.e. number of channels).\n        in_channels (int, optional): Number of input channels. Defaults to 3.\n        out_channels (int, optional): Number of output channels. Defaults to 1.\n        lifting_channels (int, optional): Number of hidden channels of the lifting block of the FNO.\n            Defaults to 256.\n        projection_channels (int, optional): Number of hidden channels of the projection block of the FNO.\n            Defaults to 256.\n        n_layers (int, optional): Number of Fourier Layers. Defaults to 4.\n        use_mlp (bool, optional): Whether to use an MLP layer after each FNO block. Defaults to False.\n        mlp (Dict[str, float], optional): Parameters of the MLP. {'expansion': float, 'dropout': float}.\n            Defaults to None.\n        non_linearity (nn.Layer, optional): Non-Linearity module to use. Defaults to F.gelu.\n        norm (F.module, optional): Normalization layer to use. Defaults to None.\n        preactivation (bool, optional): Whether to use resnet-style preactivation. Defaults to False.\n        skip (str, optional): Type of skip connection to use,{'linear', 'identity', 'soft-gating'}.\n            Defaults to \"soft-gating\".\n        separable (bool, optional): Whether to use a depthwise separable spectral convolution.\n            Defaults to  False.\n        factorization (str, optional): Tensor factorization of the parameters weight to use.\n            * If None, a dense tensor parametrizes the Spectral convolutions.\n            * Otherwise, the specified tensor factorization is used. Defaults to \"Tucker\".\n        rank (float, optional): Rank of the tensor factorization of the Fourier weights. Defaults to 1.0.\n        joint_factorization (bool, optional): Whether all the Fourier Layers should be parametrized by a\n            single tensor (vs one per layer). Defaults to False.\n        implementation (str, optional): {'factorized', 'reconstructed'}, optional. Defaults to \"factorized\".\n            If factorization is not None, forward mode to use::\n            * `reconstructed` : the full weight tensor is reconstructed from the factorization and used for the forward pass.\n            * `factorized` : the input is directly contracted with the factors of the decomposition.\n        domain_padding (str, optional): Whether to use percentage of padding. Defaults to None.\n        domain_padding_mode (str, optional): {'symmetric', 'one-sided'}, optional\n            How to perform domain padding, by default 'one-sided'. Defaults to \"one-sided\".\n        fft_norm (str, optional): The normalization mode for the FFT. Defaults to \"forward\".\n        patching_levels (int, optional): Number of patching levels to use. Defaults to 0.\n        SpectralConv (nn.layer, optional): Spectral convolution layer to use. Defaults to fno_block.\n            FactorizedSpectralConv.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        n_modes_height: int,\n        n_modes_width: int,\n        n_modes_depth: int,\n        hidden_channels: int,\n        in_channels: int = 3,\n        out_channels: int = 1,\n        lifting_channels: int = 256,\n        projection_channels: int = 256,\n        n_layers: int = 4,\n        non_linearity: nn.functional = F.gelu,\n        use_mlp: bool = False,\n        mlp: Optional[Dict[str, float]] = None,\n        norm: str = None,\n        skip: str = \"soft-gating\",\n        separable: bool = False,\n        preactivation: bool = False,\n        factorization: str = \"Tucker\",\n        rank: float = 1.0,\n        joint_factorization: bool = False,\n        implementation: str = \"factorized\",\n        domain_padding: Optional[Union[list, float, int]] = None,\n        domain_padding_mode: str = \"one-sided\",\n        fft_norm: str = \"forward\",\n        patching_levels: int = 0,\n        SpectralConv: nn.layer = fno_block.FactorizedSpectralConv,\n        **kwargs,\n    ):\n        super().__init__(\n            input_keys=input_keys,\n            output_keys=output_keys,\n            n_modes=(n_modes_height, n_modes_width, n_modes_depth),\n            hidden_channels=hidden_channels,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            lifting_channels=lifting_channels,\n            projection_channels=projection_channels,\n            n_layers=n_layers,\n            non_linearity=non_linearity,\n            use_mlp=use_mlp,\n            mlp=mlp,\n            norm=norm,\n            skip=skip,\n            separable=separable,\n            preactivation=preactivation,\n            factorization=factorization,\n            rank=rank,\n            joint_factorization=joint_factorization,\n            implementation=implementation,\n            domain_padding=domain_padding,\n            domain_padding_mode=domain_padding_mode,\n            fft_norm=fft_norm,\n            patching_levels=patching_levels,\n            SpectralConv=SpectralConv,\n        )\n        self.n_modes_height = n_modes_height\n        self.n_modes_width = n_modes_width\n        self.n_modes_height = n_modes_height\n</code></pre>"},{"location":"zh/api/autodiff/","title":"ppsci.autodiff","text":""},{"location":"zh/api/autodiff/#autodiff","title":"AutoDiff(\u81ea\u52a8\u5fae\u5206) \u6a21\u5757","text":""},{"location":"zh/api/autodiff/#ppsci.autodiff.ad","title":"<code>ppsci.autodiff.ad</code>","text":"<p>This module is adapted from https://github.com/lululxvi/deepxde</p>"},{"location":"zh/api/autodiff/#ppsci.autodiff.ad.jacobian","title":"<code>jacobian: Callable[['paddle.Tensor', Union['paddle.Tensor', List['paddle.Tensor']], int, Optional[int], Optional[bool], bool], Union['paddle.Tensor', List['paddle.Tensor']]] = Jacobians()</code>  <code>module-attribute</code>","text":""},{"location":"zh/api/autodiff/#ppsci.autodiff.ad.hessian","title":"<code>hessian: Callable[['paddle.Tensor', 'paddle.Tensor', Optional[int], int, int, Optional['paddle.Tensor'], Optional[bool], bool], 'paddle.Tensor'] = Hessians()</code>  <code>module-attribute</code>","text":""},{"location":"zh/api/autodiff/#ppsci.autodiff.ad.Jacobians","title":"<code>Jacobians</code>","text":"<p>Compute multiple Jacobians.</p> \\[ \\rm Jacobian(ys, xs, i, j) = \\dfrac{\\partial ys_i}{\\partial xs_j} \\] <p>A new instance will be created for a new pair of (output, input). For the (output, input) pair that has been computed before, it will reuse the previous instance, rather than creating a new one.</p> Source code in <code>ppsci/autodiff/ad.py</code> <pre><code>class Jacobians:\n    r\"\"\"Compute multiple Jacobians.\n\n    $$\n    \\rm Jacobian(ys, xs, i, j) = \\dfrac{\\partial ys_i}{\\partial xs_j}\n    $$\n\n    A new instance will be created for a new pair of (output, input). For the (output,\n    input) pair that has been computed before, it will reuse the previous instance,\n    rather than creating a new one.\n    \"\"\"\n\n    def __init__(self):\n        self.Js = {}\n\n    def __call__(\n        self,\n        ys: \"paddle.Tensor\",\n        xs: Union[\"paddle.Tensor\", List[\"paddle.Tensor\"]],\n        i: int = 0,\n        j: Optional[int] = None,\n        retain_graph: Optional[bool] = None,\n        create_graph: bool = True,\n    ) -&gt; Union[\"paddle.Tensor\", List[\"paddle.Tensor\"]]:\n        \"\"\"Compute jacobians for given ys and xs.\n\n        Args:\n            ys (paddle.Tensor): Output tensor.\n            xs (Union[paddle.Tensor, List[paddle.Tensor]]): Input tensor(s).\n            i (int, optional): i-th output variable. Defaults to 0.\n            j (Optional[int]): j-th input variable. Defaults to None.\n            retain_graph (Optional[bool]): Whether to retain the forward graph which\n                is used to calculate the gradient. When it is True, the graph would\n                be retained, in which way users can calculate backward twice for the\n                same graph. When it is False, the graph would be freed. Default None,\n                which means it is equal to `create_graph`.\n            create_graph (bool, optional): Whether to create the gradient graphs of\n                the computing process. When it is True, higher order derivatives are\n                supported to compute; when it is False, the gradient graphs of the\n                computing process would be discarded. Default False.\n\n        Returns:\n            paddle.Tensor: Jacobian matrix of ys[i] to xs[j].\n\n        Examples:\n            &gt;&gt;&gt; import paddle\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; x = paddle.randn([4, 1])\n            &gt;&gt;&gt; x.stop_gradient = False\n            &gt;&gt;&gt; y = x * x\n            &gt;&gt;&gt; dy_dx = ppsci.autodiff.jacobian(y, x)\n            &gt;&gt;&gt; print(dy_dx.shape)\n            [4, 1]\n        \"\"\"\n        if not isinstance(xs, (list, tuple)):\n            key = (ys, xs)\n            if key not in self.Js:\n                self.Js[key] = _Jacobian(ys, xs)\n            return self.Js[key](i, j, retain_graph, create_graph)\n        else:\n            xs_require = [xs[i] for i in range(len(xs)) if (ys, xs[i]) not in self.Js]\n            grads_require = paddle.grad(\n                ys,\n                xs_require,\n                create_graph=create_graph,\n                retain_graph=retain_graph,\n            )\n\n            idx = 0\n            Js_list = []\n            for k, xs_ in enumerate(xs):\n                key = (ys, xs_)\n                assert xs_.shape[-1] == 1, (\n                    f\"The last dim of each xs should be 1, but xs[{k}] has shape \"\n                    f\"{xs_.shape}\"\n                )\n                if key not in self.Js:\n                    self.Js[key] = _Jacobian(ys, xs_, {0: grads_require[idx]})\n                    idx += 1\n                Js_list.append(self.Js[key](i, j, retain_graph, create_graph))\n            return Js_list\n\n    def _clear(self):\n        \"\"\"Clear cached Jacobians.\"\"\"\n        self.Js = {}\n</code></pre>"},{"location":"zh/api/autodiff/#ppsci.autodiff.ad.Jacobians.__call__","title":"<code>__call__(ys, xs, i=0, j=None, retain_graph=None, create_graph=True)</code>","text":"<p>Compute jacobians for given ys and xs.</p> <p>Parameters:</p> Name Type Description Default <code>ys</code> <code>Tensor</code> <p>Output tensor.</p> required <code>xs</code> <code>Union[Tensor, List[Tensor]]</code> <p>Input tensor(s).</p> required <code>i</code> <code>int</code> <p>i-th output variable. Defaults to 0.</p> <code>0</code> <code>j</code> <code>Optional[int]</code> <p>j-th input variable. Defaults to None.</p> <code>None</code> <code>retain_graph</code> <code>Optional[bool]</code> <p>Whether to retain the forward graph which is used to calculate the gradient. When it is True, the graph would be retained, in which way users can calculate backward twice for the same graph. When it is False, the graph would be freed. Default None, which means it is equal to <code>create_graph</code>.</p> <code>None</code> <code>create_graph</code> <code>bool</code> <p>Whether to create the gradient graphs of the computing process. When it is True, higher order derivatives are supported to compute; when it is False, the gradient graphs of the computing process would be discarded. Default False.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union['paddle.Tensor', List['paddle.Tensor']]</code> <p>paddle.Tensor: Jacobian matrix of ys[i] to xs[j].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; x = paddle.randn([4, 1])\n&gt;&gt;&gt; x.stop_gradient = False\n&gt;&gt;&gt; y = x * x\n&gt;&gt;&gt; dy_dx = ppsci.autodiff.jacobian(y, x)\n&gt;&gt;&gt; print(dy_dx.shape)\n[4, 1]\n</code></pre> Source code in <code>ppsci/autodiff/ad.py</code> <pre><code>def __call__(\n    self,\n    ys: \"paddle.Tensor\",\n    xs: Union[\"paddle.Tensor\", List[\"paddle.Tensor\"]],\n    i: int = 0,\n    j: Optional[int] = None,\n    retain_graph: Optional[bool] = None,\n    create_graph: bool = True,\n) -&gt; Union[\"paddle.Tensor\", List[\"paddle.Tensor\"]]:\n    \"\"\"Compute jacobians for given ys and xs.\n\n    Args:\n        ys (paddle.Tensor): Output tensor.\n        xs (Union[paddle.Tensor, List[paddle.Tensor]]): Input tensor(s).\n        i (int, optional): i-th output variable. Defaults to 0.\n        j (Optional[int]): j-th input variable. Defaults to None.\n        retain_graph (Optional[bool]): Whether to retain the forward graph which\n            is used to calculate the gradient. When it is True, the graph would\n            be retained, in which way users can calculate backward twice for the\n            same graph. When it is False, the graph would be freed. Default None,\n            which means it is equal to `create_graph`.\n        create_graph (bool, optional): Whether to create the gradient graphs of\n            the computing process. When it is True, higher order derivatives are\n            supported to compute; when it is False, the gradient graphs of the\n            computing process would be discarded. Default False.\n\n    Returns:\n        paddle.Tensor: Jacobian matrix of ys[i] to xs[j].\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; x = paddle.randn([4, 1])\n        &gt;&gt;&gt; x.stop_gradient = False\n        &gt;&gt;&gt; y = x * x\n        &gt;&gt;&gt; dy_dx = ppsci.autodiff.jacobian(y, x)\n        &gt;&gt;&gt; print(dy_dx.shape)\n        [4, 1]\n    \"\"\"\n    if not isinstance(xs, (list, tuple)):\n        key = (ys, xs)\n        if key not in self.Js:\n            self.Js[key] = _Jacobian(ys, xs)\n        return self.Js[key](i, j, retain_graph, create_graph)\n    else:\n        xs_require = [xs[i] for i in range(len(xs)) if (ys, xs[i]) not in self.Js]\n        grads_require = paddle.grad(\n            ys,\n            xs_require,\n            create_graph=create_graph,\n            retain_graph=retain_graph,\n        )\n\n        idx = 0\n        Js_list = []\n        for k, xs_ in enumerate(xs):\n            key = (ys, xs_)\n            assert xs_.shape[-1] == 1, (\n                f\"The last dim of each xs should be 1, but xs[{k}] has shape \"\n                f\"{xs_.shape}\"\n            )\n            if key not in self.Js:\n                self.Js[key] = _Jacobian(ys, xs_, {0: grads_require[idx]})\n                idx += 1\n            Js_list.append(self.Js[key](i, j, retain_graph, create_graph))\n        return Js_list\n</code></pre>"},{"location":"zh/api/autodiff/#ppsci.autodiff.ad.Hessians","title":"<code>Hessians</code>","text":"<p>Compute multiple Hessians.</p> \\[ \\rm Hessian(ys, xs, component, i, j) = \\dfrac{\\partial ys_{component}}{\\partial xs_i \\partial xs_j} \\] <p>A new instance will be created for a new pair of (output, input). For the (output, input) pair that has been computed before, it will reuse the previous instance, rather than creating a new one.</p> Source code in <code>ppsci/autodiff/ad.py</code> <pre><code>class Hessians:\n    r\"\"\"Compute multiple Hessians.\n\n    $$\n    \\rm Hessian(ys, xs, component, i, j) = \\dfrac{\\partial ys_{component}}{\\partial xs_i \\partial xs_j}\n    $$\n\n    A new instance will be created for a new pair of (output, input). For the (output,\n    input) pair that has been computed before, it will reuse the previous instance,\n    rather than creating a new one.\n    \"\"\"\n\n    def __init__(self):\n        self.Hs = {}\n\n    def __call__(\n        self,\n        ys: \"paddle.Tensor\",\n        xs: \"paddle.Tensor\",\n        component: Optional[int] = None,\n        i: int = 0,\n        j: int = 0,\n        grad_y: Optional[\"paddle.Tensor\"] = None,\n        retain_graph: Optional[bool] = None,\n        create_graph: bool = True,\n    ) -&gt; \"paddle.Tensor\":\n        \"\"\"Compute hessian matrix for given ys and xs.\n\n        Args:\n            ys (paddle.Tensor): Output tensor.\n            xs (paddle.Tensor): Input tensor.\n            component (Optional[int]): If `y` has the shape (batch_size, dim_y &gt; 1), then `y[:, component]`\n                is used to compute the Hessian. Do not use if `y` has the shape (batch_size,\n                1). Defaults to None.\n            i (int, optional): I-th input variable. Defaults to 0.\n            j (int, optional): J-th input variable. Defaults to 0.\n            grad_y (Optional[paddle.Tensor]): The gradient of `y` w.r.t. `xs`. Provide `grad_y` if known to avoid\n                duplicate computation. Defaults to None.\n            retain_graph (Optional[bool]): Whether to retain the forward graph which\n                is used to calculate the gradient. When it is True, the graph would\n                be retained, in which way users can calculate backward twice for the\n                same graph. When it is False, the graph would be freed. Default None,\n                which means it is equal to `create_graph`.\n            create_graph (bool, optional): Whether to create the gradient graphs of\n                the computing process. When it is True, higher order derivatives are\n                supported to compute; when it is False, the gradient graphs of the\n                computing process would be discarded. Default False.\n\n        Returns:\n            paddle.Tensor: Hessian matrix.\n\n        Examples:\n            &gt;&gt;&gt; import paddle\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; x = paddle.randn([4, 3])\n            &gt;&gt;&gt; x.stop_gradient = False\n            &gt;&gt;&gt; y = (x * x).sin()\n            &gt;&gt;&gt; dy_dxx = ppsci.autodiff.hessian(y, x, component=0)\n            &gt;&gt;&gt; print(dy_dxx.shape)\n            [4, 1]\n        \"\"\"\n        key = (ys, xs, component)\n        if key not in self.Hs:\n            self.Hs[key] = _Hessian(ys, xs, component=component, grad_y=grad_y)\n        return self.Hs[key](i, j, retain_graph, create_graph)\n\n    def _clear(self):\n        \"\"\"Clear cached Hessians.\"\"\"\n        self.Hs = {}\n</code></pre>"},{"location":"zh/api/autodiff/#ppsci.autodiff.ad.Hessians.__call__","title":"<code>__call__(ys, xs, component=None, i=0, j=0, grad_y=None, retain_graph=None, create_graph=True)</code>","text":"<p>Compute hessian matrix for given ys and xs.</p> <p>Parameters:</p> Name Type Description Default <code>ys</code> <code>Tensor</code> <p>Output tensor.</p> required <code>xs</code> <code>Tensor</code> <p>Input tensor.</p> required <code>component</code> <code>Optional[int]</code> <p>If <code>y</code> has the shape (batch_size, dim_y &gt; 1), then <code>y[:, component]</code> is used to compute the Hessian. Do not use if <code>y</code> has the shape (batch_size, 1). Defaults to None.</p> <code>None</code> <code>i</code> <code>int</code> <p>I-th input variable. Defaults to 0.</p> <code>0</code> <code>j</code> <code>int</code> <p>J-th input variable. Defaults to 0.</p> <code>0</code> <code>grad_y</code> <code>Optional[Tensor]</code> <p>The gradient of <code>y</code> w.r.t. <code>xs</code>. Provide <code>grad_y</code> if known to avoid duplicate computation. Defaults to None.</p> <code>None</code> <code>retain_graph</code> <code>Optional[bool]</code> <p>Whether to retain the forward graph which is used to calculate the gradient. When it is True, the graph would be retained, in which way users can calculate backward twice for the same graph. When it is False, the graph would be freed. Default None, which means it is equal to <code>create_graph</code>.</p> <code>None</code> <code>create_graph</code> <code>bool</code> <p>Whether to create the gradient graphs of the computing process. When it is True, higher order derivatives are supported to compute; when it is False, the gradient graphs of the computing process would be discarded. Default False.</p> <code>True</code> <p>Returns:</p> Type Description <code>'paddle.Tensor'</code> <p>paddle.Tensor: Hessian matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; x = paddle.randn([4, 3])\n&gt;&gt;&gt; x.stop_gradient = False\n&gt;&gt;&gt; y = (x * x).sin()\n&gt;&gt;&gt; dy_dxx = ppsci.autodiff.hessian(y, x, component=0)\n&gt;&gt;&gt; print(dy_dxx.shape)\n[4, 1]\n</code></pre> Source code in <code>ppsci/autodiff/ad.py</code> <pre><code>def __call__(\n    self,\n    ys: \"paddle.Tensor\",\n    xs: \"paddle.Tensor\",\n    component: Optional[int] = None,\n    i: int = 0,\n    j: int = 0,\n    grad_y: Optional[\"paddle.Tensor\"] = None,\n    retain_graph: Optional[bool] = None,\n    create_graph: bool = True,\n) -&gt; \"paddle.Tensor\":\n    \"\"\"Compute hessian matrix for given ys and xs.\n\n    Args:\n        ys (paddle.Tensor): Output tensor.\n        xs (paddle.Tensor): Input tensor.\n        component (Optional[int]): If `y` has the shape (batch_size, dim_y &gt; 1), then `y[:, component]`\n            is used to compute the Hessian. Do not use if `y` has the shape (batch_size,\n            1). Defaults to None.\n        i (int, optional): I-th input variable. Defaults to 0.\n        j (int, optional): J-th input variable. Defaults to 0.\n        grad_y (Optional[paddle.Tensor]): The gradient of `y` w.r.t. `xs`. Provide `grad_y` if known to avoid\n            duplicate computation. Defaults to None.\n        retain_graph (Optional[bool]): Whether to retain the forward graph which\n            is used to calculate the gradient. When it is True, the graph would\n            be retained, in which way users can calculate backward twice for the\n            same graph. When it is False, the graph would be freed. Default None,\n            which means it is equal to `create_graph`.\n        create_graph (bool, optional): Whether to create the gradient graphs of\n            the computing process. When it is True, higher order derivatives are\n            supported to compute; when it is False, the gradient graphs of the\n            computing process would be discarded. Default False.\n\n    Returns:\n        paddle.Tensor: Hessian matrix.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; x = paddle.randn([4, 3])\n        &gt;&gt;&gt; x.stop_gradient = False\n        &gt;&gt;&gt; y = (x * x).sin()\n        &gt;&gt;&gt; dy_dxx = ppsci.autodiff.hessian(y, x, component=0)\n        &gt;&gt;&gt; print(dy_dxx.shape)\n        [4, 1]\n    \"\"\"\n    key = (ys, xs, component)\n    if key not in self.Hs:\n        self.Hs[key] = _Hessian(ys, xs, component=component, grad_y=grad_y)\n    return self.Hs[key](i, j, retain_graph, create_graph)\n</code></pre>"},{"location":"zh/api/autodiff/#ppsci.autodiff.ad.clear","title":"<code>clear()</code>","text":"<p>Clear cached Jacobians and Hessians.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; x = paddle.randn([4, 3])\n&gt;&gt;&gt; x.stop_gradient = False\n&gt;&gt;&gt; y = (x * x).sin()\n&gt;&gt;&gt; dy_dxx = ppsci.autodiff.hessian(y, x, component=0)\n&gt;&gt;&gt; ppsci.autodiff.clear()\n&gt;&gt;&gt; print(ppsci.autodiff.hessian.Hs)\n{}\n</code></pre> Source code in <code>ppsci/autodiff/ad.py</code> <pre><code>def clear():\n    \"\"\"Clear cached Jacobians and Hessians.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; x = paddle.randn([4, 3])\n        &gt;&gt;&gt; x.stop_gradient = False\n        &gt;&gt;&gt; y = (x * x).sin()\n        &gt;&gt;&gt; dy_dxx = ppsci.autodiff.hessian(y, x, component=0)\n        &gt;&gt;&gt; ppsci.autodiff.clear()\n        &gt;&gt;&gt; print(ppsci.autodiff.hessian.Hs)\n        {}\n    \"\"\"\n    jacobian._clear()\n    hessian._clear()\n</code></pre>"},{"location":"zh/api/constraint/","title":"ppsci.constraint","text":""},{"location":"zh/api/constraint/#constraint","title":"Constraint(\u7ea6\u675f\u6761\u4ef6) \u6a21\u5757","text":""},{"location":"zh/api/constraint/#ppsci.constraint","title":"<code>ppsci.constraint</code>","text":""},{"location":"zh/api/constraint/#ppsci.constraint.Constraint","title":"<code>Constraint</code>","text":"<p>Base class for constraint.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset.</p> required <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Dataloader config.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>name</code> <code>str</code> <p>Name of constraint.</p> required Source code in <code>ppsci/constraint/base.py</code> <pre><code>class Constraint:\n    \"\"\"Base class for constraint.\n\n    Args:\n        dataset (io.Dataset): Dataset.\n        dataloader_cfg (Dict[str, Any]): Dataloader config.\n        loss (loss.Loss): Loss functor.\n        name (str): Name of constraint.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: io.Dataset,\n        dataloader_cfg: Dict[str, Any],\n        loss: \"loss.Loss\",\n        name: str,\n    ):\n        self.data_loader = data.build_dataloader(dataset, dataloader_cfg)\n        self.data_iter = iter(self.data_loader)\n        self.loss = loss\n        self.name = name\n\n    def __str__(self):\n        return \", \".join(\n            [\n                self.__class__.__name__,\n                f\"name = {self.name}\",\n                f\"input_keys = {self.input_keys}\",\n                f\"output_keys = {self.output_keys}\",\n                f\"output_expr = {self.output_expr}\",\n                f\"label_dict = {self.label_dict}\",\n                f\"loss = {self.loss}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/constraint/#ppsci.constraint.BoundaryConstraint","title":"<code>BoundaryConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Class for boundary constraint.</p> <p>Parameters:</p> Name Type Description Default <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Function in dict for computing output. e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u will be multiplied by model output v and the result will be named \"u_mul_v\".</p> required <code>label_dict</code> <code>Dict[str, Union[float, Callable]]</code> <p>Function in dict for computing label, which will be a reference value to participate in the loss calculation.</p> required <code>geom</code> <code>Geometry</code> <p>Geometry where data sampled from.</p> required <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Dataloader config.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method for sampling data in geometry. Defaults to \"pseudo\".</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable]</code> <p>Criteria for refining specified boundaries. Defaults to None.</p> <code>None</code> <code>evenly</code> <code>bool</code> <p>Whether to use evenly distribution sampling. Defaults to False.</p> <code>False</code> <code>weight_dict</code> <code>Optional[Dict[str, Union[float, Callable]]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of constraint object. Defaults to \"BC\".</p> <code>'BC'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; rect = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; bc = ppsci.constraint.BoundaryConstraint(\n...     {\"u\": lambda out: out[\"u\"]},\n...     {\"u\": 0},\n...     rect,\n...     {\n...         \"dataset\": \"IterableNamedArrayDataset\",\n...         \"iters_per_epoch\": 1,\n...         \"batch_size\": 16,\n...     },\n...     ppsci.loss.MSELoss(\"mean\"),\n...     name=\"BC\",\n... )\n</code></pre> Source code in <code>ppsci/constraint/boundary_constraint.py</code> <pre><code>class BoundaryConstraint(base.Constraint):\n    \"\"\"Class for boundary constraint.\n\n    Args:\n        output_expr (Dict[str, Callable]): Function in dict for computing output.\n            e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u\n            will be multiplied by model output v and the result will be named \"u_mul_v\".\n        label_dict (Dict[str, Union[float, Callable]]): Function in dict for computing\n            label, which will be a reference value to participate in the loss calculation.\n        geom (geometry.Geometry): Geometry where data sampled from.\n        dataloader_cfg (Dict[str, Any]): Dataloader config.\n        loss (loss.Loss): Loss functor.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"], optional): Random method for sampling data in\n            geometry. Defaults to \"pseudo\".\n        criteria (Optional[Callable]): Criteria for refining specified boundaries.\n            Defaults to None.\n        evenly (bool, optional): Whether to use evenly distribution sampling.\n            Defaults to False.\n        weight_dict (Optional[Dict[str, Union[float, Callable]]]): Define the weight of each\n            constraint variable. Defaults to None.\n        name (str, optional): Name of constraint object. Defaults to \"BC\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; rect = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; bc = ppsci.constraint.BoundaryConstraint(\n        ...     {\"u\": lambda out: out[\"u\"]},\n        ...     {\"u\": 0},\n        ...     rect,\n        ...     {\n        ...         \"dataset\": \"IterableNamedArrayDataset\",\n        ...         \"iters_per_epoch\": 1,\n        ...         \"batch_size\": 16,\n        ...     },\n        ...     ppsci.loss.MSELoss(\"mean\"),\n        ...     name=\"BC\",\n        ... ) # doctest: +SKIP\n    \"\"\"\n\n    def __init__(\n        self,\n        output_expr: Dict[str, Callable],\n        label_dict: Dict[str, Union[float, Callable]],\n        geom: geometry.Geometry,\n        dataloader_cfg: Dict[str, Any],\n        loss: \"loss.Loss\",\n        random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n        criteria: Optional[Callable] = None,\n        evenly: bool = False,\n        weight_dict: Optional[Dict[str, Union[float, Callable]]] = None,\n        name: str = \"BC\",\n    ):\n        self.label_dict = label_dict\n        self.input_keys = geom.dim_keys\n        self.output_keys = tuple(label_dict.keys())\n        self.output_expr = {\n            k: v for k, v in output_expr.items() if k in self.output_keys\n        }\n\n        if isinstance(criteria, str):\n            criteria = eval(criteria)\n\n        # prepare input\n        input = geom.sample_boundary(\n            dataloader_cfg[\"batch_size\"] * dataloader_cfg[\"iters_per_epoch\"],\n            random,\n            criteria,\n            evenly,\n        )\n        if \"area\" in input:\n            input[\"area\"] *= dataloader_cfg[\"iters_per_epoch\"]\n\n        # prepare label\n        label = {}\n        for key, value in label_dict.items():\n            if isinstance(value, (int, float)):\n                label[key] = np.full_like(next(iter(input.values())), value)\n            elif isinstance(value, sympy.Basic):\n                func = sympy.lambdify(\n                    sympy.symbols(geom.dim_keys),\n                    value,\n                    [{\"amax\": lambda xy, axis: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                )\n                label[key] = func(\n                    **{k: v for k, v in input.items() if k in geom.dim_keys}\n                )\n            elif callable(value):\n                func = value\n                label[key] = func(input)\n                if isinstance(label[key], (int, float)):\n                    label[key] = np.full_like(next(iter(input.values())), label[key])\n            else:\n                raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        # prepare weight\n        weight = None\n        if weight_dict is not None:\n            weight = {key: np.ones_like(next(iter(label.values()))) for key in label}\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    weight[key] = np.full_like(next(iter(label.values())), value)\n                elif isinstance(value, sympy.Basic):\n                    func = sympy.lambdify(\n                        [sympy.Symbol(k) for k in geom.dim_keys],\n                        value,\n                        [{\"amax\": lambda xy, _: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                    )\n                    weight[key] = func(**{k: input[k] for k in geom.dim_keys})\n                elif callable(value):\n                    func = value\n                    weight[key] = func(input)\n                    if isinstance(weight[key], (int, float)):\n                        weight[key] = np.full_like(\n                            next(iter(input.values())), weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        # wrap input, label, weight into a dataset\n        if isinstance(dataloader_cfg[\"dataset\"], str):\n            dataloader_cfg[\"dataset\"] = {\"name\": dataloader_cfg[\"dataset\"]}\n        dataloader_cfg[\"dataset\"].update(\n            {\"input\": input, \"label\": label, \"weight\": weight}\n        )\n        _dataset = dataset.build_dataset(dataloader_cfg[\"dataset\"])\n\n        # construct dataloader with dataset and dataloader_cfg\n        super().__init__(_dataset, dataloader_cfg, loss, name)\n</code></pre>"},{"location":"zh/api/constraint/#ppsci.constraint.InitialConstraint","title":"<code>InitialConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Class for initial interior constraint.</p> <p>Parameters:</p> Name Type Description Default <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Function in dict for computing output. e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u will be multiplied by model output v and the result will be named \"u_mul_v\".</p> required <code>label_dict</code> <code>Dict[str, Union[float, Callable]]</code> <p>Function in dict for computing label, which will be a reference value to participate in the loss calculation.</p> required <code>geom</code> <code>TimeXGeometry</code> <p>Geometry where data sampled from.</p> required <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Dataloader config.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method for sampling data in geometry. Defaults to \"pseudo\".</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable]</code> <p>Criteria for refining specified boundaries. Defaults to None.</p> <code>None</code> <code>evenly</code> <code>bool</code> <p>Whether to use evenly distribution sampling. Defaults to False.</p> <code>False</code> <code>weight_dict</code> <code>Optional[Dict[str, Callable]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>compute_sdf_derivatives</code> <code>Optional[bool]</code> <p>Whether compute derivatives for SDF. Defaults to False.</p> <code>False</code> <code>name</code> <code>str</code> <p>Name of constraint object. Defaults to \"IC\".</p> <code>'IC'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; rect = ppsci.geometry.TimeXGeometry(\n...     ppsci.geometry.TimeDomain(0, 1),\n...     ppsci.geometry.Rectangle((0, 0), (1, 1)),\n... )\n&gt;&gt;&gt; ic = ppsci.constraint.InitialConstraint(\n...     {\"u\": lambda out: out[\"u\"]},\n...     {\"u\": 0},\n...     rect,\n...     {\n...         \"dataset\": \"IterableNamedArrayDataset\",\n...         \"iters_per_epoch\": 1,\n...         \"batch_size\": 16,\n...     },\n...     ppsci.loss.MSELoss(\"mean\"),\n...     name=\"IC\",\n... )\n</code></pre> Source code in <code>ppsci/constraint/initial_constraint.py</code> <pre><code>class InitialConstraint(base.Constraint):\n    \"\"\"Class for initial interior constraint.\n\n    Args:\n        output_expr (Dict[str, Callable]): Function in dict for computing output.\n            e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u\n            will be multiplied by model output v and the result will be named \"u_mul_v\".\n        label_dict (Dict[str, Union[float, Callable]]): Function in dict for computing\n            label, which will be a reference value to participate in the loss calculation.\n        geom (geometry.TimeXGeometry): Geometry where data sampled from.\n        dataloader_cfg (Dict[str, Any]): Dataloader config.\n        loss (loss.Loss): Loss functor.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"], optional): Random method for sampling data in\n            geometry. Defaults to \"pseudo\".\n        criteria (Optional[Callable]): Criteria for refining specified boundaries.\n            Defaults to None.\n        evenly (bool, optional): Whether to use evenly distribution sampling.\n            Defaults to False.\n        weight_dict (Optional[Dict[str, Callable]]): Define the weight of each\n            constraint variable. Defaults to None.\n        compute_sdf_derivatives (Optional[bool]): Whether compute derivatives for SDF.\n            Defaults to False.\n        name (str, optional): Name of constraint object. Defaults to \"IC\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; rect = ppsci.geometry.TimeXGeometry(\n        ...     ppsci.geometry.TimeDomain(0, 1),\n        ...     ppsci.geometry.Rectangle((0, 0), (1, 1)),\n        ... )\n        &gt;&gt;&gt; ic = ppsci.constraint.InitialConstraint(\n        ...     {\"u\": lambda out: out[\"u\"]},\n        ...     {\"u\": 0},\n        ...     rect,\n        ...     {\n        ...         \"dataset\": \"IterableNamedArrayDataset\",\n        ...         \"iters_per_epoch\": 1,\n        ...         \"batch_size\": 16,\n        ...     },\n        ...     ppsci.loss.MSELoss(\"mean\"),\n        ...     name=\"IC\",\n        ... ) # doctest: +SKIP\n    \"\"\"\n\n    def __init__(\n        self,\n        output_expr: Dict[str, Callable],\n        label_dict: Dict[str, Union[float, Callable]],\n        geom: geometry.TimeXGeometry,\n        dataloader_cfg: Dict[str, Any],\n        loss: \"loss.Loss\",\n        random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n        criteria: Optional[Callable] = None,\n        evenly: bool = False,\n        weight_dict: Optional[Dict[str, Callable]] = None,\n        compute_sdf_derivatives: bool = False,\n        name: str = \"IC\",\n    ):\n        self.label_dict = label_dict\n        self.input_keys = geom.dim_keys\n        self.output_keys = tuple(label_dict.keys())\n        self.output_expr = {\n            k: v for k, v in output_expr.items() if k in self.output_keys\n        }\n\n        if isinstance(criteria, str):\n            criteria = eval(criteria)\n\n        # prepare input\n        input = geom.sample_initial_interior(\n            dataloader_cfg[\"batch_size\"] * dataloader_cfg[\"iters_per_epoch\"],\n            random,\n            criteria,\n            evenly,\n            compute_sdf_derivatives,\n        )\n        if \"area\" in input:\n            input[\"area\"] *= dataloader_cfg[\"iters_per_epoch\"]\n\n        # prepare label\n        label = {}\n        for key, value in label_dict.items():\n            if isinstance(value, (int, float)):\n                label[key] = np.full_like(next(iter(input.values())), value)\n            elif isinstance(value, sympy.Basic):\n                func = sympy.lambdify(\n                    sympy.symbols(geom.dim_keys),\n                    value,\n                    [{\"amax\": lambda xy, _: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                )\n                label[key] = func(\n                    **{k: v for k, v in input.items() if k in geom.dim_keys}\n                )\n            elif callable(value):\n                func = value\n                label[key] = func(input)\n                if isinstance(label[key], (int, float)):\n                    label[key] = np.full_like(next(iter(input.values())), label[key])\n            else:\n                raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        # prepare weight\n        weight = None\n        if weight_dict is not None:\n            weight = {key: np.ones_like(next(iter(label.values()))) for key in label}\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    weight[key] = np.full_like(next(iter(label.values())), value)\n                elif isinstance(value, sympy.Basic):\n                    func = sympy.lambdify(\n                        sympy.symbols(geom.dim_keys),\n                        value,\n                        [{\"amax\": lambda xy, _: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                    )\n                    weight[key] = func(\n                        **{k: v for k, v in input.items() if k in geom.dim_keys}\n                    )\n                elif callable(value):\n                    func = value\n                    weight[key] = func(input)\n                    if isinstance(weight[key], (int, float)):\n                        weight[key] = np.full_like(\n                            next(iter(input.values())), weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        # wrap input, label, weight into a dataset\n        if isinstance(dataloader_cfg[\"dataset\"], str):\n            dataloader_cfg[\"dataset\"] = {\"name\": dataloader_cfg[\"dataset\"]}\n        dataloader_cfg[\"dataset\"].update(\n            {\"input\": input, \"label\": label, \"weight\": weight}\n        )\n        _dataset = dataset.build_dataset(dataloader_cfg[\"dataset\"])\n\n        # construct dataloader with dataset and dataloader_cfg\n        super().__init__(_dataset, dataloader_cfg, loss, name)\n</code></pre>"},{"location":"zh/api/constraint/#ppsci.constraint.IntegralConstraint","title":"<code>IntegralConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Class for integral constraint.</p> <p>Parameters:</p> Name Type Description Default <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Function in dict for computing output. e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u will be multiplied by model output v and the result will be named \"u_mul_v\".</p> required <code>label_dict</code> <code>Dict[str, Union[float, Callable]]</code> <p>Function in dict for computing label, which will be a reference value to participate in the loss calculation.</p> required <code>geom</code> <code>Geometry</code> <p>Geometry where data sampled from.</p> required <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Dataloader config.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method for sampling data in geometry. Defaults to \"pseudo\".</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable]</code> <p>Criteria for refining specified boundaries. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, Callable]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of constraint object. Defaults to \"IgC\".</p> <code>'IgC'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; rect = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; igc = ppsci.constraint.IntegralConstraint(\n...     {\"u\": lambda out: out[\"u\"]},\n...     {\"u\": 0},\n...     rect,\n...     {\n...         \"dataset\": \"IterableNamedArrayDataset\",\n...         \"iters_per_epoch\": 1,\n...         \"batch_size\": 16,\n...         \"integral_batch_size\": 8,\n...     },\n...     ppsci.loss.MSELoss(\"mean\"),\n...     name=\"IgC\",\n... )\n</code></pre> Source code in <code>ppsci/constraint/integral_constraint.py</code> <pre><code>class IntegralConstraint(base.Constraint):\n    \"\"\"Class for integral constraint.\n\n    Args:\n        output_expr (Dict[str, Callable]): Function in dict for computing output.\n            e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u\n            will be multiplied by model output v and the result will be named \"u_mul_v\".\n        label_dict (Dict[str, Union[float, Callable]]): Function in dict for computing\n            label, which will be a reference value to participate in the loss calculation.\n        geom (geometry.Geometry): Geometry where data sampled from.\n        dataloader_cfg (Dict[str, Any]): Dataloader config.\n        loss (loss.Loss): Loss functor.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"], optional): Random method for sampling data in\n            geometry. Defaults to \"pseudo\".\n        criteria (Optional[Callable]): Criteria for refining specified boundaries.\n            Defaults to None.\n        weight_dict (Optional[Dict[str, Callable]]): Define the weight of each\n            constraint variable. Defaults to None.\n        name (str, optional): Name of constraint object. Defaults to \"IgC\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; rect = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; igc = ppsci.constraint.IntegralConstraint(\n        ...     {\"u\": lambda out: out[\"u\"]},\n        ...     {\"u\": 0},\n        ...     rect,\n        ...     {\n        ...         \"dataset\": \"IterableNamedArrayDataset\",\n        ...         \"iters_per_epoch\": 1,\n        ...         \"batch_size\": 16,\n        ...         \"integral_batch_size\": 8,\n        ...     },\n        ...     ppsci.loss.MSELoss(\"mean\"),\n        ...     name=\"IgC\",\n        ... ) # doctest: +SKIP\n    \"\"\"\n\n    def __init__(\n        self,\n        output_expr: Dict[str, Callable],\n        label_dict: Dict[str, Union[float, Callable]],\n        geom: geometry.Geometry,\n        dataloader_cfg: Dict[str, Any],\n        loss: \"loss.Loss\",\n        random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n        criteria: Optional[Callable] = None,\n        weight_dict: Optional[Dict[str, Callable]] = None,\n        name: str = \"IgC\",\n    ):\n        self.label_dict = label_dict\n        self.input_keys = geom.dim_keys\n        self.output_keys = tuple(label_dict.keys())\n        self.output_expr = {\n            k: v for k, v in output_expr.items() if k in self.output_keys\n        }\n\n        if isinstance(criteria, str):\n            criteria = eval(criteria)\n\n        # prepare input\n        input_list: List[Dict[str, np.ndarray]] = []\n        for _ in range(\n            dataloader_cfg[\"batch_size\"] * dataloader_cfg[\"iters_per_epoch\"]\n        ):\n            input = geom.sample_boundary(\n                dataloader_cfg[\"integral_batch_size\"], random, criteria\n            )\n            input_list.append(input)\n        input = misc.stack_dict_list(input_list)\n        # shape of each input is [batch_size, integral_batch_size, ndim]\n\n        # prepare label\n        # shape of each label is [batch_size, ndim]\n        label = {}\n        for key, value in label_dict.items():\n            if isinstance(value, (int, float)):\n                label[key] = np.full(\n                    (next(iter(input.values())).shape[0], 1),\n                    value,\n                    paddle.get_default_dtype(),\n                )\n            elif isinstance(value, sympy.Basic):\n                func = sympy.lambdify(\n                    sympy.symbols(geom.dim_keys),\n                    value,\n                    [{\"amax\": lambda xy, _: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                )\n                label[key] = func(\n                    **{k: v for k, v in input.items() if k in geom.dim_keys}\n                )\n            elif callable(value):\n                func = value\n                label[key] = func(input)\n                if isinstance(label[key], (int, float)):\n                    label[key] = np.full(\n                        (next(iter(input.values())).shape[0], 1),\n                        label[key],\n                        paddle.get_default_dtype(),\n                    )\n            else:\n                raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        # prepare weight\n        # shape of each weight is [batch_size, ndim]\n        weight = None\n        if weight_dict is not None:\n            weight = {key: np.ones_like(next(iter(label.values()))) for key in label}\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    weight[key] = np.full_like(next(iter(label.values())), value)\n                elif isinstance(value, sympy.Basic):\n                    func = sympy.lambdify(\n                        sympy.symbols(geom.dim_keys),\n                        value,\n                        [{\"amax\": lambda xy, _: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                    )\n                    weight[key] = func(\n                        **{k: v for k, v in input.items() if k in geom.dim_keys}\n                    )\n                elif callable(value):\n                    func = value\n                    weight[key] = func(input)\n                    if isinstance(weight[key], (int, float)):\n                        weight[key] = np.full_like(\n                            next(iter(input.values())), weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        # wrap input, label, weight into a dataset\n        if isinstance(dataloader_cfg[\"dataset\"], str):\n            dataloader_cfg[\"dataset\"] = {\"name\": dataloader_cfg[\"dataset\"]}\n        dataloader_cfg[\"dataset\"].update(\n            {\"input\": input, \"label\": label, \"weight\": weight}\n        )\n        _dataset = dataset.build_dataset(dataloader_cfg[\"dataset\"])\n\n        # construct dataloader with dataset and dataloader_cfg\n        super().__init__(_dataset, dataloader_cfg, loss, name)\n</code></pre>"},{"location":"zh/api/constraint/#ppsci.constraint.InteriorConstraint","title":"<code>InteriorConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Class for interior constraint.</p> <p>Parameters:</p> Name Type Description Default <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Function in dict for computing output. e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u will be multiplied by model output v and the result will be named \"u_mul_v\".</p> required <code>label_dict</code> <code>Dict[str, Union[float, Callable]]</code> <p>Function in dict for computing label, which will be a reference value to participate in the loss calculation.</p> required <code>geom</code> <code>Geometry</code> <p>Geometry where data sampled from.</p> required <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Dataloader config.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method for sampling data in geometry. Defaults to \"pseudo\".</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable]</code> <p>Criteria for refining specified boundaries. Defaults to None.</p> <code>None</code> <code>evenly</code> <code>bool</code> <p>Whether to use evenly distribution sampling. Defaults to False.</p> <code>False</code> <code>weight_dict</code> <code>Optional[Dict[str, Union[Callable, float]]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>compute_sdf_derivatives</code> <code>Optional[bool]</code> <p>Whether compute derivatives for SDF. Defaults to False.</p> <code>False</code> <code>name</code> <code>str</code> <p>Name of constraint object. Defaults to \"EQ\".</p> <code>'EQ'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; rect = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; pde_constraint = ppsci.constraint.InteriorConstraint(\n...     {\"u\": lambda out: out[\"u\"]},\n...     {\"u\": 0},\n...     rect,\n...     {\n...         \"dataset\": \"IterableNamedArrayDataset\",\n...         \"iters_per_epoch\": 1,\n...         \"batch_size\": 16,\n...     },\n...     ppsci.loss.MSELoss(\"mean\"),\n...     name=\"EQ\",\n... )\n</code></pre> Source code in <code>ppsci/constraint/interior_constraint.py</code> <pre><code>class InteriorConstraint(base.Constraint):\n    \"\"\"Class for interior constraint.\n\n    Args:\n        output_expr (Dict[str, Callable]): Function in dict for computing output.\n            e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u\n            will be multiplied by model output v and the result will be named \"u_mul_v\".\n        label_dict (Dict[str, Union[float, Callable]]): Function in dict for computing\n            label, which will be a reference value to participate in the loss calculation.\n        geom (geometry.Geometry): Geometry where data sampled from.\n        dataloader_cfg (Dict[str, Any]): Dataloader config.\n        loss (loss.Loss): Loss functor.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"], optional): Random method for sampling data in\n            geometry. Defaults to \"pseudo\".\n        criteria (Optional[Callable]): Criteria for refining specified boundaries.\n            Defaults to None.\n        evenly (bool, optional): Whether to use evenly distribution sampling.\n            Defaults to False.\n        weight_dict (Optional[Dict[str, Union[Callable, float]]]): Define the\n            weight of each constraint variable. Defaults to None.\n        compute_sdf_derivatives (Optional[bool]): Whether compute derivatives for SDF.\n            Defaults to False.\n        name (str, optional): Name of constraint object. Defaults to \"EQ\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; rect = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; pde_constraint = ppsci.constraint.InteriorConstraint(\n        ...     {\"u\": lambda out: out[\"u\"]},\n        ...     {\"u\": 0},\n        ...     rect,\n        ...     {\n        ...         \"dataset\": \"IterableNamedArrayDataset\",\n        ...         \"iters_per_epoch\": 1,\n        ...         \"batch_size\": 16,\n        ...     },\n        ...     ppsci.loss.MSELoss(\"mean\"),\n        ...     name=\"EQ\",\n        ... ) # doctest: +SKIP\n    \"\"\"\n\n    def __init__(\n        self,\n        output_expr: Dict[str, Callable],\n        label_dict: Dict[str, Union[float, Callable]],\n        geom: geometry.Geometry,\n        dataloader_cfg: Dict[str, Any],\n        loss: \"loss.Loss\",\n        random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n        criteria: Optional[Callable] = None,\n        evenly: bool = False,\n        weight_dict: Optional[Dict[str, Union[Callable, float]]] = None,\n        compute_sdf_derivatives: bool = False,\n        name: str = \"EQ\",\n    ):\n        self.label_dict = label_dict\n        self.input_keys = geom.dim_keys\n        self.output_keys = tuple(label_dict.keys())\n        self.output_expr = {\n            k: v for k, v in output_expr.items() if k in self.output_keys\n        }\n\n        if isinstance(criteria, str):\n            criteria = eval(criteria)\n\n        # prepare input\n        input = geom.sample_interior(\n            dataloader_cfg[\"batch_size\"] * dataloader_cfg[\"iters_per_epoch\"],\n            random,\n            criteria,\n            evenly,\n            compute_sdf_derivatives,\n        )\n        if \"area\" in input:\n            input[\"area\"] *= dataloader_cfg[\"iters_per_epoch\"]\n\n        # prepare label\n        label = {}\n        for key, value in label_dict.items():\n            if isinstance(value, (int, float)):\n                label[key] = np.full_like(next(iter(input.values())), value)\n            elif isinstance(value, sympy.Basic):\n                func = sympy.lambdify(\n                    sympy.symbols(geom.dim_keys),\n                    value,\n                    [{\"amax\": lambda xy, _: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                )\n                label[key] = func(\n                    **{k: v for k, v in input.items() if k in geom.dim_keys}\n                )\n            elif callable(value):\n                func = value\n                label[key] = func(input)\n                if isinstance(label[key], (int, float)):\n                    label[key] = np.full_like(next(iter(input.values())), label[key])\n            else:\n                raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        # prepare weight\n        weight = None\n        if weight_dict is not None:\n            weight = {key: np.ones_like(next(iter(label.values()))) for key in label}\n            for key, value in weight_dict.items():\n                if isinstance(value, str):\n                    if value == \"sdf\":\n                        weight[key] = input[\"sdf\"]\n                    else:\n                        raise NotImplementedError(f\"string {value} is invalid yet.\")\n                elif isinstance(value, (int, float)):\n                    weight[key] = np.full_like(next(iter(label.values())), float(value))\n                elif isinstance(value, sympy.Basic):\n                    func = sympy.lambdify(\n                        sympy.symbols(geom.dim_keys),\n                        value,\n                        [{\"amax\": lambda xy, _: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                    )\n                    weight[key] = func(\n                        **{k: v for k, v in input.items() if k in geom.dim_keys}\n                    )\n                elif callable(value):\n                    func = value\n                    weight[key] = func(input)\n                    if isinstance(weight[key], (int, float)):\n                        weight[key] = np.full_like(\n                            next(iter(input.values())), weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        # wrap input, label, weight into a dataset\n        if isinstance(dataloader_cfg[\"dataset\"], str):\n            dataloader_cfg[\"dataset\"] = {\"name\": dataloader_cfg[\"dataset\"]}\n        dataloader_cfg[\"dataset\"].update(\n            {\"input\": input, \"label\": label, \"weight\": weight}\n        )\n        _dataset = dataset.build_dataset(dataloader_cfg[\"dataset\"])\n\n        # construct dataloader with dataset and dataloader_cfg\n        super().__init__(_dataset, dataloader_cfg, loss, name)\n</code></pre>"},{"location":"zh/api/constraint/#ppsci.constraint.PeriodicConstraint","title":"<code>PeriodicConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Class for periodic constraint.</p> <p>Parameters:</p> Name Type Description Default <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Function in dict for computing output. e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u will be multiplied by model output v and the result will be named \"u_mul_v\".</p> required <code>label_dict</code> <code>Dict[str, Union[float, Callable]]</code> <p>Function in dict for computing label, which will be a reference value to participate in the loss calculation.</p> required <code>geom</code> <code>Geometry</code> <p>Geometry where data sampled from.</p> required <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Dataloader config.</p> required <code>periodic_key</code> <code>str</code> <p>Name of dimension which periodic constraint applied to.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method for sampling data in geometry. Defaults to \"pseudo\".</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable]</code> <p>Criteria for refining specified boundaries. Defaults to None.</p> <code>None</code> <code>evenly</code> <code>bool</code> <p>Whether to use evenly distribution sampling. Defaults to False.</p> <code>False</code> <code>weight_dict</code> <code>Optional[Dict[str, Callable]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of constraint object. Defaults to \"PeriodicBC\".</p> <code>'PeriodicBC'</code> Source code in <code>ppsci/constraint/periodic_constraint.py</code> <pre><code>class PeriodicConstraint(base.Constraint):\n    \"\"\"Class for periodic constraint.\n\n    Args:\n        output_expr (Dict[str, Callable]): Function in dict for computing output.\n            e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u\n            will be multiplied by model output v and the result will be named \"u_mul_v\".\n        label_dict (Dict[str, Union[float, Callable]]): Function in dict for computing\n            label, which will be a reference value to participate in the loss calculation.\n        geom (geometry.Geometry): Geometry where data sampled from.\n        dataloader_cfg (Dict[str, Any]): Dataloader config.\n        periodic_key (str): Name of dimension which periodic constraint applied to.\n        loss (loss.Loss): Loss functor.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"], optional): Random method for sampling data in\n            geometry. Defaults to \"pseudo\".\n        criteria (Optional[Callable]): Criteria for refining specified boundaries.\n            Defaults to None.\n        evenly (bool, optional):  Whether to use evenly distribution sampling.\n            Defaults to False.\n        weight_dict (Optional[Dict[str, Callable]]): Define the weight of each\n            constraint variable. Defaults to None.\n        name (str, optional): Name of constraint object. Defaults to \"PeriodicBC\".\n    \"\"\"\n\n    def __init__(\n        self,\n        output_expr: Dict[str, Callable],\n        label_dict: Dict[str, Union[float, Callable]],\n        geom: geometry.Geometry,\n        periodic_key: str,\n        dataloader_cfg: Dict[str, Any],\n        loss: \"loss.Loss\",\n        random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n        criteria: Optional[Callable] = None,\n        evenly: bool = False,\n        weight_dict: Optional[Dict[str, Callable]] = None,\n        name: str = \"PeriodicBC\",\n    ):\n        self.input_keys = geom.dim_keys\n        self.output_keys = tuple(output_expr.keys())\n        self.output_expr = {\n            k: v for k, v in output_expr.items() if k in self.output_keys\n        }\n\n        if isinstance(criteria, str):\n            criteria = eval(criteria)\n\n        if dataloader_cfg[\"batch_size\"] % 2 &gt; 0:\n            raise ValueError(\n                f\"batch_size({dataloader_cfg['sampler']['batch_size']}) \"\n                \"should be positive and even when using PeriodicConstraint\"\n            )\n        if dataloader_cfg.get(\"shuffle\", False):\n            raise ValueError(\n                f\"shuffle({dataloader_cfg['sampler']['batch_size']}) \"\n                \"should be False when using PeriodicConstraint\"\n            )\n\n        # prepare input\n        _bs_half = dataloader_cfg[\"batch_size\"] // 2\n        input = geom.sample_boundary(\n            _bs_half * dataloader_cfg[\"iters_per_epoch\"],\n            random,\n            criteria,\n            evenly,\n        )\n        if \"area\" in input:\n            input[\"area\"] *= dataloader_cfg[\"iters_per_epoch\"]\n\n        input_periodic = geom.periodic_point(\n            input,\n            geom.geometry.dim_keys.index(periodic_key)\n            if isinstance(geom, geometry.TimeXGeometry)\n            else geom.dim_keys.index(periodic_key),\n        )\n        # concatenate original data next to periodic data, i.e.\n        # [orignal1, periodic1, orignal2, periodic2, ..., orignalN, periodicN]\n        mixed_input = {}\n        for key in input:\n            mixed_input[key] = []\n            for iter_id in range(dataloader_cfg[\"iters_per_epoch\"]):\n                mixed_input[key].append(\n                    input[key][iter_id * _bs_half : (iter_id + 1) * _bs_half]\n                )\n                mixed_input[key].append(\n                    input_periodic[key][iter_id * _bs_half : (iter_id + 1) * _bs_half]\n                )\n            mixed_input[key] = np.vstack(mixed_input[key])\n\n        # prepare label, keep label the same shape as input_periodic\n        label = {}\n        for key, value in label_dict.items():\n            # set all label's to zero for dummy data.\n            label[key] = np.full(\n                (next(iter(mixed_input.values())).shape[0], 1),\n                0,\n                paddle.get_default_dtype(),\n            )\n\n        # # prepare weight, keep weight the same shape as input_periodic\n        weight = None\n        if weight_dict is not None:\n            weight = {key: np.ones_like(next(iter(label.values()))) for key in label}\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    weight[key] = np.full_like(next(iter(label.values())), value)\n                elif isinstance(value, sympy.Basic):\n                    func = sympy.lambdify(\n                        [sympy.Symbol(k) for k in geom.dim_keys],\n                        value,\n                        [{\"amax\": lambda xy, _: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                    )\n                    weight[key] = func(**{k: mixed_input[k] for k in geom.dim_keys})\n                elif callable(value):\n                    func = value\n                    weight[key] = func(mixed_input)\n                    if isinstance(weight[key], (int, float)):\n                        weight[key] = np.full_like(\n                            next(iter(mixed_input.values())), weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        # wrap input, label, weight into a dataset\n        if isinstance(dataloader_cfg[\"dataset\"], str):\n            dataloader_cfg[\"dataset\"] = {\"name\": dataloader_cfg[\"dataset\"]}\n        dataloader_cfg[\"dataset\"].update(\n            {\"input\": mixed_input, \"label\": label, \"weight\": weight}\n        )\n        _dataset = dataset.build_dataset(dataloader_cfg[\"dataset\"])\n\n        # construct dataloader with dataset and dataloader_cfg\n        super().__init__(_dataset, dataloader_cfg, loss, name)\n</code></pre>"},{"location":"zh/api/constraint/#ppsci.constraint.SupervisedConstraint","title":"<code>SupervisedConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Class for supervised constraint.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Dataloader config.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>output_expr</code> <code>Optional[Dict[str, Callable]]</code> <p>List of label expression. Defaults to None.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of constraint object. Defaults to \"Sup\".</p> <code>'Sup'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; bc_sup = ppsci.constraint.SupervisedConstraint(\n...     {\n...         \"dataset\": {\n...             \"name\": \"IterableCSVDataset\",\n...             \"file_path\": \"/path/to/file.csv\",\n...             \"input_keys\": (\"x\", \"y\"),\n...             \"label_keys\": (\"u\", \"v\"),\n...         },\n...     },\n...     ppsci.loss.MSELoss(\"mean\"),\n...     name=\"bc_sup\",\n... )\n</code></pre> Source code in <code>ppsci/constraint/supervised_constraint.py</code> <pre><code>class SupervisedConstraint(base.Constraint):\n    \"\"\"Class for supervised constraint.\n\n    Args:\n        dataloader_cfg (Dict[str, Any]): Dataloader config.\n        loss (loss.Loss): Loss functor.\n        output_expr (Optional[Dict[str, Callable]]): List of label expression.\n            Defaults to None.\n        name (str, optional): Name of constraint object. Defaults to \"Sup\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; bc_sup = ppsci.constraint.SupervisedConstraint(\n        ...     {\n        ...         \"dataset\": {\n        ...             \"name\": \"IterableCSVDataset\",\n        ...             \"file_path\": \"/path/to/file.csv\",\n        ...             \"input_keys\": (\"x\", \"y\"),\n        ...             \"label_keys\": (\"u\", \"v\"),\n        ...         },\n        ...     },\n        ...     ppsci.loss.MSELoss(\"mean\"),\n        ...     name=\"bc_sup\",\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(\n        self,\n        dataloader_cfg: Dict[str, Any],\n        loss: \"loss.Loss\",\n        output_expr: Optional[Dict[str, Callable]] = None,\n        name: str = \"Sup\",\n    ):\n        # build dataset\n        _dataset = dataset.build_dataset(dataloader_cfg[\"dataset\"])\n\n        self.input_keys = _dataset.input_keys\n        self.output_keys = (\n            tuple(output_expr.keys())\n            if output_expr is not None\n            else _dataset.label_keys\n        )\n\n        self.output_expr = output_expr\n        if self.output_expr is None:\n            self.output_expr = {\n                key: (lambda out, k=key: out[k]) for key in self.output_keys\n            }\n\n        # construct dataloader with dataset and dataloader_cfg\n        super().__init__(_dataset, dataloader_cfg, loss, name)\n\n    def __str__(self):\n        return \", \".join(\n            [\n                self.__class__.__name__,\n                f\"name = {self.name}\",\n                f\"input_keys = {self.input_keys}\",\n                f\"output_keys = {self.output_keys}\",\n                f\"output_expr = {self.output_expr}\",\n                f\"loss = {self.loss}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/equation/","title":"ppsci.equation","text":""},{"location":"zh/api/equation/#equation","title":"Equation(\u65b9\u7a0b) \u6a21\u5757","text":""},{"location":"zh/api/equation/#ppsci.equation","title":"<code>ppsci.equation</code>","text":""},{"location":"zh/api/equation/#ppsci.equation.PDE","title":"<code>PDE</code>","text":"<p>Base class for Partial Differential Equation.</p> Source code in <code>ppsci/equation/pde/base.py</code> <pre><code>class PDE:\n    \"\"\"Base class for Partial Differential Equation.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.equations: Dict[str, Union[Callable, sp.Basic]] = {}\n        # for PDE which has learnable parameter(s)\n        self.learnable_parameters = nn.ParameterList()\n\n        self.detach_keys: Optional[Tuple[str, ...]] = None\n\n    @staticmethod\n    def create_symbols(\n        symbol_str: str,\n    ) -&gt; Union[sp.Symbol, Tuple[sp.Symbol, ...]]:\n        \"\"\"Create symbolic variables.\n\n        Args:\n            symbol_str (str): String contains symbols, such as \"x\", \"x y z\".\n\n        Returns:\n            Union[sympy.Symbol, Tuple[sympy.Symbol, ...]]: Created symbol(s).\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; pde = ppsci.equation.PDE()\n            &gt;&gt;&gt; symbol_x = pde.create_symbols('x')\n            &gt;&gt;&gt; symbols_xyz = pde.create_symbols('x y z')\n            &gt;&gt;&gt; print(symbol_x)\n            x\n            &gt;&gt;&gt; print(symbols_xyz)\n            (x, y, z)\n        \"\"\"\n        return sp.symbols(symbol_str)\n\n    def create_function(self, name: str, invars: Tuple[sp.Symbol, ...]) -&gt; sp.Function:\n        \"\"\"Create named function depending on given invars.\n\n        Args:\n            name (str): Function name. such as \"u\", \"v\", and \"f\".\n            invars (Tuple[sympy.Symbol, ...]): List of independent variable of function.\n\n        Returns:\n            sympy.Function: Named sympy function.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; pde = ppsci.equation.PDE()\n            &gt;&gt;&gt; x, y, z = pde.create_symbols('x y z')\n            &gt;&gt;&gt; u = pde.create_function('u', (x, y))\n            &gt;&gt;&gt; f = pde.create_function('f', (x, y, z))\n            &gt;&gt;&gt; print(u)\n            u(x, y)\n            &gt;&gt;&gt; print(f)\n            f(x, y, z)\n        \"\"\"\n        expr = sp.Function(name)(*invars)\n\n        return expr\n\n    def _apply_detach(self):\n        \"\"\"\n        Wrap detached sub_expr into detach(sub_expr) to prevent gradient back-propagation, only for those items speicified in self.detach_keys.\n\n        NOTE: This function is expected to be called after self.equations is ready in PDE.__init__.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; ns = ppsci.equation.NavierStokes(1.0, 1.0, 2, False)\n            &gt;&gt;&gt; print(ns)\n            NavierStokes\n                continuity: Derivative(u(x, y), x) + Derivative(v(x, y), y)\n                momentum_x: u(x, y)*Derivative(u(x, y), x) + v(x, y)*Derivative(u(x, y), y) + 1.0*Derivative(p(x, y), x) - 1.0*Derivative(u(x, y), (x, 2)) - 1.0*Derivative(u(x, y), (y, 2))\n                momentum_y: u(x, y)*Derivative(v(x, y), x) + v(x, y)*Derivative(v(x, y), y) + 1.0*Derivative(p(x, y), y) - 1.0*Derivative(v(x, y), (x, 2)) - 1.0*Derivative(v(x, y), (y, 2))\n            &gt;&gt;&gt; detach_keys = (\"u\", \"v__y\")\n            &gt;&gt;&gt; ns = ppsci.equation.NavierStokes(1.0, 1.0, 2, False, detach_keys=detach_keys)\n            &gt;&gt;&gt; print(ns)\n            NavierStokes\n                continuity: detach(Derivative(v(x, y), y)) + Derivative(u(x, y), x)\n                momentum_x: detach(u(x, y))*Derivative(u(x, y), x) + v(x, y)*Derivative(u(x, y), y) + 1.0*Derivative(p(x, y), x) - 1.0*Derivative(u(x, y), (x, 2)) - 1.0*Derivative(u(x, y), (y, 2))\n                momentum_y: detach(u(x, y))*Derivative(v(x, y), x) + detach(Derivative(v(x, y), y))*v(x, y) + 1.0*Derivative(p(x, y), y) - 1.0*Derivative(v(x, y), (x, 2)) - 1.0*Derivative(v(x, y), (y, 2))\n        \"\"\"\n        if self.detach_keys is None:\n            return\n\n        from copy import deepcopy\n\n        from sympy.core.traversal import postorder_traversal\n\n        from ppsci.utils.symbolic import _cvt_to_key\n\n        for name, expr in self.equations.items():\n            if not isinstance(expr, sp.Basic):\n                continue\n            # only process sympy expression\n            expr_ = deepcopy(expr)\n            for item in postorder_traversal(expr):\n                if _cvt_to_key(item) in self.detach_keys:\n                    # inplace all related sub_expr into detach(sub_expr)\n                    expr_ = expr_.replace(item, sp.Function(DETACH_FUNC_NAME)(item))\n\n                    # remove all detach wrapper for more-than-once wrapped items to prevent duplicated wrapping\n                    expr_ = expr_.replace(\n                        sp.Function(DETACH_FUNC_NAME)(\n                            sp.Function(DETACH_FUNC_NAME)(item)\n                        ),\n                        sp.Function(DETACH_FUNC_NAME)(item),\n                    )\n\n                    # remove unccessary detach wrapping for the first arg of Derivative\n                    for item_ in list(postorder_traversal(expr_)):\n                        if isinstance(item_, sp.Derivative):\n                            if item_.args[0].name == DETACH_FUNC_NAME:\n                                expr_ = expr_.replace(\n                                    item_,\n                                    sp.Derivative(\n                                        item_.args[0].args[0], *item_.args[1:]\n                                    ),\n                                )\n\n            self.equations[name] = expr_\n\n    def add_equation(self, name: str, equation: Callable):\n        \"\"\"Add an equation.\n\n        Args:\n            name (str): Name of equation\n            equation (Callable): Computation function for equation.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; import sympy\n            &gt;&gt;&gt; pde = ppsci.equation.PDE()\n            &gt;&gt;&gt; x, y = pde.create_symbols('x y')\n            &gt;&gt;&gt; u = x**2 + y**2\n            &gt;&gt;&gt; equation = sympy.diff(u, x) + sympy.diff(u, y)\n            &gt;&gt;&gt; pde.add_equation('linear_pde', equation)\n            &gt;&gt;&gt; print(pde)\n            PDE\n                linear_pde: 2*x + 2*y\n        \"\"\"\n        self.equations.update({name: equation})\n\n    def parameters(self) -&gt; List[paddle.Tensor]:\n        \"\"\"Return learnable parameters contained in PDE.\n\n        Returns:\n            List[Tensor]: A list of learnable parameters.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; pde = ppsci.equation.Vibration(2, -4, 0)\n            &gt;&gt;&gt; print(pde.parameters())\n            [Parameter containing:\n            Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n                   -4.), Parameter containing:\n            Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n                   0.)]\n        \"\"\"\n        return self.learnable_parameters.parameters()\n\n    def state_dict(self) -&gt; Dict[str, paddle.Tensor]:\n        \"\"\"Return named learnable parameters in dict.\n\n        Returns:\n            Dict[str, Tensor]: A dict of states(str) and learnable parameters(Tensor).\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; pde = ppsci.equation.Vibration(2, -4, 0)\n            &gt;&gt;&gt; print(pde.state_dict())\n            OrderedDict([('0', Parameter containing:\n            Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,\n                   -4.)), ('1', Parameter containing:\n            Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,\n                   0.))])\n        \"\"\"\n        return self.learnable_parameters.state_dict()\n\n    def set_state_dict(\n        self, state_dict: Dict[str, paddle.Tensor]\n    ) -&gt; Tuple[List[str], List[str]]:\n        \"\"\"Set state dict from dict.\n\n        Args:\n            state_dict (Dict[str, paddle.Tensor]): The state dict to be set.\n\n        Returns:\n            Tuple[List[str], List[str]]: List of missing_keys and unexpected_keys.\n                Expected to be two empty tuples mostly.\n\n        Examples:\n            &gt;&gt;&gt; import paddle\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; paddle.set_default_dtype(\"float64\")\n            &gt;&gt;&gt; pde = ppsci.equation.Vibration(2, -4, 0)\n            &gt;&gt;&gt; state = pde.state_dict()\n            &gt;&gt;&gt; state['0'] = paddle.to_tensor(-3.1)\n            &gt;&gt;&gt; pde.set_state_dict(state)\n            ([], [])\n            &gt;&gt;&gt; print(state)\n            OrderedDict([('0', Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=True,\n                   -3.10000000)), ('1', Parameter containing:\n            Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,\n                   0.))])\n        \"\"\"\n        return self.learnable_parameters.set_state_dict(state_dict)\n\n    def __str__(self):\n        return \"\\n\".join(\n            [self.__class__.__name__]\n            + [f\"    {name}: {eq}\" for name, eq in self.equations.items()]\n        )\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.PDE.add_equation","title":"<code>add_equation(name, equation)</code>","text":"<p>Add an equation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of equation</p> required <code>equation</code> <code>Callable</code> <p>Computation function for equation.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import sympy\n&gt;&gt;&gt; pde = ppsci.equation.PDE()\n&gt;&gt;&gt; x, y = pde.create_symbols('x y')\n&gt;&gt;&gt; u = x**2 + y**2\n&gt;&gt;&gt; equation = sympy.diff(u, x) + sympy.diff(u, y)\n&gt;&gt;&gt; pde.add_equation('linear_pde', equation)\n&gt;&gt;&gt; print(pde)\nPDE\n    linear_pde: 2*x + 2*y\n</code></pre> Source code in <code>ppsci/equation/pde/base.py</code> <pre><code>def add_equation(self, name: str, equation: Callable):\n    \"\"\"Add an equation.\n\n    Args:\n        name (str): Name of equation\n        equation (Callable): Computation function for equation.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import sympy\n        &gt;&gt;&gt; pde = ppsci.equation.PDE()\n        &gt;&gt;&gt; x, y = pde.create_symbols('x y')\n        &gt;&gt;&gt; u = x**2 + y**2\n        &gt;&gt;&gt; equation = sympy.diff(u, x) + sympy.diff(u, y)\n        &gt;&gt;&gt; pde.add_equation('linear_pde', equation)\n        &gt;&gt;&gt; print(pde)\n        PDE\n            linear_pde: 2*x + 2*y\n    \"\"\"\n    self.equations.update({name: equation})\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.PDE.create_function","title":"<code>create_function(name, invars)</code>","text":"<p>Create named function depending on given invars.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Function name. such as \"u\", \"v\", and \"f\".</p> required <code>invars</code> <code>Tuple[Symbol, ...]</code> <p>List of independent variable of function.</p> required <p>Returns:</p> Type Description <code>Function</code> <p>sympy.Function: Named sympy function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.PDE()\n&gt;&gt;&gt; x, y, z = pde.create_symbols('x y z')\n&gt;&gt;&gt; u = pde.create_function('u', (x, y))\n&gt;&gt;&gt; f = pde.create_function('f', (x, y, z))\n&gt;&gt;&gt; print(u)\nu(x, y)\n&gt;&gt;&gt; print(f)\nf(x, y, z)\n</code></pre> Source code in <code>ppsci/equation/pde/base.py</code> <pre><code>def create_function(self, name: str, invars: Tuple[sp.Symbol, ...]) -&gt; sp.Function:\n    \"\"\"Create named function depending on given invars.\n\n    Args:\n        name (str): Function name. such as \"u\", \"v\", and \"f\".\n        invars (Tuple[sympy.Symbol, ...]): List of independent variable of function.\n\n    Returns:\n        sympy.Function: Named sympy function.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.PDE()\n        &gt;&gt;&gt; x, y, z = pde.create_symbols('x y z')\n        &gt;&gt;&gt; u = pde.create_function('u', (x, y))\n        &gt;&gt;&gt; f = pde.create_function('f', (x, y, z))\n        &gt;&gt;&gt; print(u)\n        u(x, y)\n        &gt;&gt;&gt; print(f)\n        f(x, y, z)\n    \"\"\"\n    expr = sp.Function(name)(*invars)\n\n    return expr\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.PDE.create_symbols","title":"<code>create_symbols(symbol_str)</code>  <code>staticmethod</code>","text":"<p>Create symbolic variables.</p> <p>Parameters:</p> Name Type Description Default <code>symbol_str</code> <code>str</code> <p>String contains symbols, such as \"x\", \"x y z\".</p> required <p>Returns:</p> Type Description <code>Union[Symbol, Tuple[Symbol, ...]]</code> <p>Union[sympy.Symbol, Tuple[sympy.Symbol, ...]]: Created symbol(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.PDE()\n&gt;&gt;&gt; symbol_x = pde.create_symbols('x')\n&gt;&gt;&gt; symbols_xyz = pde.create_symbols('x y z')\n&gt;&gt;&gt; print(symbol_x)\nx\n&gt;&gt;&gt; print(symbols_xyz)\n(x, y, z)\n</code></pre> Source code in <code>ppsci/equation/pde/base.py</code> <pre><code>@staticmethod\ndef create_symbols(\n    symbol_str: str,\n) -&gt; Union[sp.Symbol, Tuple[sp.Symbol, ...]]:\n    \"\"\"Create symbolic variables.\n\n    Args:\n        symbol_str (str): String contains symbols, such as \"x\", \"x y z\".\n\n    Returns:\n        Union[sympy.Symbol, Tuple[sympy.Symbol, ...]]: Created symbol(s).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.PDE()\n        &gt;&gt;&gt; symbol_x = pde.create_symbols('x')\n        &gt;&gt;&gt; symbols_xyz = pde.create_symbols('x y z')\n        &gt;&gt;&gt; print(symbol_x)\n        x\n        &gt;&gt;&gt; print(symbols_xyz)\n        (x, y, z)\n    \"\"\"\n    return sp.symbols(symbol_str)\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.PDE.parameters","title":"<code>parameters()</code>","text":"<p>Return learnable parameters contained in PDE.</p> <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>List[Tensor]: A list of learnable parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.Vibration(2, -4, 0)\n&gt;&gt;&gt; print(pde.parameters())\n[Parameter containing:\nTensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n       -4.), Parameter containing:\nTensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n       0.)]\n</code></pre> Source code in <code>ppsci/equation/pde/base.py</code> <pre><code>def parameters(self) -&gt; List[paddle.Tensor]:\n    \"\"\"Return learnable parameters contained in PDE.\n\n    Returns:\n        List[Tensor]: A list of learnable parameters.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.Vibration(2, -4, 0)\n        &gt;&gt;&gt; print(pde.parameters())\n        [Parameter containing:\n        Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n               -4.), Parameter containing:\n        Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n               0.)]\n    \"\"\"\n    return self.learnable_parameters.parameters()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.PDE.set_state_dict","title":"<code>set_state_dict(state_dict)</code>","text":"<p>Set state dict from dict.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Tensor]</code> <p>The state dict to be set.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>Tuple[List[str], List[str]]: List of missing_keys and unexpected_keys. Expected to be two empty tuples mostly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; paddle.set_default_dtype(\"float64\")\n&gt;&gt;&gt; pde = ppsci.equation.Vibration(2, -4, 0)\n&gt;&gt;&gt; state = pde.state_dict()\n&gt;&gt;&gt; state['0'] = paddle.to_tensor(-3.1)\n&gt;&gt;&gt; pde.set_state_dict(state)\n([], [])\n&gt;&gt;&gt; print(state)\nOrderedDict([('0', Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=True,\n       -3.10000000)), ('1', Parameter containing:\nTensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,\n       0.))])\n</code></pre> Source code in <code>ppsci/equation/pde/base.py</code> <pre><code>def set_state_dict(\n    self, state_dict: Dict[str, paddle.Tensor]\n) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Set state dict from dict.\n\n    Args:\n        state_dict (Dict[str, paddle.Tensor]): The state dict to be set.\n\n    Returns:\n        Tuple[List[str], List[str]]: List of missing_keys and unexpected_keys.\n            Expected to be two empty tuples mostly.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; paddle.set_default_dtype(\"float64\")\n        &gt;&gt;&gt; pde = ppsci.equation.Vibration(2, -4, 0)\n        &gt;&gt;&gt; state = pde.state_dict()\n        &gt;&gt;&gt; state['0'] = paddle.to_tensor(-3.1)\n        &gt;&gt;&gt; pde.set_state_dict(state)\n        ([], [])\n        &gt;&gt;&gt; print(state)\n        OrderedDict([('0', Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=True,\n               -3.10000000)), ('1', Parameter containing:\n        Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,\n               0.))])\n    \"\"\"\n    return self.learnable_parameters.set_state_dict(state_dict)\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.PDE.state_dict","title":"<code>state_dict()</code>","text":"<p>Return named learnable parameters in dict.</p> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict[str, Tensor]: A dict of states(str) and learnable parameters(Tensor).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.Vibration(2, -4, 0)\n&gt;&gt;&gt; print(pde.state_dict())\nOrderedDict([('0', Parameter containing:\nTensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,\n       -4.)), ('1', Parameter containing:\nTensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,\n       0.))])\n</code></pre> Source code in <code>ppsci/equation/pde/base.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, paddle.Tensor]:\n    \"\"\"Return named learnable parameters in dict.\n\n    Returns:\n        Dict[str, Tensor]: A dict of states(str) and learnable parameters(Tensor).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.Vibration(2, -4, 0)\n        &gt;&gt;&gt; print(pde.state_dict())\n        OrderedDict([('0', Parameter containing:\n        Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,\n               -4.)), ('1', Parameter containing:\n        Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,\n               0.))])\n    \"\"\"\n    return self.learnable_parameters.state_dict()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.AllenCahn","title":"<code>AllenCahn</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Class for Allen-Cahn equation.</p> \\[ \\dfrac{\\partial u}{\\partial t} - \\epsilon^2 \\Delta u + 5u^3 - 5u = 0 \\] <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>Represents the characteristicscale of interfacial width, influencing the thickness and dynamics of phase boundaries.</p> required <code>detach_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Keys used for detach during computing. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.AllenCahn(eps=0.01)\n</code></pre> Source code in <code>ppsci/equation/pde/allen_cahn.py</code> <pre><code>class AllenCahn(base.PDE):\n    r\"\"\"Class for Allen-Cahn equation.\n\n    $$\n    \\dfrac{\\partial u}{\\partial t} - \\epsilon^2 \\Delta u + 5u^3 - 5u = 0\n    $$\n\n    Args:\n        eps (float): Represents the characteristicscale of interfacial width,\n            influencing the thickness and dynamics of phase boundaries.\n        detach_keys (Optional[Tuple[str, ...]]): Keys used for detach during computing.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.AllenCahn(eps=0.01)\n    \"\"\"\n\n    def __init__(\n        self,\n        eps: float,\n        detach_keys: Optional[Tuple[str, ...]] = None,\n    ):\n        super().__init__()\n        self.detach_keys = detach_keys\n        self.eps = eps\n        # t, x = self.create_symbols(\"t x\")\n        # invars = (t, x, )\n        # u = self.create_function(\"u\", invars)\n        # allen_cahn = u.diff(t) + 5 * u**3 - 5 * u - 0.0001 * u.diff(x, 2)\n\n        # TODO: Pow(u,3) seems cause slightly larger L2 error than multiply(u*u*u)\n        def allen_cahn(out):\n            t, x = out[\"t\"], out[\"x\"]\n            u = out[\"u\"]\n            u__t, u__x = jacobian(u, [t, x])\n            u__x__x = jacobian(u__x, x)\n\n            return u__t - (self.eps**2) * u__x__x + 5 * u * u * u - 5 * u\n\n        self.add_equation(\"allen_cahn\", allen_cahn)\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.Biharmonic","title":"<code>Biharmonic</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Class for biharmonic equation with supporting special load.</p> \\[ \\nabla^4 \\varphi = \\dfrac{q}{D} \\] <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of equation.</p> required <code>q</code> <code>Union[float, str, Basic]</code> <p>Load.</p> required <code>D</code> <code>Union[float, str]</code> <p>Rigidity.</p> required <code>detach_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Keys used for detach during computing. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.Biharmonic(2, -1.0, 1.0)\n</code></pre> Source code in <code>ppsci/equation/pde/biharmonic.py</code> <pre><code>class Biharmonic(base.PDE):\n    r\"\"\"Class for biharmonic equation with supporting special load.\n\n    $$\n    \\nabla^4 \\varphi = \\dfrac{q}{D}\n    $$\n\n    Args:\n        dim (int): Dimension of equation.\n        q (Union[float, str, sympy.Basic]): Load.\n        D (Union[float, str]): Rigidity.\n        detach_keys (Optional[Tuple[str, ...]]): Keys used for detach during computing.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.Biharmonic(2, -1.0, 1.0)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        q: Union[float, str, sympy.Basic],\n        D: Union[float, str],\n        detach_keys: Optional[Tuple[str, ...]] = None,\n    ):\n        super().__init__()\n        self.detach_keys = detach_keys\n\n        invars = self.create_symbols(\"x y z\")[:dim]\n        u = self.create_function(\"u\", invars)\n\n        if isinstance(q, str):\n            q = self.create_function(\"q\", invars)\n        if isinstance(D, str):\n            D = self.create_function(\"D\", invars)\n\n        self.dim = dim\n        self.q = q\n        self.D = D\n\n        biharmonic = -self.q / self.D\n        for invar_i in invars:\n            for invar_j in invars:\n                biharmonic += u.diff(invar_i, 2).diff(invar_j, 2)\n\n        self.add_equation(\"biharmonic\", biharmonic)\n\n        self._apply_detach()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.FractionalPoisson","title":"<code>FractionalPoisson</code>","text":"<p>               Bases: <code>PDE</code></p> <p>(TODO)Docstring of this class will be refined in the future.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Alpha.</p> required <code>geom</code> <code>Geometry</code> <p>Computation geometry.</p> required <code>resolution</code> <code>Tuple[int, ...]</code> <p>Resolution.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom_disk = ppsci.geometry.Disk([0, 0], 1)\n&gt;&gt;&gt; ALPHA = 0.5\n&gt;&gt;&gt; fpde = ppsci.equation.FractionalPoisson(ALPHA, geom_disk, [8, 100])\n</code></pre> Source code in <code>ppsci/equation/fpde/fractional_poisson.py</code> <pre><code>class FractionalPoisson(PDE):\n    \"\"\"(TODO)Docstring of this class will be refined in the future.\n\n    Args:\n        alpha (float): Alpha.\n        geom (geometry.Geometry): Computation geometry.\n        resolution (Tuple[int, ...]): Resolution.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom_disk = ppsci.geometry.Disk([0, 0], 1)\n        &gt;&gt;&gt; ALPHA = 0.5\n        &gt;&gt;&gt; fpde = ppsci.equation.FractionalPoisson(ALPHA, geom_disk, [8, 100])\n    \"\"\"\n\n    dtype = paddle.get_default_dtype()\n\n    def __init__(\n        self, alpha: float, geom: geometry.Geometry, resolution: Tuple[int, ...]\n    ):\n        super().__init__()\n        self.alpha = alpha\n        self.geom = geom\n        self.resolution = resolution\n        self._w_init = self._init_weights()\n\n        def compute_fpde_func(out):\n            x = paddle.concat((out[\"x\"], out[\"y\"]), axis=1)\n            y = out[\"u\"]\n            indices, values, shape = self.int_mat\n            int_mat = sparse.sparse_coo_tensor(\n                [[p[0] for p in indices], [p[1] for p in indices]],\n                values,\n                shape,\n                stop_gradient=False,\n            )\n            lhs = sparse.matmul(int_mat, y)\n            lhs = lhs[:, 0]\n            lhs *= (\n                special.gamma((1 - self.alpha) / 2)\n                * special.gamma((2 + self.alpha) / 2)\n                / (2 * np.pi**1.5)\n            )\n            x = x[: paddle.numel(lhs)]\n            rhs = (\n                2**self.alpha\n                * special.gamma(2 + self.alpha / 2)\n                * special.gamma(1 + self.alpha / 2)\n                * (1 - (1 + self.alpha / 2) * paddle.sum(x**2, axis=1))\n            )\n            res = lhs - rhs\n            return res\n\n        self.add_equation(\"fpde\", compute_fpde_func)\n\n    def _init_weights(self):\n        n = self._dynamic_dist2npts(self.geom.diam) + 1\n        w = [1.0]\n        for j in range(1, n):\n            w.append(w[-1] * (j - 1 - self.alpha) / j)\n        return np.array(w, dtype=self.dtype)\n\n    def get_x(self, x_f):\n        if hasattr(self, \"train_x\"):\n            return self.train_x\n\n        self.x0 = x_f\n        if np.any(self.geom.on_boundary(self.x0)):\n            raise ValueError(\"x0 contains boundary points.\")\n\n        if self.geom.ndim == 1:\n            dirns, dirn_w = [-1, 1], [1, 1]\n        elif self.geom.ndim == 2:\n            gauss_x, gauss_w = np.polynomial.legendre.leggauss(self.resolution[0])\n            gauss_x, gauss_w = gauss_x.astype(self.dtype), gauss_w.astype(self.dtype)\n            thetas = np.pi * gauss_x + np.pi\n            dirns = np.vstack((np.cos(thetas), np.sin(thetas))).T\n            dirn_w = np.pi * gauss_w\n        elif self.geom.ndim == 3:\n            gauss_x, gauss_w = np.polynomial.legendre.leggauss(max(self.resolution[:2]))\n            gauss_x, gauss_w = gauss_x.astype(self.dtype), gauss_w.astype(self.dtype)\n            thetas = (np.pi * gauss_x[: self.resolution[0]] + np.pi) / 2\n            phis = np.pi * gauss_x[: self.resolution[1]] + np.pi\n            dirns, dirn_w = [], []\n            for i in range(self.resolution[0]):\n                for j in range(self.resolution[1]):\n                    dirns.append(\n                        [\n                            np.sin(thetas[i]) * np.cos(phis[j]),\n                            np.sin(thetas[i]) * np.sin(phis[j]),\n                            np.cos(thetas[i]),\n                        ]\n                    )\n                    dirn_w.append(gauss_w[i] * gauss_w[j] * np.sin(thetas[i]))\n            dirn_w = np.pi**2 / 2 * np.array(dirn_w)\n\n        x, self.w = [], []\n        for x0i in self.x0:\n            xi = list(\n                map(\n                    lambda dirn: self.background_points(\n                        x0i, dirn, self._dynamic_dist2npts, 0\n                    ),\n                    dirns,\n                )\n            )\n            wi = list(\n                map(\n                    lambda i: dirn_w[i]\n                    * np.linalg.norm(xi[i][1] - xi[i][0]) ** (-self.alpha)\n                    * self.get_weight(len(xi[i]) - 1),\n                    range(len(dirns)),\n                )\n            )\n            # first order\n            # xi, wi = zip(self.modify_first_order(xij, wij) for xij, wij in zip(xi, wi))\n            xi, wi = zip(*map(self.modify_first_order, xi, wi))\n            # second order\n            # xi, wi = zip(*map(self.modify_second_order, xi, wi))\n            # third order\n            # xi, wi = zip(*map(self.modify_third_order, xi, wi))\n            x.append(np.vstack(xi))\n            self.w.append(np.hstack(wi))\n        self.x = np.vstack([self.x0] + x)\n        self.int_mat = self._get_int_matrix(self.x0)\n        self.train_x = misc.convert_to_dict(self.x, (\"x\", \"y\"))\n        return self.train_x\n\n    def get_weight(self, n):\n        return self._w_init[: n + 1]\n\n    def background_points(self, x, dirn, dist2npt, shift):\n        dirn = dirn / np.linalg.norm(dirn)\n        dx = self.distance2boundary_unitdirn(x, -dirn)\n        n = max(dist2npt(dx), 1)\n        h = dx / n\n        pts = x - np.arange(-shift, n - shift + 1, dtype=self.dtype)[:, None] * h * dirn\n        return pts\n\n    def distance2boundary_unitdirn(self, x, dirn):\n        # https://en.wikipedia.org/wiki/Line%E2%80%93sphere_intersection\n        xc = x - self.geom.center\n        xc = xc\n        ad = np.dot(xc, dirn)\n        return (\n            -ad + (ad**2 - np.sum(xc * xc, axis=-1) + self.geom.radius**2) ** 0.5\n        ).astype(self.dtype)\n\n    def modify_first_order(self, x, w):\n        x = np.vstack(([2 * x[0] - x[1]], x[:-1]))\n        if not self.geom.is_inside(x[0:1])[0]:\n            return x[1:], w[1:]\n        return x, w\n\n    def _dynamic_dist2npts(self, dx):\n        return int(math.ceil(self.resolution[-1] * dx))\n\n    def _get_int_matrix(self, x: np.ndarray) -&gt; np.ndarray:\n        dense_shape = (x.shape[0], self.x.shape[0])\n        indices, values = [], []\n        beg = x.shape[0]\n        for i in range(x.shape[0]):\n            for _ in range(self.w[i].shape[0]):\n                indices.append([i, beg])\n                beg += 1\n            values = np.hstack((values, self.w[i]))\n        return indices, values.astype(self.dtype), dense_shape\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.HeatExchanger","title":"<code>HeatExchanger</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Class for heat exchanger equation.</p> \\[ \\begin{aligned} &amp; L\\left(\\frac{q_m c_p}{v}\\right)_{\\mathrm{c}} \\frac{\\partial T_{\\mathrm{c}}}{\\partial \\tau}-L\\left(q_m c_p\\right)_{\\mathrm{c}} \\frac{\\partial T_{\\mathrm{c}}}{\\partial x}=\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{c}}\\left(T_{\\mathrm{w}}-T_{\\mathrm{c}}\\right), \\\\ &amp; L\\left(\\frac{q_m c_p}{v}\\right)_{\\mathrm{h}} \\frac{\\partial T_{\\mathrm{h}}}{\\partial \\tau}+L\\left(q_m c_p\\right)_{\\mathrm{h}} \\frac{\\partial T_{\\mathrm{h}}}{\\partial x}=\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{h}}\\left(T_{\\mathrm{w}}-T_{\\mathrm{h}}\\right), \\\\ &amp; \\left(M c_p\\right)_{\\mathrm{w}} \\frac{\\partial T_{\\mathrm{w}}}{\\partial \\tau}=\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{h}}\\left(T_{\\mathrm{h}}-T_{\\mathrm{w}}\\right)+\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{c}}\\left(T_{\\mathrm{c}}-T_{\\mathrm{w}}\\right). \\end{aligned} \\] <p>where:</p> <ul> <li>\\(T\\) is temperature,</li> <li>\\(q_m\\) is mass flow rate,</li> <li>\\(c_p\\) represents specific heat capacity,</li> <li>\\(v\\) denotes flow velocity,</li> <li>\\(L\\) stands for flow length,</li> <li>\\(\\eta_{\\mathrm{o}}\\) signifies fin surface efficiency,</li> <li>\\(\\alpha\\) stands for heat transfer coefficient,</li> <li>\\(A\\) indicates heat transfer area,</li> <li>\\(M\\) represents the mass of the heat transfer structure,</li> <li>\\(\\tau\\) correspond to time,</li> <li>\\(x\\) correspond flow direction,</li> <li>Subscripts \\(\\mathrm{h}\\), \\(\\mathrm{c}\\), and \\(\\mathrm{w}\\) denote the hot fluid side, cold fluid side, and heat transfer wall, respectively.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>alpha_h</code> <code>Union[float, str]</code> <p>\\(\\frac{(\\eta_o\\alpha A)_h}{L(c_p)_h}\\)</p> required <code>alpha_c</code> <code>Union[float, str]</code> <p>\\(\\frac{(\\eta_o\\alpha A)_c}{L(c_p)_c}\\)</p> required <code>v_h</code> <code>Union[float, str]</code> <p>\\(v_h\\)</p> required <code>v_c</code> <code>Union[float, str]</code> <p>\\(v_c\\)</p> required <code>w_h</code> <code>Union[float, str]</code> <p>\\(\\frac{(\\eta_o\\alpha A)_h}{M(c_p)_w}\\)</p> required <code>w_c</code> <code>Union[float, str]</code> <p>\\(\\frac{(\\eta_o\\alpha A)_c}{M(c_p)_w}\\)</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.HeatExchanger(1.0,1.0,1.0,1.0,1.0,1.0)\n</code></pre> Source code in <code>ppsci/equation/pde/heat_exchanger.py</code> <pre><code>class HeatExchanger(base.PDE):\n    r\"\"\"Class for heat exchanger equation.\n\n    $$\n    \\begin{aligned}\n    &amp; L\\left(\\frac{q_m c_p}{v}\\right)_{\\mathrm{c}} \\frac{\\partial T_{\\mathrm{c}}}{\\partial \\tau}-L\\left(q_m c_p\\right)_{\\mathrm{c}} \\frac{\\partial T_{\\mathrm{c}}}{\\partial x}=\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{c}}\\left(T_{\\mathrm{w}}-T_{\\mathrm{c}}\\right), \\\\\n    &amp; L\\left(\\frac{q_m c_p}{v}\\right)_{\\mathrm{h}} \\frac{\\partial T_{\\mathrm{h}}}{\\partial \\tau}+L\\left(q_m c_p\\right)_{\\mathrm{h}} \\frac{\\partial T_{\\mathrm{h}}}{\\partial x}=\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{h}}\\left(T_{\\mathrm{w}}-T_{\\mathrm{h}}\\right), \\\\\n    &amp; \\left(M c_p\\right)_{\\mathrm{w}} \\frac{\\partial T_{\\mathrm{w}}}{\\partial \\tau}=\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{h}}\\left(T_{\\mathrm{h}}-T_{\\mathrm{w}}\\right)+\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{c}}\\left(T_{\\mathrm{c}}-T_{\\mathrm{w}}\\right).\n    \\end{aligned}\n    $$\n\n    where:\n\n    - $T$ is temperature,\n    - $q_m$ is mass flow rate,\n    - $c_p$ represents specific heat capacity,\n    - $v$ denotes flow velocity,\n    - $L$ stands for flow length,\n    - $\\eta_{\\mathrm{o}}$ signifies fin surface efficiency,\n    - $\\alpha$ stands for heat transfer coefficient,\n    - $A$ indicates heat transfer area,\n    - $M$ represents the mass of the heat transfer structure,\n    - $\\tau$ correspond to time,\n    - $x$ correspond flow direction,\n    - Subscripts $\\mathrm{h}$, $\\mathrm{c}$, and $\\mathrm{w}$ denote the hot fluid side, cold fluid side, and heat transfer wall, respectively.\n\n    Args:\n        alpha_h: $\\frac{(\\eta_o\\alpha A)_h}{L(c_p)_h}$\n        alpha_c: $\\frac{(\\eta_o\\alpha A)_c}{L(c_p)_c}$\n        v_h: $v_h$\n        v_c: $v_c$\n        w_h: $\\frac{(\\eta_o\\alpha A)_h}{M(c_p)_w}$\n        w_c: $\\frac{(\\eta_o\\alpha A)_c}{M(c_p)_w}$\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.HeatExchanger(1.0,1.0,1.0,1.0,1.0,1.0)\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha_h: Union[float, str],\n        alpha_c: Union[float, str],\n        v_h: Union[float, str],\n        v_c: Union[float, str],\n        w_h: Union[float, str],\n        w_c: Union[float, str],\n    ):\n        super().__init__()\n        x, t, qm_h, qm_c = self.create_symbols(\"x t qm_h qm_c\")\n\n        T_h = self.create_function(\"T_h\", (x, t, qm_h))\n        T_c = self.create_function(\"T_c\", (x, t, qm_c))\n        T_w = self.create_function(\"T_w\", (x, t))\n\n        T_h_x = T_h.diff(x)\n        T_h_t = T_h.diff(t)\n        T_c_x = T_c.diff(x)\n        T_c_t = T_c.diff(t)\n        T_w_t = T_w.diff(t)\n\n        beta_h = (alpha_h * v_h) / qm_h\n        beta_c = (alpha_c * v_c) / qm_c\n\n        heat_boundary = T_h_t + v_h * T_h_x - beta_h * (T_w - T_h)\n        cold_boundary = T_c_t - v_c * T_c_x - beta_c * (T_w - T_c)\n        wall = T_w_t - w_h * (T_h - T_w) - w_c * (T_c - T_w)\n\n        self.add_equation(\"heat_boundary\", heat_boundary)\n        self.add_equation(\"cold_boundary\", cold_boundary)\n        self.add_equation(\"wall\", wall)\n\n        self._apply_detach()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.Laplace","title":"<code>Laplace</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Class for laplace equation.</p> \\[ \\nabla^2 \\varphi = 0 \\] <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of equation.</p> required <code>detach_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Keys used for detach during computing. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.Laplace(2)\n</code></pre> Source code in <code>ppsci/equation/pde/laplace.py</code> <pre><code>class Laplace(base.PDE):\n    r\"\"\"Class for laplace equation.\n\n    $$\n    \\nabla^2 \\varphi = 0\n    $$\n\n    Args:\n        dim (int): Dimension of equation.\n        detach_keys (Optional[Tuple[str, ...]]): Keys used for detach during computing.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.Laplace(2)\n    \"\"\"\n\n    def __init__(self, dim: int, detach_keys: Optional[Tuple[str, ...]] = None):\n        super().__init__()\n        self.detach_keys = detach_keys\n\n        invars = self.create_symbols(\"x y z\")[:dim]\n        u = self.create_function(\"u\", invars)\n\n        self.dim = dim\n\n        laplace = 0\n        for invar in invars:\n            laplace += u.diff(invar, 2)\n\n        self.add_equation(\"laplace\", laplace)\n\n        self._apply_detach()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.LinearElasticity","title":"<code>LinearElasticity</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Linear elasticity equations.</p> <p>Use either (E, nu) or (lambda_, mu) to define the material properties.</p> \\[ \\begin{cases}     stress\\_disp_{xx} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial u}{\\partial x} - \\sigma_{xx} \\\\     stress\\_disp_{yy} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial v}{\\partial y} - \\sigma_{yy} \\\\     stress\\_disp_{zz} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial w}{\\partial z} - \\sigma_{zz} \\\\     stress\\_disp_{xy} = \\mu(\\dfrac{\\partial u}{\\partial y} + \\dfrac{\\partial v}{\\partial x}) - \\sigma_{xy} \\\\     stress\\_disp_{xz} = \\mu(\\dfrac{\\partial u}{\\partial z} + \\dfrac{\\partial w}{\\partial x}) - \\sigma_{xz} \\\\     stress\\_disp_{yz} = \\mu(\\dfrac{\\partial v}{\\partial z} + \\dfrac{\\partial w}{\\partial y}) - \\sigma_{yz} \\\\     equilibrium_{x} = \\rho \\dfrac{\\partial^2 u}{\\partial t^2} - (\\dfrac{\\partial \\sigma_{xx}}{\\partial x} + \\dfrac{\\partial \\sigma_{xy}}{\\partial y} + \\dfrac{\\partial \\sigma_{xz}}{\\partial z}) \\\\     equilibrium_{y} = \\rho \\dfrac{\\partial^2 u}{\\partial t^2} - (\\dfrac{\\partial \\sigma_{xy}}{\\partial x} + \\dfrac{\\partial \\sigma_{yy}}{\\partial y} + \\dfrac{\\partial \\sigma_{yz}}{\\partial z}) \\\\     equilibrium_{z} = \\rho \\dfrac{\\partial^2 u}{\\partial t^2} - (\\dfrac{\\partial \\sigma_{xz}}{\\partial x} + \\dfrac{\\partial \\sigma_{yz}}{\\partial y} + \\dfrac{\\partial \\sigma_{zz}}{\\partial z}) \\\\ \\end{cases} \\] <p>Parameters:</p> Name Type Description Default <code>E</code> <code>Optional[Union[float, str]]</code> <p>The Young's modulus. Defaults to None.</p> <code>None</code> <code>nu</code> <code>Optional[Union[float, str]]</code> <p>The Poisson's ratio. Defaults to None.</p> <code>None</code> <code>lambda_</code> <code>Optional[Union[float, str]]</code> <p>Lam\u00e9's first parameter. Defaults to None.</p> <code>None</code> <code>mu</code> <code>Optional[Union[float, str]]</code> <p>Lam\u00e9's second parameter (shear modulus). Defaults to None.</p> <code>None</code> <code>rho</code> <code>Union[float, str]</code> <p>Mass density. Defaults to 1.</p> <code>1</code> <code>dim</code> <code>int</code> <p>Dimension of the linear elasticity (2 or 3). Defaults to 3.</p> <code>3</code> <code>time</code> <code>bool</code> <p>Whether contains time data. Defaults to False.</p> <code>False</code> <code>detach_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Keys used for detach during computing. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.LinearElasticity(\n...     E=None, nu=None, lambda_=1e4, mu=100, dim=3\n... )\n</code></pre> Source code in <code>ppsci/equation/pde/linear_elasticity.py</code> <pre><code>class LinearElasticity(base.PDE):\n    r\"\"\"Linear elasticity equations.\n\n    Use either (E, nu) or (lambda_, mu) to define the material properties.\n\n    $$\n    \\begin{cases}\n        stress\\_disp_{xx} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial u}{\\partial x} - \\sigma_{xx} \\\\\n        stress\\_disp_{yy} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial v}{\\partial y} - \\sigma_{yy} \\\\\n        stress\\_disp_{zz} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial w}{\\partial z} - \\sigma_{zz} \\\\\n        stress\\_disp_{xy} = \\mu(\\dfrac{\\partial u}{\\partial y} + \\dfrac{\\partial v}{\\partial x}) - \\sigma_{xy} \\\\\n        stress\\_disp_{xz} = \\mu(\\dfrac{\\partial u}{\\partial z} + \\dfrac{\\partial w}{\\partial x}) - \\sigma_{xz} \\\\\n        stress\\_disp_{yz} = \\mu(\\dfrac{\\partial v}{\\partial z} + \\dfrac{\\partial w}{\\partial y}) - \\sigma_{yz} \\\\\n        equilibrium_{x} = \\rho \\dfrac{\\partial^2 u}{\\partial t^2} - (\\dfrac{\\partial \\sigma_{xx}}{\\partial x} + \\dfrac{\\partial \\sigma_{xy}}{\\partial y} + \\dfrac{\\partial \\sigma_{xz}}{\\partial z}) \\\\\n        equilibrium_{y} = \\rho \\dfrac{\\partial^2 u}{\\partial t^2} - (\\dfrac{\\partial \\sigma_{xy}}{\\partial x} + \\dfrac{\\partial \\sigma_{yy}}{\\partial y} + \\dfrac{\\partial \\sigma_{yz}}{\\partial z}) \\\\\n        equilibrium_{z} = \\rho \\dfrac{\\partial^2 u}{\\partial t^2} - (\\dfrac{\\partial \\sigma_{xz}}{\\partial x} + \\dfrac{\\partial \\sigma_{yz}}{\\partial y} + \\dfrac{\\partial \\sigma_{zz}}{\\partial z}) \\\\\n    \\end{cases}\n    $$\n\n    Args:\n        E (Optional[Union[float, str]]): The Young's modulus. Defaults to None.\n        nu (Optional[Union[float, str]]): The Poisson's ratio. Defaults to None.\n        lambda_ (Optional[Union[float, str]]): Lam\u00e9's first parameter. Defaults to None.\n        mu (Optional[Union[float, str]]): Lam\u00e9's second parameter (shear modulus). Defaults to None.\n        rho (Union[float, str], optional): Mass density. Defaults to 1.\n        dim (int, optional): Dimension of the linear elasticity (2 or 3). Defaults to 3.\n        time (bool, optional): Whether contains time data. Defaults to False.\n        detach_keys (Optional[Tuple[str, ...]]): Keys used for detach during computing.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.LinearElasticity(\n        ...     E=None, nu=None, lambda_=1e4, mu=100, dim=3\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        E: Optional[Union[float, str]] = None,\n        nu: Optional[Union[float, str]] = None,\n        lambda_: Optional[Union[float, str]] = None,\n        mu: Optional[Union[float, str]] = None,\n        rho: Union[float, str] = 1,\n        dim: int = 3,\n        time: bool = False,\n        detach_keys: Optional[Tuple[str, ...]] = None,\n    ):\n        super().__init__()\n        self.detach_keys = detach_keys\n        self.dim = dim\n        self.time = time\n\n        t, x, y, z = self.create_symbols(\"t x y z\")\n        normal_x, normal_y, normal_z = self.create_symbols(\"normal_x normal_y normal_z\")\n        invars = (x, y)\n        if time:\n            invars = (t,) + invars\n        if self.dim == 3:\n            invars += (z,)\n\n        u = self.create_function(\"u\", invars)\n        v = self.create_function(\"v\", invars)\n        w = self.create_function(\"w\", invars) if dim == 3 else sp.Number(0)\n\n        sigma_xx = self.create_function(\"sigma_xx\", invars)\n        sigma_yy = self.create_function(\"sigma_yy\", invars)\n        sigma_xy = self.create_function(\"sigma_xy\", invars)\n        sigma_zz = (\n            self.create_function(\"sigma_zz\", invars) if dim == 3 else sp.Number(0)\n        )\n        sigma_xz = (\n            self.create_function(\"sigma_xz\", invars) if dim == 3 else sp.Number(0)\n        )\n        sigma_yz = (\n            self.create_function(\"sigma_yz\", invars) if dim == 3 else sp.Number(0)\n        )\n\n        # compute lambda and mu\n        if lambda_ is None:\n            if isinstance(nu, str):\n                nu = self.create_function(nu, invars)\n            if isinstance(E, str):\n                E = self.create_function(E, invars)\n            lambda_ = nu * E / ((1 + nu) * (1 - 2 * nu))\n            mu = E / (2 * (1 + nu))\n        else:\n            if isinstance(lambda_, str):\n                lambda_ = self.create_function(lambda_, invars)\n            if isinstance(mu, str):\n                mu = self.create_function(mu, invars)\n\n        if isinstance(rho, str):\n            rho = self.create_function(rho, invars)\n\n        self.E = E\n        self.nu = nu\n        self.lambda_ = lambda_\n        self.mu = mu\n        self.rho = rho\n\n        # compute stress equations\n        stress_disp_xx = (\n            lambda_ * (u.diff(x) + v.diff(y) + w.diff(z))\n            + 2 * mu * u.diff(x)\n            - sigma_xx\n        )\n        stress_disp_yy = (\n            lambda_ * (u.diff(x) + v.diff(y) + w.diff(z))\n            + 2 * mu * v.diff(y)\n            - sigma_yy\n        )\n        stress_disp_zz = (\n            lambda_ * (u.diff(x) + v.diff(y) + w.diff(z))\n            + 2 * mu * w.diff(z)\n            - sigma_zz\n        )\n        stress_disp_xy = mu * (u.diff(y) + v.diff(x)) - sigma_xy\n        stress_disp_xz = mu * (u.diff(z) + w.diff(x)) - sigma_xz\n        stress_disp_yz = mu * (v.diff(z) + w.diff(y)) - sigma_yz\n\n        # compute equilibrium equations\n        equilibrium_x = rho * ((u.diff(t)).diff(t)) - (\n            sigma_xx.diff(x) + sigma_xy.diff(y) + sigma_xz.diff(z)\n        )\n        equilibrium_y = rho * ((v.diff(t)).diff(t)) - (\n            sigma_xy.diff(x) + sigma_yy.diff(y) + sigma_yz.diff(z)\n        )\n        equilibrium_z = rho * ((w.diff(t)).diff(t)) - (\n            sigma_xz.diff(x) + sigma_yz.diff(y) + sigma_zz.diff(z)\n        )\n\n        # compute traction equations\n        traction_x = normal_x * sigma_xx + normal_y * sigma_xy + normal_z * sigma_xz\n        traction_y = normal_x * sigma_xy + normal_y * sigma_yy + normal_z * sigma_yz\n        traction_z = normal_x * sigma_xz + normal_y * sigma_yz + normal_z * sigma_zz\n\n        # add stress equations\n        self.add_equation(\"stress_disp_xx\", stress_disp_xx)\n        self.add_equation(\"stress_disp_yy\", stress_disp_yy)\n        self.add_equation(\"stress_disp_xy\", stress_disp_xy)\n        if self.dim == 3:\n            self.add_equation(\"stress_disp_zz\", stress_disp_zz)\n            self.add_equation(\"stress_disp_xz\", stress_disp_xz)\n            self.add_equation(\"stress_disp_yz\", stress_disp_yz)\n\n        # add equilibrium equations\n        self.add_equation(\"equilibrium_x\", equilibrium_x)\n        self.add_equation(\"equilibrium_y\", equilibrium_y)\n        if self.dim == 3:\n            self.add_equation(\"equilibrium_z\", equilibrium_z)\n\n        # add traction equations\n        self.add_equation(\"traction_x\", traction_x)\n        self.add_equation(\"traction_y\", traction_y)\n        if self.dim == 3:\n            self.add_equation(\"traction_z\", traction_z)\n\n        self._apply_detach()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.NavierStokes","title":"<code>NavierStokes</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Class for navier-stokes equation.</p> \\[ \\begin{cases}     \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z} = 0 \\\\     \\dfrac{\\partial u}{\\partial t} + u\\dfrac{\\partial u}{\\partial x} + v\\dfrac{\\partial u}{\\partial y} + w\\dfrac{\\partial u}{\\partial z} =         - \\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial x}         + \\nu(             \\dfrac{\\partial ^2 u}{\\partial x ^2}             + \\dfrac{\\partial ^2 u}{\\partial y ^2}             + \\dfrac{\\partial ^2 u}{\\partial z ^2}         ) \\\\     \\dfrac{\\partial v}{\\partial t} + u\\dfrac{\\partial v}{\\partial x} + v\\dfrac{\\partial v}{\\partial y} + w\\dfrac{\\partial v}{\\partial z} =         - \\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial y}         + \\nu(             \\dfrac{\\partial ^2 v}{\\partial x ^2}             + \\dfrac{\\partial ^2 v}{\\partial y ^2}             + \\dfrac{\\partial ^2 v}{\\partial z ^2}         ) \\\\     \\dfrac{\\partial w}{\\partial t} + u\\dfrac{\\partial w}{\\partial x} + v\\dfrac{\\partial w}{\\partial y} + w\\dfrac{\\partial w}{\\partial z} =         - \\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial z}         + \\nu(             \\dfrac{\\partial ^2 w}{\\partial x ^2}             + \\dfrac{\\partial ^2 w}{\\partial y ^2}             + \\dfrac{\\partial ^2 w}{\\partial z ^2}         ) \\\\ \\end{cases} \\] <p>Parameters:</p> Name Type Description Default <code>nu</code> <code>Union[float, str]</code> <p>Dynamic viscosity.</p> required <code>rho</code> <code>Union[float, str]</code> <p>Density.</p> required <code>dim</code> <code>int</code> <p>Dimension of equation.</p> required <code>time</code> <code>bool</code> <p>Whether the equation is time-dependent.</p> required <code>detach_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Keys used for detach during computing. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.NavierStokes(0.1, 1.0, 3, False)\n</code></pre> Source code in <code>ppsci/equation/pde/navier_stokes.py</code> <pre><code>class NavierStokes(base.PDE):\n    r\"\"\"Class for navier-stokes equation.\n\n    $$\n    \\begin{cases}\n        \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z} = 0 \\\\\n        \\dfrac{\\partial u}{\\partial t} + u\\dfrac{\\partial u}{\\partial x} + v\\dfrac{\\partial u}{\\partial y} + w\\dfrac{\\partial u}{\\partial z} =\n            - \\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial x}\n            + \\nu(\n                \\dfrac{\\partial ^2 u}{\\partial x ^2}\n                + \\dfrac{\\partial ^2 u}{\\partial y ^2}\n                + \\dfrac{\\partial ^2 u}{\\partial z ^2}\n            ) \\\\\n        \\dfrac{\\partial v}{\\partial t} + u\\dfrac{\\partial v}{\\partial x} + v\\dfrac{\\partial v}{\\partial y} + w\\dfrac{\\partial v}{\\partial z} =\n            - \\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial y}\n            + \\nu(\n                \\dfrac{\\partial ^2 v}{\\partial x ^2}\n                + \\dfrac{\\partial ^2 v}{\\partial y ^2}\n                + \\dfrac{\\partial ^2 v}{\\partial z ^2}\n            ) \\\\\n        \\dfrac{\\partial w}{\\partial t} + u\\dfrac{\\partial w}{\\partial x} + v\\dfrac{\\partial w}{\\partial y} + w\\dfrac{\\partial w}{\\partial z} =\n            - \\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial z}\n            + \\nu(\n                \\dfrac{\\partial ^2 w}{\\partial x ^2}\n                + \\dfrac{\\partial ^2 w}{\\partial y ^2}\n                + \\dfrac{\\partial ^2 w}{\\partial z ^2}\n            ) \\\\\n    \\end{cases}\n    $$\n\n    Args:\n        nu (Union[float, str]): Dynamic viscosity.\n        rho (Union[float, str]): Density.\n        dim (int): Dimension of equation.\n        time (bool): Whether the equation is time-dependent.\n        detach_keys (Optional[Tuple[str, ...]]): Keys used for detach during computing.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.NavierStokes(0.1, 1.0, 3, False)\n    \"\"\"\n\n    def __init__(\n        self,\n        nu: Union[float, str],\n        rho: Union[float, str],\n        dim: int,\n        time: bool,\n        detach_keys: Optional[Tuple[str, ...]] = None,\n    ):\n        super().__init__()\n        self.detach_keys = detach_keys\n        self.dim = dim\n        self.time = time\n\n        t, x, y, z = self.create_symbols(\"t x y z\")\n        invars = (x, y)\n        if time:\n            invars = (t,) + invars\n        if dim == 3:\n            invars += (z,)\n\n        if isinstance(nu, str):\n            nu = sp_parser.parse_expr(nu)\n            if isinstance(nu, sp.Symbol):\n                invars += (nu,)\n\n        if isinstance(rho, str):\n            rho = sp_parser.parse_expr(rho)\n            if isinstance(rho, sp.Symbol):\n                invars += (rho,)\n\n        self.nu = nu\n        self.rho = rho\n\n        u = self.create_function(\"u\", invars)\n        v = self.create_function(\"v\", invars)\n        w = self.create_function(\"w\", invars) if dim == 3 else sp.Number(0)\n        p = self.create_function(\"p\", invars)\n\n        continuity = u.diff(x) + v.diff(y) + w.diff(z)\n        momentum_x = (\n            u.diff(t)\n            + u * u.diff(x)\n            + v * u.diff(y)\n            + w * u.diff(z)\n            - (\n                (nu * u.diff(x)).diff(x)\n                + (nu * u.diff(y)).diff(y)\n                + (nu * u.diff(z)).diff(z)\n            )\n            + 1 / rho * p.diff(x)\n        )\n        momentum_y = (\n            v.diff(t)\n            + u * v.diff(x)\n            + v * v.diff(y)\n            + w * v.diff(z)\n            - (\n                (nu * v.diff(x)).diff(x)\n                + (nu * v.diff(y)).diff(y)\n                + (nu * v.diff(z)).diff(z)\n            )\n            + 1 / rho * p.diff(y)\n        )\n        momentum_z = (\n            w.diff(t)\n            + u * w.diff(x)\n            + v * w.diff(y)\n            + w * w.diff(z)\n            - (\n                (nu * w.diff(x)).diff(x)\n                + (nu * w.diff(y)).diff(y)\n                + (nu * w.diff(z)).diff(z)\n            )\n            + 1 / rho * p.diff(z)\n        )\n        self.add_equation(\"continuity\", continuity)\n        self.add_equation(\"momentum_x\", momentum_x)\n        self.add_equation(\"momentum_y\", momentum_y)\n        if self.dim == 3:\n            self.add_equation(\"momentum_z\", momentum_z)\n\n        self._apply_detach()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.NormalDotVec","title":"<code>NormalDotVec</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Normal Dot Vector.</p> \\[ \\mathbf{n} \\cdot \\mathbf{v} = 0 \\] <p>Parameters:</p> Name Type Description Default <code>vec_keys</code> <code>Tuple[str, ...]</code> <p>Keys for vectors, such as (\"u\", \"v\", \"w\") for velocity vector.</p> required <code>detach_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Keys used for detach during computing. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.NormalDotVec((\"u\", \"v\", \"w\"))\n</code></pre> Source code in <code>ppsci/equation/pde/normal_dot_vec.py</code> <pre><code>class NormalDotVec(base.PDE):\n    r\"\"\"Normal Dot Vector.\n\n    $$\n    \\mathbf{n} \\cdot \\mathbf{v} = 0\n    $$\n\n    Args:\n        vec_keys (Tuple[str, ...]): Keys for vectors, such as (\"u\", \"v\", \"w\") for\n            velocity vector.\n        detach_keys (Optional[Tuple[str, ...]]): Keys used for detach during computing.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.NormalDotVec((\"u\", \"v\", \"w\"))\n    \"\"\"\n\n    def __init__(\n        self, vec_keys: Tuple[str, ...], detach_keys: Optional[Tuple[str, ...]] = None\n    ):\n        super().__init__()\n        self.detach_keys = detach_keys\n        if not vec_keys:\n            raise ValueError(f\"len(vec_keys)({len(vec_keys)}) should be larger than 0.\")\n\n        self.vec_keys = vec_keys\n        vec_vars = self.create_symbols(\" \".join(vec_keys))\n        normals = self.create_symbols(\"normal_x normal_y normal_z\")\n\n        normal_dot_vec = 0\n        for (normal, vec) in zip(normals, vec_vars):\n            normal_dot_vec += normal * vec\n\n        self.add_equation(\"normal_dot_vec\", normal_dot_vec)\n\n        self._apply_detach()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.Poisson","title":"<code>Poisson</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Class for poisson equation.</p> \\[ \\nabla^2 \\varphi = C \\] <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of equation.</p> required <code>detach_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Keys used for detach during computing. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.Poisson(2)\n</code></pre> Source code in <code>ppsci/equation/pde/poisson.py</code> <pre><code>class Poisson(base.PDE):\n    r\"\"\"Class for poisson equation.\n\n    $$\n    \\nabla^2 \\varphi = C\n    $$\n\n    Args:\n        dim (int): Dimension of equation.\n        detach_keys (Optional[Tuple[str, ...]]): Keys used for detach during computing.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.Poisson(2)\n    \"\"\"\n\n    def __init__(self, dim: int, detach_keys: Optional[Tuple[str, ...]] = None):\n        super().__init__()\n        self.detach_keys = detach_keys\n        invars = self.create_symbols(\"x y z\")[:dim]\n        p = self.create_function(\"p\", invars)\n        self.dim = dim\n\n        poisson = 0\n        for invar in invars:\n            poisson += p.diff(invar, 2)\n\n        self.add_equation(\"poisson\", poisson)\n\n        self._apply_detach()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.Vibration","title":"<code>Vibration</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Vortex induced vibration equation.</p> \\[ \\rho \\dfrac{\\partial^2 \\eta}{\\partial t^2} + e^{k1} \\dfrac{\\partial \\eta}{\\partial t} + e^{k2} \\eta = f \\] <p>Parameters:</p> Name Type Description Default <code>rho</code> <code>float</code> <p>Generalized mass.</p> required <code>k1</code> <code>float</code> <p>Learnable parameter for modal damping.</p> required <code>k2</code> <code>float</code> <p>Learnable parameter for generalized stiffness.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.Vibration(1.0, 4.0, -1.0)\n</code></pre> Source code in <code>ppsci/equation/pde/viv.py</code> <pre><code>class Vibration(base.PDE):\n    r\"\"\"Vortex induced vibration equation.\n\n    $$\n    \\rho \\dfrac{\\partial^2 \\eta}{\\partial t^2} + e^{k1} \\dfrac{\\partial \\eta}{\\partial t} + e^{k2} \\eta = f\n    $$\n\n    Args:\n        rho (float): Generalized mass.\n        k1 (float): Learnable parameter for modal damping.\n        k2 (float): Learnable parameter for generalized stiffness.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.Vibration(1.0, 4.0, -1.0)\n    \"\"\"\n\n    def __init__(self, rho: float, k1: float, k2: float):\n        super().__init__()\n        self.rho = rho\n        self.k1 = paddle.create_parameter(\n            shape=[],\n            dtype=paddle.get_default_dtype(),\n            default_initializer=initializer.Constant(k1),\n        )\n        self.k2 = paddle.create_parameter(\n            shape=[],\n            dtype=paddle.get_default_dtype(),\n            default_initializer=initializer.Constant(k2),\n        )\n        self.learnable_parameters.append(self.k1)\n        self.learnable_parameters.append(self.k2)\n\n        t_f = self.create_symbols(\"t_f\")\n        eta = self.create_function(\"eta\", (t_f,))\n        k1 = self.create_symbols(self.k1.name)\n        k2 = self.create_symbols(self.k2.name)\n        f = self.rho * eta.diff(t_f, 2) + sp.exp(k1) * eta.diff(t_f) + sp.exp(k2) * eta\n        self.add_equation(\"f\", f)\n\n        self._apply_detach()\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.Volterra","title":"<code>Volterra</code>","text":"<p>               Bases: <code>PDE</code></p> <p>A second kind of volterra integral equation with Gaussian quadrature algorithm.</p> \\[ x(t) - f(t)=\\int_a^t K(t, s) x(s) d s \\] <p>Volterra integral equation</p> <p>Gaussian quadrature</p> <p>Parameters:</p> Name Type Description Default <code>bound</code> <code>float</code> <p>Lower bound <code>a</code> for Volterra integral equation.</p> required <code>num_points</code> <code>int</code> <p>Sampled points in integral interval.</p> required <code>quad_deg</code> <code>int</code> <p>Number of quadrature.</p> required <code>kernel_func</code> <code>Callable</code> <p>Kernel func <code>K(t,s)</code>.</p> required <code>func</code> <code>Callable</code> <p><code>x(t) - f(t)</code> in Volterra integral equation.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; vol_eq = ppsci.equation.Volterra(\n...     0, 12, 20, lambda t, s: np.exp(s - t), lambda out: out[\"u\"],\n... )\n</code></pre> Source code in <code>ppsci/equation/ide/volterra.py</code> <pre><code>class Volterra(PDE):\n    r\"\"\"A second kind of volterra integral equation with Gaussian quadrature algorithm.\n\n    $$\n    x(t) - f(t)=\\int_a^t K(t, s) x(s) d s\n    $$\n\n    [Volterra integral equation](https://en.wikipedia.org/wiki/Volterra_integral_equation)\n\n    [Gaussian quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature#Change_of_interval)\n\n    Args:\n        bound (float): Lower bound `a` for Volterra integral equation.\n        num_points (int): Sampled points in integral interval.\n        quad_deg (int): Number of quadrature.\n        kernel_func (Callable): Kernel func `K(t,s)`.\n        func (Callable): `x(t) - f(t)` in Volterra integral equation.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; vol_eq = ppsci.equation.Volterra(\n        ...     0, 12, 20, lambda t, s: np.exp(s - t), lambda out: out[\"u\"],\n        ... )\n    \"\"\"\n\n    dtype = paddle.get_default_dtype()\n\n    def __init__(\n        self,\n        bound: float,\n        num_points: int,\n        quad_deg: int,\n        kernel_func: Callable,\n        func: Callable,\n    ):\n        super().__init__()\n        self.bound = bound\n        self.num_points = num_points\n        self.quad_deg = quad_deg\n        self.kernel_func = kernel_func\n        self.func = func\n\n        self.quad_x, self.quad_w = np.polynomial.legendre.leggauss(quad_deg)\n        self.quad_x = self.quad_x.astype(Volterra.dtype).reshape([-1, 1])  # [Q, 1]\n        self.quad_x = paddle.to_tensor(self.quad_x)  # [Q, 1]\n\n        self.quad_w = self.quad_w.astype(Volterra.dtype)  # [Q, ]\n\n        def compute_volterra_func(out):\n            x, u = out[\"x\"], out[\"u\"]\n            lhs = self.func(out)\n\n            int_mat = paddle.to_tensor(self._get_int_matrix(x), stop_gradient=False)\n            rhs = paddle.mm(int_mat, u)  # (N, 1)\n\n            volterra = lhs[: len(rhs)] - rhs\n            return volterra\n\n        self.add_equation(\"volterra\", compute_volterra_func)\n\n    def get_quad_points(self, t: paddle.Tensor) -&gt; paddle.Tensor:\n        \"\"\"Scale and transform quad_x from [-1, 1] to range [a, b].\n\n        reference: https://en.wikipedia.org/wiki/Gaussian_quadrature#Change_of_interval\n\n        Args:\n            t (paddle.Tensor): Tensor array of upper bounds 't' for integral.\n\n        Returns:\n            paddle.Tensor: Transformed points in desired range with shape of [N, Q].\n        \"\"\"\n        a, b = self.bound, t\n        return ((b - a) / 2) @ self.quad_x.T + (b + a) / 2\n\n    def _get_quad_weights(self, t: float) -&gt; np.ndarray:\n        \"\"\"Scale weights to range according to given t and lower bound of integral.\n\n        reference: https://en.wikipedia.org/wiki/Gaussian_quadrature#Change_of_interval\n\n        Args:\n            t (float): Array of upper bound 't' for integral.\n\n        Returns:\n            np.ndarray: Transformed weights in desired range with shape of [Q, ].\n        \"\"\"\n        a, b = self.bound, t\n        return (b - a) / 2 * self.quad_w\n\n    def _get_int_matrix(self, x: np.ndarray) -&gt; np.ndarray:\n        int_mat = np.zeros(\n            (self.num_points, self.num_points + (self.num_points * self.quad_deg)),\n            dtype=Volterra.dtype,\n        )\n        for i in range(self.num_points):\n            xi = float(x[i])\n            beg = self.num_points + self.quad_deg * i\n            end = self.num_points + self.quad_deg * (i + 1)\n            K = np.ravel(\n                self.kernel_func(np.full((self.quad_deg, 1), xi), x[beg:end].numpy())\n            )\n            int_mat[i, beg:end] = self._get_quad_weights(xi) * K\n        return int_mat\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.Volterra.get_quad_points","title":"<code>get_quad_points(t)</code>","text":"<p>Scale and transform quad_x from [-1, 1] to range [a, b].</p> <p>reference: https://en.wikipedia.org/wiki/Gaussian_quadrature#Change_of_interval</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor array of upper bounds 't' for integral.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Transformed points in desired range with shape of [N, Q].</p> Source code in <code>ppsci/equation/ide/volterra.py</code> <pre><code>def get_quad_points(self, t: paddle.Tensor) -&gt; paddle.Tensor:\n    \"\"\"Scale and transform quad_x from [-1, 1] to range [a, b].\n\n    reference: https://en.wikipedia.org/wiki/Gaussian_quadrature#Change_of_interval\n\n    Args:\n        t (paddle.Tensor): Tensor array of upper bounds 't' for integral.\n\n    Returns:\n        paddle.Tensor: Transformed points in desired range with shape of [N, Q].\n    \"\"\"\n    a, b = self.bound, t\n    return ((b - a) / 2) @ self.quad_x.T + (b + a) / 2\n</code></pre>"},{"location":"zh/api/equation/#ppsci.equation.NLSMB","title":"<code>NLSMB</code>","text":"<p>               Bases: <code>PDE</code></p> <p>Class for nonlinear Schrodinger-Maxwell-Bloch equation.</p> \\[ \\begin{cases}     \\dfrac{\\partial E}{\\partial x} = i \\alpha_1 \\dfrac{\\partial^2 E}{\\partial t ^2} - i \\alpha_2 |E|^2 E+2 p \\\\     \\dfrac{\\partial p}{\\partial t} = 2 i \\omega_0 p+2 E \\eta \\\\     \\dfrac{\\partial \\eta}{\\partial t} = -(E p^* + E^* p) \\end{cases} \\] <p>Parameters:</p> Name Type Description Default <code>alpha_1</code> <code>Union[float, str]</code> <p>Group velocity dispersion.</p> required <code>alpha_2</code> <code>Union[float, str]</code> <p>Kerr nonlinearity.</p> required <code>omega_0</code> <code>Union[float, str]</code> <p>The offset of resonance frequency.</p> required <code>time</code> <code>bool</code> <p>Whether the equation is time-dependent.</p> required <code>detach_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Keys used for detach during computing. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; pde = ppsci.equation.NLSMB(0.5, -1.0, 0.5, True)\n</code></pre> Source code in <code>ppsci/equation/pde/nls_m_b.py</code> <pre><code>class NLSMB(base.PDE):\n    r\"\"\"Class for nonlinear Schrodinger-Maxwell-Bloch equation.\n\n    $$\n    \\begin{cases}\n        \\dfrac{\\partial E}{\\partial x} = i \\alpha_1 \\dfrac{\\partial^2 E}{\\partial t ^2} - i \\alpha_2 |E|^2 E+2 p \\\\\n        \\dfrac{\\partial p}{\\partial t} = 2 i \\omega_0 p+2 E \\eta \\\\\n        \\dfrac{\\partial \\eta}{\\partial t} = -(E p^* + E^* p)\n    \\end{cases}\n    $$\n\n    Args:\n        alpha_1 (Union[float, str]): Group velocity dispersion.\n        alpha_2 (Union[float, str]): Kerr nonlinearity.\n        omega_0 (Union[float, str]): The offset of resonance frequency.\n        time (bool): Whether the equation is time-dependent.\n        detach_keys (Optional[Tuple[str, ...]]): Keys used for detach during computing.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; pde = ppsci.equation.NLSMB(0.5, -1.0, 0.5, True)\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha_1: Union[float, str],\n        alpha_2: Union[float, str],\n        omega_0: Union[float, str],\n        time: bool,\n        detach_keys: Optional[Tuple[str, ...]] = None,\n    ):\n        super().__init__()\n        self.detach_keys = detach_keys\n        self.time = time\n\n        t, x = self.create_symbols(\"t x\")\n        invars = (x,)\n        if time:\n            invars = (t,) + invars\n\n        self.alpha_1 = alpha_1\n        self.alpha_2 = alpha_2\n        self.omega_0 = omega_0\n\n        Eu = self.create_function(\"Eu\", invars)\n        Ev = self.create_function(\"Ev\", invars)\n        pu = self.create_function(\"pu\", invars)\n        pv = self.create_function(\"pv\", invars)\n        eta = self.create_function(\"eta\", invars)\n\n        pu_t = pu.diff(t)\n        pv_t = pv.diff(t)\n        eta_t = eta.diff(t)\n\n        Eu_x = Eu.diff(x)\n        Ev_x = Ev.diff(x)\n\n        Eu_tt = Eu.diff(t).diff(t)\n        Ev_tt = Ev.diff(t).diff(t)\n\n        Schrodinger_1 = (\n            alpha_1 * Eu_tt - alpha_2 * Eu * (Eu**2 + Ev**2) + 2 * pv - Ev_x\n        )\n        Schrodinger_2 = (\n            alpha_1 * Ev_tt - alpha_2 * Ev * (Eu**2 + Ev**2) - 2 * pu + Eu_x\n        )\n        Maxwell_1 = 2 * Ev * eta - pv_t + 2 * pu * omega_0\n        Maxwell_2 = -2 * Eu * eta + pu_t + 2 * pv * omega_0\n        Bloch = 2 * pv * Ev + 2 * pu * Eu + eta_t\n\n        self.add_equation(\"Schrodinger_1\", Schrodinger_1)\n        self.add_equation(\"Schrodinger_2\", Schrodinger_2)\n        self.add_equation(\"Maxwell_1\", Maxwell_1)\n        self.add_equation(\"Maxwell_2\", Maxwell_2)\n        self.add_equation(\"Bloch\", Bloch)\n\n        self._apply_detach()\n</code></pre>"},{"location":"zh/api/experimental/","title":"ppsci.experimental","text":""},{"location":"zh/api/experimental/#experimental-api","title":"Experimental(\u5b9e\u9a8c\u6027 API) \u6a21\u5757","text":"<p>Experimental</p> <p>Experimental \u6a21\u5757\u4e0b\u5747\u4e3a\u5b9e\u9a8c\u6027 API\uff0c\u5176\u7b7e\u540d\u548c\u4f4d\u7f6e\u5728\u672a\u6765\u53ef\u80fd\u53d1\u751f\u53d8\u52a8</p>"},{"location":"zh/api/experimental/#ppsci.experimental.math_module","title":"<code>ppsci.experimental.math_module</code>","text":""},{"location":"zh/api/experimental/#ppsci.experimental.math_module.bessel_i0","title":"<code>bessel_i0(x)</code>","text":"<p>Zero-order modified B\u00e9zier curve functions of the first kind.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data of the formula.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; res = ppsci.experimental.bessel_i0(paddle.to_tensor([0, 1, 2, 3, 4], dtype=\"float32\"))\n</code></pre> Source code in <code>ppsci/experimental/math_module.py</code> <pre><code>def bessel_i0(x: paddle.Tensor) -&gt; paddle.Tensor:\n    \"\"\"Zero-order modified B\u00e9zier curve functions of the first kind.\n\n    Args:\n        x (paddle.Tensor): Input data of the formula.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; res = ppsci.experimental.bessel_i0(paddle.to_tensor([0, 1, 2, 3, 4], dtype=\"float32\"))\n    \"\"\"\n    return paddle.i0(x)\n</code></pre>"},{"location":"zh/api/experimental/#ppsci.experimental.math_module.bessel_i0e","title":"<code>bessel_i0e(x)</code>","text":"<p>Exponentially scaled zero-order modified B\u00e9zier curve functions of the first kind.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data of the formula.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; res = ppsci.experimental.bessel_i0e(paddle.to_tensor([0, 1, 2, 3, 4], dtype=\"float32\"))\n</code></pre> Source code in <code>ppsci/experimental/math_module.py</code> <pre><code>def bessel_i0e(x: paddle.Tensor) -&gt; paddle.Tensor:\n    \"\"\"Exponentially scaled zero-order modified B\u00e9zier curve functions of the first kind.\n\n    Args:\n        x (paddle.Tensor): Input data of the formula.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; res = ppsci.experimental.bessel_i0e(paddle.to_tensor([0, 1, 2, 3, 4], dtype=\"float32\"))\n    \"\"\"\n    return paddle.i0e(x)\n</code></pre>"},{"location":"zh/api/experimental/#ppsci.experimental.math_module.bessel_i1","title":"<code>bessel_i1(x)</code>","text":"<p>First-order modified B\u00e9zier curve functions of the first kind.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data of the formula.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; res = ppsci.experimental.bessel_i1(paddle.to_tensor([0, 1, 2, 3, 4], dtype=\"float32\"))\n</code></pre> Source code in <code>ppsci/experimental/math_module.py</code> <pre><code>def bessel_i1(x: paddle.Tensor) -&gt; paddle.Tensor:\n    \"\"\"First-order modified B\u00e9zier curve functions of the first kind.\n\n    Args:\n        x (paddle.Tensor): Input data of the formula.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; res = ppsci.experimental.bessel_i1(paddle.to_tensor([0, 1, 2, 3, 4], dtype=\"float32\"))\n    \"\"\"\n    return paddle.i1(x)\n</code></pre>"},{"location":"zh/api/experimental/#ppsci.experimental.math_module.bessel_i1e","title":"<code>bessel_i1e(x)</code>","text":"<p>Exponentially scaled first-order modified B\u00e9zier curve functions of the first kind.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data of the formula.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; res = ppsci.experimental.bessel_i1e(paddle.to_tensor([0, 1, 2, 3, 4], dtype=\"float32\"))\n</code></pre> Source code in <code>ppsci/experimental/math_module.py</code> <pre><code>def bessel_i1e(x: paddle.Tensor) -&gt; paddle.Tensor:\n    \"\"\"Exponentially scaled first-order modified B\u00e9zier curve functions of the first kind.\n\n    Args:\n        x (paddle.Tensor): Input data of the formula.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; res = ppsci.experimental.bessel_i1e(paddle.to_tensor([0, 1, 2, 3, 4], dtype=\"float32\"))\n    \"\"\"\n    return paddle.i1e(x)\n</code></pre>"},{"location":"zh/api/experimental/#ppsci.experimental.math_module.fractional_diff","title":"<code>fractional_diff(func, alpha, a, t, h, dtype='float64')</code>","text":"<p>Compute fractional derivative of given function at point t with fractional order alpha using Caputo derivative of fractional.</p> \\[ D_t^\\alpha f(t)=\\frac{1}{\\Gamma(n-\\alpha)} \\int_0^t \\frac{f^{(n)}(s)}{(t-s)^{\\alpha+1-n}} d s . \\] \\[ s.t. 0 \\lt \\alpha \\lt 1 . \\] <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to compute the fractional derivative of.</p> required <code>alpha</code> <code>float</code> <p>Fractional order.</p> required <code>t</code> <code>float</code> <p>Point to compute the fractional derivative at.</p> required <code>a</code> <code>float</code> <p>Start point of the fractional integral.</p> required <code>h</code> <code>float</code> <p>Step size for finite difference.</p> required <code>dtype</code> <code>str</code> <p>Data dtype during computation. Defaults to \"float64\".</p> <code>'float64'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Fractional derivative result of the function at t.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ppsci.experimental import fractional_diff\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # define f(x) = x^2\n&gt;&gt;&gt; def f(x):\n...     return x * x\n&gt;&gt;&gt; # compute 0.5-order fractional derivative of f(x) at t=1.0 with step size h=1e-6\n&gt;&gt;&gt; res = fractional_diff(f, alpha=0.5, a=0, t=1.0, h=1e-6, dtype=\"float64\")\n&gt;&gt;&gt; np.testing.assert_allclose(float(res), 1.503547, 1e-6)\n</code></pre> Source code in <code>ppsci/experimental/math_module.py</code> <pre><code>def fractional_diff(\n    func: Callable, alpha: float, a: float, t: float, h: float, dtype=\"float64\"\n) -&gt; paddle.Tensor:\n    r\"\"\"Compute fractional derivative of given function at point t with fractional order\n    alpha using [Caputo derivative of fractional](https://en.wikipedia.org/wiki/Fractional_calculus#Caputo_fractional_derivative).\n\n    $$\n    D_t^\\alpha f(t)=\\frac{1}{\\Gamma(n-\\alpha)} \\int_0^t \\frac{f^{(n)}(s)}{(t-s)^{\\alpha+1-n}} d s .\n    $$\n\n    $$\n    s.t. 0 \\lt \\alpha \\lt 1 .\n    $$\n\n    Args:\n        func (Callable): Function to compute the fractional derivative of.\n        alpha (float): Fractional order.\n        t (float): Point to compute the fractional derivative at.\n        a (float): Start point of the fractional integral.\n        h (float): Step size for finite difference.\n        dtype (str, optional): Data dtype during computation. Defaults to \"float64\".\n\n    Returns:\n        paddle.Tensor: Fractional derivative result of the function at t.\n\n    Examples:\n        &gt;&gt;&gt; from ppsci.experimental import fractional_diff\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; # define f(x) = x^2\n        &gt;&gt;&gt; def f(x):\n        ...     return x * x\n        &gt;&gt;&gt; # compute 0.5-order fractional derivative of f(x) at t=1.0 with step size h=1e-6\n        &gt;&gt;&gt; res = fractional_diff(f, alpha=0.5, a=0, t=1.0, h=1e-6, dtype=\"float64\")\n        &gt;&gt;&gt; np.testing.assert_allclose(float(res), 1.503547, 1e-6)\n    \"\"\"\n\n    if not (0 &lt; alpha &lt; 1):\n        raise NotImplementedError(\n            f\"Given alpha should be in range (0, 1), but got {alpha}\"\n        )\n\n    def _finite_derivative(\n        func: Callable, x: paddle.Tensor, dx: float\n    ) -&gt; paddle.Tensor:\n        \"\"\"Compute the finite difference of a function at x using centered difference.\n\n        Args:\n            func (Callable): Function to compute the finite difference of.\n            x (paddle.Tensor): Point to compute the finite difference at.\n            dx (float): Delta to use for the finite difference.\n\n        Returns:\n            paddle.Tensor: First-order Finite difference of the function at x.\n        \"\"\"\n        return (func(x + dx) - func(x - dx)) / (2 * dx)\n\n    def int_func(s):\n        return _finite_derivative(func, s, dx=h) / (t - s) ** (alpha)\n\n    result = (\n        1.0 / paddle.exp(paddle.lgamma(paddle.to_tensor(1.0 - alpha, dtype=dtype)))\n    ) * gaussian_integrate(\n        int_func, dim=1, N=2**10 + 1, integration_domains=[[a, t]], dtype=dtype\n    )\n    return result\n</code></pre>"},{"location":"zh/api/experimental/#ppsci.experimental.math_module.gaussian_integrate","title":"<code>gaussian_integrate(fn, dim, N, integration_domains, dtype='float64')</code>","text":"<p>Integrate given function using gaussian quadrature.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[Any], Tensor]</code> <p>Function to be integrated.</p> required <code>dim</code> <code>int</code> <p>Dimensionality of the integrand.</p> required <code>N</code> <code>int</code> <p>Number of dicretization points.</p> required <code>integration_domains</code> <code>List[List[float]]</code> <p>Intergration domains.</p> required <code>dtype</code> <code>Literal['float32', 'float64']</code> <p>Dtype used during computation. Defaults to \"float64\".</p> <code>'float64'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Integral result.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci.experimental\n&gt;&gt;&gt; func = lambda x: paddle.sin(x)\n&gt;&gt;&gt; dim = 1\n&gt;&gt;&gt; N = 500\n&gt;&gt;&gt; integration_domains = [[0, np.pi]]\n&gt;&gt;&gt; result = ppsci.experimental.gaussian_integrate(func, dim, N, integration_domains)\n&gt;&gt;&gt; np.testing.assert_allclose(float(result), 2.0, 1e-6)\n&gt;&gt;&gt; print(float(result))\n1.9999999999999576\n</code></pre> Source code in <code>ppsci/experimental/math_module.py</code> <pre><code>def gaussian_integrate(\n    fn: Callable[[Any], paddle.Tensor],\n    dim: int,\n    N: int,\n    integration_domains: List[List[float]],\n    dtype: Literal[\"float32\", \"float64\"] = \"float64\",\n) -&gt; paddle.Tensor:\n    \"\"\"Integrate given function using gaussian quadrature.\n\n    Args:\n        fn (Callable[[Any], paddle.Tensor]): Function to be integrated.\n        dim (int): Dimensionality of the integrand.\n        N (int): Number of dicretization points.\n        integration_domains (List[List[float]]): Intergration domains.\n        dtype (Literal[\"float32\", \"float64\"], optional): Dtype used during computation. Defaults to \"float64\".\n\n    Returns:\n        paddle.Tensor: Integral result.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci.experimental\n        &gt;&gt;&gt; func = lambda x: paddle.sin(x)\n        &gt;&gt;&gt; dim = 1\n        &gt;&gt;&gt; N = 500\n        &gt;&gt;&gt; integration_domains = [[0, np.pi]]\n        &gt;&gt;&gt; result = ppsci.experimental.gaussian_integrate(func, dim, N, integration_domains)\n        &gt;&gt;&gt; np.testing.assert_allclose(float(result), 2.0, 1e-6)\n        &gt;&gt;&gt; print(float(result))\n        1.9999999999999576\n    \"\"\"\n\n    def _compatible_meshgrid(*args: paddle.Tensor, **kwargs: paddle.Tensor):\n        # TODO(HydrogenSulfate): paddle.meshgrid do not support single Tensor,\n        # which will be fixed in paddle framework.\n        if len(args) == 1:\n            return args\n        else:\n            return paddle.meshgrid(*args, **kwargs)\n\n    def _roots(N: int) -&gt; np.ndarray:\n        return np.polynomial.legendre.leggauss(N)[0]\n\n    def _calculate_grid(\n        N: int,\n        integration_domains: paddle.Tensor,\n    ) -&gt; Tuple[paddle.Tensor, paddle.Tensor, int]:\n        \"\"\"Calculate grid points, widths and N per dim\n\n        Args:\n            N (int): Number of points.\n            integration_domain (paddle.Tensor): Integration domain.\n\n        Returns:\n            Tuple[paddle.Tensor, paddle.Tensor, int]: Grid points, grid widths and\n                Number of grid slices per dimension.\n        \"\"\"\n        # Create grid and assemble evaluation points\n        grid_1d = []\n        _dim = integration_domains.shape[0]\n        n_per_dim = int(N ** (1.0 / _dim) + 1e-8)\n\n        # Determine for each dimension grid points and mesh width\n        def _resize_roots(\n            integration_domain: Tuple[float, float], roots: np.ndarray\n        ):  # scale from [-1,1] to [a,b]\n            a = integration_domain[0]\n            b = integration_domain[1]\n            return ((b - a) / 2) * roots + ((a + b) / 2)\n\n        for dim in range(_dim):\n            grid_1d.append(_resize_roots(integration_domains[dim], _roots(n_per_dim)))\n        h = paddle.stack([grid_1d[dim][1] - grid_1d[dim][0] for dim in range(_dim)])\n\n        # Get grid points\n        points = _compatible_meshgrid(*grid_1d)\n        points = paddle.stack([mg.reshape([-1]) for mg in points], axis=1)\n\n        return points, h, n_per_dim\n\n    def _evaluate_integrand(fn, points, weights=None, fn_args=None) -&gt; paddle.Tensor:\n        \"\"\"Evaluate the integrand function at the passed points.\n\n        Args:\n            fn (function): Integrand function.\n            points (paddle.Tensor): Integration points.\n            weights (paddle.Tensor, optional): Integration weights. Defaults to None.\n            fn_args (list or tuple, optional): Any arguments required by the function. Defaults to None.\n\n        Returns:\n            paddle.Tensor: Integral result.\n        \"\"\"\n        if fn_args is None:\n            fn_args = ()\n\n        result = fn(points, *fn_args)\n        if not str(result.dtype).endswith(dtype):\n            result = result.astype(dtype)\n\n        if result.shape[0] != points.shape[0]:\n            raise ValueError(\n                f\"The passed function was given {points.shape[0]} points but only returned {result.shape[0]} value(s).\"\n                f\"Please ensure that your function is vectorized, i.e. can be called with multiple evaluation points at once. It should return a tensor \"\n                f\"where first dimension matches length of passed elements. \"\n            )\n\n        if weights is not None:\n            if (\n                len(result.shape) &gt; 1\n            ):  # if the the integrand is multi-dimensional, we need to reshape/repeat weights so they can be broadcast in the *=\n                integrand_shape = result.shape[1:]\n                weights = paddle.repeat_interleave(\n                    paddle.unsqueeze(weights, axis=1), np.prod(integrand_shape)\n                ).reshape((weights.shape[0], *(integrand_shape)))\n            result *= weights\n\n        return result\n\n    def _weights(N, dim):\n        \"\"\"Return the weights, broadcast across the dimensions, generated from the polynomial of choice.\n\n        Args:\n            N (int): Number of nodes.\n            dim (int): Number of dimensions.\n\n        Returns:\n            paddle.Tensor: Integration weights.\n        \"\"\"\n        weights = paddle.to_tensor(np.polynomial.legendre.leggauss(N)[1], dtype=dtype)\n        return paddle.prod(\n            paddle.stack(_compatible_meshgrid(*([weights] * dim)), axis=0),\n            axis=0,\n        ).reshape([-1])\n\n    def _apply_composite_rule(cur_dim_areas, dim, hs, domain):\n        \"\"\"Apply \"composite\" rule for gaussian integrals\n\n        cur_dim_areas will contain the areas per dimension\n        \"\"\"\n        # We collapse dimension by dimension\n        for cur_dim in range(dim):\n            cur_dim_areas = (\n                0.5\n                * (domain[cur_dim][1] - domain[cur_dim][0])\n                * paddle.sum(\n                    cur_dim_areas, axis=len(cur_dim_areas.shape) - 1, dtype=dtype\n                )\n            )\n        return cur_dim_areas\n\n    @expand_func_values_and_squeeze_integral\n    def _calculate_result(\n        function_values: paddle.Tensor,\n        dim: int,\n        n_per_dim: int,\n        hs: paddle.Tensor,\n        integration_domains: paddle.Tensor,\n    ) -&gt; paddle.Tensor:\n        \"\"\"Apply the \"composite rule\" to calculate a result from the evaluated integrand.\n\n        Args:\n            function_values (paddle.Tensor): Output of the integrand.\n            dim (int): Dimensionality.\n            n_per_dim (int): Number of grid slices per dimension.\n            hs (paddle.Tensor): Distances between grid slices for each dimension.\n\n        Returns:\n            paddle.Tensor: Quadrature result.\n        \"\"\"\n        # Reshape the output to be [integrand_dim,N,N,...] points instead of [integrand_dim,dim*N] points\n        integrand_shape = function_values.shape[1:]\n        dim_shape = [n_per_dim] * dim\n        new_shape = [*integrand_shape, *dim_shape]\n\n        perm = list(range(len(function_values.shape)))\n        if len(perm) &gt;= 2:\n            perm.append(perm.pop(0))\n        reshaped_function_values = paddle.transpose(function_values, perm)\n        reshaped_function_values = reshaped_function_values.reshape(new_shape)\n\n        assert new_shape == list(\n            reshaped_function_values.shape\n        ), f\"reshaping produced shape {reshaped_function_values.shape}, expected shape was {new_shape}\"\n\n        result = _apply_composite_rule(\n            reshaped_function_values, dim, hs, integration_domains\n        )\n        return result\n\n    assert dtype in [\n        \"float32\",\n        \"float64\",\n    ], f\"dtype must be either 'float32' or 'float64', but got {dtype}\"\n\n    neg = False\n    for i, (a, b) in enumerate(integration_domains):\n        if a &gt; b:\n            neg = not neg\n            integration_domains[i] = [b, a]\n\n    integration_domains = paddle.to_tensor(\n        integration_domains,\n        dtype=dtype,\n    )\n\n    if integration_domains.shape[0] != dim:\n        raise ValueError(\n            f\"The number of integration domain({integration_domains.shape[0]}) \"\n            f\"must be equal to the given 'dim'({dim}).\"\n        )\n    if integration_domains.shape[1] != 2:\n        raise ValueError(\n            f\"integration_domain should be in format of [[a_1, b_1], [a_2, b_2], ..., \"\n            f\"[a_dim, b_dim]], but got each range of integration is {integration_domains[0]}\"\n        )\n    grid_points, hs, n_per_dim = _calculate_grid(N, integration_domains)\n\n    function_values = _evaluate_integrand(\n        fn, grid_points, weights=_weights(n_per_dim, dim)\n    )\n\n    result = _calculate_result(function_values, dim, n_per_dim, hs, integration_domains)\n    return result if (not neg) else -result\n</code></pre>"},{"location":"zh/api/experimental/#ppsci.experimental.math_module.trapezoid_integrate","title":"<code>trapezoid_integrate(y, x=None, dx=None, axis=-1, mode='sum')</code>","text":"<p>Integrate along the given axis using the composite trapezoidal rule. Use the sum method.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Tensor</code> <p>Input to be integrated.</p> required <code>x</code> <code>Tensor</code> <p>The sample points corresponding to the input samples. its shape should be (1) input.shape; (2) the input.shape[axis] if axis is not default. Defaults to None. dx (float, optional): The sample points are assumed to be evenly spaced and it is the spacing between sample points. If 'x' and 'dx' are both default, 'dx' is set to 1 by default. Defaults to None.</p> <code>None</code> <code>axis</code> <code>int</code> <p>The axis along which to integrate. Defaults to -1.</p> <code>-1</code> <code>mode</code> <code>Literal['sum', 'cumsum']</code> <p>Which type cumulative sum function used. Defaults to \"sum\".</p> <code>'sum'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Integral result. If dim of input is N, return is N-1 dim.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; y = paddle.to_tensor([[0, 1, 2], [3, 4, 5]], dtype=\"float32\")\n&gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(y)\n&gt;&gt;&gt; print(res)\nTensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [2., 8.])\n&gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(y, mode=\"cumsum\")\n&gt;&gt;&gt; print(res)\nTensor(shape=[2, 2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [[0.50000000, 2.        ],\n        [3.50000000, 8.        ]])\n&gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(\n...     y, x=paddle.to_tensor([[0, 1, 2], [3, 4, 5]], dtype=\"float32\")\n... )\n&gt;&gt;&gt; print(res)\nTensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [2., 8.])\n&gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(\n...     y, x=paddle.to_tensor([0, 1], dtype=\"float32\"), axis=0\n... )\n&gt;&gt;&gt; print(res)\nTensor(shape=[3], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [1.50000000, 2.50000000, 3.50000000])\n&gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(\n...     y, x=paddle.to_tensor([0, 1, 2], dtype=\"float32\"), axis=1\n... )\n&gt;&gt;&gt; print(res)\nTensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [2., 8.])\n&gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(y, dx=2)\n&gt;&gt;&gt; print(res)\nTensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [4. , 16.])\n</code></pre> Source code in <code>ppsci/experimental/math_module.py</code> <pre><code>def trapezoid_integrate(\n    y: paddle.Tensor,\n    x: paddle.Tensor = None,\n    dx: float = None,\n    axis: int = -1,\n    mode: Literal[\"sum\", \"cumsum\"] = \"sum\",\n) -&gt; paddle.Tensor:\n    \"\"\"\n    Integrate along the given axis using the composite trapezoidal rule. Use the sum method.\n\n    Args:\n        y (paddle.Tensor): Input to be integrated.\n        x (paddle.Tensor, optional): The sample points corresponding to the input samples. its shape should be\n            (1) input.shape; (2) the input.shape[axis] if axis is not default. Defaults to None.\n            dx (float, optional): The sample points are assumed to be evenly spaced and it is the spacing between sample points.\n            If 'x' and 'dx' are both default, 'dx' is set to 1 by default. Defaults to None.\n        axis (int, optional): The axis along which to integrate. Defaults to -1.\n        mode (Literal[\"sum\", \"cumsum\"], optional): Which type cumulative sum function used. Defaults to \"sum\".\n\n    Returns:\n        paddle.Tensor: Integral result. If dim of input is N, return is N-1 dim.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; y = paddle.to_tensor([[0, 1, 2], [3, 4, 5]], dtype=\"float32\")\n        &gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(y)\n        &gt;&gt;&gt; print(res)\n        Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [2., 8.])\n        &gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(y, mode=\"cumsum\")\n        &gt;&gt;&gt; print(res)\n        Tensor(shape=[2, 2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [[0.50000000, 2.        ],\n                [3.50000000, 8.        ]])\n        &gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(\n        ...     y, x=paddle.to_tensor([[0, 1, 2], [3, 4, 5]], dtype=\"float32\")\n        ... )\n        &gt;&gt;&gt; print(res)\n        Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [2., 8.])\n        &gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(\n        ...     y, x=paddle.to_tensor([0, 1], dtype=\"float32\"), axis=0\n        ... )\n        &gt;&gt;&gt; print(res)\n        Tensor(shape=[3], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [1.50000000, 2.50000000, 3.50000000])\n        &gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(\n        ...     y, x=paddle.to_tensor([0, 1, 2], dtype=\"float32\"), axis=1\n        ... )\n        &gt;&gt;&gt; print(res)\n        Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [2., 8.])\n        &gt;&gt;&gt; res = ppsci.experimental.trapezoid_integrate(y, dx=2)\n        &gt;&gt;&gt; print(res)\n        Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [4. , 16.])\n    \"\"\"\n    if mode == \"sum\":\n        return paddle.trapezoid(y, x, dx, axis)\n    elif mode == \"cumsum\":\n        return paddle.cumulative_trapezoid(y, x, dx, axis)\n    else:\n        raise ValueError(f'mode should be \"sum\" or \"cumsum\", but got {mode}')\n</code></pre>"},{"location":"zh/api/geometry/","title":"ppsci.geometry","text":""},{"location":"zh/api/geometry/#geometry","title":"Geometry(\u51e0\u4f55) \u6a21\u5757","text":""},{"location":"zh/api/geometry/#ppsci.geometry","title":"<code>ppsci.geometry</code>","text":""},{"location":"zh/api/geometry/#ppsci.geometry.Geometry","title":"<code>Geometry</code>","text":"<p>Base class for geometry.</p> <p>Parameters:</p> Name Type Description Default <code>ndim</code> <code>int</code> <p>Number of geometry dimension.</p> required <code>bbox</code> <code>Tuple[ndarray, ndarray]</code> <p>Bounding box of upper and lower.</p> required <code>diam</code> <code>float</code> <p>Diameter of geometry.</p> required Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>class Geometry:\n    \"\"\"Base class for geometry.\n\n    Args:\n        ndim (int): Number of geometry dimension.\n        bbox (Tuple[np.ndarray, np.ndarray]): Bounding box of upper and lower.\n        diam (float): Diameter of geometry.\n    \"\"\"\n\n    def __init__(self, ndim: int, bbox: Tuple[np.ndarray, np.ndarray], diam: float):\n        self.ndim = ndim\n        self.bbox = bbox\n        self.diam = min(diam, np.linalg.norm(bbox[1] - bbox[0]))\n\n    @property\n    def dim_keys(self):\n        return (\"x\", \"y\", \"z\")[: self.ndim]\n\n    @abc.abstractmethod\n    def is_inside(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Returns a boolean array where x is inside the geometry.\n\n        Args:\n            x (np.ndarray): Points to check if inside the geometry. The shape is [N, D],\n                where D is the number of dimension of geometry.\n\n        Returns:\n            np.ndarray: Boolean array where x is inside the geometry. The shape is [N].\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; x = np.array([[0], [0.5], [1.5]])\n            &gt;&gt;&gt; interval.is_inside(x)\n            array([ True,  True, False])\n            &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.5, 0.5], [1.5, 1.5]])\n            &gt;&gt;&gt; rectangle.is_inside(x)\n            array([ True,  True, False])\n            &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n            &gt;&gt;&gt; x = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1.5, 1.5, 1.5]])\n            &gt;&gt;&gt; cuboid.is_inside(x)\n            array([ True,  True, False])\n        \"\"\"\n\n    @abc.abstractmethod\n    def on_boundary(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Returns a boolean array where x is on geometry boundary.\n\n        Args:\n            x (np.ndarray): Points to check if on the geometry boundary. The shape is [N, D],\n                where D is the number of dimension of geometry.\n\n        Returns:\n            np.ndarray: Boolean array where x is on the geometry boundary. The shape is [N].\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; x = np.array([[0], [0.5], [1.5]])\n            &gt;&gt;&gt; interval.on_boundary(x)\n            array([ True, False, False])\n            &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; x = np.array([[0, 0], [0.5, 0.5], [1, 1.5]])\n            &gt;&gt;&gt; rectangle.on_boundary(x)\n            array([ True, False, False])\n            &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n            &gt;&gt;&gt; x = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1, 1, 1.5]])\n            &gt;&gt;&gt; cuboid.on_boundary(x)\n            array([ True, False, False])\n        \"\"\"\n\n    def boundary_normal(self, x):\n        \"\"\"Compute the unit normal at x.\"\"\"\n        raise NotImplementedError(f\"{self}.boundary_normal is not implemented\")\n\n    def uniform_points(self, n: int, boundary: bool = True) -&gt; np.ndarray:\n        \"\"\"Compute the equi-spaced points in the geometry.\n\n        Warings:\n            This function is not implemented, please use random_points instead.\n\n        Args:\n            n (int): Number of points.\n            boundary (bool): Include boundary points. Defaults to True.\n\n        Returns:\n            np.ndarray: Random points in the geometry. The shape is [N, D].\n        \"\"\"\n        logger.warning(\n            f\"{self}.uniform_points not implemented. \" f\"Use random_points instead.\"\n        )\n        return self.random_points(n)\n\n    def sample_interior(\n        self,\n        n: int,\n        random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n        criteria: Optional[Callable[..., np.ndarray]] = None,\n        evenly: bool = False,\n        compute_sdf_derivatives: bool = False,\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Sample random points in the geometry and return those meet criteria.\n\n        Args:\n            n (int): Number of points.\n            random (Literal[\"pseudo\", \"Halton\", \"LHS\"]): Random method. Defaults to \"pseudo\".\n                pseudo: Pseudo random.\n                Halton: Halton sequence.\n                LHS: Latin Hypercube Sampling.\n            criteria (Optional[Callable[..., np.ndarray]]): Criteria function. Given\n                coords from differnet dimension and return a boolean array with shape [n,].\n                Defaults to None.\n            evenly (bool): Evenly sample points. Defaults to False.\n            compute_sdf_derivatives (bool): Compute SDF derivatives. Defaults to False.\n\n        Returns:\n            Dict[str, np.ndarray]: Random points in the geometry. The shape is [N, D].\n                                   their signed distance function. The shape is [N, 1].\n                                   their derivatives of SDF(optional). The shape is [N, D].\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; np.random.seed(42)\n            &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; interval.sample_interior(2)\n            {'x': array([[0.37454012],\n                   [0.9507143 ]], dtype=float32), 'sdf': array([[0.37454012],\n                   [0.04928571]], dtype=float32)}\n            &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; rectangle.sample_interior(2, \"pseudo\", None, False, True)\n            {'x': array([[0.7319939 ],\n                   [0.15601864]], dtype=float32), 'y': array([[0.5986585 ],\n                   [0.15599452]], dtype=float32), 'sdf': array([[0.2680061 ],\n                   [0.15599453]], dtype=float32), 'sdf__x': array([[-1.0001659 ],\n                   [ 0.25868416]], dtype=float32), 'sdf__y': array([[-0.        ],\n                   [ 0.74118376]], dtype=float32)}\n            &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n            &gt;&gt;&gt; cuboid.sample_interior(2, \"pseudo\", None, True, True)\n            {'x': array([[0.],\n                   [0.]], dtype=float32), 'y': array([[0.],\n                   [0.]], dtype=float32), 'z': array([[0.],\n                   [1.]], dtype=float32), 'sdf': array([[0.],\n                   [0.]], dtype=float32), 'sdf__x': array([[0.50008297],\n                   [0.50008297]], dtype=float32), 'sdf__y': array([[0.50008297],\n                   [0.50008297]], dtype=float32), 'sdf__z': array([[ 0.50008297],\n                   [-0.49948692]], dtype=float32)}\n        \"\"\"\n        x = np.empty(shape=(n, self.ndim), dtype=paddle.get_default_dtype())\n        _size, _ntry, _nsuc = 0, 0, 0\n        while _size &lt; n:\n            if evenly:\n                points = self.uniform_points(n)\n            else:\n                if misc.typename(self) == \"TimeXGeometry\":\n                    points = self.random_points(n, random, criteria)\n                else:\n                    points = self.random_points(n, random)\n\n            if criteria is not None:\n                criteria_mask = criteria(*np.split(points, self.ndim, axis=1)).flatten()\n                points = points[criteria_mask]\n\n            if len(points) &gt; n - _size:\n                points = points[: n - _size]\n            x[_size : _size + len(points)] = points\n\n            _size += len(points)\n            _ntry += 1\n            if len(points) &gt; 0:\n                _nsuc += 1\n\n            if _ntry &gt;= 1000 and _nsuc == 0:\n                raise ValueError(\n                    \"Sample interior points failed, \"\n                    \"please check correctness of geometry and given criteria.\"\n                )\n\n        # if sdf_func added, return x_dict and sdf_dict, else, only return the x_dict\n        if hasattr(self, \"sdf_func\"):\n            sdf = -self.sdf_func(x)\n            sdf_dict = misc.convert_to_dict(sdf, (\"sdf\",))\n            sdf_derives_dict = {}\n            if compute_sdf_derivatives:\n                sdf_derives = -self.sdf_derivatives(x)\n                sdf_derives_dict = misc.convert_to_dict(\n                    sdf_derives, tuple(f\"sdf__{key}\" for key in self.dim_keys)\n                )\n        else:\n            sdf_dict = {}\n            sdf_derives_dict = {}\n        x_dict = misc.convert_to_dict(x, self.dim_keys)\n\n        return {**x_dict, **sdf_dict, **sdf_derives_dict}\n\n    def sample_boundary(\n        self,\n        n: int,\n        random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n        criteria: Optional[Callable[..., np.ndarray]] = None,\n        evenly: bool = False,\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Compute the random points in the geometry and return those meet criteria.\n\n        Args:\n            n (int): Number of points.\n            random (Literal[\"pseudo\", \"Halton\", \"LHS\"]): Random method. Defaults to \"pseudo\".\n                pseudo: Pseudo random.\n                Halton: Halton sequence.\n                LHS: Latin Hypercube Sampling.\n            criteria (Optional[Callable[..., np.ndarray]]): Criteria function. Given\n                coords from differnet dimension and return a boolean array with shape [n,].\n                Defaults to None.\n            evenly (bool): Evenly sample points. Defaults to False.\n\n        Returns:\n            Dict[str, np.ndarray]: Random points in the geometry. The shape is [N, D].\n                                   their normal vectors. The shape is [N, D].\n                                   their area. The shape is [N, 1].(only if the geometry is a mesh)\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; np.random.seed(42)\n            &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; interval.sample_boundary(2)\n            {'x': array([[0.],\n                   [1.]], dtype=float32), 'normal_x': array([[-1.],\n                   [ 1.]], dtype=float32)}\n            &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; rectangle.sample_boundary(2)\n            {'x': array([[1.],\n                   [0.]], dtype=float32), 'y': array([[0.49816048],\n                   [0.19714284]], dtype=float32), 'normal_x': array([[ 1.],\n                   [-1.]], dtype=float32), 'normal_y': array([[0.],\n                   [0.]], dtype=float32)}\n            &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n            &gt;&gt;&gt; cuboid.sample_boundary(2)\n            {'x': array([[0.83244264],\n                   [0.18182497]], dtype=float32), 'y': array([[0.21233912],\n                   [0.1834045 ]], dtype=float32), 'z': array([[0.],\n                   [1.]], dtype=float32), 'normal_x': array([[0.],\n                   [0.]], dtype=float32), 'normal_y': array([[0.],\n                   [0.]], dtype=float32), 'normal_z': array([[-1.],\n                   [ 1.]], dtype=float32)}\n        \"\"\"\n        x = np.empty(shape=(n, self.ndim), dtype=paddle.get_default_dtype())\n        _size, _ntry, _nsuc = 0, 0, 0\n        while _size &lt; n:\n            if evenly:\n                if (\n                    misc.typename(self) == \"TimeXGeometry\"\n                    and misc.typename(self.geometry) == \"Mesh\"\n                ):\n                    points, normal, area = self.uniform_boundary_points(n)\n                else:\n                    points = self.uniform_boundary_points(n)\n            else:\n                if (\n                    misc.typename(self) == \"TimeXGeometry\"\n                    and misc.typename(self.geometry) == \"Mesh\"\n                ):\n                    points, normal, area = self.random_boundary_points(n, random)\n                else:\n                    if misc.typename(self) == \"TimeXGeometry\":\n                        points = self.random_boundary_points(n, random, criteria)\n                    else:\n                        points = self.random_boundary_points(n, random)\n\n            if criteria is not None:\n                criteria_mask = criteria(*np.split(points, self.ndim, axis=1)).flatten()\n                points = points[criteria_mask]\n\n            if len(points) &gt; n - _size:\n                points = points[: n - _size]\n            x[_size : _size + len(points)] = points\n\n            _size += len(points)\n            _ntry += 1\n            if len(points) &gt; 0:\n                _nsuc += 1\n\n            if _ntry &gt;= 10000 and _nsuc == 0:\n                raise ValueError(\n                    \"Sample boundary points failed, \"\n                    \"please check correctness of geometry and given criteria.\"\n                )\n\n        if not (\n            misc.typename(self) == \"TimeXGeometry\"\n            and misc.typename(self.geometry) == \"Mesh\"\n        ):\n            normal = self.boundary_normal(x)\n\n        normal_dict = misc.convert_to_dict(\n            normal[:, 1:] if \"t\" in self.dim_keys else normal,\n            [f\"normal_{key}\" for key in self.dim_keys if key != \"t\"],\n        )\n        x_dict = misc.convert_to_dict(x, self.dim_keys)\n        if (\n            misc.typename(self) == \"TimeXGeometry\"\n            and misc.typename(self.geometry) == \"Mesh\"\n        ):\n            area_dict = misc.convert_to_dict(area[:, 1:], [\"area\"])\n            return {**x_dict, **normal_dict, **area_dict}\n\n        return {**x_dict, **normal_dict}\n\n    @abc.abstractmethod\n    def random_points(\n        self, n: int, random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\"\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the random points in the geometry.\n\n        Args:\n            n (int): Number of points.\n            random (Literal[\"pseudo\", \"Halton\", \"LHS\"]): Random method. Defaults to \"pseudo\".\n                pseudo: Pseudo random.\n                Halton: Halton sequence.\n                LHS: Latin Hypercube Sampling.\n\n        Returns:\n            np.ndarray: Random points in the geometry. The shape is [N, D].\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; np.random.seed(42)\n            &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; interval.random_points(2)\n            array([[0.37454012],\n                   [0.9507143 ]], dtype=float32)\n            &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; rectangle.random_points(2)\n            array([[0.7319939 , 0.5986585 ],\n                   [0.15601864, 0.15599452]], dtype=float32)\n            &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n            &gt;&gt;&gt; cuboid.random_points(2)\n            array([[0.05808361, 0.8661761 , 0.601115  ],\n                   [0.7080726 , 0.02058449, 0.96990985]], dtype=float32)\n        \"\"\"\n\n    def uniform_boundary_points(self, n: int) -&gt; np.ndarray:\n        \"\"\"Compute the equi-spaced points on the boundary(not implemented).\n\n        Warings:\n            This function is not implemented, please use random_boundary_points instead.\n\n        Args:\n            n (int): Number of points.\n\n        Returns:\n            np.ndarray: Random points on the boundary. The shape is [N, D].\n        \"\"\"\n        logger.warning(\n            f\"{self}.uniform_boundary_points not implemented. \"\n            f\"Use random_boundary_points instead.\"\n        )\n        return self.random_boundary_points(n)\n\n    @abc.abstractmethod\n    def random_boundary_points(\n        self, n: int, random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\"\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the random points on the boundary.\n\n        Args:\n            n (int): Number of points.\n            random (Literal[\"pseudo\", \"Halton\", \"LHS\"]): Random method. Defaults to \"pseudo\".\n                pseudo: Pseudo random.\n                Halton: Halton sequence.\n                LHS: Latin Hypercube Sampling.\n\n        Returns:\n            np.ndarray: Random points on the boundary. The shape is [N, D].\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; np.random.seed(42)\n            &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; interval.random_boundary_points(2)\n            array([[0.],\n                   [1.]], dtype=float32)\n            &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; rectangle.random_boundary_points(2)\n            array([[1.        , 0.49816048],\n                   [0.        , 0.19714284]], dtype=float32)\n            &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n            &gt;&gt;&gt; cuboid.random_boundary_points(2)\n            array([[0.83244264, 0.21233912, 0.        ],\n                   [0.18182497, 0.1834045 , 1.        ]], dtype=float32)\n        \"\"\"\n\n    def periodic_point(self, x: np.ndarray, component: int):\n        \"\"\"Compute the periodic image of x(not implemented).\n\n        Warings:\n            This function is not implemented.\n        \"\"\"\n        raise NotImplementedError(f\"{self}.periodic_point to be implemented\")\n\n    def sdf_derivatives(self, x: np.ndarray, epsilon: float = 1e-4) -&gt; np.ndarray:\n        \"\"\"Compute derivatives of SDF function.\n\n        Args:\n            x (np.ndarray): Points for computing SDF derivatives using central\n                difference. The shape is [N, D], D is the number of dimension of\n                geometry.\n            epsilon (float): Derivative step. Defaults to 1e-4.\n\n        Returns:\n            np.ndarray: Derivatives of corresponding SDF function.\n                The shape is [N, D]. D is the number of dimension of geometry.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; x = np.array([[0], [0.5], [1.5]])\n            &gt;&gt;&gt; interval.sdf_derivatives(x)\n            array([[-1.],\n                   [ 0.],\n                   [ 1.]])\n            &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.5, 0.5], [1.5, 1.5]])\n            &gt;&gt;&gt; rectangle.sdf_derivatives(x)\n            array([[-0.5       , -0.5       ],\n                   [ 0.        ,  0.        ],\n                   [ 0.70710678,  0.70710678]])\n            &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n            &gt;&gt;&gt; x = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1, 1, 1]])\n            &gt;&gt;&gt; cuboid.sdf_derivatives(x)\n            array([[-0.5, -0.5, -0.5],\n                   [ 0. ,  0. ,  0. ],\n                   [ 0.5,  0.5,  0.5]])\n        \"\"\"\n        if not hasattr(self, \"sdf_func\"):\n            raise NotImplementedError(\n                f\"{misc.typename(self)}.sdf_func should be implemented \"\n                \"when using 'sdf_derivatives'.\"\n            )\n        # Only compute sdf derivatives for those already implement `sdf_func` method.\n        sdf_derives = np.empty_like(x)\n        for i in range(self.ndim):\n            h = np.zeros_like(x)\n            h[:, i] += epsilon / 2\n            derives_at_i = (self.sdf_func(x + h) - self.sdf_func(x - h)) / epsilon\n            sdf_derives[:, i : i + 1] = derives_at_i\n        return sdf_derives\n\n    def union(self, other: \"Geometry\") -&gt; \"Geometry\":\n        \"\"\"CSG Union.\n\n        Args:\n            other (Geometry): The other geometry.\n\n        Returns:\n            Geometry: The union of two geometries.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n            &gt;&gt;&gt; union = interval1.union(interval2)\n            &gt;&gt;&gt; union.bbox\n            (array([[0.]]), array([[1.5]]))\n            &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0, 0), (2, 3))\n            &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0, 0), (3, 2))\n            &gt;&gt;&gt; union = rectangle1.union(rectangle2)\n            &gt;&gt;&gt; union.bbox\n            (array([0., 0.], dtype=float32), array([3., 3.], dtype=float32))\n            &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n            &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n            &gt;&gt;&gt; union = cuboid1 | cuboid2\n            &gt;&gt;&gt; union.bbox\n            (array([0., 0., 0.], dtype=float32), array([2., 2., 2.], dtype=float32))\n        \"\"\"\n        from ppsci.geometry import csg\n\n        return csg.CSGUnion(self, other)\n\n    def __or__(self, other: \"Geometry\") -&gt; \"Geometry\":\n        \"\"\"CSG Union.\n\n        Args:\n            other (Geometry): The other geometry.\n\n        Returns:\n            Geometry: The union of two geometries.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n            &gt;&gt;&gt; union = interval1.__or__(interval2)\n            &gt;&gt;&gt; union.bbox\n            (array([[0.]]), array([[1.5]]))\n            &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0, 0), (2, 3))\n            &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0, 0), (3, 2))\n            &gt;&gt;&gt; union = rectangle1.__or__(rectangle2)\n            &gt;&gt;&gt; union.bbox\n            (array([0., 0.], dtype=float32), array([3., 3.], dtype=float32))\n            &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n            &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n            &gt;&gt;&gt; union = cuboid1 | cuboid2\n            &gt;&gt;&gt; union.bbox\n            (array([0., 0., 0.], dtype=float32), array([2., 2., 2.], dtype=float32))\n        \"\"\"\n        from ppsci.geometry import csg\n\n        return csg.CSGUnion(self, other)\n\n    def difference(self, other: \"Geometry\") -&gt; \"Geometry\":\n        \"\"\"CSG Difference.\n\n        Args:\n            other (Geometry): The other geometry.\n\n        Returns:\n            Geometry: The difference of two geometries.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 2.0)\n            &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(1.0, 3.0)\n            &gt;&gt;&gt; difference = interval1.difference(interval2)\n            &gt;&gt;&gt; difference.bbox\n            (array([[0.]]), array([[2.]]))\n            &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n            &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((1.0, 1.0), (2.0, 2.0))\n            &gt;&gt;&gt; difference = rectangle1.difference(rectangle2)\n            &gt;&gt;&gt; difference.bbox\n            (array([0., 0.], dtype=float32), array([2., 3.], dtype=float32))\n            &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n            &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n            &gt;&gt;&gt; difference = cuboid1 - cuboid2\n            &gt;&gt;&gt; difference.bbox\n            (array([0., 0., 0.], dtype=float32), array([1., 2., 2.], dtype=float32))\n        \"\"\"\n        from ppsci.geometry import csg\n\n        return csg.CSGDifference(self, other)\n\n    def __sub__(self, other: \"Geometry\") -&gt; \"Geometry\":\n        \"\"\"CSG Difference.\n\n        Args:\n            other (Geometry): The other geometry.\n\n        Returns:\n            Geometry: The difference of two geometries.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 2.0)\n            &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(1.0, 3.0)\n            &gt;&gt;&gt; difference = interval1.__sub__(interval2)\n            &gt;&gt;&gt; difference.bbox\n            (array([[0.]]), array([[2.]]))\n            &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n            &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((1.0, 1.0), (2.0, 2.0))\n            &gt;&gt;&gt; difference = rectangle1.__sub__(rectangle2)\n            &gt;&gt;&gt; difference.bbox\n            (array([0., 0.], dtype=float32), array([2., 3.], dtype=float32))\n            &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n            &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n            &gt;&gt;&gt; difference = cuboid1 - cuboid2\n            &gt;&gt;&gt; difference.bbox\n            (array([0., 0., 0.], dtype=float32), array([1., 2., 2.], dtype=float32))\n        \"\"\"\n        from ppsci.geometry import csg\n\n        return csg.CSGDifference(self, other)\n\n    def intersection(self, other: \"Geometry\") -&gt; \"Geometry\":\n        \"\"\"CSG Intersection.\n\n        Args:\n            other (Geometry): The other geometry.\n\n        Returns:\n            Geometry: The intersection of two geometries.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 1.0)\n            &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n            &gt;&gt;&gt; intersection = interval1.intersection(interval2)\n            &gt;&gt;&gt; intersection.bbox\n            (array([[0.5]]), array([[1.]]))\n            &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n            &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0.0, 0.0), (3.0, 2.0))\n            &gt;&gt;&gt; intersection = rectangle1.intersection(rectangle2)\n            &gt;&gt;&gt; intersection.bbox\n            (array([0., 0.], dtype=float32), array([2., 2.], dtype=float32))\n            &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n            &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n            &gt;&gt;&gt; intersection = cuboid1 &amp; cuboid2\n            &gt;&gt;&gt; intersection.bbox\n            (array([0., 0., 0.], dtype=float32), array([1., 1., 1.], dtype=float32))\n        \"\"\"\n        from ppsci.geometry import csg\n\n        return csg.CSGIntersection(self, other)\n\n    def __and__(self, other: \"Geometry\") -&gt; \"Geometry\":\n        \"\"\"CSG Intersection.\n\n        Args:\n            other (Geometry): The other geometry.\n\n        Returns:\n            Geometry: The intersection of two geometries.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 1.0)\n            &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n            &gt;&gt;&gt; intersection = interval1.__and__(interval2)\n            &gt;&gt;&gt; intersection.bbox\n            (array([[0.5]]), array([[1.]]))\n            &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n            &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0.0, 0.0), (3.0, 2.0))\n            &gt;&gt;&gt; intersection = rectangle1.__and__(rectangle2)\n            &gt;&gt;&gt; intersection.bbox\n            (array([0., 0.], dtype=float32), array([2., 2.], dtype=float32))\n            &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n            &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n            &gt;&gt;&gt; intersection = cuboid1 &amp; cuboid2\n            &gt;&gt;&gt; intersection.bbox\n            (array([0., 0., 0.], dtype=float32), array([1., 1., 1.], dtype=float32))\n        \"\"\"\n        from ppsci.geometry import csg\n\n        return csg.CSGIntersection(self, other)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the name of class.\n\n        Returns:\n            str: Meta information of geometry.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n            &gt;&gt;&gt; interval.__str__()\n            \"Interval, ndim = 1, bbox = (array([[0]]), array([[1]])), diam = 1, dim_keys = ('x',)\"\n            &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; rectangle.__str__()\n            \"Rectangle, ndim = 2, bbox = (array([0., 0.], dtype=float32), array([1., 1.], dtype=float32)), diam = 1.4142135381698608, dim_keys = ('x', 'y')\"\n            &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n            &gt;&gt;&gt; cuboid.__str__()\n            \"Cuboid, ndim = 3, bbox = (array([0., 0., 0.], dtype=float32), array([1., 1., 1.], dtype=float32)), diam = 1.7320507764816284, dim_keys = ('x', 'y', 'z')\"\n        \"\"\"\n        return \", \".join(\n            [\n                self.__class__.__name__,\n                f\"ndim = {self.ndim}\",\n                f\"bbox = {self.bbox}\",\n                f\"diam = {self.diam}\",\n                f\"dim_keys = {self.dim_keys}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.__and__","title":"<code>__and__(other)</code>","text":"<p>CSG Intersection.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Geometry</code> <p>The other geometry.</p> required <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The intersection of two geometries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 1.0)\n&gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n&gt;&gt;&gt; intersection = interval1.__and__(interval2)\n&gt;&gt;&gt; intersection.bbox\n(array([[0.5]]), array([[1.]]))\n&gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n&gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0.0, 0.0), (3.0, 2.0))\n&gt;&gt;&gt; intersection = rectangle1.__and__(rectangle2)\n&gt;&gt;&gt; intersection.bbox\n(array([0., 0.], dtype=float32), array([2., 2.], dtype=float32))\n&gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n&gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n&gt;&gt;&gt; intersection = cuboid1 &amp; cuboid2\n&gt;&gt;&gt; intersection.bbox\n(array([0., 0., 0.], dtype=float32), array([1., 1., 1.], dtype=float32))\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def __and__(self, other: \"Geometry\") -&gt; \"Geometry\":\n    \"\"\"CSG Intersection.\n\n    Args:\n        other (Geometry): The other geometry.\n\n    Returns:\n        Geometry: The intersection of two geometries.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 1.0)\n        &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n        &gt;&gt;&gt; intersection = interval1.__and__(interval2)\n        &gt;&gt;&gt; intersection.bbox\n        (array([[0.5]]), array([[1.]]))\n        &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n        &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0.0, 0.0), (3.0, 2.0))\n        &gt;&gt;&gt; intersection = rectangle1.__and__(rectangle2)\n        &gt;&gt;&gt; intersection.bbox\n        (array([0., 0.], dtype=float32), array([2., 2.], dtype=float32))\n        &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n        &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n        &gt;&gt;&gt; intersection = cuboid1 &amp; cuboid2\n        &gt;&gt;&gt; intersection.bbox\n        (array([0., 0., 0.], dtype=float32), array([1., 1., 1.], dtype=float32))\n    \"\"\"\n    from ppsci.geometry import csg\n\n    return csg.CSGIntersection(self, other)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.__or__","title":"<code>__or__(other)</code>","text":"<p>CSG Union.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Geometry</code> <p>The other geometry.</p> required <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The union of two geometries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n&gt;&gt;&gt; union = interval1.__or__(interval2)\n&gt;&gt;&gt; union.bbox\n(array([[0.]]), array([[1.5]]))\n&gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0, 0), (2, 3))\n&gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0, 0), (3, 2))\n&gt;&gt;&gt; union = rectangle1.__or__(rectangle2)\n&gt;&gt;&gt; union.bbox\n(array([0., 0.], dtype=float32), array([3., 3.], dtype=float32))\n&gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n&gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n&gt;&gt;&gt; union = cuboid1 | cuboid2\n&gt;&gt;&gt; union.bbox\n(array([0., 0., 0.], dtype=float32), array([2., 2., 2.], dtype=float32))\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def __or__(self, other: \"Geometry\") -&gt; \"Geometry\":\n    \"\"\"CSG Union.\n\n    Args:\n        other (Geometry): The other geometry.\n\n    Returns:\n        Geometry: The union of two geometries.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n        &gt;&gt;&gt; union = interval1.__or__(interval2)\n        &gt;&gt;&gt; union.bbox\n        (array([[0.]]), array([[1.5]]))\n        &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0, 0), (2, 3))\n        &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0, 0), (3, 2))\n        &gt;&gt;&gt; union = rectangle1.__or__(rectangle2)\n        &gt;&gt;&gt; union.bbox\n        (array([0., 0.], dtype=float32), array([3., 3.], dtype=float32))\n        &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n        &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n        &gt;&gt;&gt; union = cuboid1 | cuboid2\n        &gt;&gt;&gt; union.bbox\n        (array([0., 0., 0.], dtype=float32), array([2., 2., 2.], dtype=float32))\n    \"\"\"\n    from ppsci.geometry import csg\n\n    return csg.CSGUnion(self, other)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.__str__","title":"<code>__str__()</code>","text":"<p>Return the name of class.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Meta information of geometry.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; interval.__str__()\n\"Interval, ndim = 1, bbox = (array([[0]]), array([[1]])), diam = 1, dim_keys = ('x',)\"\n&gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; rectangle.__str__()\n\"Rectangle, ndim = 2, bbox = (array([0., 0.], dtype=float32), array([1., 1.], dtype=float32)), diam = 1.4142135381698608, dim_keys = ('x', 'y')\"\n&gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n&gt;&gt;&gt; cuboid.__str__()\n\"Cuboid, ndim = 3, bbox = (array([0., 0., 0.], dtype=float32), array([1., 1., 1.], dtype=float32)), diam = 1.7320507764816284, dim_keys = ('x', 'y', 'z')\"\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the name of class.\n\n    Returns:\n        str: Meta information of geometry.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; interval.__str__()\n        \"Interval, ndim = 1, bbox = (array([[0]]), array([[1]])), diam = 1, dim_keys = ('x',)\"\n        &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; rectangle.__str__()\n        \"Rectangle, ndim = 2, bbox = (array([0., 0.], dtype=float32), array([1., 1.], dtype=float32)), diam = 1.4142135381698608, dim_keys = ('x', 'y')\"\n        &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n        &gt;&gt;&gt; cuboid.__str__()\n        \"Cuboid, ndim = 3, bbox = (array([0., 0., 0.], dtype=float32), array([1., 1., 1.], dtype=float32)), diam = 1.7320507764816284, dim_keys = ('x', 'y', 'z')\"\n    \"\"\"\n    return \", \".join(\n        [\n            self.__class__.__name__,\n            f\"ndim = {self.ndim}\",\n            f\"bbox = {self.bbox}\",\n            f\"diam = {self.diam}\",\n            f\"dim_keys = {self.dim_keys}\",\n        ]\n    )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.__sub__","title":"<code>__sub__(other)</code>","text":"<p>CSG Difference.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Geometry</code> <p>The other geometry.</p> required <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The difference of two geometries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 2.0)\n&gt;&gt;&gt; interval2 = ppsci.geometry.Interval(1.0, 3.0)\n&gt;&gt;&gt; difference = interval1.__sub__(interval2)\n&gt;&gt;&gt; difference.bbox\n(array([[0.]]), array([[2.]]))\n&gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n&gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((1.0, 1.0), (2.0, 2.0))\n&gt;&gt;&gt; difference = rectangle1.__sub__(rectangle2)\n&gt;&gt;&gt; difference.bbox\n(array([0., 0.], dtype=float32), array([2., 3.], dtype=float32))\n&gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n&gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n&gt;&gt;&gt; difference = cuboid1 - cuboid2\n&gt;&gt;&gt; difference.bbox\n(array([0., 0., 0.], dtype=float32), array([1., 2., 2.], dtype=float32))\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def __sub__(self, other: \"Geometry\") -&gt; \"Geometry\":\n    \"\"\"CSG Difference.\n\n    Args:\n        other (Geometry): The other geometry.\n\n    Returns:\n        Geometry: The difference of two geometries.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 2.0)\n        &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(1.0, 3.0)\n        &gt;&gt;&gt; difference = interval1.__sub__(interval2)\n        &gt;&gt;&gt; difference.bbox\n        (array([[0.]]), array([[2.]]))\n        &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n        &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((1.0, 1.0), (2.0, 2.0))\n        &gt;&gt;&gt; difference = rectangle1.__sub__(rectangle2)\n        &gt;&gt;&gt; difference.bbox\n        (array([0., 0.], dtype=float32), array([2., 3.], dtype=float32))\n        &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n        &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n        &gt;&gt;&gt; difference = cuboid1 - cuboid2\n        &gt;&gt;&gt; difference.bbox\n        (array([0., 0., 0.], dtype=float32), array([1., 2., 2.], dtype=float32))\n    \"\"\"\n    from ppsci.geometry import csg\n\n    return csg.CSGDifference(self, other)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.boundary_normal","title":"<code>boundary_normal(x)</code>","text":"<p>Compute the unit normal at x.</p> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def boundary_normal(self, x):\n    \"\"\"Compute the unit normal at x.\"\"\"\n    raise NotImplementedError(f\"{self}.boundary_normal is not implemented\")\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.difference","title":"<code>difference(other)</code>","text":"<p>CSG Difference.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Geometry</code> <p>The other geometry.</p> required <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The difference of two geometries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 2.0)\n&gt;&gt;&gt; interval2 = ppsci.geometry.Interval(1.0, 3.0)\n&gt;&gt;&gt; difference = interval1.difference(interval2)\n&gt;&gt;&gt; difference.bbox\n(array([[0.]]), array([[2.]]))\n&gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n&gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((1.0, 1.0), (2.0, 2.0))\n&gt;&gt;&gt; difference = rectangle1.difference(rectangle2)\n&gt;&gt;&gt; difference.bbox\n(array([0., 0.], dtype=float32), array([2., 3.], dtype=float32))\n&gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n&gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n&gt;&gt;&gt; difference = cuboid1 - cuboid2\n&gt;&gt;&gt; difference.bbox\n(array([0., 0., 0.], dtype=float32), array([1., 2., 2.], dtype=float32))\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def difference(self, other: \"Geometry\") -&gt; \"Geometry\":\n    \"\"\"CSG Difference.\n\n    Args:\n        other (Geometry): The other geometry.\n\n    Returns:\n        Geometry: The difference of two geometries.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 2.0)\n        &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(1.0, 3.0)\n        &gt;&gt;&gt; difference = interval1.difference(interval2)\n        &gt;&gt;&gt; difference.bbox\n        (array([[0.]]), array([[2.]]))\n        &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n        &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((1.0, 1.0), (2.0, 2.0))\n        &gt;&gt;&gt; difference = rectangle1.difference(rectangle2)\n        &gt;&gt;&gt; difference.bbox\n        (array([0., 0.], dtype=float32), array([2., 3.], dtype=float32))\n        &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n        &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n        &gt;&gt;&gt; difference = cuboid1 - cuboid2\n        &gt;&gt;&gt; difference.bbox\n        (array([0., 0., 0.], dtype=float32), array([1., 2., 2.], dtype=float32))\n    \"\"\"\n    from ppsci.geometry import csg\n\n    return csg.CSGDifference(self, other)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.intersection","title":"<code>intersection(other)</code>","text":"<p>CSG Intersection.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Geometry</code> <p>The other geometry.</p> required <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The intersection of two geometries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 1.0)\n&gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n&gt;&gt;&gt; intersection = interval1.intersection(interval2)\n&gt;&gt;&gt; intersection.bbox\n(array([[0.5]]), array([[1.]]))\n&gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n&gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0.0, 0.0), (3.0, 2.0))\n&gt;&gt;&gt; intersection = rectangle1.intersection(rectangle2)\n&gt;&gt;&gt; intersection.bbox\n(array([0., 0.], dtype=float32), array([2., 2.], dtype=float32))\n&gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n&gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n&gt;&gt;&gt; intersection = cuboid1 &amp; cuboid2\n&gt;&gt;&gt; intersection.bbox\n(array([0., 0., 0.], dtype=float32), array([1., 1., 1.], dtype=float32))\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def intersection(self, other: \"Geometry\") -&gt; \"Geometry\":\n    \"\"\"CSG Intersection.\n\n    Args:\n        other (Geometry): The other geometry.\n\n    Returns:\n        Geometry: The intersection of two geometries.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0.0, 1.0)\n        &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n        &gt;&gt;&gt; intersection = interval1.intersection(interval2)\n        &gt;&gt;&gt; intersection.bbox\n        (array([[0.5]]), array([[1.]]))\n        &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0.0, 0.0), (2.0, 3.0))\n        &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0.0, 0.0), (3.0, 2.0))\n        &gt;&gt;&gt; intersection = rectangle1.intersection(rectangle2)\n        &gt;&gt;&gt; intersection.bbox\n        (array([0., 0.], dtype=float32), array([2., 2.], dtype=float32))\n        &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n        &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n        &gt;&gt;&gt; intersection = cuboid1 &amp; cuboid2\n        &gt;&gt;&gt; intersection.bbox\n        (array([0., 0., 0.], dtype=float32), array([1., 1., 1.], dtype=float32))\n    \"\"\"\n    from ppsci.geometry import csg\n\n    return csg.CSGIntersection(self, other)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.is_inside","title":"<code>is_inside(x)</code>  <code>abstractmethod</code>","text":"<p>Returns a boolean array where x is inside the geometry.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Points to check if inside the geometry. The shape is [N, D], where D is the number of dimension of geometry.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Boolean array where x is inside the geometry. The shape is [N].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; x = np.array([[0], [0.5], [1.5]])\n&gt;&gt;&gt; interval.is_inside(x)\narray([ True,  True, False])\n&gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.5, 0.5], [1.5, 1.5]])\n&gt;&gt;&gt; rectangle.is_inside(x)\narray([ True,  True, False])\n&gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n&gt;&gt;&gt; x = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1.5, 1.5, 1.5]])\n&gt;&gt;&gt; cuboid.is_inside(x)\narray([ True,  True, False])\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>@abc.abstractmethod\ndef is_inside(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Returns a boolean array where x is inside the geometry.\n\n    Args:\n        x (np.ndarray): Points to check if inside the geometry. The shape is [N, D],\n            where D is the number of dimension of geometry.\n\n    Returns:\n        np.ndarray: Boolean array where x is inside the geometry. The shape is [N].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; x = np.array([[0], [0.5], [1.5]])\n        &gt;&gt;&gt; interval.is_inside(x)\n        array([ True,  True, False])\n        &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.5, 0.5], [1.5, 1.5]])\n        &gt;&gt;&gt; rectangle.is_inside(x)\n        array([ True,  True, False])\n        &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n        &gt;&gt;&gt; x = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1.5, 1.5, 1.5]])\n        &gt;&gt;&gt; cuboid.is_inside(x)\n        array([ True,  True, False])\n    \"\"\"\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.on_boundary","title":"<code>on_boundary(x)</code>  <code>abstractmethod</code>","text":"<p>Returns a boolean array where x is on geometry boundary.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Points to check if on the geometry boundary. The shape is [N, D], where D is the number of dimension of geometry.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Boolean array where x is on the geometry boundary. The shape is [N].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; x = np.array([[0], [0.5], [1.5]])\n&gt;&gt;&gt; interval.on_boundary(x)\narray([ True, False, False])\n&gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; x = np.array([[0, 0], [0.5, 0.5], [1, 1.5]])\n&gt;&gt;&gt; rectangle.on_boundary(x)\narray([ True, False, False])\n&gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n&gt;&gt;&gt; x = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1, 1, 1.5]])\n&gt;&gt;&gt; cuboid.on_boundary(x)\narray([ True, False, False])\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>@abc.abstractmethod\ndef on_boundary(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Returns a boolean array where x is on geometry boundary.\n\n    Args:\n        x (np.ndarray): Points to check if on the geometry boundary. The shape is [N, D],\n            where D is the number of dimension of geometry.\n\n    Returns:\n        np.ndarray: Boolean array where x is on the geometry boundary. The shape is [N].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; x = np.array([[0], [0.5], [1.5]])\n        &gt;&gt;&gt; interval.on_boundary(x)\n        array([ True, False, False])\n        &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; x = np.array([[0, 0], [0.5, 0.5], [1, 1.5]])\n        &gt;&gt;&gt; rectangle.on_boundary(x)\n        array([ True, False, False])\n        &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n        &gt;&gt;&gt; x = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1, 1, 1.5]])\n        &gt;&gt;&gt; cuboid.on_boundary(x)\n        array([ True, False, False])\n    \"\"\"\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.periodic_point","title":"<code>periodic_point(x, component)</code>","text":"<p>Compute the periodic image of x(not implemented).</p> Warings <p>This function is not implemented.</p> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def periodic_point(self, x: np.ndarray, component: int):\n    \"\"\"Compute the periodic image of x(not implemented).\n\n    Warings:\n        This function is not implemented.\n    \"\"\"\n    raise NotImplementedError(f\"{self}.periodic_point to be implemented\")\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.random_boundary_points","title":"<code>random_boundary_points(n, random='pseudo')</code>  <code>abstractmethod</code>","text":"<p>Compute the random points on the boundary.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method. Defaults to \"pseudo\". pseudo: Pseudo random. Halton: Halton sequence. LHS: Latin Hypercube Sampling.</p> <code>'pseudo'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Random points on the boundary. The shape is [N, D].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; interval.random_boundary_points(2)\narray([[0.],\n       [1.]], dtype=float32)\n&gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; rectangle.random_boundary_points(2)\narray([[1.        , 0.49816048],\n       [0.        , 0.19714284]], dtype=float32)\n&gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n&gt;&gt;&gt; cuboid.random_boundary_points(2)\narray([[0.83244264, 0.21233912, 0.        ],\n       [0.18182497, 0.1834045 , 1.        ]], dtype=float32)\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>@abc.abstractmethod\ndef random_boundary_points(\n    self, n: int, random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\"\n) -&gt; np.ndarray:\n    \"\"\"Compute the random points on the boundary.\n\n    Args:\n        n (int): Number of points.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"]): Random method. Defaults to \"pseudo\".\n            pseudo: Pseudo random.\n            Halton: Halton sequence.\n            LHS: Latin Hypercube Sampling.\n\n    Returns:\n        np.ndarray: Random points on the boundary. The shape is [N, D].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; interval.random_boundary_points(2)\n        array([[0.],\n               [1.]], dtype=float32)\n        &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; rectangle.random_boundary_points(2)\n        array([[1.        , 0.49816048],\n               [0.        , 0.19714284]], dtype=float32)\n        &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n        &gt;&gt;&gt; cuboid.random_boundary_points(2)\n        array([[0.83244264, 0.21233912, 0.        ],\n               [0.18182497, 0.1834045 , 1.        ]], dtype=float32)\n    \"\"\"\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.random_points","title":"<code>random_points(n, random='pseudo')</code>  <code>abstractmethod</code>","text":"<p>Compute the random points in the geometry.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method. Defaults to \"pseudo\". pseudo: Pseudo random. Halton: Halton sequence. LHS: Latin Hypercube Sampling.</p> <code>'pseudo'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Random points in the geometry. The shape is [N, D].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; interval.random_points(2)\narray([[0.37454012],\n       [0.9507143 ]], dtype=float32)\n&gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; rectangle.random_points(2)\narray([[0.7319939 , 0.5986585 ],\n       [0.15601864, 0.15599452]], dtype=float32)\n&gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n&gt;&gt;&gt; cuboid.random_points(2)\narray([[0.05808361, 0.8661761 , 0.601115  ],\n       [0.7080726 , 0.02058449, 0.96990985]], dtype=float32)\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>@abc.abstractmethod\ndef random_points(\n    self, n: int, random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\"\n) -&gt; np.ndarray:\n    \"\"\"Compute the random points in the geometry.\n\n    Args:\n        n (int): Number of points.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"]): Random method. Defaults to \"pseudo\".\n            pseudo: Pseudo random.\n            Halton: Halton sequence.\n            LHS: Latin Hypercube Sampling.\n\n    Returns:\n        np.ndarray: Random points in the geometry. The shape is [N, D].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; interval.random_points(2)\n        array([[0.37454012],\n               [0.9507143 ]], dtype=float32)\n        &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; rectangle.random_points(2)\n        array([[0.7319939 , 0.5986585 ],\n               [0.15601864, 0.15599452]], dtype=float32)\n        &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n        &gt;&gt;&gt; cuboid.random_points(2)\n        array([[0.05808361, 0.8661761 , 0.601115  ],\n               [0.7080726 , 0.02058449, 0.96990985]], dtype=float32)\n    \"\"\"\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.sample_boundary","title":"<code>sample_boundary(n, random='pseudo', criteria=None, evenly=False)</code>","text":"<p>Compute the random points in the geometry and return those meet criteria.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method. Defaults to \"pseudo\". pseudo: Pseudo random. Halton: Halton sequence. LHS: Latin Hypercube Sampling.</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable[..., ndarray]]</code> <p>Criteria function. Given coords from differnet dimension and return a boolean array with shape [n,]. Defaults to None.</p> <code>None</code> <code>evenly</code> <code>bool</code> <p>Evenly sample points. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: Random points in the geometry. The shape is [N, D].                    their normal vectors. The shape is [N, D].                    their area. The shape is [N, 1].(only if the geometry is a mesh)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; interval.sample_boundary(2)\n{'x': array([[0.],\n       [1.]], dtype=float32), 'normal_x': array([[-1.],\n       [ 1.]], dtype=float32)}\n&gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; rectangle.sample_boundary(2)\n{'x': array([[1.],\n       [0.]], dtype=float32), 'y': array([[0.49816048],\n       [0.19714284]], dtype=float32), 'normal_x': array([[ 1.],\n       [-1.]], dtype=float32), 'normal_y': array([[0.],\n       [0.]], dtype=float32)}\n&gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n&gt;&gt;&gt; cuboid.sample_boundary(2)\n{'x': array([[0.83244264],\n       [0.18182497]], dtype=float32), 'y': array([[0.21233912],\n       [0.1834045 ]], dtype=float32), 'z': array([[0.],\n       [1.]], dtype=float32), 'normal_x': array([[0.],\n       [0.]], dtype=float32), 'normal_y': array([[0.],\n       [0.]], dtype=float32), 'normal_z': array([[-1.],\n       [ 1.]], dtype=float32)}\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def sample_boundary(\n    self,\n    n: int,\n    random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n    criteria: Optional[Callable[..., np.ndarray]] = None,\n    evenly: bool = False,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Compute the random points in the geometry and return those meet criteria.\n\n    Args:\n        n (int): Number of points.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"]): Random method. Defaults to \"pseudo\".\n            pseudo: Pseudo random.\n            Halton: Halton sequence.\n            LHS: Latin Hypercube Sampling.\n        criteria (Optional[Callable[..., np.ndarray]]): Criteria function. Given\n            coords from differnet dimension and return a boolean array with shape [n,].\n            Defaults to None.\n        evenly (bool): Evenly sample points. Defaults to False.\n\n    Returns:\n        Dict[str, np.ndarray]: Random points in the geometry. The shape is [N, D].\n                               their normal vectors. The shape is [N, D].\n                               their area. The shape is [N, 1].(only if the geometry is a mesh)\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; interval.sample_boundary(2)\n        {'x': array([[0.],\n               [1.]], dtype=float32), 'normal_x': array([[-1.],\n               [ 1.]], dtype=float32)}\n        &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; rectangle.sample_boundary(2)\n        {'x': array([[1.],\n               [0.]], dtype=float32), 'y': array([[0.49816048],\n               [0.19714284]], dtype=float32), 'normal_x': array([[ 1.],\n               [-1.]], dtype=float32), 'normal_y': array([[0.],\n               [0.]], dtype=float32)}\n        &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n        &gt;&gt;&gt; cuboid.sample_boundary(2)\n        {'x': array([[0.83244264],\n               [0.18182497]], dtype=float32), 'y': array([[0.21233912],\n               [0.1834045 ]], dtype=float32), 'z': array([[0.],\n               [1.]], dtype=float32), 'normal_x': array([[0.],\n               [0.]], dtype=float32), 'normal_y': array([[0.],\n               [0.]], dtype=float32), 'normal_z': array([[-1.],\n               [ 1.]], dtype=float32)}\n    \"\"\"\n    x = np.empty(shape=(n, self.ndim), dtype=paddle.get_default_dtype())\n    _size, _ntry, _nsuc = 0, 0, 0\n    while _size &lt; n:\n        if evenly:\n            if (\n                misc.typename(self) == \"TimeXGeometry\"\n                and misc.typename(self.geometry) == \"Mesh\"\n            ):\n                points, normal, area = self.uniform_boundary_points(n)\n            else:\n                points = self.uniform_boundary_points(n)\n        else:\n            if (\n                misc.typename(self) == \"TimeXGeometry\"\n                and misc.typename(self.geometry) == \"Mesh\"\n            ):\n                points, normal, area = self.random_boundary_points(n, random)\n            else:\n                if misc.typename(self) == \"TimeXGeometry\":\n                    points = self.random_boundary_points(n, random, criteria)\n                else:\n                    points = self.random_boundary_points(n, random)\n\n        if criteria is not None:\n            criteria_mask = criteria(*np.split(points, self.ndim, axis=1)).flatten()\n            points = points[criteria_mask]\n\n        if len(points) &gt; n - _size:\n            points = points[: n - _size]\n        x[_size : _size + len(points)] = points\n\n        _size += len(points)\n        _ntry += 1\n        if len(points) &gt; 0:\n            _nsuc += 1\n\n        if _ntry &gt;= 10000 and _nsuc == 0:\n            raise ValueError(\n                \"Sample boundary points failed, \"\n                \"please check correctness of geometry and given criteria.\"\n            )\n\n    if not (\n        misc.typename(self) == \"TimeXGeometry\"\n        and misc.typename(self.geometry) == \"Mesh\"\n    ):\n        normal = self.boundary_normal(x)\n\n    normal_dict = misc.convert_to_dict(\n        normal[:, 1:] if \"t\" in self.dim_keys else normal,\n        [f\"normal_{key}\" for key in self.dim_keys if key != \"t\"],\n    )\n    x_dict = misc.convert_to_dict(x, self.dim_keys)\n    if (\n        misc.typename(self) == \"TimeXGeometry\"\n        and misc.typename(self.geometry) == \"Mesh\"\n    ):\n        area_dict = misc.convert_to_dict(area[:, 1:], [\"area\"])\n        return {**x_dict, **normal_dict, **area_dict}\n\n    return {**x_dict, **normal_dict}\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.sample_interior","title":"<code>sample_interior(n, random='pseudo', criteria=None, evenly=False, compute_sdf_derivatives=False)</code>","text":"<p>Sample random points in the geometry and return those meet criteria.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method. Defaults to \"pseudo\". pseudo: Pseudo random. Halton: Halton sequence. LHS: Latin Hypercube Sampling.</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable[..., ndarray]]</code> <p>Criteria function. Given coords from differnet dimension and return a boolean array with shape [n,]. Defaults to None.</p> <code>None</code> <code>evenly</code> <code>bool</code> <p>Evenly sample points. Defaults to False.</p> <code>False</code> <code>compute_sdf_derivatives</code> <code>bool</code> <p>Compute SDF derivatives. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: Random points in the geometry. The shape is [N, D].                    their signed distance function. The shape is [N, 1].                    their derivatives of SDF(optional). The shape is [N, D].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; interval.sample_interior(2)\n{'x': array([[0.37454012],\n       [0.9507143 ]], dtype=float32), 'sdf': array([[0.37454012],\n       [0.04928571]], dtype=float32)}\n&gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; rectangle.sample_interior(2, \"pseudo\", None, False, True)\n{'x': array([[0.7319939 ],\n       [0.15601864]], dtype=float32), 'y': array([[0.5986585 ],\n       [0.15599452]], dtype=float32), 'sdf': array([[0.2680061 ],\n       [0.15599453]], dtype=float32), 'sdf__x': array([[-1.0001659 ],\n       [ 0.25868416]], dtype=float32), 'sdf__y': array([[-0.        ],\n       [ 0.74118376]], dtype=float32)}\n&gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n&gt;&gt;&gt; cuboid.sample_interior(2, \"pseudo\", None, True, True)\n{'x': array([[0.],\n       [0.]], dtype=float32), 'y': array([[0.],\n       [0.]], dtype=float32), 'z': array([[0.],\n       [1.]], dtype=float32), 'sdf': array([[0.],\n       [0.]], dtype=float32), 'sdf__x': array([[0.50008297],\n       [0.50008297]], dtype=float32), 'sdf__y': array([[0.50008297],\n       [0.50008297]], dtype=float32), 'sdf__z': array([[ 0.50008297],\n       [-0.49948692]], dtype=float32)}\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def sample_interior(\n    self,\n    n: int,\n    random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n    criteria: Optional[Callable[..., np.ndarray]] = None,\n    evenly: bool = False,\n    compute_sdf_derivatives: bool = False,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Sample random points in the geometry and return those meet criteria.\n\n    Args:\n        n (int): Number of points.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"]): Random method. Defaults to \"pseudo\".\n            pseudo: Pseudo random.\n            Halton: Halton sequence.\n            LHS: Latin Hypercube Sampling.\n        criteria (Optional[Callable[..., np.ndarray]]): Criteria function. Given\n            coords from differnet dimension and return a boolean array with shape [n,].\n            Defaults to None.\n        evenly (bool): Evenly sample points. Defaults to False.\n        compute_sdf_derivatives (bool): Compute SDF derivatives. Defaults to False.\n\n    Returns:\n        Dict[str, np.ndarray]: Random points in the geometry. The shape is [N, D].\n                               their signed distance function. The shape is [N, 1].\n                               their derivatives of SDF(optional). The shape is [N, D].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; interval.sample_interior(2)\n        {'x': array([[0.37454012],\n               [0.9507143 ]], dtype=float32), 'sdf': array([[0.37454012],\n               [0.04928571]], dtype=float32)}\n        &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; rectangle.sample_interior(2, \"pseudo\", None, False, True)\n        {'x': array([[0.7319939 ],\n               [0.15601864]], dtype=float32), 'y': array([[0.5986585 ],\n               [0.15599452]], dtype=float32), 'sdf': array([[0.2680061 ],\n               [0.15599453]], dtype=float32), 'sdf__x': array([[-1.0001659 ],\n               [ 0.25868416]], dtype=float32), 'sdf__y': array([[-0.        ],\n               [ 0.74118376]], dtype=float32)}\n        &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n        &gt;&gt;&gt; cuboid.sample_interior(2, \"pseudo\", None, True, True)\n        {'x': array([[0.],\n               [0.]], dtype=float32), 'y': array([[0.],\n               [0.]], dtype=float32), 'z': array([[0.],\n               [1.]], dtype=float32), 'sdf': array([[0.],\n               [0.]], dtype=float32), 'sdf__x': array([[0.50008297],\n               [0.50008297]], dtype=float32), 'sdf__y': array([[0.50008297],\n               [0.50008297]], dtype=float32), 'sdf__z': array([[ 0.50008297],\n               [-0.49948692]], dtype=float32)}\n    \"\"\"\n    x = np.empty(shape=(n, self.ndim), dtype=paddle.get_default_dtype())\n    _size, _ntry, _nsuc = 0, 0, 0\n    while _size &lt; n:\n        if evenly:\n            points = self.uniform_points(n)\n        else:\n            if misc.typename(self) == \"TimeXGeometry\":\n                points = self.random_points(n, random, criteria)\n            else:\n                points = self.random_points(n, random)\n\n        if criteria is not None:\n            criteria_mask = criteria(*np.split(points, self.ndim, axis=1)).flatten()\n            points = points[criteria_mask]\n\n        if len(points) &gt; n - _size:\n            points = points[: n - _size]\n        x[_size : _size + len(points)] = points\n\n        _size += len(points)\n        _ntry += 1\n        if len(points) &gt; 0:\n            _nsuc += 1\n\n        if _ntry &gt;= 1000 and _nsuc == 0:\n            raise ValueError(\n                \"Sample interior points failed, \"\n                \"please check correctness of geometry and given criteria.\"\n            )\n\n    # if sdf_func added, return x_dict and sdf_dict, else, only return the x_dict\n    if hasattr(self, \"sdf_func\"):\n        sdf = -self.sdf_func(x)\n        sdf_dict = misc.convert_to_dict(sdf, (\"sdf\",))\n        sdf_derives_dict = {}\n        if compute_sdf_derivatives:\n            sdf_derives = -self.sdf_derivatives(x)\n            sdf_derives_dict = misc.convert_to_dict(\n                sdf_derives, tuple(f\"sdf__{key}\" for key in self.dim_keys)\n            )\n    else:\n        sdf_dict = {}\n        sdf_derives_dict = {}\n    x_dict = misc.convert_to_dict(x, self.dim_keys)\n\n    return {**x_dict, **sdf_dict, **sdf_derives_dict}\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.sdf_derivatives","title":"<code>sdf_derivatives(x, epsilon=0.0001)</code>","text":"<p>Compute derivatives of SDF function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Points for computing SDF derivatives using central difference. The shape is [N, D], D is the number of dimension of geometry.</p> required <code>epsilon</code> <code>float</code> <p>Derivative step. Defaults to 1e-4.</p> <code>0.0001</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Derivatives of corresponding SDF function. The shape is [N, D]. D is the number of dimension of geometry.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; x = np.array([[0], [0.5], [1.5]])\n&gt;&gt;&gt; interval.sdf_derivatives(x)\narray([[-1.],\n       [ 0.],\n       [ 1.]])\n&gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.5, 0.5], [1.5, 1.5]])\n&gt;&gt;&gt; rectangle.sdf_derivatives(x)\narray([[-0.5       , -0.5       ],\n       [ 0.        ,  0.        ],\n       [ 0.70710678,  0.70710678]])\n&gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n&gt;&gt;&gt; x = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1, 1, 1]])\n&gt;&gt;&gt; cuboid.sdf_derivatives(x)\narray([[-0.5, -0.5, -0.5],\n       [ 0. ,  0. ,  0. ],\n       [ 0.5,  0.5,  0.5]])\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def sdf_derivatives(self, x: np.ndarray, epsilon: float = 1e-4) -&gt; np.ndarray:\n    \"\"\"Compute derivatives of SDF function.\n\n    Args:\n        x (np.ndarray): Points for computing SDF derivatives using central\n            difference. The shape is [N, D], D is the number of dimension of\n            geometry.\n        epsilon (float): Derivative step. Defaults to 1e-4.\n\n    Returns:\n        np.ndarray: Derivatives of corresponding SDF function.\n            The shape is [N, D]. D is the number of dimension of geometry.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; x = np.array([[0], [0.5], [1.5]])\n        &gt;&gt;&gt; interval.sdf_derivatives(x)\n        array([[-1.],\n               [ 0.],\n               [ 1.]])\n        &gt;&gt;&gt; rectangle = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.5, 0.5], [1.5, 1.5]])\n        &gt;&gt;&gt; rectangle.sdf_derivatives(x)\n        array([[-0.5       , -0.5       ],\n               [ 0.        ,  0.        ],\n               [ 0.70710678,  0.70710678]])\n        &gt;&gt;&gt; cuboid = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n        &gt;&gt;&gt; x = np.array([[0, 0, 0], [0.5, 0.5, 0.5], [1, 1, 1]])\n        &gt;&gt;&gt; cuboid.sdf_derivatives(x)\n        array([[-0.5, -0.5, -0.5],\n               [ 0. ,  0. ,  0. ],\n               [ 0.5,  0.5,  0.5]])\n    \"\"\"\n    if not hasattr(self, \"sdf_func\"):\n        raise NotImplementedError(\n            f\"{misc.typename(self)}.sdf_func should be implemented \"\n            \"when using 'sdf_derivatives'.\"\n        )\n    # Only compute sdf derivatives for those already implement `sdf_func` method.\n    sdf_derives = np.empty_like(x)\n    for i in range(self.ndim):\n        h = np.zeros_like(x)\n        h[:, i] += epsilon / 2\n        derives_at_i = (self.sdf_func(x + h) - self.sdf_func(x - h)) / epsilon\n        sdf_derives[:, i : i + 1] = derives_at_i\n    return sdf_derives\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.uniform_boundary_points","title":"<code>uniform_boundary_points(n)</code>","text":"<p>Compute the equi-spaced points on the boundary(not implemented).</p> Warings <p>This function is not implemented, please use random_boundary_points instead.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Random points on the boundary. The shape is [N, D].</p> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def uniform_boundary_points(self, n: int) -&gt; np.ndarray:\n    \"\"\"Compute the equi-spaced points on the boundary(not implemented).\n\n    Warings:\n        This function is not implemented, please use random_boundary_points instead.\n\n    Args:\n        n (int): Number of points.\n\n    Returns:\n        np.ndarray: Random points on the boundary. The shape is [N, D].\n    \"\"\"\n    logger.warning(\n        f\"{self}.uniform_boundary_points not implemented. \"\n        f\"Use random_boundary_points instead.\"\n    )\n    return self.random_boundary_points(n)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.uniform_points","title":"<code>uniform_points(n, boundary=True)</code>","text":"<p>Compute the equi-spaced points in the geometry.</p> Warings <p>This function is not implemented, please use random_points instead.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points.</p> required <code>boundary</code> <code>bool</code> <p>Include boundary points. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Random points in the geometry. The shape is [N, D].</p> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def uniform_points(self, n: int, boundary: bool = True) -&gt; np.ndarray:\n    \"\"\"Compute the equi-spaced points in the geometry.\n\n    Warings:\n        This function is not implemented, please use random_points instead.\n\n    Args:\n        n (int): Number of points.\n        boundary (bool): Include boundary points. Defaults to True.\n\n    Returns:\n        np.ndarray: Random points in the geometry. The shape is [N, D].\n    \"\"\"\n    logger.warning(\n        f\"{self}.uniform_points not implemented. \" f\"Use random_points instead.\"\n    )\n    return self.random_points(n)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Geometry.union","title":"<code>union(other)</code>","text":"<p>CSG Union.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Geometry</code> <p>The other geometry.</p> required <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The union of two geometries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0, 1)\n&gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n&gt;&gt;&gt; union = interval1.union(interval2)\n&gt;&gt;&gt; union.bbox\n(array([[0.]]), array([[1.5]]))\n&gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0, 0), (2, 3))\n&gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0, 0), (3, 2))\n&gt;&gt;&gt; union = rectangle1.union(rectangle2)\n&gt;&gt;&gt; union.bbox\n(array([0., 0.], dtype=float32), array([3., 3.], dtype=float32))\n&gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n&gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n&gt;&gt;&gt; union = cuboid1 | cuboid2\n&gt;&gt;&gt; union.bbox\n(array([0., 0., 0.], dtype=float32), array([2., 2., 2.], dtype=float32))\n</code></pre> Source code in <code>ppsci/geometry/geometry.py</code> <pre><code>def union(self, other: \"Geometry\") -&gt; \"Geometry\":\n    \"\"\"CSG Union.\n\n    Args:\n        other (Geometry): The other geometry.\n\n    Returns:\n        Geometry: The union of two geometries.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; interval1 = ppsci.geometry.Interval(0, 1)\n        &gt;&gt;&gt; interval2 = ppsci.geometry.Interval(0.5, 1.5)\n        &gt;&gt;&gt; union = interval1.union(interval2)\n        &gt;&gt;&gt; union.bbox\n        (array([[0.]]), array([[1.5]]))\n        &gt;&gt;&gt; rectangle1 = ppsci.geometry.Rectangle((0, 0), (2, 3))\n        &gt;&gt;&gt; rectangle2 = ppsci.geometry.Rectangle((0, 0), (3, 2))\n        &gt;&gt;&gt; union = rectangle1.union(rectangle2)\n        &gt;&gt;&gt; union.bbox\n        (array([0., 0.], dtype=float32), array([3., 3.], dtype=float32))\n        &gt;&gt;&gt; cuboid1 = ppsci.geometry.Cuboid((0, 0, 0), (1, 2, 2))\n        &gt;&gt;&gt; cuboid2 = ppsci.geometry.Cuboid((0, 0, 0), (2, 1, 1))\n        &gt;&gt;&gt; union = cuboid1 | cuboid2\n        &gt;&gt;&gt; union.bbox\n        (array([0., 0., 0.], dtype=float32), array([2., 2., 2.], dtype=float32))\n    \"\"\"\n    from ppsci.geometry import csg\n\n    return csg.CSGUnion(self, other)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Interval","title":"<code>Interval</code>","text":"<p>               Bases: <code>Geometry</code></p> <p>Class for interval.</p> <p>Parameters:</p> Name Type Description Default <code>l</code> <code>float</code> <p>Left position of interval.</p> required <code>r</code> <code>float</code> <p>Right position of interval.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.Interval(-1, 1)\n</code></pre> Source code in <code>ppsci/geometry/geometry_1d.py</code> <pre><code>class Interval(geometry.Geometry):\n    \"\"\"Class for interval.\n\n    Args:\n        l (float): Left position of interval.\n        r (float): Right position of interval.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.Interval(-1, 1)\n    \"\"\"\n\n    def __init__(self, l: float, r: float):\n        super().__init__(1, (np.array([[l]]), np.array([[r]])), r - l)\n        self.l = l\n        self.r = r\n\n    def is_inside(self, x: np.ndarray):\n        return ((self.l &lt;= x) &amp; (x &lt;= self.r)).flatten()\n\n    def on_boundary(self, x: np.ndarray):\n        return (np.isclose(x, self.l) | np.isclose(x, self.r)).flatten()\n\n    def boundary_normal(self, x: np.ndarray):\n        return -np.isclose(x, self.l).astype(paddle.get_default_dtype()) + np.isclose(\n            x, self.r\n        ).astype(paddle.get_default_dtype())\n\n    def uniform_points(self, n: int, boundary: bool = True):\n        if boundary:\n            return np.linspace(\n                self.l, self.r, n, dtype=paddle.get_default_dtype()\n            ).reshape([-1, 1])\n        return np.linspace(\n            self.l, self.r, n + 1, endpoint=False, dtype=paddle.get_default_dtype()\n        )[1:].reshape([-1, 1])\n\n    def random_points(self, n: int, random: str = \"pseudo\"):\n        x = sample(n, 1, random)\n        return (self.l + x * self.diam).astype(paddle.get_default_dtype())\n\n    def uniform_boundary_points(self, n: int):\n        if n == 1:\n            return np.array([[self.l]], dtype=paddle.get_default_dtype())\n        xl = np.full([n // 2, 1], self.l, dtype=paddle.get_default_dtype())\n        xr = np.full([n - n // 2, 1], self.r, dtype=paddle.get_default_dtype())\n        return np.concatenate((xl, xr), axis=0)\n\n    def random_boundary_points(self, n: int, random: str = \"pseudo\"):\n        if n == 2:\n            return np.array([[self.l], [self.r]], dtype=paddle.get_default_dtype())\n        return (\n            np.random.choice([self.l, self.r], n)\n            .reshape([-1, 1])\n            .astype(paddle.get_default_dtype())\n        )\n\n    def periodic_point(self, x: np.ndarray, component: int = 0):\n        x_array = misc.convert_to_array(x, self.dim_keys)\n        periodic_x = x_array\n        periodic_x[np.isclose(x_array, self.l)] = self.r\n        periodic_x[np.isclose(x_array, self.r)] = self.l\n        periodic_x_normal = self.boundary_normal(periodic_x)\n\n        periodic_x = misc.convert_to_dict(periodic_x, self.dim_keys)\n        periodic_x_normal = misc.convert_to_dict(\n            periodic_x_normal, [f\"normal_{k}\" for k in self.dim_keys]\n        )\n        return {**periodic_x, **periodic_x_normal}\n\n    def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute signed distance field\n\n        Args:\n            points (np.ndarray): The coordinate points used to calculate the SDF value,\n                the shape is [N, 1]\n\n        Returns:\n            np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n        NOTE: This function usually returns ndarray with negative values, because\n        according to the definition of SDF, the SDF value of the coordinate point inside\n        the object(interior points) is negative, the outside is positive, and the edge\n        is 0. Therefore, when used for weighting, a negative sign is often added before\n        the result of this function.\n        \"\"\"\n        if points.shape[1] != self.ndim:\n            raise ValueError(\n                f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n            )\n        return -((self.r - self.l) / 2 - np.abs(points - (self.l + self.r) / 2))\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Interval.sdf_func","title":"<code>sdf_func(points)</code>","text":"<p>Compute signed distance field</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The coordinate points used to calculate the SDF value, the shape is [N, 1]</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: SDF values of input points without squared, the shape is [N, 1].</p> <p>NOTE: This function usually returns ndarray with negative values, because according to the definition of SDF, the SDF value of the coordinate point inside the object(interior points) is negative, the outside is positive, and the edge is 0. Therefore, when used for weighting, a negative sign is often added before the result of this function.</p> Source code in <code>ppsci/geometry/geometry_1d.py</code> <pre><code>def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute signed distance field\n\n    Args:\n        points (np.ndarray): The coordinate points used to calculate the SDF value,\n            the shape is [N, 1]\n\n    Returns:\n        np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n    NOTE: This function usually returns ndarray with negative values, because\n    according to the definition of SDF, the SDF value of the coordinate point inside\n    the object(interior points) is negative, the outside is positive, and the edge\n    is 0. Therefore, when used for weighting, a negative sign is often added before\n    the result of this function.\n    \"\"\"\n    if points.shape[1] != self.ndim:\n        raise ValueError(\n            f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n        )\n    return -((self.r - self.l) / 2 - np.abs(points - (self.l + self.r) / 2))\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Disk","title":"<code>Disk</code>","text":"<p>               Bases: <code>Geometry</code></p> <p>Class for disk geometry</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>Tuple[float, float]</code> <p>Center point of disk [x0, y0].</p> required <code>radius</code> <code>float</code> <p>Radius of disk.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.Disk((0.0, 0.0), 1.0)\n</code></pre> Source code in <code>ppsci/geometry/geometry_2d.py</code> <pre><code>class Disk(geometry.Geometry):\n    \"\"\"Class for disk geometry\n\n    Args:\n        center (Tuple[float, float]): Center point of disk [x0, y0].\n        radius (float): Radius of disk.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.Disk((0.0, 0.0), 1.0)\n    \"\"\"\n\n    def __init__(self, center: Tuple[float, float], radius: float):\n        self.center = np.array(center, dtype=paddle.get_default_dtype())\n        self.radius = radius\n        super().__init__(2, (self.center - radius, self.center + radius), 2 * radius)\n\n    def is_inside(self, x):\n        return np.linalg.norm(x - self.center, axis=1) &lt;= self.radius\n\n    def on_boundary(self, x):\n        return np.isclose(np.linalg.norm(x - self.center, axis=1), self.radius)\n\n    def boundary_normal(self, x):\n        ox = x - self.center\n        ox_len = np.linalg.norm(ox, axis=1, keepdims=True)\n        ox = (ox / ox_len) * np.isclose(ox_len, self.radius).astype(\n            paddle.get_default_dtype()\n        )\n        return ox\n\n    def random_points(self, n, random=\"pseudo\"):\n        # http://mathworld.wolfram.com/DiskPointPicking.html\n        rng = sampler.sample(n, 2, random)\n        r, theta = rng[:, 0], 2 * np.pi * rng[:, 1]\n        x = np.sqrt(r) * np.cos(theta)\n        y = np.sqrt(r) * np.sin(theta)\n        return self.radius * np.stack((x, y), axis=1) + self.center\n\n    def uniform_boundary_points(self, n):\n        theta = np.linspace(\n            0, 2 * np.pi, num=n, endpoint=False, dtype=paddle.get_default_dtype()\n        )\n        X = np.stack((np.cos(theta), np.sin(theta)), axis=1)\n        return self.radius * X + self.center\n\n    def random_boundary_points(self, n, random=\"pseudo\"):\n        theta = 2 * np.pi * sampler.sample(n, 1, random)\n        X = np.concatenate((np.cos(theta), np.sin(theta)), axis=1)\n        return self.radius * X + self.center\n\n    def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute signed distance field.\n\n        Args:\n            points (np.ndarray): The coordinate points used to calculate the SDF value,\n                the shape is [N, 2]\n\n        Returns:\n            np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n        NOTE: This function usually returns ndarray with negative values, because\n        according to the definition of SDF, the SDF value of the coordinate point inside\n        the object(interior points) is negative, the outside is positive, and the edge\n        is 0. Therefore, when used for weighting, a negative sign is often added before\n        the result of this function.\n        \"\"\"\n        if points.shape[1] != self.ndim:\n            raise ValueError(\n                f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n            )\n        sdf = self.radius - np.linalg.norm(points - self.center, axis=1)\n        sdf = -sdf[..., np.newaxis]\n        return sdf\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Disk.sdf_func","title":"<code>sdf_func(points)</code>","text":"<p>Compute signed distance field.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The coordinate points used to calculate the SDF value, the shape is [N, 2]</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: SDF values of input points without squared, the shape is [N, 1].</p> <p>NOTE: This function usually returns ndarray with negative values, because according to the definition of SDF, the SDF value of the coordinate point inside the object(interior points) is negative, the outside is positive, and the edge is 0. Therefore, when used for weighting, a negative sign is often added before the result of this function.</p> Source code in <code>ppsci/geometry/geometry_2d.py</code> <pre><code>def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute signed distance field.\n\n    Args:\n        points (np.ndarray): The coordinate points used to calculate the SDF value,\n            the shape is [N, 2]\n\n    Returns:\n        np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n    NOTE: This function usually returns ndarray with negative values, because\n    according to the definition of SDF, the SDF value of the coordinate point inside\n    the object(interior points) is negative, the outside is positive, and the edge\n    is 0. Therefore, when used for weighting, a negative sign is often added before\n    the result of this function.\n    \"\"\"\n    if points.shape[1] != self.ndim:\n        raise ValueError(\n            f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n        )\n    sdf = self.radius - np.linalg.norm(points - self.center, axis=1)\n    sdf = -sdf[..., np.newaxis]\n    return sdf\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Polygon","title":"<code>Polygon</code>","text":"<p>               Bases: <code>Geometry</code></p> <p>Class for simple polygon.</p> <p>Parameters:</p> Name Type Description Default <code>vertices</code> <code>Tuple[Tuple[float, float], ...]</code> <p>The order of vertices can be in a clockwise or counter-clockwise direction. The vertices will be re-ordered in counterclockwise (right hand rule).</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.Polygon(((0, 0), (1, 0), (2, 1), (2, 2), (0, 2)))\n</code></pre> Source code in <code>ppsci/geometry/geometry_2d.py</code> <pre><code>class Polygon(geometry.Geometry):\n    \"\"\"Class for simple polygon.\n\n    Args:\n        vertices (Tuple[Tuple[float, float], ...]): The order of vertices can be in a\n            clockwise or counter-clockwise direction. The vertices will be re-ordered in\n            counterclockwise (right hand rule).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.Polygon(((0, 0), (1, 0), (2, 1), (2, 2), (0, 2)))\n    \"\"\"\n\n    def __init__(self, vertices):\n        self.vertices = np.array(vertices, dtype=paddle.get_default_dtype())\n        if len(vertices) == 3:\n            raise ValueError(\"The polygon is a triangle. Use Triangle instead.\")\n        if Rectangle.is_valid(self.vertices):\n            raise ValueError(\"The polygon is a rectangle. Use Rectangle instead.\")\n\n        self.area = polygon_signed_area(self.vertices)\n        # Clockwise\n        if self.area &lt; 0:\n            self.area = -self.area\n            self.vertices = np.flipud(self.vertices)\n\n        self.diagonals = spatial.distance.squareform(\n            spatial.distance.pdist(self.vertices)\n        )\n        super().__init__(\n            2,\n            (np.amin(self.vertices, axis=0), np.amax(self.vertices, axis=0)),\n            np.max(self.diagonals),\n        )\n        self.nvertices = len(self.vertices)\n        self.perimeter = np.sum(\n            [self.diagonals[i, i + 1] for i in range(-1, self.nvertices - 1)]\n        )\n        self.bbox = np.array(\n            [np.min(self.vertices, axis=0), np.max(self.vertices, axis=0)],\n            dtype=paddle.get_default_dtype(),\n        )\n\n        self.segments = self.vertices[1:] - self.vertices[:-1]\n        self.segments = np.vstack((self.vertices[0] - self.vertices[-1], self.segments))\n        self.normal = clockwise_rotation_90(self.segments.T).T\n        self.normal = self.normal / np.linalg.norm(self.normal, axis=1).reshape(-1, 1)\n\n    def is_inside(self, x):\n        def wn_PnPoly(P, V):\n            \"\"\"Winding number algorithm.\n\n            https://en.wikipedia.org/wiki/Point_in_polygon\n            http://geomalgorithms.com/a03-_inclusion.html\n\n            Args:\n                P: A point.\n                V: Vertex points of a polygon.\n\n            Returns:\n                wn: Winding number (=0 only if P is outside polygon).\n            \"\"\"\n            wn = np.zeros(len(P))  # Winding number counter\n\n            # Repeat the first vertex at end\n            # Loop through all edges of the polygon\n            for i in range(-1, self.nvertices - 1):  # Edge from V[i] to V[i+1]\n                tmp = np.all(\n                    np.hstack(\n                        [\n                            V[i, 1] &lt;= P[:, 1:2],  # Start y &lt;= P[1]\n                            V[i + 1, 1] &gt; P[:, 1:2],  # An upward crossing\n                            is_left(V[i], V[i + 1], P) &gt; 0,  # P left of edge\n                        ]\n                    ),\n                    axis=-1,\n                )\n                wn[tmp] += 1  # Have a valid up intersect\n                tmp = np.all(\n                    np.hstack(\n                        [\n                            V[i, 1] &gt; P[:, 1:2],  # Start y &gt; P[1]\n                            V[i + 1, 1] &lt;= P[:, 1:2],  # A downward crossing\n                            is_left(V[i], V[i + 1], P) &lt; 0,  # P right of edge\n                        ]\n                    ),\n                    axis=-1,\n                )\n                wn[tmp] -= 1  # Have a valid down intersect\n            return wn\n\n        return wn_PnPoly(x, self.vertices) != 0\n\n    def on_boundary(self, x):\n        _on = np.zeros(shape=len(x), dtype=np.int)\n        for i in range(-1, self.nvertices - 1):\n            l1 = np.linalg.norm(self.vertices[i] - x, axis=-1)\n            l2 = np.linalg.norm(self.vertices[i + 1] - x, axis=-1)\n            _on[np.isclose(l1 + l2, self.diagonals[i, i + 1])] += 1\n        return _on &gt; 0\n\n    def random_points(self, n, random=\"pseudo\"):\n        x = np.empty((0, 2), dtype=paddle.get_default_dtype())\n        vbbox = self.bbox[1] - self.bbox[0]\n        while len(x) &lt; n:\n            x_new = sampler.sample(n, 2, \"pseudo\") * vbbox + self.bbox[0]\n            x = np.vstack((x, x_new[self.is_inside(x_new)]))\n        return x[:n]\n\n    def uniform_boundary_points(self, n):\n        density = n / self.perimeter\n        x = []\n        for i in range(-1, self.nvertices - 1):\n            x.append(\n                np.linspace(\n                    0,\n                    1,\n                    num=int(np.ceil(density * self.diagonals[i, i + 1])),\n                    endpoint=False,\n                    dtype=paddle.get_default_dtype(),\n                )[:, None]\n                * (self.vertices[i + 1] - self.vertices[i])\n                + self.vertices[i]\n            )\n        x = np.vstack(x)\n        if len(x) &gt; n:\n            x = x[0:n]\n        return x\n\n    def random_boundary_points(self, n, random=\"pseudo\"):\n        u = np.ravel(sampler.sample(n + self.nvertices, 1, random))\n        # Remove the possible points very close to the corners\n        l = 0\n        for i in range(0, self.nvertices - 1):\n            l += self.diagonals[i, i + 1]\n            u = u[np.logical_not(np.isclose(u, l / self.perimeter))]\n        u = u[:n]\n        u *= self.perimeter\n        u.sort()\n\n        x = []\n        i = -1\n        l0 = 0\n        l1 = l0 + self.diagonals[i, i + 1]\n        v = (self.vertices[i + 1] - self.vertices[i]) / self.diagonals[i, i + 1]\n        for l in u:\n            if l &gt; l1:\n                i += 1\n                l0, l1 = l1, l1 + self.diagonals[i, i + 1]\n                v = (self.vertices[i + 1] - self.vertices[i]) / self.diagonals[i, i + 1]\n            x.append((l - l0) * v + self.vertices[i])\n        return np.vstack(x)\n\n    def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute signed distance field.\n\n        Args:\n            points (np.ndarray): The coordinate points used to calculate the SDF value,\n                the shape is [N, 2]\n        Returns:\n            np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n        NOTE: This function usually returns ndarray with negative values, because\n        according to the definition of SDF, the SDF value of the coordinate point inside\n        the object(interior points) is negative, the outside is positive, and the edge\n        is 0. Therefore, when used for weighting, a negative sign is often added before\n        the result of this function.\n        \"\"\"\n        if points.shape[1] != self.ndim:\n            raise ValueError(\n                f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n            )\n        sdf_value = np.empty((points.shape[0], 1), dtype=paddle.get_default_dtype())\n        for n in range(points.shape[0]):\n            distance = np.dot(\n                points[n] - self.vertices[0], points[n] - self.vertices[0]\n            )\n            inside_tag = 1.0\n            for i in range(self.vertices.shape[0]):\n                j = (self.vertices.shape[0] - 1) if i == 0 else (i - 1)\n                # Calculate the shortest distance from point P to each edge.\n                vector_ij = self.vertices[j] - self.vertices[i]\n                vector_in = points[n] - self.vertices[i]\n                distance_vector = vector_in - vector_ij * np.clip(\n                    np.dot(vector_in, vector_ij) / np.dot(vector_ij, vector_ij),\n                    0.0,\n                    1.0,\n                )\n                distance = np.minimum(\n                    distance, np.dot(distance_vector, distance_vector)\n                )\n                # Calculate the inside and outside using the Odd-even rule\n                odd_even_rule_number = np.array(\n                    [\n                        points[n][1] &gt;= self.vertices[i][1],\n                        points[n][1] &lt; self.vertices[j][1],\n                        vector_ij[0] * vector_in[1] &gt; vector_ij[1] * vector_in[0],\n                    ]\n                )\n                if odd_even_rule_number.all() or np.all(~odd_even_rule_number):\n                    inside_tag *= -1.0\n            sdf_value[n] = inside_tag * np.sqrt(distance)\n        return -sdf_value\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Polygon.sdf_func","title":"<code>sdf_func(points)</code>","text":"<p>Compute signed distance field.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The coordinate points used to calculate the SDF value, the shape is [N, 2]</p> required <p>Returns:     np.ndarray: SDF values of input points without squared, the shape is [N, 1].</p> <p>NOTE: This function usually returns ndarray with negative values, because according to the definition of SDF, the SDF value of the coordinate point inside the object(interior points) is negative, the outside is positive, and the edge is 0. Therefore, when used for weighting, a negative sign is often added before the result of this function.</p> Source code in <code>ppsci/geometry/geometry_2d.py</code> <pre><code>def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute signed distance field.\n\n    Args:\n        points (np.ndarray): The coordinate points used to calculate the SDF value,\n            the shape is [N, 2]\n    Returns:\n        np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n    NOTE: This function usually returns ndarray with negative values, because\n    according to the definition of SDF, the SDF value of the coordinate point inside\n    the object(interior points) is negative, the outside is positive, and the edge\n    is 0. Therefore, when used for weighting, a negative sign is often added before\n    the result of this function.\n    \"\"\"\n    if points.shape[1] != self.ndim:\n        raise ValueError(\n            f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n        )\n    sdf_value = np.empty((points.shape[0], 1), dtype=paddle.get_default_dtype())\n    for n in range(points.shape[0]):\n        distance = np.dot(\n            points[n] - self.vertices[0], points[n] - self.vertices[0]\n        )\n        inside_tag = 1.0\n        for i in range(self.vertices.shape[0]):\n            j = (self.vertices.shape[0] - 1) if i == 0 else (i - 1)\n            # Calculate the shortest distance from point P to each edge.\n            vector_ij = self.vertices[j] - self.vertices[i]\n            vector_in = points[n] - self.vertices[i]\n            distance_vector = vector_in - vector_ij * np.clip(\n                np.dot(vector_in, vector_ij) / np.dot(vector_ij, vector_ij),\n                0.0,\n                1.0,\n            )\n            distance = np.minimum(\n                distance, np.dot(distance_vector, distance_vector)\n            )\n            # Calculate the inside and outside using the Odd-even rule\n            odd_even_rule_number = np.array(\n                [\n                    points[n][1] &gt;= self.vertices[i][1],\n                    points[n][1] &lt; self.vertices[j][1],\n                    vector_ij[0] * vector_in[1] &gt; vector_ij[1] * vector_in[0],\n                ]\n            )\n            if odd_even_rule_number.all() or np.all(~odd_even_rule_number):\n                inside_tag *= -1.0\n        sdf_value[n] = inside_tag * np.sqrt(distance)\n    return -sdf_value\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Rectangle","title":"<code>Rectangle</code>","text":"<p>               Bases: <code>Hypercube</code></p> <p>Class for rectangle geometry</p> <p>Parameters:</p> Name Type Description Default <code>xmin</code> <code>Tuple[float, float]</code> <p>Bottom left corner point, [x0, y0].</p> required <code>xmax</code> <code>Tuple[float, float]</code> <p>Top right corner point, [x1, y1].</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0.0, 0.0), (1.0, 1.0))\n</code></pre> Source code in <code>ppsci/geometry/geometry_2d.py</code> <pre><code>class Rectangle(geometry_nd.Hypercube):\n    \"\"\"Class for rectangle geometry\n\n    Args:\n        xmin (Tuple[float, float]): Bottom left corner point, [x0, y0].\n        xmax (Tuple[float, float]): Top right corner point, [x1, y1].\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0.0, 0.0), (1.0, 1.0))\n    \"\"\"\n\n    def __init__(self, xmin, xmax):\n        super().__init__(xmin, xmax)\n        self.perimeter = 2 * np.sum(self.xmax - self.xmin)\n        self.area = np.prod(self.xmax - self.xmin)\n\n    def uniform_boundary_points(self, n):\n        nx, ny = np.ceil(n / self.perimeter * (self.xmax - self.xmin)).astype(int)\n        bottom = np.hstack(\n            (\n                np.linspace(\n                    self.xmin[0],\n                    self.xmax[0],\n                    nx,\n                    endpoint=False,\n                    dtype=paddle.get_default_dtype(),\n                ).reshape([nx, 1]),\n                np.full([nx, 1], self.xmin[1], dtype=paddle.get_default_dtype()),\n            )\n        )\n        right = np.hstack(\n            (\n                np.full([ny, 1], self.xmax[0], dtype=paddle.get_default_dtype()),\n                np.linspace(\n                    self.xmin[1],\n                    self.xmax[1],\n                    ny,\n                    endpoint=False,\n                    dtype=paddle.get_default_dtype(),\n                ).reshape([ny, 1]),\n            )\n        )\n        top = np.hstack(\n            (\n                np.linspace(\n                    self.xmin[0], self.xmax[0], nx + 1, dtype=paddle.get_default_dtype()\n                )[1:].reshape([nx, 1]),\n                np.full([nx, 1], self.xmax[1], dtype=paddle.get_default_dtype()),\n            )\n        )\n        left = np.hstack(\n            (\n                np.full([ny, 1], self.xmin[0], dtype=paddle.get_default_dtype()),\n                np.linspace(\n                    self.xmin[1], self.xmax[1], ny + 1, dtype=paddle.get_default_dtype()\n                )[1:].reshape([ny, 1]),\n            )\n        )\n        x = np.vstack((bottom, right, top, left))\n        if len(x) &gt; n:\n            x = x[0:n]\n        return x\n\n    def random_boundary_points(self, n, random=\"pseudo\"):\n        l1 = self.xmax[0] - self.xmin[0]\n        l2 = l1 + self.xmax[1] - self.xmin[1]\n        l3 = l2 + l1\n        u = np.ravel(sampler.sample(n + 10, 1, random))\n        # Remove the possible points very close to the corners\n        u = u[~np.isclose(u, l1 / self.perimeter)]\n        u = u[~np.isclose(u, l3 / self.perimeter)]\n        u = u[0:n]\n\n        u *= self.perimeter\n        x = []\n        for l in u:\n            if l &lt; l1:\n                x.append([self.xmin[0] + l, self.xmin[1]])\n            elif l &lt; l2:\n                x.append([self.xmax[0], self.xmin[1] + (l - l1)])\n            elif l &lt; l3:\n                x.append([self.xmax[0] - (l - l2), self.xmax[1]])\n            else:\n                x.append([self.xmin[0], self.xmax[1] - (l - l3)])\n        return np.vstack(x)\n\n    @staticmethod\n    def is_valid(vertices):\n        \"\"\"Check if the geometry is a Rectangle.\"\"\"\n        return (\n            len(vertices) == 4\n            and np.isclose(np.prod(vertices[1] - vertices[0]), 0)\n            and np.isclose(np.prod(vertices[2] - vertices[1]), 0)\n            and np.isclose(np.prod(vertices[3] - vertices[2]), 0)\n            and np.isclose(np.prod(vertices[0] - vertices[3]), 0)\n        )\n\n    def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute signed distance field.\n\n        Args:\n            points (np.ndarray): The coordinate points used to calculate the SDF value,\n                the shape of the array is [N, 2].\n\n        Returns:\n            np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n        NOTE: This function usually returns ndarray with negative values, because\n        according to the definition of SDF, the SDF value of the coordinate point inside\n        the object(interior points) is negative, the outside is positive, and the edge\n        is 0. Therefore, when used for weighting, a negative sign is often added before\n        the result of this function.\n        \"\"\"\n        if points.shape[1] != self.ndim:\n            raise ValueError(\n                f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n            )\n        center = (self.xmin + self.xmax) / 2\n        dist_to_boundary = (\n            np.abs(points - center) - np.array([self.xmax - self.xmin]) / 2\n        )\n        return (\n            np.linalg.norm(np.maximum(dist_to_boundary, 0), axis=1)\n            + np.minimum(np.max(dist_to_boundary, axis=1), 0)\n        ).reshape(-1, 1)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Rectangle.is_valid","title":"<code>is_valid(vertices)</code>  <code>staticmethod</code>","text":"<p>Check if the geometry is a Rectangle.</p> Source code in <code>ppsci/geometry/geometry_2d.py</code> <pre><code>@staticmethod\ndef is_valid(vertices):\n    \"\"\"Check if the geometry is a Rectangle.\"\"\"\n    return (\n        len(vertices) == 4\n        and np.isclose(np.prod(vertices[1] - vertices[0]), 0)\n        and np.isclose(np.prod(vertices[2] - vertices[1]), 0)\n        and np.isclose(np.prod(vertices[3] - vertices[2]), 0)\n        and np.isclose(np.prod(vertices[0] - vertices[3]), 0)\n    )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Rectangle.sdf_func","title":"<code>sdf_func(points)</code>","text":"<p>Compute signed distance field.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The coordinate points used to calculate the SDF value, the shape of the array is [N, 2].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: SDF values of input points without squared, the shape is [N, 1].</p> <p>NOTE: This function usually returns ndarray with negative values, because according to the definition of SDF, the SDF value of the coordinate point inside the object(interior points) is negative, the outside is positive, and the edge is 0. Therefore, when used for weighting, a negative sign is often added before the result of this function.</p> Source code in <code>ppsci/geometry/geometry_2d.py</code> <pre><code>def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute signed distance field.\n\n    Args:\n        points (np.ndarray): The coordinate points used to calculate the SDF value,\n            the shape of the array is [N, 2].\n\n    Returns:\n        np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n    NOTE: This function usually returns ndarray with negative values, because\n    according to the definition of SDF, the SDF value of the coordinate point inside\n    the object(interior points) is negative, the outside is positive, and the edge\n    is 0. Therefore, when used for weighting, a negative sign is often added before\n    the result of this function.\n    \"\"\"\n    if points.shape[1] != self.ndim:\n        raise ValueError(\n            f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n        )\n    center = (self.xmin + self.xmax) / 2\n    dist_to_boundary = (\n        np.abs(points - center) - np.array([self.xmax - self.xmin]) / 2\n    )\n    return (\n        np.linalg.norm(np.maximum(dist_to_boundary, 0), axis=1)\n        + np.minimum(np.max(dist_to_boundary, axis=1), 0)\n    ).reshape(-1, 1)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Triangle","title":"<code>Triangle</code>","text":"<p>               Bases: <code>Geometry</code></p> <p>Class for Triangle</p> <p>The order of vertices can be in a clockwise or counterclockwise direction. The vertices will be re-ordered in counterclockwise (right hand rule).</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>Tuple[float, float]</code> <p>First point of Triangle [x0, y0].</p> required <code>x2</code> <code>Tuple[float, float]</code> <p>Second point of Triangle [x1, y1].</p> required <code>x3</code> <code>Tuple[float, float]</code> <p>Third point of Triangle [x2, y2].</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.Triangle((0, 0), (1, 0), (0, 1))\n</code></pre> Source code in <code>ppsci/geometry/geometry_2d.py</code> <pre><code>class Triangle(geometry.Geometry):\n    \"\"\"Class for Triangle\n\n    The order of vertices can be in a clockwise or counterclockwise direction. The\n    vertices will be re-ordered in counterclockwise (right hand rule).\n\n    Args:\n        x1 (Tuple[float, float]): First point of Triangle [x0, y0].\n        x2 (Tuple[float, float]): Second point of Triangle [x1, y1].\n        x3 (Tuple[float, float]): Third point of Triangle [x2, y2].\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.Triangle((0, 0), (1, 0), (0, 1))\n    \"\"\"\n\n    def __init__(self, x1, x2, x3):\n        self.area = polygon_signed_area([x1, x2, x3])\n        # Clockwise\n        if self.area &lt; 0:\n            self.area = -self.area\n            x2, x3 = x3, x2\n\n        self.x1 = np.array(x1, dtype=paddle.get_default_dtype())\n        self.x2 = np.array(x2, dtype=paddle.get_default_dtype())\n        self.x3 = np.array(x3, dtype=paddle.get_default_dtype())\n\n        self.v12 = self.x2 - self.x1\n        self.v23 = self.x3 - self.x2\n        self.v31 = self.x1 - self.x3\n        self.l12 = np.linalg.norm(self.v12)\n        self.l23 = np.linalg.norm(self.v23)\n        self.l31 = np.linalg.norm(self.v31)\n        self.n12 = self.v12 / self.l12\n        self.n23 = self.v23 / self.l23\n        self.n31 = self.v31 / self.l31\n        self.n12_normal = clockwise_rotation_90(self.n12)\n        self.n23_normal = clockwise_rotation_90(self.n23)\n        self.n31_normal = clockwise_rotation_90(self.n31)\n        self.perimeter = self.l12 + self.l23 + self.l31\n\n        super().__init__(\n            2,\n            (np.minimum(x1, np.minimum(x2, x3)), np.maximum(x1, np.maximum(x2, x3))),\n            self.l12\n            * self.l23\n            * self.l31\n            / (\n                self.perimeter\n                * (self.l12 + self.l23 - self.l31)\n                * (self.l23 + self.l31 - self.l12)\n                * (self.l31 + self.l12 - self.l23)\n            )\n            ** 0.5,\n        )\n\n    def is_inside(self, x):\n        # https://stackoverflow.com/a/2049593/12679294\n        _sign = np.stack(\n            [\n                np.cross(self.v12, x - self.x1),\n                np.cross(self.v23, x - self.x2),\n                np.cross(self.v31, x - self.x3),\n            ],\n            axis=1,\n        )\n        return ~(np.any(_sign &gt; 0, axis=-1) &amp; np.any(_sign &lt; 0, axis=-1))\n\n    def on_boundary(self, x):\n        l1 = np.linalg.norm(x - self.x1, axis=-1)\n        l2 = np.linalg.norm(x - self.x2, axis=-1)\n        l3 = np.linalg.norm(x - self.x3, axis=-1)\n        return np.any(\n            np.isclose(\n                [l1 + l2 - self.l12, l2 + l3 - self.l23, l3 + l1 - self.l31],\n                0,\n                atol=1e-6,\n            ),\n            axis=0,\n        )\n\n    def boundary_normal(self, x):\n        l1 = np.linalg.norm(x - self.x1, axis=-1, keepdims=True)\n        l2 = np.linalg.norm(x - self.x2, axis=-1, keepdims=True)\n        l3 = np.linalg.norm(x - self.x3, axis=-1, keepdims=True)\n        on12 = np.isclose(l1 + l2, self.l12)\n        on23 = np.isclose(l2 + l3, self.l23)\n        on31 = np.isclose(l3 + l1, self.l31)\n        # Check points on the vertexes\n        if np.any(np.count_nonzero(np.hstack([on12, on23, on31]), axis=-1) &gt; 1):\n            raise ValueError(\n                \"{}.boundary_normal do not accept points on the vertexes.\".format(\n                    self.__class__.__name__\n                )\n            )\n        return self.n12_normal * on12 + self.n23_normal * on23 + self.n31_normal * on31\n\n    def random_points(self, n, random=\"pseudo\"):\n        # There are two methods for triangle point picking.\n        # Method 1 (used here):\n        # - https://math.stackexchange.com/questions/18686/uniform-random-point-in-triangle\n        # Method 2:\n        # - http://mathworld.wolfram.com/TrianglePointPicking.html\n        # - https://hbfs.wordpress.com/2010/10/05/random-points-in-a-triangle-generating-random-sequences-ii/\n        # - https://stackoverflow.com/questions/19654251/random-point-inside-triangle-inside-java\n        sqrt_r1 = np.sqrt(np.random.rand(n, 1))\n        r2 = np.random.rand(n, 1)\n        return (\n            (1 - sqrt_r1) * self.x1\n            + sqrt_r1 * (1 - r2) * self.x2\n            + r2 * sqrt_r1 * self.x3\n        )\n\n    def uniform_boundary_points(self, n):\n        density = n / self.perimeter\n        x12 = (\n            np.linspace(\n                0,\n                1,\n                num=int(np.ceil(density * self.l12)),\n                endpoint=False,\n                dtype=paddle.get_default_dtype(),\n            )[:, None]\n            * self.v12\n            + self.x1\n        )\n        x23 = (\n            np.linspace(\n                0,\n                1,\n                num=int(np.ceil(density * self.l23)),\n                endpoint=False,\n                dtype=paddle.get_default_dtype(),\n            )[:, None]\n            * self.v23\n            + self.x2\n        )\n        x31 = (\n            np.linspace(\n                0,\n                1,\n                num=int(np.ceil(density * self.l31)),\n                endpoint=False,\n                dtype=paddle.get_default_dtype(),\n            )[:, None]\n            * self.v31\n            + self.x3\n        )\n        x = np.vstack((x12, x23, x31))\n        if len(x) &gt; n:\n            x = x[0:n]\n        return x\n\n    def random_boundary_points(self, n, random=\"pseudo\"):\n        u = np.ravel(sampler.sample(n + 2, 1, random))\n        # Remove the possible points very close to the corners\n        u = u[np.logical_not(np.isclose(u, self.l12 / self.perimeter))]\n        u = u[np.logical_not(np.isclose(u, (self.l12 + self.l23) / self.perimeter))]\n        u = u[:n]\n\n        u *= self.perimeter\n        x = []\n        for l in u:\n            if l &lt; self.l12:\n                x.append(l * self.n12 + self.x1)\n            elif l &lt; self.l12 + self.l23:\n                x.append((l - self.l12) * self.n23 + self.x2)\n            else:\n                x.append((l - self.l12 - self.l23) * self.n31 + self.x3)\n        return np.vstack(x)\n\n    def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute signed distance field.\n\n        Args:\n            points (np.ndarray): The coordinate points used to calculate the SDF value,\n                the shape of the array is [N, 2].\n\n        Returns:\n            np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n        NOTE: This function usually returns ndarray with negative values, because\n        according to the definition of SDF, the SDF value of the coordinate point inside\n        the object(interior points) is negative, the outside is positive, and the edge\n        is 0. Therefore, when used for weighting, a negative sign is often added before\n        the result of this function.\n        \"\"\"\n        if points.shape[1] != self.ndim:\n            raise ValueError(\n                f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n            )\n        v1p = points - self.x1  # v1p: vector from x1 to points\n        v2p = points - self.x2\n        v3p = points - self.x3\n        # vv12_p: vertical vector of points to v12(If the vertical point is in the extension of v12,\n        # the vector will be the vector from x1 to points)\n        vv12_p = (\n            self.v12\n            * np.clip(np.dot(v1p, self.v12.reshape(2, -1)) / self.l12**2, 0, 1)\n            - v1p\n        )\n        vv23_p = (\n            self.v23\n            * np.clip(np.dot(v2p, self.v23.reshape(2, -1)) / self.l23**2, 0, 1)\n            - v2p\n        )\n        vv31_p = (\n            self.v31\n            * np.clip(np.dot(v3p, self.v31.reshape(2, -1)) / self.l31**2, 0, 1)\n            - v3p\n        )\n        is_inside = self.is_inside(points).reshape(-1, 1) * 2 - 1\n        len_vv12_p = np.linalg.norm(vv12_p, axis=1, keepdims=True)\n        len_vv23_p = np.linalg.norm(vv23_p, axis=1, keepdims=True)\n        len_vv31_p = np.linalg.norm(vv31_p, axis=1, keepdims=True)\n        mini_dist = np.minimum(np.minimum(len_vv12_p, len_vv23_p), len_vv31_p)\n        return is_inside * mini_dist\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Triangle.sdf_func","title":"<code>sdf_func(points)</code>","text":"<p>Compute signed distance field.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The coordinate points used to calculate the SDF value, the shape of the array is [N, 2].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: SDF values of input points without squared, the shape is [N, 1].</p> <p>NOTE: This function usually returns ndarray with negative values, because according to the definition of SDF, the SDF value of the coordinate point inside the object(interior points) is negative, the outside is positive, and the edge is 0. Therefore, when used for weighting, a negative sign is often added before the result of this function.</p> Source code in <code>ppsci/geometry/geometry_2d.py</code> <pre><code>def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute signed distance field.\n\n    Args:\n        points (np.ndarray): The coordinate points used to calculate the SDF value,\n            the shape of the array is [N, 2].\n\n    Returns:\n        np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n    NOTE: This function usually returns ndarray with negative values, because\n    according to the definition of SDF, the SDF value of the coordinate point inside\n    the object(interior points) is negative, the outside is positive, and the edge\n    is 0. Therefore, when used for weighting, a negative sign is often added before\n    the result of this function.\n    \"\"\"\n    if points.shape[1] != self.ndim:\n        raise ValueError(\n            f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n        )\n    v1p = points - self.x1  # v1p: vector from x1 to points\n    v2p = points - self.x2\n    v3p = points - self.x3\n    # vv12_p: vertical vector of points to v12(If the vertical point is in the extension of v12,\n    # the vector will be the vector from x1 to points)\n    vv12_p = (\n        self.v12\n        * np.clip(np.dot(v1p, self.v12.reshape(2, -1)) / self.l12**2, 0, 1)\n        - v1p\n    )\n    vv23_p = (\n        self.v23\n        * np.clip(np.dot(v2p, self.v23.reshape(2, -1)) / self.l23**2, 0, 1)\n        - v2p\n    )\n    vv31_p = (\n        self.v31\n        * np.clip(np.dot(v3p, self.v31.reshape(2, -1)) / self.l31**2, 0, 1)\n        - v3p\n    )\n    is_inside = self.is_inside(points).reshape(-1, 1) * 2 - 1\n    len_vv12_p = np.linalg.norm(vv12_p, axis=1, keepdims=True)\n    len_vv23_p = np.linalg.norm(vv23_p, axis=1, keepdims=True)\n    len_vv31_p = np.linalg.norm(vv31_p, axis=1, keepdims=True)\n    mini_dist = np.minimum(np.minimum(len_vv12_p, len_vv23_p), len_vv31_p)\n    return is_inside * mini_dist\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Cuboid","title":"<code>Cuboid</code>","text":"<p>               Bases: <code>Hypercube</code></p> <p>Class for Cuboid</p> <p>Parameters:</p> Name Type Description Default <code>xmin</code> <code>Tuple[float, float, float]</code> <p>Bottom left corner point [x0, y0, z0].</p> required <code>xmax</code> <code>Tuple[float, float, float]</code> <p>Top right corner point [x1, y1, z1].</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n</code></pre> Source code in <code>ppsci/geometry/geometry_3d.py</code> <pre><code>class Cuboid(geometry_nd.Hypercube):\n    \"\"\"Class for Cuboid\n\n    Args:\n        xmin (Tuple[float, float, float]): Bottom left corner point [x0, y0, z0].\n        xmax (Tuple[float, float, float]): Top right corner point [x1, y1, z1].\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.Cuboid((0, 0, 0), (1, 1, 1))\n    \"\"\"\n\n    def __init__(\n        self, xmin: Tuple[float, float, float], xmax: Tuple[float, float, float]\n    ):\n        super().__init__(xmin, xmax)\n        dx = self.xmax - self.xmin\n        self.area = 2 * np.sum(dx * np.roll(dx, 2))\n\n    def random_boundary_points(self, n, random=\"pseudo\"):\n        pts = []\n        density = n / self.area\n        rect = geometry_2d.Rectangle(self.xmin[:-1], self.xmax[:-1])\n        for z in [self.xmin[-1], self.xmax[-1]]:\n            u = rect.random_points(int(np.ceil(density * rect.area)), random=random)\n            pts.append(\n                np.hstack(\n                    (u, np.full((len(u), 1), z, dtype=paddle.get_default_dtype()))\n                )\n            )\n        rect = geometry_2d.Rectangle(self.xmin[::2], self.xmax[::2])\n        for y in [self.xmin[1], self.xmax[1]]:\n            u = rect.random_points(int(np.ceil(density * rect.area)), random=random)\n            pts.append(\n                np.hstack(\n                    (\n                        u[:, 0:1],\n                        np.full((len(u), 1), y, dtype=paddle.get_default_dtype()),\n                        u[:, 1:],\n                    )\n                )\n            )\n        rect = geometry_2d.Rectangle(self.xmin[1:], self.xmax[1:])\n        for x in [self.xmin[0], self.xmax[0]]:\n            u = rect.random_points(int(np.ceil(density * rect.area)), random=random)\n            pts.append(\n                np.hstack(\n                    (np.full((len(u), 1), x, dtype=paddle.get_default_dtype()), u)\n                )\n            )\n        pts = np.vstack(pts)\n        if len(pts) &gt; n:\n            return pts[np.random.choice(len(pts), size=n, replace=False)]\n        return pts\n\n    def uniform_boundary_points(self, n):\n        h = (self.area / n) ** 0.5\n        nx, ny, nz = np.ceil((self.xmax - self.xmin) / h).astype(int) + 1\n        x = np.linspace(\n            self.xmin[0], self.xmax[0], num=nx, dtype=paddle.get_default_dtype()\n        )\n        y = np.linspace(\n            self.xmin[1], self.xmax[1], num=ny, dtype=paddle.get_default_dtype()\n        )\n        z = np.linspace(\n            self.xmin[2], self.xmax[2], num=nz, dtype=paddle.get_default_dtype()\n        )\n\n        pts = []\n        for v in [self.xmin[-1], self.xmax[-1]]:\n            u = list(itertools.product(x, y))\n            pts.append(\n                np.hstack(\n                    (u, np.full((len(u), 1), v, dtype=paddle.get_default_dtype()))\n                )\n            )\n        if nz &gt; 2:\n            for v in [self.xmin[1], self.xmax[1]]:\n                u = np.array(\n                    list(itertools.product(x, z[1:-1])),\n                    dtype=paddle.get_default_dtype(),\n                )\n                pts.append(\n                    np.hstack(\n                        (\n                            u[:, 0:1],\n                            np.full((len(u), 1), v, dtype=paddle.get_default_dtype()),\n                            u[:, 1:],\n                        )\n                    )\n                )\n        if ny &gt; 2 and nz &gt; 2:\n            for v in [self.xmin[0], self.xmax[0]]:\n                u = list(itertools.product(y[1:-1], z[1:-1]))\n                pts.append(\n                    np.hstack(\n                        (np.full((len(u), 1), v, dtype=paddle.get_default_dtype()), u)\n                    )\n                )\n        pts = np.vstack(pts)\n        if len(pts) &gt; n:\n            return pts[np.random.choice(len(pts), size=n, replace=False)]\n        return pts\n\n    def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute signed distance field.\n\n        Args:\n            points (np.ndarray): The coordinate points used to calculate the SDF value,\n                the shape is [N, 3]\n\n        Returns:\n            np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n        NOTE: This function usually returns ndarray with negative values, because\n        according to the definition of SDF, the SDF value of the coordinate point inside\n        the object(interior points) is negative, the outside is positive, and the edge\n        is 0. Therefore, when used for weighting, a negative sign is often added before\n        the result of this function.\n        \"\"\"\n        if points.shape[1] != self.ndim:\n            raise ValueError(\n                f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n            )\n        sdf = (\n            ((self.xmax - self.xmin) / 2 - abs(points - (self.xmin + self.xmax) / 2))\n        ).min(axis=1)\n        sdf = -sdf[..., np.newaxis]\n        return sdf\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Cuboid.sdf_func","title":"<code>sdf_func(points)</code>","text":"<p>Compute signed distance field.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The coordinate points used to calculate the SDF value, the shape is [N, 3]</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: SDF values of input points without squared, the shape is [N, 1].</p> <p>NOTE: This function usually returns ndarray with negative values, because according to the definition of SDF, the SDF value of the coordinate point inside the object(interior points) is negative, the outside is positive, and the edge is 0. Therefore, when used for weighting, a negative sign is often added before the result of this function.</p> Source code in <code>ppsci/geometry/geometry_3d.py</code> <pre><code>def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute signed distance field.\n\n    Args:\n        points (np.ndarray): The coordinate points used to calculate the SDF value,\n            the shape is [N, 3]\n\n    Returns:\n        np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n    NOTE: This function usually returns ndarray with negative values, because\n    according to the definition of SDF, the SDF value of the coordinate point inside\n    the object(interior points) is negative, the outside is positive, and the edge\n    is 0. Therefore, when used for weighting, a negative sign is often added before\n    the result of this function.\n    \"\"\"\n    if points.shape[1] != self.ndim:\n        raise ValueError(\n            f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n        )\n    sdf = (\n        ((self.xmax - self.xmin) / 2 - abs(points - (self.xmin + self.xmax) / 2))\n    ).min(axis=1)\n    sdf = -sdf[..., np.newaxis]\n    return sdf\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Sphere","title":"<code>Sphere</code>","text":"<p>               Bases: <code>Hypersphere</code></p> <p>Class for Sphere</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>Tuple[float, float, float]</code> <p>Center of the sphere [x0, y0, z0].</p> required <code>radius</code> <code>float</code> <p>Radius of the sphere.</p> required Source code in <code>ppsci/geometry/geometry_3d.py</code> <pre><code>class Sphere(geometry_nd.Hypersphere):\n    \"\"\"Class for Sphere\n\n    Args:\n        center (Tuple[float, float, float]): Center of the sphere [x0, y0, z0].\n        radius (float): Radius of the sphere.\n    \"\"\"\n\n    def __init__(self, center, radius):\n        super().__init__(center, radius)\n\n    def uniform_boundary_points(self, n: int):\n        nl = np.arange(1, n + 1).astype(paddle.get_default_dtype())\n        g = (np.sqrt(5) - 1) / 2\n        z = (2 * nl - 1) / n - 1\n        x = np.sqrt(1 - z**2) * np.cos(2 * np.pi * nl * g)\n        y = np.sqrt(1 - z**2) * np.sin(2 * np.pi * nl * g)\n        return np.stack((x, y, z), axis=-1)\n\n    def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute signed distance field.\n\n        Args:\n            points (np.ndarray): The coordinate points used to calculate the SDF value,\n                the shape is [N, 3]\n\n        Returns:\n            np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n        NOTE: This function usually returns ndarray with negative values, because\n        according to the definition of SDF, the SDF value of the coordinate point inside\n        the object(interior points) is negative, the outside is positive, and the edge\n        is 0. Therefore, when used for weighting, a negative sign is often added before\n        the result of this function.\n        \"\"\"\n        if points.shape[1] != self.ndim:\n            raise ValueError(\n                f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n            )\n        sdf = self.radius - (((points - self.center) ** 2).sum(axis=1)) ** 0.5\n        sdf = -sdf[..., np.newaxis]\n        return sdf\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Sphere.sdf_func","title":"<code>sdf_func(points)</code>","text":"<p>Compute signed distance field.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The coordinate points used to calculate the SDF value, the shape is [N, 3]</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: SDF values of input points without squared, the shape is [N, 1].</p> <p>NOTE: This function usually returns ndarray with negative values, because according to the definition of SDF, the SDF value of the coordinate point inside the object(interior points) is negative, the outside is positive, and the edge is 0. Therefore, when used for weighting, a negative sign is often added before the result of this function.</p> Source code in <code>ppsci/geometry/geometry_3d.py</code> <pre><code>def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute signed distance field.\n\n    Args:\n        points (np.ndarray): The coordinate points used to calculate the SDF value,\n            the shape is [N, 3]\n\n    Returns:\n        np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n    NOTE: This function usually returns ndarray with negative values, because\n    according to the definition of SDF, the SDF value of the coordinate point inside\n    the object(interior points) is negative, the outside is positive, and the edge\n    is 0. Therefore, when used for weighting, a negative sign is often added before\n    the result of this function.\n    \"\"\"\n    if points.shape[1] != self.ndim:\n        raise ValueError(\n            f\"Shape of given points should be [*, {self.ndim}], but got {points.shape}\"\n        )\n    sdf = self.radius - (((points - self.center) ** 2).sum(axis=1)) ** 0.5\n    sdf = -sdf[..., np.newaxis]\n    return sdf\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Hypercube","title":"<code>Hypercube</code>","text":"<p>               Bases: <code>Geometry</code></p> <p>Multi-dimensional hyper cube.</p> <p>Parameters:</p> Name Type Description Default <code>xmin</code> <code>Tuple[float, ...]</code> <p>Lower corner point.</p> required <code>xmax</code> <code>Tuple[float, ...]</code> <p>Upper corner point.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.Hypercube((0, 0, 0, 0), (1, 1, 1, 1))\n</code></pre> Source code in <code>ppsci/geometry/geometry_nd.py</code> <pre><code>class Hypercube(geometry.Geometry):\n    \"\"\"Multi-dimensional hyper cube.\n\n    Args:\n        xmin (Tuple[float, ...]): Lower corner point.\n        xmax (Tuple[float, ...]): Upper corner point.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.Hypercube((0, 0, 0, 0), (1, 1, 1, 1))\n    \"\"\"\n\n    def __init__(self, xmin: Tuple[float, ...], xmax: Tuple[float, ...]):\n        if len(xmin) != len(xmax):\n            raise ValueError(\"Dimensions of xmin and xmax do not match.\")\n\n        self.xmin = np.array(xmin, dtype=paddle.get_default_dtype())\n        self.xmax = np.array(xmax, dtype=paddle.get_default_dtype())\n        if np.any(self.xmin &gt;= self.xmax):\n            raise ValueError(\"xmin &gt;= xmax\")\n\n        self.side_length = self.xmax - self.xmin\n        super().__init__(\n            len(xmin), (self.xmin, self.xmax), np.linalg.norm(self.side_length)\n        )\n        self.volume = np.prod(self.side_length, dtype=paddle.get_default_dtype())\n\n    def is_inside(self, x):\n        return np.logical_and(\n            np.all(x &gt;= self.xmin, axis=-1), np.all(x &lt;= self.xmax, axis=-1)\n        )\n\n    def on_boundary(self, x):\n        _on_boundary = np.logical_or(\n            np.any(np.isclose(x, self.xmin), axis=-1),\n            np.any(np.isclose(x, self.xmax), axis=-1),\n        )\n        return np.logical_and(self.is_inside(x), _on_boundary)\n\n    def boundary_normal(self, x):\n        _n = -np.isclose(x, self.xmin).astype(paddle.get_default_dtype()) + np.isclose(\n            x, self.xmax\n        )\n        # For vertices, the normal is averaged for all directions\n        idx = np.count_nonzero(_n, axis=-1) &gt; 1\n        if np.any(idx):\n            l = np.linalg.norm(_n[idx], axis=-1, keepdims=True)\n            _n[idx] /= l\n        return _n\n\n    def uniform_points(self, n, boundary=True):\n        dx = (self.volume / n) ** (1 / self.ndim)\n        xi = []\n        for i in range(self.ndim):\n            ni = int(np.ceil(self.side_length[i] / dx))\n            if boundary:\n                xi.append(\n                    np.linspace(\n                        self.xmin[i],\n                        self.xmax[i],\n                        num=ni,\n                        dtype=paddle.get_default_dtype(),\n                    )\n                )\n            else:\n                xi.append(\n                    np.linspace(\n                        self.xmin[i],\n                        self.xmax[i],\n                        num=ni + 1,\n                        endpoint=False,\n                        dtype=paddle.get_default_dtype(),\n                    )[1:]\n                )\n        x = np.array(list(itertools.product(*xi)), dtype=paddle.get_default_dtype())\n        if len(x) &gt; n:\n            x = x[0:n]\n        return x\n\n    def random_points(self, n, random=\"pseudo\"):\n        x = sampler.sample(n, self.ndim, random)\n        # print(f\"Hypercube's range: {self.__class__.__name__}\", self.xmin, self.xmax)\n        return (self.xmax - self.xmin) * x + self.xmin\n\n    def random_boundary_points(self, n, random=\"pseudo\"):\n        x = sampler.sample(n, self.ndim, random)\n        # Randomly pick a dimension\n        rand_dim = np.random.randint(self.ndim, size=n)\n        # Replace value of the randomly picked dimension with the nearest boundary value (0 or 1)\n        x[np.arange(n), rand_dim] = np.round(x[np.arange(n), rand_dim])\n        return (self.xmax - self.xmin) * x + self.xmin\n\n    def periodic_point(self, x, component):\n        y = misc.convert_to_array(x, self.dim_keys)\n        _on_xmin = np.isclose(y[:, component], self.xmin[component])\n        _on_xmax = np.isclose(y[:, component], self.xmax[component])\n        y[:, component][_on_xmin] = self.xmax[component]\n        y[:, component][_on_xmax] = self.xmin[component]\n        y_normal = self.boundary_normal(y)\n\n        y = misc.convert_to_dict(y, self.dim_keys)\n        y_normal = misc.convert_to_dict(\n            y_normal, [f\"normal_{k}\" for k in self.dim_keys]\n        )\n        return {**y, **y_normal}\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Hypersphere","title":"<code>Hypersphere</code>","text":"<p>               Bases: <code>Geometry</code></p> <p>Multi-dimensional hyper sphere.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>Tuple[float, ...]</code> <p>Center point coordinate.</p> required <code>radius</code> <code>Tuple[float, ...]</code> <p>Radius along each dimension.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.Hypersphere((0, 0, 0, 0), 1.0)\n</code></pre> Source code in <code>ppsci/geometry/geometry_nd.py</code> <pre><code>class Hypersphere(geometry.Geometry):\n    \"\"\"Multi-dimensional hyper sphere.\n\n    Args:\n        center (Tuple[float, ...]): Center point coordinate.\n        radius (Tuple[float, ...]): Radius along each dimension.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.Hypersphere((0, 0, 0, 0), 1.0)\n    \"\"\"\n\n    def __init__(self, center, radius):\n        self.center = np.array(center, dtype=paddle.get_default_dtype())\n        self.radius = radius\n        super().__init__(\n            len(center), (self.center - radius, self.center + radius), 2 * radius\n        )\n\n        self._r2 = radius**2\n\n    def is_inside(self, x):\n        return np.linalg.norm(x - self.center, axis=-1) &lt;= self.radius\n\n    def on_boundary(self, x):\n        return np.isclose(np.linalg.norm(x - self.center, axis=-1), self.radius)\n\n    def boundary_normal(self, x):\n        _n = x - self.center\n        l = np.linalg.norm(_n, axis=-1, keepdims=True)\n        _n = _n / l * np.isclose(l, self.radius)\n        return _n\n\n    def random_points(self, n, random=\"pseudo\"):\n        # https://math.stackexchange.com/questions/87230/picking-random-points-in-the-volume-of-sphere-with-uniform-probability\n        if random == \"pseudo\":\n            U = np.random.rand(n, 1).astype(paddle.get_default_dtype())\n            X = np.random.normal(size=(n, self.ndim)).astype(paddle.get_default_dtype())\n        else:\n            rng = sampler.sample(n, self.ndim + 1, random)\n            U, X = rng[:, 0:1], rng[:, 1:]  # Error if X = [0, 0, ...]\n            X = stats.norm.ppf(X).astype(paddle.get_default_dtype())\n        X = preprocessing.normalize(X)\n        X = U ** (1 / self.ndim) * X\n        return self.radius * X + self.center\n\n    def random_boundary_points(self, n, random=\"pseudo\"):\n        # http://mathworld.wolfram.com/HyperspherePointPicking.html\n        if random == \"pseudo\":\n            X = np.random.normal(size=(n, self.ndim)).astype(paddle.get_default_dtype())\n        else:\n            U = sampler.sample(\n                n, self.ndim, random\n            )  # Error for [0, 0, ...] or [0.5, 0.5, ...]\n            X = stats.norm.ppf(U).astype(paddle.get_default_dtype())\n        X = preprocessing.normalize(X)\n        return self.radius * X + self.center\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Mesh","title":"<code>Mesh</code>","text":"<p>               Bases: <code>Geometry</code></p> <p>Class for mesh geometry.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Union[str, Mesh]</code> <p>Mesh file path or mesh object, such as \"/path/to/mesh.stl\".</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.Mesh(\"/path/to/mesh.stl\")\n</code></pre> Source code in <code>ppsci/geometry/mesh.py</code> <pre><code>class Mesh(geometry.Geometry):\n    \"\"\"Class for mesh geometry.\n\n    Args:\n        mesh (Union[str, Mesh]): Mesh file path or mesh object, such as \"/path/to/mesh.stl\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.Mesh(\"/path/to/mesh.stl\")  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, mesh: Union[\"pymesh.Mesh\", str]):\n        # check if pymesh is installed when using Mesh Class\n        if not checker.dynamic_import_to_globals([\"pymesh\"]):\n            raise ImportError(\n                \"Could not import pymesh python package.\"\n                \"Please install it as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n            )\n        import pymesh\n\n        if isinstance(mesh, str):\n            self.py_mesh = pymesh.meshio.load_mesh(mesh)\n        elif isinstance(mesh, pymesh.Mesh):\n            self.py_mesh = mesh\n        else:\n            raise ValueError(\"arg `mesh` should be path string or `pymesh.Mesh`\")\n\n        self.init_mesh()\n\n    @classmethod\n    def from_pymesh(cls, mesh: \"pymesh.Mesh\") -&gt; \"Mesh\":\n        \"\"\"Instantiate Mesh object with given PyMesh object.\n\n        Args:\n            mesh (pymesh.Mesh): PyMesh object.\n\n        Returns:\n            Mesh: Instantiated ppsci.geometry.Mesh object.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; import pymesh  # doctest: +SKIP\n            &gt;&gt;&gt; import numpy as np  # doctest: +SKIP\n            &gt;&gt;&gt; box = pymesh.generate_box_mesh(np.array([0, 0, 0]), np.array([1, 1, 1]))  # doctest: +SKIP\n            &gt;&gt;&gt; mesh = ppsci.geometry.Mesh.from_pymesh(box)  # doctest: +SKIP\n            &gt;&gt;&gt; print(mesh.vertices)  # doctest: +SKIP\n            [[0. 0. 0.]\n             [1. 0. 0.]\n             [1. 1. 0.]\n             [0. 1. 0.]\n             [0. 0. 1.]\n             [1. 0. 1.]\n             [1. 1. 1.]\n             [0. 1. 1.]]\n        \"\"\"\n        # check if pymesh is installed when using Mesh Class\n        if not checker.dynamic_import_to_globals([\"pymesh\"]):\n            raise ImportError(\n                \"Could not import pymesh python package.\"\n                \"Please install it as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n            )\n        import pymesh\n\n        if isinstance(mesh, pymesh.Mesh):\n            return cls(mesh)\n        else:\n            raise ValueError(\n                f\"arg `mesh` should be type of `pymesh.Mesh`, but got {type(mesh)}\"\n            )\n\n    def init_mesh(self):\n        \"\"\"Initialize necessary variables for mesh\"\"\"\n        if \"face_normal\" not in self.py_mesh.get_attribute_names():\n            self.py_mesh.add_attribute(\"face_normal\")\n        self.face_normal = self.py_mesh.get_attribute(\"face_normal\").reshape([-1, 3])\n\n        if not checker.dynamic_import_to_globals([\"open3d\"]):\n            raise ImportError(\n                \"Could not import open3d python package. \"\n                \"Please install it with `pip install open3d`.\"\n            )\n        import open3d\n\n        self.open3d_mesh = open3d.geometry.TriangleMesh(\n            open3d.utility.Vector3dVector(np.array(self.py_mesh.vertices)),\n            open3d.utility.Vector3iVector(np.array(self.py_mesh.faces)),\n        )\n        self.open3d_mesh.compute_vertex_normals()\n\n        self.vertices = self.py_mesh.vertices\n        self.faces = self.py_mesh.faces\n        self.vectors = self.vertices[self.faces]\n        super().__init__(\n            self.vertices.shape[-1],\n            (np.amin(self.vertices, axis=0), np.amax(self.vertices, axis=0)),\n            np.inf,\n        )\n        self.v0 = self.vectors[:, 0]\n        self.v1 = self.vectors[:, 1]\n        self.v2 = self.vectors[:, 2]\n        self.num_vertices = self.py_mesh.num_vertices\n        self.num_faces = self.py_mesh.num_faces\n\n        if not checker.dynamic_import_to_globals([\"pysdf\"]):\n            raise ImportError(\n                \"Could not import pysdf python package. \"\n                \"Please install open3d with `pip install pysdf`.\"\n            )\n        import pysdf\n\n        self.pysdf = pysdf.SDF(self.vertices, self.faces)\n        self.bounds = (\n            ((np.min(self.vectors[:, :, 0])), np.max(self.vectors[:, :, 0])),\n            ((np.min(self.vectors[:, :, 1])), np.max(self.vectors[:, :, 1])),\n            ((np.min(self.vectors[:, :, 2])), np.max(self.vectors[:, :, 2])),\n        )\n\n    def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute signed distance field.\n\n        Args:\n            points (np.ndarray): The coordinate points used to calculate the SDF value,\n                the shape is [N, 3]\n\n        Returns:\n            np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n        NOTE: This function usually returns ndarray with negative values, because\n        according to the definition of SDF, the SDF value of the coordinate point inside\n        the object(interior points) is negative, the outside is positive, and the edge\n        is 0. Therefore, when used for weighting, a negative sign is often added before\n        the result of this function.\n        \"\"\"\n        if not checker.dynamic_import_to_globals([\"pymesh\"]):\n            raise ImportError(\n                \"Could not import pymesh python package.\"\n                \"Please install it as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n            )\n        import pymesh\n\n        sdf, _, _, _ = pymesh.signed_distance_to_mesh(self.py_mesh, points)\n        sdf = sdf[..., np.newaxis].astype(paddle.get_default_dtype())\n        return sdf\n\n    def is_inside(self, x):\n        # NOTE: point on boundary is included\n        return self.pysdf.contains(x)\n\n    def on_boundary(self, x):\n        return np.isclose(self.sdf_func(x), 0.0).flatten()\n\n    def translate(self, translation: np.ndarray, relative: bool = True) -&gt; \"Mesh\":\n        \"\"\"Translate by given offsets.\n\n        NOTE: This API generate a completely new Mesh object with translated geometry,\n        without modifying original Mesh object inplace.\n\n        Args:\n            translation (np.ndarray): Translation offsets, numpy array of shape (3,):\n                [offset_x, offset_y, offset_z].\n            relative (bool, optional): Whether translate relatively. Defaults to True.\n\n        Returns:\n            Mesh: Translated Mesh object.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; import pymesh  # doctest: +SKIP\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; box = pymesh.generate_box_mesh(np.array([0, 0, 0]), np.array([1, 1, 1]))  # doctest: +SKIP\n            &gt;&gt;&gt; mesh = ppsci.geometry.Mesh(box)  # doctest: +SKIP\n            &gt;&gt;&gt; print(mesh.vertices)  # doctest: +SKIP\n            [[0. 0. 0.]\n             [1. 0. 0.]\n             [1. 1. 0.]\n             [0. 1. 0.]\n             [0. 0. 1.]\n             [1. 0. 1.]\n             [1. 1. 1.]\n             [0. 1. 1.]]\n            &gt;&gt;&gt; print(mesh.translate((-0.5, 0, 0.5), False).vertices) # the center is moved to the translation vector.  # doctest: +SKIP\n            [[-1.  -0.5  0. ]\n             [ 0.  -0.5  0. ]\n             [ 0.   0.5  0. ]\n             [-1.   0.5  0. ]\n             [-1.  -0.5  1. ]\n             [ 0.  -0.5  1. ]\n             [ 0.   0.5  1. ]\n             [-1.   0.5  1. ]]\n            &gt;&gt;&gt; print(mesh.translate((-0.5, 0, 0.5), True).vertices) # the translation vector is directly added to the geometry coordinates  # doctest: +SKIP\n            [[-0.5  0.   0.5]\n             [ 0.5  0.   0.5]\n             [ 0.5  1.   0.5]\n             [-0.5  1.   0.5]\n             [-0.5  0.   1.5]\n             [ 0.5  0.   1.5]\n             [ 0.5  1.   1.5]\n             [-0.5  1.   1.5]]\n        \"\"\"\n        vertices = np.array(self.vertices, dtype=paddle.get_default_dtype())\n        faces = np.array(self.faces)\n\n        if not checker.dynamic_import_to_globals((\"open3d\", \"pymesh\")):\n            raise ImportError(\n                \"Could not import open3d and pymesh python package. \"\n                \"Please install open3d with `pip install open3d` and \"\n                \"pymesh as https://paddlescience-docs.readthedocs.io/zh/latest/zh/install_setup/#__tabbed_4_1\"\n            )\n        import open3d  # isort:skip\n        import pymesh  # isort:skip\n\n        open3d_mesh = open3d.geometry.TriangleMesh(\n            open3d.utility.Vector3dVector(vertices),\n            open3d.utility.Vector3iVector(faces),\n        )\n        open3d_mesh = open3d_mesh.translate(translation, relative)\n        translated_mesh = pymesh.form_mesh(\n            np.asarray(open3d_mesh.vertices, dtype=paddle.get_default_dtype()), faces\n        )\n        # Generate a new Mesh object using class method\n        return Mesh.from_pymesh(translated_mesh)\n\n    def scale(\n        self, scale: float, center: Tuple[float, float, float] = (0, 0, 0)\n    ) -&gt; \"Mesh\":\n        \"\"\"Scale by given scale coefficient and center coordinate.\n\n        NOTE: This API generate a completely new Mesh object with scaled geometry,\n        without modifying original Mesh object inplace.\n\n        Args:\n            scale (float): Scale coefficient.\n            center (Tuple[float,float,float], optional): Center coordinate, [x, y, z].\n                Defaults to (0, 0, 0).\n\n        Returns:\n            Mesh: Scaled Mesh object.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; import pymesh  # doctest: +SKIP\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; box = pymesh.generate_box_mesh(np.array([0, 0, 0]), np.array([1, 1, 1]))  # doctest: +SKIP\n            &gt;&gt;&gt; mesh = ppsci.geometry.Mesh(box)  # doctest: +SKIP\n            &gt;&gt;&gt; print(mesh.vertices)  # doctest: +SKIP\n            [[0. 0. 0.]\n             [1. 0. 0.]\n             [1. 1. 0.]\n             [0. 1. 0.]\n             [0. 0. 1.]\n             [1. 0. 1.]\n             [1. 1. 1.]\n             [0. 1. 1.]]\n            &gt;&gt;&gt; mesh = mesh.scale(2, (0.25, 0.5, 0.75))  # doctest: +SKIP\n            &gt;&gt;&gt; print(mesh.vertices)  # doctest: +SKIP\n            [[-0.25 -0.5  -0.75]\n             [ 1.75 -0.5  -0.75]\n             [ 1.75  1.5  -0.75]\n             [-0.25  1.5  -0.75]\n             [-0.25 -0.5   1.25]\n             [ 1.75 -0.5   1.25]\n             [ 1.75  1.5   1.25]\n             [-0.25  1.5   1.25]]\n        \"\"\"\n        vertices = np.array(self.vertices, dtype=paddle.get_default_dtype())\n        faces = np.array(self.faces, dtype=paddle.get_default_dtype())\n\n        if not checker.dynamic_import_to_globals((\"open3d\", \"pymesh\")):\n            raise ImportError(\n                \"Could not import open3d and pymesh python package. \"\n                \"Please install open3d with `pip install open3d` and \"\n                \"pymesh as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n            )\n        import open3d  # isort:skip\n        import pymesh  # isort:skip\n\n        open3d_mesh = open3d.geometry.TriangleMesh(\n            open3d.utility.Vector3dVector(vertices),\n            open3d.utility.Vector3iVector(faces),\n        )\n        open3d_mesh = open3d_mesh.scale(scale, center)\n        scaled_pymesh = pymesh.form_mesh(\n            np.asarray(open3d_mesh.vertices, dtype=paddle.get_default_dtype()), faces\n        )\n        # Generate a new Mesh object using class method\n        return Mesh.from_pymesh(scaled_pymesh)\n\n    def uniform_boundary_points(self, n: int):\n        \"\"\"Compute the equi-spaced points on the boundary.\"\"\"\n        return self.pysdf.sample_surface(n)\n\n    def inflated_random_points(self, n, distance, random=\"pseudo\", criteria=None):\n        if not isinstance(n, (tuple, list)):\n            n = [n]\n        if not isinstance(distance, (tuple, list)):\n            distance = [distance]\n        if len(n) != len(distance):\n            raise ValueError(\n                f\"len(n)({len(n)}) should be equal to len(distance)({len(distance)})\"\n            )\n\n        from ppsci.geometry import inflation\n\n        all_points = []\n        all_areas = []\n        for _n, _dist in zip(n, distance):\n            inflated_mesh = Mesh(inflation.pymesh_inflation(self.py_mesh, _dist))\n            points, areas = inflated_mesh.random_points(_n, random, criteria)\n            all_points.append(points)\n            all_areas.append(areas)\n\n        all_points = np.concatenate(all_points, axis=0)\n        all_areas = np.concatenate(all_areas, axis=0)\n        return all_points, all_areas\n\n    def _approximate_area(\n        self,\n        random: Literal[\"pseudo\"] = \"pseudo\",\n        criteria: Optional[Callable] = None,\n        n_appr: int = 10000,\n    ) -&gt; float:\n        \"\"\"Approximate area with given `criteria` and `n_appr` points by Monte Carlo\n        algorithm.\n\n        Args:\n            random (str, optional): Random method. Defaults to \"pseudo\".\n            criteria (Optional[Callable]): Criteria function. Defaults to None.\n            n_appr (int): Number of points for approximating area. Defaults to 10000.\n\n        Returns:\n            np.ndarray: Approximated areas with shape of [n_faces, ].\n        \"\"\"\n        triangle_areas = area_of_triangles(self.v0, self.v1, self.v2)\n        triangle_probabilities = triangle_areas / np.linalg.norm(triangle_areas, ord=1)\n        triangle_index = np.arange(triangle_probabilities.shape[0])\n        npoint_per_triangle = np.random.choice(\n            triangle_index, n_appr, p=triangle_probabilities\n        )\n        npoint_per_triangle, _ = np.histogram(\n            npoint_per_triangle,\n            np.arange(triangle_probabilities.shape[0] + 1) - 0.5,\n        )\n\n        appr_areas = []\n        if criteria is not None:\n            aux_points = []\n\n        for i, npoint in enumerate(npoint_per_triangle):\n            if npoint == 0:\n                continue\n            # sample points for computing criteria mask if criteria is given\n            if criteria is not None:\n                points_at_triangle_i = sample_in_triangle(\n                    self.v0[i], self.v1[i], self.v2[i], npoint, random\n                )\n                aux_points.append(points_at_triangle_i)\n\n            appr_areas.append(\n                np.full(\n                    (npoint, 1), triangle_areas[i] / npoint, paddle.get_default_dtype()\n                )\n            )\n        appr_areas = np.concatenate(appr_areas, axis=0)  # [n_appr, 1]\n\n        # set invalid area to 0 by computing criteria mask with auxiliary points\n        if criteria is not None:\n            aux_points = np.concatenate(aux_points, axis=0)  # [n_appr, 3]\n            criteria_mask = criteria(*np.split(aux_points, self.ndim, 1))\n            appr_areas *= criteria_mask\n        return appr_areas.sum()\n\n    def random_boundary_points(self, n, random=\"pseudo\"):\n        triangle_area = area_of_triangles(self.v0, self.v1, self.v2)\n        triangle_prob = triangle_area / np.linalg.norm(triangle_area, ord=1)\n        npoint_per_triangle = np.random.choice(\n            np.arange(len(triangle_prob)), n, p=triangle_prob\n        )\n        npoint_per_triangle, _ = np.histogram(\n            npoint_per_triangle, np.arange(len(triangle_prob) + 1) - 0.5\n        )\n\n        points = []\n        normal = []\n        areas = []\n        for i, npoint in enumerate(npoint_per_triangle):\n            if npoint == 0:\n                continue\n            points_at_triangle_i = sample_in_triangle(\n                self.v0[i], self.v1[i], self.v2[i], npoint, random\n            )\n            normal_at_triangle_i = np.tile(self.face_normal[i], (npoint, 1)).astype(\n                paddle.get_default_dtype()\n            )\n            areas_at_triangle_i = np.full(\n                (npoint, 1),\n                triangle_area[i] / npoint,\n                dtype=paddle.get_default_dtype(),\n            )\n\n            points.append(points_at_triangle_i)\n            normal.append(normal_at_triangle_i)\n            areas.append(areas_at_triangle_i)\n\n        points = np.concatenate(points, axis=0)\n        normal = np.concatenate(normal, axis=0)\n        areas = np.concatenate(areas, axis=0)\n\n        return points, normal, areas\n\n    def sample_boundary(\n        self,\n        n: int,\n        random: Literal[\"pseudo\"] = \"pseudo\",\n        criteria: Optional[Callable[..., np.ndarray]] = None,\n        evenly: bool = False,\n        inflation_dist: Union[float, Tuple[float, ...]] = None,\n    ) -&gt; Dict[str, np.ndarray]:\n        # TODO(sensen): Support for time-dependent points(repeat data in time)\n        if inflation_dist is not None:\n            if not isinstance(n, (tuple, list)):\n                n = [n]\n            if not isinstance(inflation_dist, (tuple, list)):\n                inflation_dist = [inflation_dist]\n            if len(n) != len(inflation_dist):\n                raise ValueError(\n                    f\"len(n)({len(n)}) should be equal to len(inflation_dist)({len(inflation_dist)})\"\n                )\n\n            from ppsci.geometry import inflation\n\n            inflated_data_dict = {}\n            for _n, _dist in zip(n, inflation_dist):\n                # 1. manually inflate mesh at first\n                inflated_mesh = Mesh(inflation.pymesh_inflation(self.py_mesh, _dist))\n                # 2. compute all data by sample_boundary with `inflation_dist=None`\n                data_dict = inflated_mesh.sample_boundary(\n                    _n,\n                    random,\n                    criteria,\n                    evenly,\n                    inflation_dist=None,\n                )\n                for key, value in data_dict.items():\n                    if key not in inflated_data_dict:\n                        inflated_data_dict[key] = value\n                    else:\n                        inflated_data_dict[key] = np.concatenate(\n                            (inflated_data_dict[key], value), axis=0\n                        )\n            return inflated_data_dict\n        else:\n            if evenly:\n                raise ValueError(\n                    \"Can't sample evenly on mesh now, please set evenly=False.\"\n                )\n            _size, _ntry, _nsuc = 0, 0, 0\n            all_points = []\n            all_normal = []\n            while _size &lt; n:\n                points, normal, _ = self.random_boundary_points(n, random)\n                if criteria is not None:\n                    criteria_mask = criteria(\n                        *np.split(points, self.ndim, axis=1)\n                    ).flatten()\n                    points = points[criteria_mask]\n                    normal = normal[criteria_mask]\n\n                if len(points) &gt; n - _size:\n                    points = points[: n - _size]\n                    normal = normal[: n - _size]\n\n                all_points.append(points)\n                all_normal.append(normal)\n\n                _size += len(points)\n                _ntry += 1\n                if len(points) &gt; 0:\n                    _nsuc += 1\n\n                if _ntry &gt;= 1000 and _nsuc == 0:\n                    raise ValueError(\n                        \"Sample boundary points failed, \"\n                        \"please check correctness of geometry and given criteria.\"\n                    )\n\n            all_points = np.concatenate(all_points, axis=0)\n            all_normal = np.concatenate(all_normal, axis=0)\n            appr_area = self._approximate_area(random, criteria)\n            all_areas = np.full((n, 1), appr_area / n, paddle.get_default_dtype())\n\n        x_dict = misc.convert_to_dict(all_points, self.dim_keys)\n        normal_dict = misc.convert_to_dict(\n            all_normal, [f\"normal_{key}\" for key in self.dim_keys if key != \"t\"]\n        )\n        area_dict = misc.convert_to_dict(all_areas, [\"area\"])\n        return {**x_dict, **normal_dict, **area_dict}\n\n    def random_points(self, n, random=\"pseudo\", criteria=None):\n        _size = 0\n        all_points = []\n        cuboid = geometry_3d.Cuboid(\n            [bound[0] for bound in self.bounds],\n            [bound[1] for bound in self.bounds],\n        )\n        _nsample, _nvalid = 0, 0\n        while _size &lt; n:\n            random_points = cuboid.random_points(n, random)\n            valid_mask = self.is_inside(random_points)\n\n            if criteria:\n                valid_mask &amp;= criteria(\n                    *np.split(random_points, self.ndim, axis=1)\n                ).flatten()\n            valid_points = random_points[valid_mask]\n            _nvalid += len(valid_points)\n\n            if len(valid_points) &gt; n - _size:\n                valid_points = valid_points[: n - _size]\n\n            all_points.append(valid_points)\n            _size += len(valid_points)\n            _nsample += n\n\n        all_points = np.concatenate(all_points, axis=0)\n        cuboid_volume = np.prod([b[1] - b[0] for b in self.bounds])\n        all_areas = np.full(\n            (n, 1), cuboid_volume * (_nvalid / _nsample) / n, paddle.get_default_dtype()\n        )\n        return all_points, all_areas\n\n    def sample_interior(\n        self,\n        n: int,\n        random: Literal[\"pseudo\"] = \"pseudo\",\n        criteria: Optional[Callable[..., np.ndarray]] = None,\n        evenly: bool = False,\n        compute_sdf_derivatives: bool = False,\n    ):\n        \"\"\"Sample random points in the geometry and return those meet criteria.\"\"\"\n        if evenly:\n            # TODO(sensen): Implement uniform sample for mesh interior.\n            raise NotImplementedError(\n                \"uniformly sample for interior in mesh is not support yet, \"\n                \"you may need to set evenly=False in config dict of constraint\"\n            )\n        points, areas = self.random_points(n, random, criteria)\n\n        x_dict = misc.convert_to_dict(points, self.dim_keys)\n        area_dict = misc.convert_to_dict(areas, (\"area\",))\n\n        # NOTE: add negative to the sdf values because weight should be positive.\n        sdf = -self.sdf_func(points)\n        sdf_dict = misc.convert_to_dict(sdf, (\"sdf\",))\n\n        sdf_derives_dict = {}\n        if compute_sdf_derivatives:\n            sdf_derives = -self.sdf_derivatives(points)\n            sdf_derives_dict = misc.convert_to_dict(\n                sdf_derives, tuple(f\"sdf__{key}\" for key in self.dim_keys)\n            )\n\n        return {**x_dict, **area_dict, **sdf_dict, **sdf_derives_dict}\n\n    def union(self, other: \"Mesh\"):\n        if not checker.dynamic_import_to_globals([\"pymesh\"]):\n            raise ImportError(\n                \"Could not import pymesh python package. \"\n                \"Please install it as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n            )\n        import pymesh\n\n        csg = pymesh.CSGTree(\n            {\"union\": [{\"mesh\": self.py_mesh}, {\"mesh\": other.py_mesh}]}\n        )\n        return Mesh(csg.mesh)\n\n    def __or__(self, other: \"Mesh\"):\n        return self.union(other)\n\n    def __add__(self, other: \"Mesh\"):\n        return self.union(other)\n\n    def difference(self, other: \"Mesh\"):\n        if not checker.dynamic_import_to_globals([\"pymesh\"]):\n            raise ImportError(\n                \"Could not import pymesh python package. \"\n                \"Please install it as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n            )\n        import pymesh\n\n        csg = pymesh.CSGTree(\n            {\"difference\": [{\"mesh\": self.py_mesh}, {\"mesh\": other.py_mesh}]}\n        )\n        return Mesh(csg.mesh)\n\n    def __sub__(self, other: \"Mesh\"):\n        return self.difference(other)\n\n    def intersection(self, other: \"Mesh\"):\n        if not checker.dynamic_import_to_globals([\"pymesh\"]):\n            raise ImportError(\n                \"Could not import pymesh python package. \"\n                \"Please install it as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n            )\n        import pymesh\n\n        csg = pymesh.CSGTree(\n            {\"intersection\": [{\"mesh\": self.py_mesh}, {\"mesh\": other.py_mesh}]}\n        )\n        return Mesh(csg.mesh)\n\n    def __and__(self, other: \"Mesh\"):\n        return self.intersection(other)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the name of class\"\"\"\n        return \", \".join(\n            [\n                self.__class__.__name__,\n                f\"num_vertices = {self.num_vertices}\",\n                f\"num_faces = {self.num_faces}\",\n                f\"bounds = {self.bounds}\",\n                f\"dim_keys = {self.dim_keys}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Mesh.__str__","title":"<code>__str__()</code>","text":"<p>Return the name of class</p> Source code in <code>ppsci/geometry/mesh.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the name of class\"\"\"\n    return \", \".join(\n        [\n            self.__class__.__name__,\n            f\"num_vertices = {self.num_vertices}\",\n            f\"num_faces = {self.num_faces}\",\n            f\"bounds = {self.bounds}\",\n            f\"dim_keys = {self.dim_keys}\",\n        ]\n    )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Mesh.from_pymesh","title":"<code>from_pymesh(mesh)</code>  <code>classmethod</code>","text":"<p>Instantiate Mesh object with given PyMesh object.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Mesh</code> <p>PyMesh object.</p> required <p>Returns:</p> Name Type Description <code>Mesh</code> <code>'Mesh'</code> <p>Instantiated ppsci.geometry.Mesh object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import pymesh\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; box = pymesh.generate_box_mesh(np.array([0, 0, 0]), np.array([1, 1, 1]))\n&gt;&gt;&gt; mesh = ppsci.geometry.Mesh.from_pymesh(box)\n&gt;&gt;&gt; print(mesh.vertices)\n[[0. 0. 0.]\n [1. 0. 0.]\n [1. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 1.]\n [1. 1. 1.]\n [0. 1. 1.]]\n</code></pre> Source code in <code>ppsci/geometry/mesh.py</code> <pre><code>@classmethod\ndef from_pymesh(cls, mesh: \"pymesh.Mesh\") -&gt; \"Mesh\":\n    \"\"\"Instantiate Mesh object with given PyMesh object.\n\n    Args:\n        mesh (pymesh.Mesh): PyMesh object.\n\n    Returns:\n        Mesh: Instantiated ppsci.geometry.Mesh object.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import pymesh  # doctest: +SKIP\n        &gt;&gt;&gt; import numpy as np  # doctest: +SKIP\n        &gt;&gt;&gt; box = pymesh.generate_box_mesh(np.array([0, 0, 0]), np.array([1, 1, 1]))  # doctest: +SKIP\n        &gt;&gt;&gt; mesh = ppsci.geometry.Mesh.from_pymesh(box)  # doctest: +SKIP\n        &gt;&gt;&gt; print(mesh.vertices)  # doctest: +SKIP\n        [[0. 0. 0.]\n         [1. 0. 0.]\n         [1. 1. 0.]\n         [0. 1. 0.]\n         [0. 0. 1.]\n         [1. 0. 1.]\n         [1. 1. 1.]\n         [0. 1. 1.]]\n    \"\"\"\n    # check if pymesh is installed when using Mesh Class\n    if not checker.dynamic_import_to_globals([\"pymesh\"]):\n        raise ImportError(\n            \"Could not import pymesh python package.\"\n            \"Please install it as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n        )\n    import pymesh\n\n    if isinstance(mesh, pymesh.Mesh):\n        return cls(mesh)\n    else:\n        raise ValueError(\n            f\"arg `mesh` should be type of `pymesh.Mesh`, but got {type(mesh)}\"\n        )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Mesh.init_mesh","title":"<code>init_mesh()</code>","text":"<p>Initialize necessary variables for mesh</p> Source code in <code>ppsci/geometry/mesh.py</code> <pre><code>def init_mesh(self):\n    \"\"\"Initialize necessary variables for mesh\"\"\"\n    if \"face_normal\" not in self.py_mesh.get_attribute_names():\n        self.py_mesh.add_attribute(\"face_normal\")\n    self.face_normal = self.py_mesh.get_attribute(\"face_normal\").reshape([-1, 3])\n\n    if not checker.dynamic_import_to_globals([\"open3d\"]):\n        raise ImportError(\n            \"Could not import open3d python package. \"\n            \"Please install it with `pip install open3d`.\"\n        )\n    import open3d\n\n    self.open3d_mesh = open3d.geometry.TriangleMesh(\n        open3d.utility.Vector3dVector(np.array(self.py_mesh.vertices)),\n        open3d.utility.Vector3iVector(np.array(self.py_mesh.faces)),\n    )\n    self.open3d_mesh.compute_vertex_normals()\n\n    self.vertices = self.py_mesh.vertices\n    self.faces = self.py_mesh.faces\n    self.vectors = self.vertices[self.faces]\n    super().__init__(\n        self.vertices.shape[-1],\n        (np.amin(self.vertices, axis=0), np.amax(self.vertices, axis=0)),\n        np.inf,\n    )\n    self.v0 = self.vectors[:, 0]\n    self.v1 = self.vectors[:, 1]\n    self.v2 = self.vectors[:, 2]\n    self.num_vertices = self.py_mesh.num_vertices\n    self.num_faces = self.py_mesh.num_faces\n\n    if not checker.dynamic_import_to_globals([\"pysdf\"]):\n        raise ImportError(\n            \"Could not import pysdf python package. \"\n            \"Please install open3d with `pip install pysdf`.\"\n        )\n    import pysdf\n\n    self.pysdf = pysdf.SDF(self.vertices, self.faces)\n    self.bounds = (\n        ((np.min(self.vectors[:, :, 0])), np.max(self.vectors[:, :, 0])),\n        ((np.min(self.vectors[:, :, 1])), np.max(self.vectors[:, :, 1])),\n        ((np.min(self.vectors[:, :, 2])), np.max(self.vectors[:, :, 2])),\n    )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Mesh.sample_interior","title":"<code>sample_interior(n, random='pseudo', criteria=None, evenly=False, compute_sdf_derivatives=False)</code>","text":"<p>Sample random points in the geometry and return those meet criteria.</p> Source code in <code>ppsci/geometry/mesh.py</code> <pre><code>def sample_interior(\n    self,\n    n: int,\n    random: Literal[\"pseudo\"] = \"pseudo\",\n    criteria: Optional[Callable[..., np.ndarray]] = None,\n    evenly: bool = False,\n    compute_sdf_derivatives: bool = False,\n):\n    \"\"\"Sample random points in the geometry and return those meet criteria.\"\"\"\n    if evenly:\n        # TODO(sensen): Implement uniform sample for mesh interior.\n        raise NotImplementedError(\n            \"uniformly sample for interior in mesh is not support yet, \"\n            \"you may need to set evenly=False in config dict of constraint\"\n        )\n    points, areas = self.random_points(n, random, criteria)\n\n    x_dict = misc.convert_to_dict(points, self.dim_keys)\n    area_dict = misc.convert_to_dict(areas, (\"area\",))\n\n    # NOTE: add negative to the sdf values because weight should be positive.\n    sdf = -self.sdf_func(points)\n    sdf_dict = misc.convert_to_dict(sdf, (\"sdf\",))\n\n    sdf_derives_dict = {}\n    if compute_sdf_derivatives:\n        sdf_derives = -self.sdf_derivatives(points)\n        sdf_derives_dict = misc.convert_to_dict(\n            sdf_derives, tuple(f\"sdf__{key}\" for key in self.dim_keys)\n        )\n\n    return {**x_dict, **area_dict, **sdf_dict, **sdf_derives_dict}\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Mesh.scale","title":"<code>scale(scale, center=(0, 0, 0))</code>","text":"<p>Scale by given scale coefficient and center coordinate.</p> <p>NOTE: This API generate a completely new Mesh object with scaled geometry, without modifying original Mesh object inplace.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>Scale coefficient.</p> required <code>center</code> <code>Tuple[float, float, float]</code> <p>Center coordinate, [x, y, z]. Defaults to (0, 0, 0).</p> <code>(0, 0, 0)</code> <p>Returns:</p> Name Type Description <code>Mesh</code> <code>'Mesh'</code> <p>Scaled Mesh object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import pymesh\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; box = pymesh.generate_box_mesh(np.array([0, 0, 0]), np.array([1, 1, 1]))\n&gt;&gt;&gt; mesh = ppsci.geometry.Mesh(box)\n&gt;&gt;&gt; print(mesh.vertices)\n[[0. 0. 0.]\n [1. 0. 0.]\n [1. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 1.]\n [1. 1. 1.]\n [0. 1. 1.]]\n&gt;&gt;&gt; mesh = mesh.scale(2, (0.25, 0.5, 0.75))\n&gt;&gt;&gt; print(mesh.vertices)\n[[-0.25 -0.5  -0.75]\n [ 1.75 -0.5  -0.75]\n [ 1.75  1.5  -0.75]\n [-0.25  1.5  -0.75]\n [-0.25 -0.5   1.25]\n [ 1.75 -0.5   1.25]\n [ 1.75  1.5   1.25]\n [-0.25  1.5   1.25]]\n</code></pre> Source code in <code>ppsci/geometry/mesh.py</code> <pre><code>def scale(\n    self, scale: float, center: Tuple[float, float, float] = (0, 0, 0)\n) -&gt; \"Mesh\":\n    \"\"\"Scale by given scale coefficient and center coordinate.\n\n    NOTE: This API generate a completely new Mesh object with scaled geometry,\n    without modifying original Mesh object inplace.\n\n    Args:\n        scale (float): Scale coefficient.\n        center (Tuple[float,float,float], optional): Center coordinate, [x, y, z].\n            Defaults to (0, 0, 0).\n\n    Returns:\n        Mesh: Scaled Mesh object.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import pymesh  # doctest: +SKIP\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; box = pymesh.generate_box_mesh(np.array([0, 0, 0]), np.array([1, 1, 1]))  # doctest: +SKIP\n        &gt;&gt;&gt; mesh = ppsci.geometry.Mesh(box)  # doctest: +SKIP\n        &gt;&gt;&gt; print(mesh.vertices)  # doctest: +SKIP\n        [[0. 0. 0.]\n         [1. 0. 0.]\n         [1. 1. 0.]\n         [0. 1. 0.]\n         [0. 0. 1.]\n         [1. 0. 1.]\n         [1. 1. 1.]\n         [0. 1. 1.]]\n        &gt;&gt;&gt; mesh = mesh.scale(2, (0.25, 0.5, 0.75))  # doctest: +SKIP\n        &gt;&gt;&gt; print(mesh.vertices)  # doctest: +SKIP\n        [[-0.25 -0.5  -0.75]\n         [ 1.75 -0.5  -0.75]\n         [ 1.75  1.5  -0.75]\n         [-0.25  1.5  -0.75]\n         [-0.25 -0.5   1.25]\n         [ 1.75 -0.5   1.25]\n         [ 1.75  1.5   1.25]\n         [-0.25  1.5   1.25]]\n    \"\"\"\n    vertices = np.array(self.vertices, dtype=paddle.get_default_dtype())\n    faces = np.array(self.faces, dtype=paddle.get_default_dtype())\n\n    if not checker.dynamic_import_to_globals((\"open3d\", \"pymesh\")):\n        raise ImportError(\n            \"Could not import open3d and pymesh python package. \"\n            \"Please install open3d with `pip install open3d` and \"\n            \"pymesh as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n        )\n    import open3d  # isort:skip\n    import pymesh  # isort:skip\n\n    open3d_mesh = open3d.geometry.TriangleMesh(\n        open3d.utility.Vector3dVector(vertices),\n        open3d.utility.Vector3iVector(faces),\n    )\n    open3d_mesh = open3d_mesh.scale(scale, center)\n    scaled_pymesh = pymesh.form_mesh(\n        np.asarray(open3d_mesh.vertices, dtype=paddle.get_default_dtype()), faces\n    )\n    # Generate a new Mesh object using class method\n    return Mesh.from_pymesh(scaled_pymesh)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Mesh.sdf_func","title":"<code>sdf_func(points)</code>","text":"<p>Compute signed distance field.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The coordinate points used to calculate the SDF value, the shape is [N, 3]</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: SDF values of input points without squared, the shape is [N, 1].</p> <p>NOTE: This function usually returns ndarray with negative values, because according to the definition of SDF, the SDF value of the coordinate point inside the object(interior points) is negative, the outside is positive, and the edge is 0. Therefore, when used for weighting, a negative sign is often added before the result of this function.</p> Source code in <code>ppsci/geometry/mesh.py</code> <pre><code>def sdf_func(self, points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute signed distance field.\n\n    Args:\n        points (np.ndarray): The coordinate points used to calculate the SDF value,\n            the shape is [N, 3]\n\n    Returns:\n        np.ndarray: SDF values of input points without squared, the shape is [N, 1].\n\n    NOTE: This function usually returns ndarray with negative values, because\n    according to the definition of SDF, the SDF value of the coordinate point inside\n    the object(interior points) is negative, the outside is positive, and the edge\n    is 0. Therefore, when used for weighting, a negative sign is often added before\n    the result of this function.\n    \"\"\"\n    if not checker.dynamic_import_to_globals([\"pymesh\"]):\n        raise ImportError(\n            \"Could not import pymesh python package.\"\n            \"Please install it as https://pymesh.readthedocs.io/en/latest/installation.html.\"\n        )\n    import pymesh\n\n    sdf, _, _, _ = pymesh.signed_distance_to_mesh(self.py_mesh, points)\n    sdf = sdf[..., np.newaxis].astype(paddle.get_default_dtype())\n    return sdf\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Mesh.translate","title":"<code>translate(translation, relative=True)</code>","text":"<p>Translate by given offsets.</p> <p>NOTE: This API generate a completely new Mesh object with translated geometry, without modifying original Mesh object inplace.</p> <p>Parameters:</p> Name Type Description Default <code>translation</code> <code>ndarray</code> <p>Translation offsets, numpy array of shape (3,): [offset_x, offset_y, offset_z].</p> required <code>relative</code> <code>bool</code> <p>Whether translate relatively. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Mesh</code> <code>'Mesh'</code> <p>Translated Mesh object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import pymesh\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; box = pymesh.generate_box_mesh(np.array([0, 0, 0]), np.array([1, 1, 1]))\n&gt;&gt;&gt; mesh = ppsci.geometry.Mesh(box)\n&gt;&gt;&gt; print(mesh.vertices)\n[[0. 0. 0.]\n [1. 0. 0.]\n [1. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 1.]\n [1. 1. 1.]\n [0. 1. 1.]]\n&gt;&gt;&gt; print(mesh.translate((-0.5, 0, 0.5), False).vertices) # the center is moved to the translation vector.\n[[-1.  -0.5  0. ]\n [ 0.  -0.5  0. ]\n [ 0.   0.5  0. ]\n [-1.   0.5  0. ]\n [-1.  -0.5  1. ]\n [ 0.  -0.5  1. ]\n [ 0.   0.5  1. ]\n [-1.   0.5  1. ]]\n&gt;&gt;&gt; print(mesh.translate((-0.5, 0, 0.5), True).vertices) # the translation vector is directly added to the geometry coordinates\n[[-0.5  0.   0.5]\n [ 0.5  0.   0.5]\n [ 0.5  1.   0.5]\n [-0.5  1.   0.5]\n [-0.5  0.   1.5]\n [ 0.5  0.   1.5]\n [ 0.5  1.   1.5]\n [-0.5  1.   1.5]]\n</code></pre> Source code in <code>ppsci/geometry/mesh.py</code> <pre><code>def translate(self, translation: np.ndarray, relative: bool = True) -&gt; \"Mesh\":\n    \"\"\"Translate by given offsets.\n\n    NOTE: This API generate a completely new Mesh object with translated geometry,\n    without modifying original Mesh object inplace.\n\n    Args:\n        translation (np.ndarray): Translation offsets, numpy array of shape (3,):\n            [offset_x, offset_y, offset_z].\n        relative (bool, optional): Whether translate relatively. Defaults to True.\n\n    Returns:\n        Mesh: Translated Mesh object.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import pymesh  # doctest: +SKIP\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; box = pymesh.generate_box_mesh(np.array([0, 0, 0]), np.array([1, 1, 1]))  # doctest: +SKIP\n        &gt;&gt;&gt; mesh = ppsci.geometry.Mesh(box)  # doctest: +SKIP\n        &gt;&gt;&gt; print(mesh.vertices)  # doctest: +SKIP\n        [[0. 0. 0.]\n         [1. 0. 0.]\n         [1. 1. 0.]\n         [0. 1. 0.]\n         [0. 0. 1.]\n         [1. 0. 1.]\n         [1. 1. 1.]\n         [0. 1. 1.]]\n        &gt;&gt;&gt; print(mesh.translate((-0.5, 0, 0.5), False).vertices) # the center is moved to the translation vector.  # doctest: +SKIP\n        [[-1.  -0.5  0. ]\n         [ 0.  -0.5  0. ]\n         [ 0.   0.5  0. ]\n         [-1.   0.5  0. ]\n         [-1.  -0.5  1. ]\n         [ 0.  -0.5  1. ]\n         [ 0.   0.5  1. ]\n         [-1.   0.5  1. ]]\n        &gt;&gt;&gt; print(mesh.translate((-0.5, 0, 0.5), True).vertices) # the translation vector is directly added to the geometry coordinates  # doctest: +SKIP\n        [[-0.5  0.   0.5]\n         [ 0.5  0.   0.5]\n         [ 0.5  1.   0.5]\n         [-0.5  1.   0.5]\n         [-0.5  0.   1.5]\n         [ 0.5  0.   1.5]\n         [ 0.5  1.   1.5]\n         [-0.5  1.   1.5]]\n    \"\"\"\n    vertices = np.array(self.vertices, dtype=paddle.get_default_dtype())\n    faces = np.array(self.faces)\n\n    if not checker.dynamic_import_to_globals((\"open3d\", \"pymesh\")):\n        raise ImportError(\n            \"Could not import open3d and pymesh python package. \"\n            \"Please install open3d with `pip install open3d` and \"\n            \"pymesh as https://paddlescience-docs.readthedocs.io/zh/latest/zh/install_setup/#__tabbed_4_1\"\n        )\n    import open3d  # isort:skip\n    import pymesh  # isort:skip\n\n    open3d_mesh = open3d.geometry.TriangleMesh(\n        open3d.utility.Vector3dVector(vertices),\n        open3d.utility.Vector3iVector(faces),\n    )\n    open3d_mesh = open3d_mesh.translate(translation, relative)\n    translated_mesh = pymesh.form_mesh(\n        np.asarray(open3d_mesh.vertices, dtype=paddle.get_default_dtype()), faces\n    )\n    # Generate a new Mesh object using class method\n    return Mesh.from_pymesh(translated_mesh)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.Mesh.uniform_boundary_points","title":"<code>uniform_boundary_points(n)</code>","text":"<p>Compute the equi-spaced points on the boundary.</p> Source code in <code>ppsci/geometry/mesh.py</code> <pre><code>def uniform_boundary_points(self, n: int):\n    \"\"\"Compute the equi-spaced points on the boundary.\"\"\"\n    return self.pysdf.sample_surface(n)\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.PointCloud","title":"<code>PointCloud</code>","text":"<p>               Bases: <code>Geometry</code></p> <p>Class for point cloud geometry, i.e. a set of points from given file or array.</p> <p>Parameters:</p> Name Type Description Default <code>interior</code> <code>Dict[str, ndarray]</code> <p>Filepath or dict data, which store interior points of a point cloud, such as {\"x\": np.ndarray, \"y\": np.ndarray}.</p> required <code>coord_keys</code> <code>Tuple[str, ...]</code> <p>Tuple of coordinate keys, such as (\"x\", \"y\").</p> required <code>boundary</code> <code>Dict[str, ndarray]</code> <p>Boundary points of a point cloud. Defaults to None.</p> <code>None</code> <code>boundary_normal</code> <code>Dict[str, ndarray]</code> <p>Boundary normal points of a point cloud. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; interior_points = {\"x\": np.linspace(-1, 1, dtype=\"float32\").reshape((-1, 1))}\n&gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n</code></pre> Source code in <code>ppsci/geometry/pointcloud.py</code> <pre><code>class PointCloud(geometry.Geometry):\n    \"\"\"Class for point cloud geometry, i.e. a set of points from given file or array.\n\n    Args:\n        interior (Dict[str, np.ndarray]): Filepath or dict data, which store interior points of a point cloud, such as {\"x\": np.ndarray, \"y\": np.ndarray}.\n        coord_keys (Tuple[str, ...]): Tuple of coordinate keys, such as (\"x\", \"y\").\n        boundary (Dict[str, np.ndarray]): Boundary points of a point cloud. Defaults to None.\n        boundary_normal (Dict[str, np.ndarray]): Boundary normal points of a point cloud. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; interior_points = {\"x\": np.linspace(-1, 1, dtype=\"float32\").reshape((-1, 1))}\n        &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n    \"\"\"\n\n    def __init__(\n        self,\n        interior: Dict[str, np.ndarray],\n        coord_keys: Tuple[str, ...],\n        boundary: Optional[Dict[str, np.ndarray]] = None,\n        boundary_normal: Optional[Dict[str, np.ndarray]] = None,\n    ):\n        # Interior points\n        self.interior = misc.convert_to_array(interior, coord_keys)\n        self.len = self.interior.shape[0]\n\n        # Boundary points\n        self.boundary = boundary\n        if self.boundary is not None:\n            self.boundary = misc.convert_to_array(self.boundary, coord_keys)\n\n        # Boundary normal points\n        self.normal = boundary_normal\n        if self.normal is not None:\n            self.normal = misc.convert_to_array(\n                self.normal, tuple(f\"{key}_normal\" for key in coord_keys)\n            )\n            if list(self.normal.shape) != list(self.boundary.shape):\n                raise ValueError(\n                    f\"boundary's shape({self.boundary.shape}) must equal \"\n                    f\"to normal's shape({self.normal.shape})\"\n                )\n\n        self.input_keys = coord_keys\n        super().__init__(\n            len(coord_keys),\n            (np.amin(self.interior, axis=0), np.amax(self.interior, axis=0)),\n            np.inf,\n        )\n\n    @property\n    def dim_keys(self):\n        return self.input_keys\n\n    def is_inside(self, x):\n        # NOTE: point on boundary is included\n        return (\n            np.isclose((x[:, None, :] - self.interior[None, :, :]), 0, atol=1e-6)\n            .all(axis=2)\n            .any(axis=1)\n        )\n\n    def on_boundary(self, x):\n        if not self.boundary:\n            raise ValueError(\n                \"self.boundary must be initialized\" \" when call 'on_boundary' function\"\n            )\n        return (\n            np.isclose(\n                (x[:, None, :] - self.boundary[None, :, :]),\n                0,\n                atol=1e-6,\n            )\n            .all(axis=2)\n            .any(axis=1)\n        )\n\n    def translate(self, translation: np.ndarray) -&gt; \"PointCloud\":\n        \"\"\"\n        Translate the geometry by the given offset.\n\n        Args:\n            translation (np.ndarray): Translation offset.The shape of translation must be the same as the shape of the interior points.\n\n        Returns:\n            PointCloud: Translated point cloud.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n            &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n            &gt;&gt;&gt; translation = np.array([1.0])\n            &gt;&gt;&gt; print(geom.translate(translation).interior)\n            [[1. ]\n             [1.5]\n             [2. ]\n             [2.5]\n             [3. ]]\n            &gt;&gt;&gt; interior_points_2d = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1)),\n            ...                       \"y\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n            &gt;&gt;&gt; geom_2d = ppsci.geometry.PointCloud(interior_points_2d, (\"x\", \"y\"))\n            &gt;&gt;&gt; translation_2d = np.array([1.0, 3.0])\n            &gt;&gt;&gt; print(geom_2d.translate(translation_2d).interior)\n            [[1.  3. ]\n             [1.5 3.5]\n             [2.  4. ]\n             [2.5 4.5]\n             [3.  5. ]]\n        \"\"\"\n        for i, offset in enumerate(translation):\n            self.interior[:, i] += offset\n            if self.boundary:\n                self.boundary += offset\n        return self\n\n    def scale(self, scale: np.ndarray) -&gt; \"PointCloud\":\n        \"\"\"\n        Scale the geometry by the given factor.\n\n        Args:\n            scale (np.ndarray): Scale factor.The shape of scale must be the same as the shape of the interior points.\n\n        Returns:\n            PointCloud: Scaled point cloud.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n            &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n            &gt;&gt;&gt; scale = np.array([2.0])\n            &gt;&gt;&gt; print(geom.scale(scale).interior)\n            [[0.]\n             [1.]\n             [2.]\n             [3.]\n             [4.]]\n            &gt;&gt;&gt; interior_points_2d = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1)),\n            ...                       \"y\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n            &gt;&gt;&gt; geom_2d = ppsci.geometry.PointCloud(interior_points_2d, (\"x\", \"y\"))\n            &gt;&gt;&gt; scale_2d = np.array([2.0, 0.5])\n            &gt;&gt;&gt; print(geom_2d.scale(scale_2d).interior)\n            [[0.   0.  ]\n             [1.   0.25]\n             [2.   0.5 ]\n             [3.   0.75]\n             [4.   1.  ]]\n        \"\"\"\n        for i, _scale in enumerate(scale):\n            self.interior[:, i] *= _scale\n            if self.boundary:\n                self.boundary[:, i] *= _scale\n            if self.normal:\n                self.normal[:, i] *= _scale\n        return self\n\n    def uniform_boundary_points(self, n: int):\n        \"\"\"Compute the equi-spaced points on the boundary.\"\"\"\n        raise NotImplementedError(\n            \"PointCloud do not have 'uniform_boundary_points' method\"\n        )\n\n    def random_boundary_points(self, n: int, random: str = \"pseudo\") -&gt; np.ndarray:\n        \"\"\"Randomly sample points on the boundary.\n\n        Args:\n            n (int): Number of sample points.\n            random (str): Random method. Defaults to \"pseudo\".\n\n        Returns:\n            np.ndarray: Randomly sampled points on the boundary.The shape of the returned array is (n, ndim).\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; np.random.seed(0)\n            &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n            &gt;&gt;&gt; boundary_points = {\"x\": np.array([0.0, 2.0], dtype=\"float32\").reshape((-1, 1))}\n            &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",), boundary_points)\n            &gt;&gt;&gt; print(geom.random_boundary_points(1))\n            [[2.]]\n        \"\"\"\n        assert self.boundary is not None, (\n            \"boundary points can't be empty when call \"\n            \"'random_boundary_points' method\"\n        )\n        assert n &lt;= len(self.boundary), (\n            f\"number of sample points({n}) \"\n            f\"can't be more than that in boundary({len(self.boundary)})\"\n        )\n        return self.boundary[\n            np.random.choice(len(self.boundary), size=n, replace=False)\n        ]\n\n    def random_points(self, n: int, random: str = \"pseudo\") -&gt; np.ndarray:\n        \"\"\"Randomly sample points in the geometry.\n\n        Args:\n            n (int): Number of sample points.\n            random (str): Random method. Defaults to \"pseudo\".\n\n        Returns:\n            np.ndarray: Randomly sampled points in the geometry.The shape of the returned array is (n, ndim).\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; np.random.seed(0)\n            &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n            &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n            &gt;&gt;&gt; print(geom.random_points(2))\n            [[1.]\n             [0.]]\n        \"\"\"\n        assert n &lt;= len(self.interior), (\n            f\"number of sample points({n}) \"\n            f\"can't be more than that in points({len(self.interior)})\"\n        )\n        return self.interior[\n            np.random.choice(len(self.interior), size=n, replace=False)\n        ]\n\n    def uniform_points(self, n: int, boundary: bool = True) -&gt; np.ndarray:\n        \"\"\"Compute the equi-spaced points in the geometry.\n\n        Args:\n            n (int): Number of sample points.\n            boundary (bool): Whether to include boundary points. Defaults to True.\n\n        Returns:\n            np.ndarray: Equi-spaced points in the geometry.The shape of the returned array is (n, ndim).\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n            &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n            &gt;&gt;&gt; print(geom.uniform_points(2))\n            [[0. ]\n             [0.5]]\n        \"\"\"\n        return self.interior[:n]\n\n    def union(self, other):\n        raise NotImplementedError(\n            \"Union operation for PointCloud is not supported yet.\"\n        )\n\n    def __or__(self, other):\n        raise NotImplementedError(\n            \"Union operation for PointCloud is not supported yet.\"\n        )\n\n    def difference(self, other):\n        raise NotImplementedError(\n            \"Subtraction operation for PointCloud is not supported yet.\"\n        )\n\n    def __sub__(self, other):\n        raise NotImplementedError(\n            \"Subtraction operation for PointCloud is not supported yet.\"\n        )\n\n    def intersection(self, other):\n        raise NotImplementedError(\n            \"Intersection operation for PointCloud is not supported yet.\"\n        )\n\n    def __and__(self, other):\n        raise NotImplementedError(\n            \"Intersection operation for PointCloud is not supported yet.\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the name of class.\"\"\"\n        return \", \".join(\n            [\n                self.__class__.__name__,\n                f\"num_points = {len(self.interior)}\",\n                f\"ndim = {self.ndim}\",\n                f\"bbox = {self.bbox}\",\n                f\"dim_keys = {self.dim_keys}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.PointCloud.__str__","title":"<code>__str__()</code>","text":"<p>Return the name of class.</p> Source code in <code>ppsci/geometry/pointcloud.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the name of class.\"\"\"\n    return \", \".join(\n        [\n            self.__class__.__name__,\n            f\"num_points = {len(self.interior)}\",\n            f\"ndim = {self.ndim}\",\n            f\"bbox = {self.bbox}\",\n            f\"dim_keys = {self.dim_keys}\",\n        ]\n    )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.PointCloud.random_boundary_points","title":"<code>random_boundary_points(n, random='pseudo')</code>","text":"<p>Randomly sample points on the boundary.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of sample points.</p> required <code>random</code> <code>str</code> <p>Random method. Defaults to \"pseudo\".</p> <code>'pseudo'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Randomly sampled points on the boundary.The shape of the returned array is (n, ndim).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(0)\n&gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n&gt;&gt;&gt; boundary_points = {\"x\": np.array([0.0, 2.0], dtype=\"float32\").reshape((-1, 1))}\n&gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",), boundary_points)\n&gt;&gt;&gt; print(geom.random_boundary_points(1))\n[[2.]]\n</code></pre> Source code in <code>ppsci/geometry/pointcloud.py</code> <pre><code>def random_boundary_points(self, n: int, random: str = \"pseudo\") -&gt; np.ndarray:\n    \"\"\"Randomly sample points on the boundary.\n\n    Args:\n        n (int): Number of sample points.\n        random (str): Random method. Defaults to \"pseudo\".\n\n    Returns:\n        np.ndarray: Randomly sampled points on the boundary.The shape of the returned array is (n, ndim).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; np.random.seed(0)\n        &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n        &gt;&gt;&gt; boundary_points = {\"x\": np.array([0.0, 2.0], dtype=\"float32\").reshape((-1, 1))}\n        &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",), boundary_points)\n        &gt;&gt;&gt; print(geom.random_boundary_points(1))\n        [[2.]]\n    \"\"\"\n    assert self.boundary is not None, (\n        \"boundary points can't be empty when call \"\n        \"'random_boundary_points' method\"\n    )\n    assert n &lt;= len(self.boundary), (\n        f\"number of sample points({n}) \"\n        f\"can't be more than that in boundary({len(self.boundary)})\"\n    )\n    return self.boundary[\n        np.random.choice(len(self.boundary), size=n, replace=False)\n    ]\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.PointCloud.random_points","title":"<code>random_points(n, random='pseudo')</code>","text":"<p>Randomly sample points in the geometry.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of sample points.</p> required <code>random</code> <code>str</code> <p>Random method. Defaults to \"pseudo\".</p> <code>'pseudo'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Randomly sampled points in the geometry.The shape of the returned array is (n, ndim).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(0)\n&gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n&gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n&gt;&gt;&gt; print(geom.random_points(2))\n[[1.]\n [0.]]\n</code></pre> Source code in <code>ppsci/geometry/pointcloud.py</code> <pre><code>def random_points(self, n: int, random: str = \"pseudo\") -&gt; np.ndarray:\n    \"\"\"Randomly sample points in the geometry.\n\n    Args:\n        n (int): Number of sample points.\n        random (str): Random method. Defaults to \"pseudo\".\n\n    Returns:\n        np.ndarray: Randomly sampled points in the geometry.The shape of the returned array is (n, ndim).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; np.random.seed(0)\n        &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n        &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n        &gt;&gt;&gt; print(geom.random_points(2))\n        [[1.]\n         [0.]]\n    \"\"\"\n    assert n &lt;= len(self.interior), (\n        f\"number of sample points({n}) \"\n        f\"can't be more than that in points({len(self.interior)})\"\n    )\n    return self.interior[\n        np.random.choice(len(self.interior), size=n, replace=False)\n    ]\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.PointCloud.scale","title":"<code>scale(scale)</code>","text":"<p>Scale the geometry by the given factor.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>ndarray</code> <p>Scale factor.The shape of scale must be the same as the shape of the interior points.</p> required <p>Returns:</p> Name Type Description <code>PointCloud</code> <code>'PointCloud'</code> <p>Scaled point cloud.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n&gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n&gt;&gt;&gt; scale = np.array([2.0])\n&gt;&gt;&gt; print(geom.scale(scale).interior)\n[[0.]\n [1.]\n [2.]\n [3.]\n [4.]]\n&gt;&gt;&gt; interior_points_2d = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1)),\n...                       \"y\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n&gt;&gt;&gt; geom_2d = ppsci.geometry.PointCloud(interior_points_2d, (\"x\", \"y\"))\n&gt;&gt;&gt; scale_2d = np.array([2.0, 0.5])\n&gt;&gt;&gt; print(geom_2d.scale(scale_2d).interior)\n[[0.   0.  ]\n [1.   0.25]\n [2.   0.5 ]\n [3.   0.75]\n [4.   1.  ]]\n</code></pre> Source code in <code>ppsci/geometry/pointcloud.py</code> <pre><code>def scale(self, scale: np.ndarray) -&gt; \"PointCloud\":\n    \"\"\"\n    Scale the geometry by the given factor.\n\n    Args:\n        scale (np.ndarray): Scale factor.The shape of scale must be the same as the shape of the interior points.\n\n    Returns:\n        PointCloud: Scaled point cloud.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n        &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n        &gt;&gt;&gt; scale = np.array([2.0])\n        &gt;&gt;&gt; print(geom.scale(scale).interior)\n        [[0.]\n         [1.]\n         [2.]\n         [3.]\n         [4.]]\n        &gt;&gt;&gt; interior_points_2d = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1)),\n        ...                       \"y\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n        &gt;&gt;&gt; geom_2d = ppsci.geometry.PointCloud(interior_points_2d, (\"x\", \"y\"))\n        &gt;&gt;&gt; scale_2d = np.array([2.0, 0.5])\n        &gt;&gt;&gt; print(geom_2d.scale(scale_2d).interior)\n        [[0.   0.  ]\n         [1.   0.25]\n         [2.   0.5 ]\n         [3.   0.75]\n         [4.   1.  ]]\n    \"\"\"\n    for i, _scale in enumerate(scale):\n        self.interior[:, i] *= _scale\n        if self.boundary:\n            self.boundary[:, i] *= _scale\n        if self.normal:\n            self.normal[:, i] *= _scale\n    return self\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.PointCloud.translate","title":"<code>translate(translation)</code>","text":"<p>Translate the geometry by the given offset.</p> <p>Parameters:</p> Name Type Description Default <code>translation</code> <code>ndarray</code> <p>Translation offset.The shape of translation must be the same as the shape of the interior points.</p> required <p>Returns:</p> Name Type Description <code>PointCloud</code> <code>'PointCloud'</code> <p>Translated point cloud.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n&gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n&gt;&gt;&gt; translation = np.array([1.0])\n&gt;&gt;&gt; print(geom.translate(translation).interior)\n[[1. ]\n [1.5]\n [2. ]\n [2.5]\n [3. ]]\n&gt;&gt;&gt; interior_points_2d = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1)),\n...                       \"y\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n&gt;&gt;&gt; geom_2d = ppsci.geometry.PointCloud(interior_points_2d, (\"x\", \"y\"))\n&gt;&gt;&gt; translation_2d = np.array([1.0, 3.0])\n&gt;&gt;&gt; print(geom_2d.translate(translation_2d).interior)\n[[1.  3. ]\n [1.5 3.5]\n [2.  4. ]\n [2.5 4.5]\n [3.  5. ]]\n</code></pre> Source code in <code>ppsci/geometry/pointcloud.py</code> <pre><code>def translate(self, translation: np.ndarray) -&gt; \"PointCloud\":\n    \"\"\"\n    Translate the geometry by the given offset.\n\n    Args:\n        translation (np.ndarray): Translation offset.The shape of translation must be the same as the shape of the interior points.\n\n    Returns:\n        PointCloud: Translated point cloud.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n        &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n        &gt;&gt;&gt; translation = np.array([1.0])\n        &gt;&gt;&gt; print(geom.translate(translation).interior)\n        [[1. ]\n         [1.5]\n         [2. ]\n         [2.5]\n         [3. ]]\n        &gt;&gt;&gt; interior_points_2d = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1)),\n        ...                       \"y\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n        &gt;&gt;&gt; geom_2d = ppsci.geometry.PointCloud(interior_points_2d, (\"x\", \"y\"))\n        &gt;&gt;&gt; translation_2d = np.array([1.0, 3.0])\n        &gt;&gt;&gt; print(geom_2d.translate(translation_2d).interior)\n        [[1.  3. ]\n         [1.5 3.5]\n         [2.  4. ]\n         [2.5 4.5]\n         [3.  5. ]]\n    \"\"\"\n    for i, offset in enumerate(translation):\n        self.interior[:, i] += offset\n        if self.boundary:\n            self.boundary += offset\n    return self\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.PointCloud.uniform_boundary_points","title":"<code>uniform_boundary_points(n)</code>","text":"<p>Compute the equi-spaced points on the boundary.</p> Source code in <code>ppsci/geometry/pointcloud.py</code> <pre><code>def uniform_boundary_points(self, n: int):\n    \"\"\"Compute the equi-spaced points on the boundary.\"\"\"\n    raise NotImplementedError(\n        \"PointCloud do not have 'uniform_boundary_points' method\"\n    )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.PointCloud.uniform_points","title":"<code>uniform_points(n, boundary=True)</code>","text":"<p>Compute the equi-spaced points in the geometry.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of sample points.</p> required <code>boundary</code> <code>bool</code> <p>Whether to include boundary points. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Equi-spaced points in the geometry.The shape of the returned array is (n, ndim).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n&gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n&gt;&gt;&gt; print(geom.uniform_points(2))\n[[0. ]\n [0.5]]\n</code></pre> Source code in <code>ppsci/geometry/pointcloud.py</code> <pre><code>def uniform_points(self, n: int, boundary: bool = True) -&gt; np.ndarray:\n    \"\"\"Compute the equi-spaced points in the geometry.\n\n    Args:\n        n (int): Number of sample points.\n        boundary (bool): Whether to include boundary points. Defaults to True.\n\n    Returns:\n        np.ndarray: Equi-spaced points in the geometry.The shape of the returned array is (n, ndim).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; interior_points = {\"x\": np.linspace(0, 2, 5, dtype=\"float32\").reshape((-1, 1))}\n        &gt;&gt;&gt; geom = ppsci.geometry.PointCloud(interior_points, (\"x\",))\n        &gt;&gt;&gt; print(geom.uniform_points(2))\n        [[0. ]\n         [0.5]]\n    \"\"\"\n    return self.interior[:n]\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeDomain","title":"<code>TimeDomain</code>","text":"<p>               Bases: <code>Interval</code></p> <p>Class for timedomain, an special interval geometry.</p> <p>Parameters:</p> Name Type Description Default <code>t0</code> <code>float</code> <p>Start of time.</p> required <code>t1</code> <code>float</code> <p>End of time.</p> required <code>time_step</code> <code>Optional[float]</code> <p>Step interval of time. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>Optional[Tuple[float, ...]]</code> <p>List of timestamps. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.TimeDomain(0, 1)\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>class TimeDomain(geometry_1d.Interval):\n    \"\"\"Class for timedomain, an special interval geometry.\n\n    Args:\n        t0 (float): Start of time.\n        t1 (float): End of time.\n        time_step (Optional[float]): Step interval of time. Defaults to None.\n        timestamps (Optional[Tuple[float, ...]]): List of timestamps.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.TimeDomain(0, 1)\n    \"\"\"\n\n    def __init__(\n        self,\n        t0: float,\n        t1: float,\n        time_step: Optional[float] = None,\n        timestamps: Optional[Tuple[float, ...]] = None,\n    ):\n        super().__init__(t0, t1)\n        self.t0 = t0\n        self.t1 = t1\n        self.time_step = time_step\n        if timestamps is None:\n            self.timestamps = None\n        else:\n            self.timestamps = np.array(\n                timestamps, dtype=paddle.get_default_dtype()\n            ).reshape([-1])\n        if time_step is not None:\n            if time_step &lt;= 0:\n                raise ValueError(f\"time_step({time_step}) must be larger than 0.\")\n            self.num_timestamps = int(np.ceil((t1 - t0) / time_step)) + 1\n        elif timestamps is not None:\n            self.num_timestamps = len(timestamps)\n\n    def on_initial(self, t: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Check if a specific time is on the initial time point.\n\n        Args:\n            t (np.ndarray): The time to be checked.\n\n        Returns:\n            np.ndarray: Bool numpy array of whether the specific time is on the initial time point.\n\n        Examples:\n            &gt;&gt;&gt; import paddle\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; geom = ppsci.geometry.TimeDomain(0, 1)\n            &gt;&gt;&gt; T = [0, 0.01, 0.126, 0.2, 0.3]\n            &gt;&gt;&gt; check = geom.on_initial(T)\n            &gt;&gt;&gt; print(check)\n            [ True False False False False]\n        \"\"\"\n        return np.isclose(t, self.t0).flatten()\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeDomain.on_initial","title":"<code>on_initial(t)</code>","text":"<p>Check if a specific time is on the initial time point.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>The time to be checked.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Bool numpy array of whether the specific time is on the initial time point.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; geom = ppsci.geometry.TimeDomain(0, 1)\n&gt;&gt;&gt; T = [0, 0.01, 0.126, 0.2, 0.3]\n&gt;&gt;&gt; check = geom.on_initial(T)\n&gt;&gt;&gt; print(check)\n[ True False False False False]\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def on_initial(self, t: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Check if a specific time is on the initial time point.\n\n    Args:\n        t (np.ndarray): The time to be checked.\n\n    Returns:\n        np.ndarray: Bool numpy array of whether the specific time is on the initial time point.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; geom = ppsci.geometry.TimeDomain(0, 1)\n        &gt;&gt;&gt; T = [0, 0.01, 0.126, 0.2, 0.3]\n        &gt;&gt;&gt; check = geom.on_initial(T)\n        &gt;&gt;&gt; print(check)\n        [ True False False False False]\n    \"\"\"\n    return np.isclose(t, self.t0).flatten()\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry","title":"<code>TimeXGeometry</code>","text":"<p>               Bases: <code>Geometry</code></p> <p>Class for combination of time and geometry.</p> <p>Parameters:</p> Name Type Description Default <code>timedomain</code> <code>TimeDomain</code> <p>TimeDomain object.</p> required <code>geometry</code> <code>Geometry</code> <p>Geometry object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>class TimeXGeometry(geometry.Geometry):\n    \"\"\"Class for combination of time and geometry.\n\n    Args:\n        timedomain (TimeDomain): TimeDomain object.\n        geometry (geometry.Geometry): Geometry object.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n    \"\"\"\n\n    def __init__(self, timedomain: TimeDomain, geometry: geometry.Geometry):\n        self.timedomain = timedomain\n        self.geometry = geometry\n        self.ndim = geometry.ndim + timedomain.ndim\n\n    @property\n    def dim_keys(self):\n        return (\"t\",) + self.geometry.dim_keys\n\n    def on_boundary(self, x):\n        # [N, ndim(txyz)]\n        return self.geometry.on_boundary(x[:, 1:])\n\n    def on_initial(self, x):\n        # [N, 1(t)]\n        return self.timedomain.on_initial(x[:, :1])\n\n    def boundary_normal(self, x):\n        # x: [N, ndim(txyz)]\n        normal = self.geometry.boundary_normal(x[:, 1:])\n        return np.hstack((x[:, :1], normal))\n\n    def uniform_points(self, n: int, boundary: bool = True) -&gt; np.ndarray:\n        \"\"\"Uniform points on the spatial-temporal domain.\n        Geometry volume ~ bbox.\n        Time volume ~ diam.\n\n        Args:\n            n (int): The total number of sample points to be generated.\n            boundary (bool): Indicates whether boundary points are included, default is True.\n\n        Returns:\n            np.ndarray: a set of spatial-temporal coordinate points 'tx' that represent sample points evenly distributed within the spatial-temporal domain.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.001)\n            &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n            &gt;&gt;&gt; ts = time_geom.uniform_points(1000)\n            &gt;&gt;&gt; print(ts.shape)\n            (1000, 3)\n        \"\"\"\n        if self.timedomain.time_step is not None:\n            # exclude start time t0\n            nt = int(np.ceil(self.timedomain.diam / self.timedomain.time_step))\n            nx = int(np.ceil(n / nt))\n        elif self.timedomain.timestamps is not None:\n            # exclude start time t0\n            nt = self.timedomain.num_timestamps - 1\n            nx = int(np.ceil(n / nt))\n        else:\n            nx = int(\n                np.ceil(\n                    (\n                        n\n                        * np.prod(self.geometry.bbox[1] - self.geometry.bbox[0])\n                        / self.timedomain.diam\n                    )\n                    ** 0.5\n                )\n            )\n            nt = int(np.ceil(n / nx))\n        x = self.geometry.uniform_points(nx, boundary=boundary)\n        nx = len(x)\n        if boundary and (\n            self.timedomain.time_step is None and self.timedomain.timestamps is None\n        ):\n            t = self.timedomain.uniform_points(nt, boundary=True)\n        else:\n            if self.timedomain.time_step is not None:\n                t = np.linspace(\n                    self.timedomain.t1,\n                    self.timedomain.t0,\n                    num=nt,\n                    endpoint=boundary,\n                    dtype=paddle.get_default_dtype(),\n                )[:, None][::-1]\n            else:\n                t = self.timedomain.timestamps[1:]\n        tx = []\n        for ti in t:\n            tx.append(\n                np.hstack((np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x))\n            )\n        tx = np.vstack(tx)\n        if len(tx) &gt; n:\n            tx = tx[:n]\n        return tx\n\n    def random_points(\n        self, n: int, random: str = \"pseudo\", criteria: Optional[Callable] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Generate random points on the spatial-temporal domain.\n\n        Args:\n            n (int): The total number of random points to generate.\n            random (str): Specifies the way to generate random points, default is \"pseudo\" , which means that a pseudo-random number generator is used.\n            criteria (Optional[Callable]): A method that filters on the generated random points. Defaults to None.\n\n        Returns:\n            np.ndarray: A set of random spatial-temporal points.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.001)\n            &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n            &gt;&gt;&gt; ts = time_geom.random_points(1000)\n            &gt;&gt;&gt; print(ts.shape)\n            (1000, 3)\n        \"\"\"\n        if self.timedomain.time_step is None and self.timedomain.timestamps is None:\n            raise ValueError(\"Either time_step or timestamps must be provided.\")\n        # time evenly and geometry random, if time_step if specified\n        if self.timedomain.time_step is not None:\n            nt = int(np.ceil(self.timedomain.diam / self.timedomain.time_step))\n            t = np.linspace(\n                self.timedomain.t1,\n                self.timedomain.t0,\n                num=nt,\n                endpoint=False,\n                dtype=paddle.get_default_dtype(),\n            )[:, None][\n                ::-1\n            ]  # [nt, 1]\n            # 1. sample nx points in static geometry with criteria\n            nx = int(np.ceil(n / nt))\n            _size, _ntry, _nsuc = 0, 0, 0\n            x = np.empty(\n                shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype()\n            )\n            while _size &lt; nx:\n                _x = self.geometry.random_points(nx, random)\n                if criteria is not None:\n                    # fix arg 't' to None in criteria there\n                    criteria_mask = criteria(\n                        None, *np.split(_x, self.geometry.ndim, axis=1)\n                    ).flatten()\n                    _x = _x[criteria_mask]\n                if len(_x) &gt; nx - _size:\n                    _x = _x[: nx - _size]\n                x[_size : _size + len(_x)] = _x\n\n                _size += len(_x)\n                _ntry += 1\n                if len(_x) &gt; 0:\n                    _nsuc += 1\n\n                if _ntry &gt;= 1000 and _nsuc == 0:\n                    raise ValueError(\n                        \"Sample points failed, \"\n                        \"please check correctness of geometry and given criteria.\"\n                    )\n\n            # 2. repeat spatial points along time\n            tx = []\n            for ti in t:\n                tx.append(\n                    np.hstack(\n                        (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x)\n                    )\n                )\n            tx = np.vstack(tx)\n            if len(tx) &gt; n:\n                tx = tx[:n]\n            return tx\n        elif self.timedomain.timestamps is not None:\n            nt = self.timedomain.num_timestamps - 1\n            t = self.timedomain.timestamps[1:]\n            nx = int(np.ceil(n / nt))\n\n            _size, _ntry, _nsuc = 0, 0, 0\n            x = np.empty(\n                shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype()\n            )\n            while _size &lt; nx:\n                _x = self.geometry.random_points(nx, random)\n                if criteria is not None:\n                    # fix arg 't' to None in criteria there\n                    criteria_mask = criteria(\n                        None, *np.split(_x, self.geometry.ndim, axis=1)\n                    ).flatten()\n                    _x = _x[criteria_mask]\n                if len(_x) &gt; nx - _size:\n                    _x = _x[: nx - _size]\n                x[_size : _size + len(_x)] = _x\n\n                _size += len(_x)\n                _ntry += 1\n                if len(_x) &gt; 0:\n                    _nsuc += 1\n\n                if _ntry &gt;= 1000 and _nsuc == 0:\n                    raise ValueError(\n                        \"Sample interior points failed, \"\n                        \"please check correctness of geometry and given criteria.\"\n                    )\n\n            tx = []\n            for ti in t:\n                tx.append(\n                    np.hstack(\n                        (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x)\n                    )\n                )\n            tx = np.vstack(tx)\n            if len(tx) &gt; n:\n                tx = tx[:n]\n            return tx\n\n        if isinstance(self.geometry, geometry_1d.Interval):\n            geom = geometry_2d.Rectangle(\n                [self.timedomain.t0, self.geometry.l],\n                [self.timedomain.t1, self.geometry.r],\n            )\n            return geom.random_points(n, random=random)\n\n        if isinstance(self.geometry, geometry_2d.Rectangle):\n            geom = geometry_3d.Cuboid(\n                [self.timedomain.t0, self.geometry.xmin[0], self.geometry.xmin[1]],\n                [self.timedomain.t1, self.geometry.xmax[0], self.geometry.xmax[1]],\n            )\n            return geom.random_points(n, random=random)\n\n        if isinstance(self.geometry, (geometry_3d.Cuboid, geometry_nd.Hypercube)):\n            geom = geometry_nd.Hypercube(\n                np.append(self.timedomain.t0, self.geometry.xmin),\n                np.append(self.timedomain.t1, self.geometry.xmax),\n            )\n            return geom.random_points(n, random=random)\n\n        x = self.geometry.random_points(n, random=random)\n        t = self.timedomain.random_points(n, random=random)\n        t = np.random.permutation(t)\n        return np.hstack((t, x))\n\n    def uniform_boundary_points(\n        self, n: int, criteria: Optional[Callable] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Uniform boundary points on the spatial-temporal domain.\n        Geometry surface area ~ bbox.\n        Time surface area ~ diam.\n\n        Args:\n            n (int): The total number of boundary points on the spatial-temporal domain to be generated that are evenly distributed across geometry boundaries.\n            criteria (Optional[Callable]): Used to filter the generated boundary points, only points that meet certain conditions are retained. Default is None.\n\n        Returns:\n            np.ndarray: A set of  point coordinates evenly distributed across geometry boundaries on the spatial-temporal domain.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n            &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n            &gt;&gt;&gt; ts = time_geom.uniform_boundary_points(1000)\n            &gt;&gt;&gt; print(ts.shape)\n            (1000, 3)\n        \"\"\"\n        if self.geometry.ndim == 1:\n            nx = 2\n        else:\n            s = 2 * sum(\n                map(\n                    lambda l: l[0] * l[1],\n                    itertools.combinations(\n                        self.geometry.bbox[1] - self.geometry.bbox[0], 2\n                    ),\n                )\n            )\n            nx = int((n * s / self.timedomain.diam) ** 0.5)\n        nt = int(np.ceil(n / nx))\n\n        _size, _ntry, _nsuc = 0, 0, 0\n        x = np.empty(shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype())\n        while _size &lt; nx:\n            _x = self.geometry.uniform_boundary_points(nx)\n            if criteria is not None:\n                # fix arg 't' to None in criteria there\n                criteria_mask = criteria(\n                    None, *np.split(_x, self.geometry.ndim, axis=1)\n                ).flatten()\n                _x = _x[criteria_mask]\n            if len(_x) &gt; nx - _size:\n                _x = _x[: nx - _size]\n            x[_size : _size + len(_x)] = _x\n\n            _size += len(_x)\n            _ntry += 1\n            if len(_x) &gt; 0:\n                _nsuc += 1\n\n            if _ntry &gt;= 1000 and _nsuc == 0:\n                raise ValueError(\n                    \"Sample boundary points failed, \"\n                    \"please check correctness of geometry and given criteria.\"\n                )\n\n        nx = len(x)\n        t = np.linspace(\n            self.timedomain.t1,\n            self.timedomain.t0,\n            num=nt,\n            endpoint=False,\n            dtype=paddle.get_default_dtype(),\n        )[:, None][::-1]\n        tx = []\n        for ti in t:\n            tx.append(\n                np.hstack((np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x))\n            )\n        tx = np.vstack(tx)\n        if len(tx) &gt; n:\n            tx = tx[:n]\n        return tx\n\n    def random_boundary_points(\n        self, n: int, random: str = \"pseudo\", criteria: Optional[Callable] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Random boundary points on the spatial-temporal domain.\n\n        Args:\n            n (int): The total number of spatial-temporal points generated on a given geometry boundary.\n            random (str): Controls the way to generate random points. Default is \"pseudo\".\n            criteria (Optional[Callable]): Used to filter the generated boundary points, only points that meet certain conditions are retained. Default is None.\n\n        Returns:\n            np.ndarray: A set of point coordinates randomly distributed across geometry boundaries on the spatial-temporal domain.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.001)\n            &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n            &gt;&gt;&gt; ts = time_geom.random_boundary_points(1000)\n            &gt;&gt;&gt; print(ts.shape)\n            (1000, 3)\n        \"\"\"\n        if self.timedomain.time_step is None and self.timedomain.timestamps is None:\n            raise ValueError(\"Either time_step or timestamps must be provided.\")\n        if self.timedomain.time_step is not None:\n            # exclude start time t0\n            nt = int(np.ceil(self.timedomain.diam / self.timedomain.time_step))\n            t = np.linspace(\n                self.timedomain.t1,\n                self.timedomain.t0,\n                num=nt,\n                endpoint=False,\n                dtype=paddle.get_default_dtype(),\n            )[:, None][::-1]\n            nx = int(np.ceil(n / nt))\n\n            if isinstance(self.geometry, mesh.Mesh):\n                x, _n, a = self.geometry.random_boundary_points(nx, random=random)\n            else:\n                _size, _ntry, _nsuc = 0, 0, 0\n                x = np.empty(\n                    shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype()\n                )\n                while _size &lt; nx:\n                    _x = self.geometry.random_boundary_points(nx, random)\n                    if criteria is not None:\n                        # fix arg 't' to None in criteria there\n                        criteria_mask = criteria(\n                            None, *np.split(_x, self.geometry.ndim, axis=1)\n                        ).flatten()\n                        _x = _x[criteria_mask]\n                    if len(_x) &gt; nx - _size:\n                        _x = _x[: nx - _size]\n                    x[_size : _size + len(_x)] = _x\n\n                    _size += len(_x)\n                    _ntry += 1\n                    if len(_x) &gt; 0:\n                        _nsuc += 1\n\n                    if _ntry &gt;= 1000 and _nsuc == 0:\n                        raise ValueError(\n                            \"Sample boundary points failed, \"\n                            \"please check correctness of geometry and given criteria.\"\n                        )\n\n            t_x = []\n            if isinstance(self.geometry, mesh.Mesh):\n                t_normal = []\n                t_area = []\n\n            for ti in t:\n                t_x.append(\n                    np.hstack(\n                        (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x)\n                    )\n                )\n                if isinstance(self.geometry, mesh.Mesh):\n                    t_normal.append(\n                        np.hstack(\n                            (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), _n)\n                        )\n                    )\n                    t_area.append(\n                        np.hstack(\n                            (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), a)\n                        )\n                    )\n\n            t_x = np.vstack(t_x)\n            if isinstance(self.geometry, mesh.Mesh):\n                t_normal = np.vstack(t_normal)\n                t_area = np.vstack(t_area)\n\n            if len(t_x) &gt; n:\n                t_x = t_x[:n]\n                if isinstance(self.geometry, mesh.Mesh):\n                    t_normal = t_normal[:n]\n                    t_area = t_area[:n]\n\n            if isinstance(self.geometry, mesh.Mesh):\n                return t_x, t_normal, t_area\n            else:\n                return t_x\n        elif self.timedomain.timestamps is not None:\n            # exclude start time t0\n            nt = self.timedomain.num_timestamps - 1\n            t = self.timedomain.timestamps[1:]\n            nx = int(np.ceil(n / nt))\n\n            if isinstance(self.geometry, mesh.Mesh):\n                x, _n, a = self.geometry.random_boundary_points(nx, random=random)\n            else:\n                _size, _ntry, _nsuc = 0, 0, 0\n                x = np.empty(\n                    shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype()\n                )\n                while _size &lt; nx:\n                    _x = self.geometry.random_boundary_points(nx, random)\n                    if criteria is not None:\n                        # fix arg 't' to None in criteria there\n                        criteria_mask = criteria(\n                            None, *np.split(_x, self.geometry.ndim, axis=1)\n                        ).flatten()\n                        _x = _x[criteria_mask]\n                    if len(_x) &gt; nx - _size:\n                        _x = _x[: nx - _size]\n                    x[_size : _size + len(_x)] = _x\n\n                    _size += len(_x)\n                    _ntry += 1\n                    if len(_x) &gt; 0:\n                        _nsuc += 1\n\n                    if _ntry &gt;= 1000 and _nsuc == 0:\n                        raise ValueError(\n                            \"Sample boundary points failed, \"\n                            \"please check correctness of geometry and given criteria.\"\n                        )\n\n            t_x = []\n            if isinstance(self.geometry, mesh.Mesh):\n                t_normal = []\n                t_area = []\n\n            for ti in t:\n                t_x.append(\n                    np.hstack(\n                        (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x)\n                    )\n                )\n                if isinstance(self.geometry, mesh.Mesh):\n                    t_normal.append(\n                        np.hstack(\n                            (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), _n)\n                        )\n                    )\n                    t_area.append(\n                        np.hstack(\n                            (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), a)\n                        )\n                    )\n\n            t_x = np.vstack(t_x)\n            if isinstance(self.geometry, mesh.Mesh):\n                t_normal = np.vstack(t_normal)\n                t_area = np.vstack(t_area)\n\n            if len(t_x) &gt; n:\n                t_x = t_x[:n]\n                if isinstance(self.geometry, mesh.Mesh):\n                    t_normal = t_normal[:n]\n                    t_area = t_area[:n]\n\n            if isinstance(self.geometry, mesh.Mesh):\n                return t_x, t_normal, t_area\n            else:\n                return t_x\n        else:\n            if isinstance(self.geometry, mesh.Mesh):\n                x, _n, a = self.geometry.random_boundary_points(n, random=random)\n            else:\n                x = self.geometry.random_boundary_points(n, random=random)\n\n            t = self.timedomain.random_points(n, random=random)\n            t = np.random.permutation(t)\n\n            t_x = np.hstack((t, x))\n\n            if isinstance(self.geometry, mesh.Mesh):\n                t_normal = np.hstack((_n, t))\n                t_area = np.hstack((_n, t))\n                return t_x, t_normal, t_area\n            else:\n                return t_x\n\n    def uniform_initial_points(self, n: int) -&gt; np.ndarray:\n        \"\"\"Generate evenly distributed point coordinates on the spatial-temporal domain at the initial moment.\n\n        Args:\n            n (int): The total number of generated points.\n\n        Returns:\n           np.ndarray: A set of point coordinates evenly distributed on the spatial-temporal domain at the initial moment.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n            &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n            &gt;&gt;&gt; ts = time_geom.uniform_initial_points(1000)\n            &gt;&gt;&gt; print(ts.shape)\n            (1000, 3)\n        \"\"\"\n        x = self.geometry.uniform_points(n, True)\n        t = self.timedomain.t0\n        if len(x) &gt; n:\n            x = x[:n]\n        return np.hstack((np.full([n, 1], t, dtype=paddle.get_default_dtype()), x))\n\n    def random_initial_points(self, n: int, random: str = \"pseudo\") -&gt; np.ndarray:\n        \"\"\"Generate randomly distributed point coordinates on the spatial-temporal domain at the initial moment.\n\n        Args:\n            n (int): The total number of generated points.\n            random (str): Controls the way to generate random points. Default is \"pseudo\".\n\n        Returns:\n            np.ndarray: A set of point coordinates randomly distributed on the spatial-temporal domain at the initial moment.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n            &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n            &gt;&gt;&gt; ts = time_geom.random_initial_points(1000)\n            &gt;&gt;&gt; print(ts.shape)\n            (1000, 3)\n        \"\"\"\n        x = self.geometry.random_points(n, random=random)\n        t = self.timedomain.t0\n        return np.hstack((np.full([n, 1], t, dtype=paddle.get_default_dtype()), x))\n\n    def periodic_point(\n        self, x: Dict[str, np.ndarray], component: int\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Process given point coordinates to satisfy the periodic boundary conditions of the geometry.\n\n        Args:\n            x (Dict[str, np.ndarray]): Contains the coordinates and timestamps of the points. It represents the coordinates of the point to be processed.\n            component (int): Specifies the components or dimensions of specific spatial coordinates that are periodically processed.\n\n        Returns:\n            Dict[str, np.ndarray] : contains the original timestamps and the coordinates of the spatial point after periodic processing.\n\n        Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.1)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n        &gt;&gt;&gt; ts = time_geom.sample_boundary(1000)\n        &gt;&gt;&gt; result = time_geom.periodic_point(ts, 0)\n        &gt;&gt;&gt; for k,v in result.items():\n        ...     print(k, v.shape)\n        t (1000, 1)\n        x (1000, 1)\n        y (1000, 1)\n        normal_x (1000, 1)\n        normal_y (1000, 1)\n        \"\"\"\n        xp = self.geometry.periodic_point(x, component)\n        txp = {\"t\": x[\"t\"], **xp}\n        return txp\n\n    def sample_initial_interior(\n        self,\n        n: int,\n        random: str = \"pseudo\",\n        criteria: Optional[Callable] = None,\n        evenly: bool = False,\n        compute_sdf_derivatives: bool = False,\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Sample random points in the time-geometry and return those meet criteria.\n\n        Args:\n            n (int): The total number of interior points generated.\n            random (str): The method used to specify the initial point of generation. Default is \"pseudo\".\n            criteria (Optional[Callable]): Used to filter the generated interior points, only points that meet certain conditions are retained. Default is None.\n            evenly (bool): Indicates whether the initial points are generated evenly. Default is False.\n            compute_sdf_derivatives (bool): Indicates whether to calculate the derivative of signed distance function or not. Default is False.\n\n        Returns:\n            np.ndarray: Contains the coordinates of the initial internal point generated, as well as the potentially computed signed distance function and its derivative.\n\n        Examples:\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n            &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n            &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n            &gt;&gt;&gt; ts = time_geom.sample_initial_interior(1000)\n            &gt;&gt;&gt; for k,v in ts.items():\n            ...     print(k, v.shape)\n            t (1000, 1)\n            x (1000, 1)\n            y (1000, 1)\n            sdf (1000, 1)\n        \"\"\"\n        x = np.empty(shape=(n, self.ndim), dtype=paddle.get_default_dtype())\n        _size, _ntry, _nsuc = 0, 0, 0\n        while _size &lt; n:\n            if evenly:\n                points = self.uniform_initial_points(n)\n            else:\n                points = self.random_initial_points(n, random)\n\n            if criteria is not None:\n                criteria_mask = criteria(*np.split(points, self.ndim, axis=1)).flatten()\n                points = points[criteria_mask]\n\n            if len(points) &gt; n - _size:\n                points = points[: n - _size]\n            x[_size : _size + len(points)] = points\n\n            _size += len(points)\n            _ntry += 1\n            if len(points) &gt; 0:\n                _nsuc += 1\n\n            if _ntry &gt;= 1000 and _nsuc == 0:\n                raise ValueError(\n                    \"Sample initial interior points failed, \"\n                    \"please check correctness of geometry and given criteria.\"\n                )\n\n        # if sdf_func added, return x_dict and sdf_dict, else, only return the x_dict\n        if hasattr(self.geometry, \"sdf_func\"):\n            # compute sdf excluding time t\n            sdf = -self.geometry.sdf_func(x[..., 1:])\n            sdf_dict = misc.convert_to_dict(sdf, (\"sdf\",))\n            sdf_derives_dict = {}\n            if compute_sdf_derivatives:\n                # compute sdf derivatives excluding time t\n                sdf_derives = -self.geometry.sdf_derivatives(x[..., 1:])\n                sdf_derives_dict = misc.convert_to_dict(\n                    sdf_derives, tuple(f\"sdf__{key}\" for key in self.geometry.dim_keys)\n                )\n        else:\n            sdf_dict = {}\n            sdf_derives_dict = {}\n        x_dict = misc.convert_to_dict(x, self.dim_keys)\n\n        return {**x_dict, **sdf_dict, **sdf_derives_dict}\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the name of class\"\"\"\n        return \", \".join(\n            [\n                self.__class__.__name__,\n                f\"ndim = {self.ndim}\",\n                f\"bbox = (time){self.timedomain.bbox} x (space){self.geometry.bbox}\",\n                f\"diam = (time){self.timedomain.diam} x (space){self.geometry.diam}\",\n                f\"dim_keys = {self.dim_keys}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry.__str__","title":"<code>__str__()</code>","text":"<p>Return the name of class</p> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the name of class\"\"\"\n    return \", \".join(\n        [\n            self.__class__.__name__,\n            f\"ndim = {self.ndim}\",\n            f\"bbox = (time){self.timedomain.bbox} x (space){self.geometry.bbox}\",\n            f\"diam = (time){self.timedomain.diam} x (space){self.geometry.diam}\",\n            f\"dim_keys = {self.dim_keys}\",\n        ]\n    )\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry.periodic_point","title":"<code>periodic_point(x, component)</code>","text":"<p>Process given point coordinates to satisfy the periodic boundary conditions of the geometry.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict[str, ndarray]</code> <p>Contains the coordinates and timestamps of the points. It represents the coordinates of the point to be processed.</p> required <code>component</code> <code>int</code> <p>Specifies the components or dimensions of specific spatial coordinates that are periodically processed.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray] : contains the original timestamps and the coordinates of the spatial point after periodic processing.</p> <p>Examples:</p> <p>import ppsci timedomain = ppsci.geometry.TimeDomain(0, 1, 0.1) geom = ppsci.geometry.Rectangle((0, 0), (1, 1)) time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom) ts = time_geom.sample_boundary(1000) result = time_geom.periodic_point(ts, 0) for k,v in result.items(): ...     print(k, v.shape) t (1000, 1) x (1000, 1) y (1000, 1) normal_x (1000, 1) normal_y (1000, 1)</p> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def periodic_point(\n    self, x: Dict[str, np.ndarray], component: int\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Process given point coordinates to satisfy the periodic boundary conditions of the geometry.\n\n    Args:\n        x (Dict[str, np.ndarray]): Contains the coordinates and timestamps of the points. It represents the coordinates of the point to be processed.\n        component (int): Specifies the components or dimensions of specific spatial coordinates that are periodically processed.\n\n    Returns:\n        Dict[str, np.ndarray] : contains the original timestamps and the coordinates of the spatial point after periodic processing.\n\n    Examples:\n    &gt;&gt;&gt; import ppsci\n    &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.1)\n    &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n    &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n    &gt;&gt;&gt; ts = time_geom.sample_boundary(1000)\n    &gt;&gt;&gt; result = time_geom.periodic_point(ts, 0)\n    &gt;&gt;&gt; for k,v in result.items():\n    ...     print(k, v.shape)\n    t (1000, 1)\n    x (1000, 1)\n    y (1000, 1)\n    normal_x (1000, 1)\n    normal_y (1000, 1)\n    \"\"\"\n    xp = self.geometry.periodic_point(x, component)\n    txp = {\"t\": x[\"t\"], **xp}\n    return txp\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry.random_boundary_points","title":"<code>random_boundary_points(n, random='pseudo', criteria=None)</code>","text":"<p>Random boundary points on the spatial-temporal domain.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The total number of spatial-temporal points generated on a given geometry boundary.</p> required <code>random</code> <code>str</code> <p>Controls the way to generate random points. Default is \"pseudo\".</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable]</code> <p>Used to filter the generated boundary points, only points that meet certain conditions are retained. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A set of point coordinates randomly distributed across geometry boundaries on the spatial-temporal domain.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.001)\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n&gt;&gt;&gt; ts = time_geom.random_boundary_points(1000)\n&gt;&gt;&gt; print(ts.shape)\n(1000, 3)\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def random_boundary_points(\n    self, n: int, random: str = \"pseudo\", criteria: Optional[Callable] = None\n) -&gt; np.ndarray:\n    \"\"\"Random boundary points on the spatial-temporal domain.\n\n    Args:\n        n (int): The total number of spatial-temporal points generated on a given geometry boundary.\n        random (str): Controls the way to generate random points. Default is \"pseudo\".\n        criteria (Optional[Callable]): Used to filter the generated boundary points, only points that meet certain conditions are retained. Default is None.\n\n    Returns:\n        np.ndarray: A set of point coordinates randomly distributed across geometry boundaries on the spatial-temporal domain.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.001)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n        &gt;&gt;&gt; ts = time_geom.random_boundary_points(1000)\n        &gt;&gt;&gt; print(ts.shape)\n        (1000, 3)\n    \"\"\"\n    if self.timedomain.time_step is None and self.timedomain.timestamps is None:\n        raise ValueError(\"Either time_step or timestamps must be provided.\")\n    if self.timedomain.time_step is not None:\n        # exclude start time t0\n        nt = int(np.ceil(self.timedomain.diam / self.timedomain.time_step))\n        t = np.linspace(\n            self.timedomain.t1,\n            self.timedomain.t0,\n            num=nt,\n            endpoint=False,\n            dtype=paddle.get_default_dtype(),\n        )[:, None][::-1]\n        nx = int(np.ceil(n / nt))\n\n        if isinstance(self.geometry, mesh.Mesh):\n            x, _n, a = self.geometry.random_boundary_points(nx, random=random)\n        else:\n            _size, _ntry, _nsuc = 0, 0, 0\n            x = np.empty(\n                shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype()\n            )\n            while _size &lt; nx:\n                _x = self.geometry.random_boundary_points(nx, random)\n                if criteria is not None:\n                    # fix arg 't' to None in criteria there\n                    criteria_mask = criteria(\n                        None, *np.split(_x, self.geometry.ndim, axis=1)\n                    ).flatten()\n                    _x = _x[criteria_mask]\n                if len(_x) &gt; nx - _size:\n                    _x = _x[: nx - _size]\n                x[_size : _size + len(_x)] = _x\n\n                _size += len(_x)\n                _ntry += 1\n                if len(_x) &gt; 0:\n                    _nsuc += 1\n\n                if _ntry &gt;= 1000 and _nsuc == 0:\n                    raise ValueError(\n                        \"Sample boundary points failed, \"\n                        \"please check correctness of geometry and given criteria.\"\n                    )\n\n        t_x = []\n        if isinstance(self.geometry, mesh.Mesh):\n            t_normal = []\n            t_area = []\n\n        for ti in t:\n            t_x.append(\n                np.hstack(\n                    (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x)\n                )\n            )\n            if isinstance(self.geometry, mesh.Mesh):\n                t_normal.append(\n                    np.hstack(\n                        (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), _n)\n                    )\n                )\n                t_area.append(\n                    np.hstack(\n                        (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), a)\n                    )\n                )\n\n        t_x = np.vstack(t_x)\n        if isinstance(self.geometry, mesh.Mesh):\n            t_normal = np.vstack(t_normal)\n            t_area = np.vstack(t_area)\n\n        if len(t_x) &gt; n:\n            t_x = t_x[:n]\n            if isinstance(self.geometry, mesh.Mesh):\n                t_normal = t_normal[:n]\n                t_area = t_area[:n]\n\n        if isinstance(self.geometry, mesh.Mesh):\n            return t_x, t_normal, t_area\n        else:\n            return t_x\n    elif self.timedomain.timestamps is not None:\n        # exclude start time t0\n        nt = self.timedomain.num_timestamps - 1\n        t = self.timedomain.timestamps[1:]\n        nx = int(np.ceil(n / nt))\n\n        if isinstance(self.geometry, mesh.Mesh):\n            x, _n, a = self.geometry.random_boundary_points(nx, random=random)\n        else:\n            _size, _ntry, _nsuc = 0, 0, 0\n            x = np.empty(\n                shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype()\n            )\n            while _size &lt; nx:\n                _x = self.geometry.random_boundary_points(nx, random)\n                if criteria is not None:\n                    # fix arg 't' to None in criteria there\n                    criteria_mask = criteria(\n                        None, *np.split(_x, self.geometry.ndim, axis=1)\n                    ).flatten()\n                    _x = _x[criteria_mask]\n                if len(_x) &gt; nx - _size:\n                    _x = _x[: nx - _size]\n                x[_size : _size + len(_x)] = _x\n\n                _size += len(_x)\n                _ntry += 1\n                if len(_x) &gt; 0:\n                    _nsuc += 1\n\n                if _ntry &gt;= 1000 and _nsuc == 0:\n                    raise ValueError(\n                        \"Sample boundary points failed, \"\n                        \"please check correctness of geometry and given criteria.\"\n                    )\n\n        t_x = []\n        if isinstance(self.geometry, mesh.Mesh):\n            t_normal = []\n            t_area = []\n\n        for ti in t:\n            t_x.append(\n                np.hstack(\n                    (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x)\n                )\n            )\n            if isinstance(self.geometry, mesh.Mesh):\n                t_normal.append(\n                    np.hstack(\n                        (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), _n)\n                    )\n                )\n                t_area.append(\n                    np.hstack(\n                        (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), a)\n                    )\n                )\n\n        t_x = np.vstack(t_x)\n        if isinstance(self.geometry, mesh.Mesh):\n            t_normal = np.vstack(t_normal)\n            t_area = np.vstack(t_area)\n\n        if len(t_x) &gt; n:\n            t_x = t_x[:n]\n            if isinstance(self.geometry, mesh.Mesh):\n                t_normal = t_normal[:n]\n                t_area = t_area[:n]\n\n        if isinstance(self.geometry, mesh.Mesh):\n            return t_x, t_normal, t_area\n        else:\n            return t_x\n    else:\n        if isinstance(self.geometry, mesh.Mesh):\n            x, _n, a = self.geometry.random_boundary_points(n, random=random)\n        else:\n            x = self.geometry.random_boundary_points(n, random=random)\n\n        t = self.timedomain.random_points(n, random=random)\n        t = np.random.permutation(t)\n\n        t_x = np.hstack((t, x))\n\n        if isinstance(self.geometry, mesh.Mesh):\n            t_normal = np.hstack((_n, t))\n            t_area = np.hstack((_n, t))\n            return t_x, t_normal, t_area\n        else:\n            return t_x\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry.random_initial_points","title":"<code>random_initial_points(n, random='pseudo')</code>","text":"<p>Generate randomly distributed point coordinates on the spatial-temporal domain at the initial moment.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The total number of generated points.</p> required <code>random</code> <code>str</code> <p>Controls the way to generate random points. Default is \"pseudo\".</p> <code>'pseudo'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A set of point coordinates randomly distributed on the spatial-temporal domain at the initial moment.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n&gt;&gt;&gt; ts = time_geom.random_initial_points(1000)\n&gt;&gt;&gt; print(ts.shape)\n(1000, 3)\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def random_initial_points(self, n: int, random: str = \"pseudo\") -&gt; np.ndarray:\n    \"\"\"Generate randomly distributed point coordinates on the spatial-temporal domain at the initial moment.\n\n    Args:\n        n (int): The total number of generated points.\n        random (str): Controls the way to generate random points. Default is \"pseudo\".\n\n    Returns:\n        np.ndarray: A set of point coordinates randomly distributed on the spatial-temporal domain at the initial moment.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n        &gt;&gt;&gt; ts = time_geom.random_initial_points(1000)\n        &gt;&gt;&gt; print(ts.shape)\n        (1000, 3)\n    \"\"\"\n    x = self.geometry.random_points(n, random=random)\n    t = self.timedomain.t0\n    return np.hstack((np.full([n, 1], t, dtype=paddle.get_default_dtype()), x))\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry.random_points","title":"<code>random_points(n, random='pseudo', criteria=None)</code>","text":"<p>Generate random points on the spatial-temporal domain.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The total number of random points to generate.</p> required <code>random</code> <code>str</code> <p>Specifies the way to generate random points, default is \"pseudo\" , which means that a pseudo-random number generator is used.</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable]</code> <p>A method that filters on the generated random points. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A set of random spatial-temporal points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.001)\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n&gt;&gt;&gt; ts = time_geom.random_points(1000)\n&gt;&gt;&gt; print(ts.shape)\n(1000, 3)\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def random_points(\n    self, n: int, random: str = \"pseudo\", criteria: Optional[Callable] = None\n) -&gt; np.ndarray:\n    \"\"\"Generate random points on the spatial-temporal domain.\n\n    Args:\n        n (int): The total number of random points to generate.\n        random (str): Specifies the way to generate random points, default is \"pseudo\" , which means that a pseudo-random number generator is used.\n        criteria (Optional[Callable]): A method that filters on the generated random points. Defaults to None.\n\n    Returns:\n        np.ndarray: A set of random spatial-temporal points.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.001)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n        &gt;&gt;&gt; ts = time_geom.random_points(1000)\n        &gt;&gt;&gt; print(ts.shape)\n        (1000, 3)\n    \"\"\"\n    if self.timedomain.time_step is None and self.timedomain.timestamps is None:\n        raise ValueError(\"Either time_step or timestamps must be provided.\")\n    # time evenly and geometry random, if time_step if specified\n    if self.timedomain.time_step is not None:\n        nt = int(np.ceil(self.timedomain.diam / self.timedomain.time_step))\n        t = np.linspace(\n            self.timedomain.t1,\n            self.timedomain.t0,\n            num=nt,\n            endpoint=False,\n            dtype=paddle.get_default_dtype(),\n        )[:, None][\n            ::-1\n        ]  # [nt, 1]\n        # 1. sample nx points in static geometry with criteria\n        nx = int(np.ceil(n / nt))\n        _size, _ntry, _nsuc = 0, 0, 0\n        x = np.empty(\n            shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype()\n        )\n        while _size &lt; nx:\n            _x = self.geometry.random_points(nx, random)\n            if criteria is not None:\n                # fix arg 't' to None in criteria there\n                criteria_mask = criteria(\n                    None, *np.split(_x, self.geometry.ndim, axis=1)\n                ).flatten()\n                _x = _x[criteria_mask]\n            if len(_x) &gt; nx - _size:\n                _x = _x[: nx - _size]\n            x[_size : _size + len(_x)] = _x\n\n            _size += len(_x)\n            _ntry += 1\n            if len(_x) &gt; 0:\n                _nsuc += 1\n\n            if _ntry &gt;= 1000 and _nsuc == 0:\n                raise ValueError(\n                    \"Sample points failed, \"\n                    \"please check correctness of geometry and given criteria.\"\n                )\n\n        # 2. repeat spatial points along time\n        tx = []\n        for ti in t:\n            tx.append(\n                np.hstack(\n                    (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x)\n                )\n            )\n        tx = np.vstack(tx)\n        if len(tx) &gt; n:\n            tx = tx[:n]\n        return tx\n    elif self.timedomain.timestamps is not None:\n        nt = self.timedomain.num_timestamps - 1\n        t = self.timedomain.timestamps[1:]\n        nx = int(np.ceil(n / nt))\n\n        _size, _ntry, _nsuc = 0, 0, 0\n        x = np.empty(\n            shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype()\n        )\n        while _size &lt; nx:\n            _x = self.geometry.random_points(nx, random)\n            if criteria is not None:\n                # fix arg 't' to None in criteria there\n                criteria_mask = criteria(\n                    None, *np.split(_x, self.geometry.ndim, axis=1)\n                ).flatten()\n                _x = _x[criteria_mask]\n            if len(_x) &gt; nx - _size:\n                _x = _x[: nx - _size]\n            x[_size : _size + len(_x)] = _x\n\n            _size += len(_x)\n            _ntry += 1\n            if len(_x) &gt; 0:\n                _nsuc += 1\n\n            if _ntry &gt;= 1000 and _nsuc == 0:\n                raise ValueError(\n                    \"Sample interior points failed, \"\n                    \"please check correctness of geometry and given criteria.\"\n                )\n\n        tx = []\n        for ti in t:\n            tx.append(\n                np.hstack(\n                    (np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x)\n                )\n            )\n        tx = np.vstack(tx)\n        if len(tx) &gt; n:\n            tx = tx[:n]\n        return tx\n\n    if isinstance(self.geometry, geometry_1d.Interval):\n        geom = geometry_2d.Rectangle(\n            [self.timedomain.t0, self.geometry.l],\n            [self.timedomain.t1, self.geometry.r],\n        )\n        return geom.random_points(n, random=random)\n\n    if isinstance(self.geometry, geometry_2d.Rectangle):\n        geom = geometry_3d.Cuboid(\n            [self.timedomain.t0, self.geometry.xmin[0], self.geometry.xmin[1]],\n            [self.timedomain.t1, self.geometry.xmax[0], self.geometry.xmax[1]],\n        )\n        return geom.random_points(n, random=random)\n\n    if isinstance(self.geometry, (geometry_3d.Cuboid, geometry_nd.Hypercube)):\n        geom = geometry_nd.Hypercube(\n            np.append(self.timedomain.t0, self.geometry.xmin),\n            np.append(self.timedomain.t1, self.geometry.xmax),\n        )\n        return geom.random_points(n, random=random)\n\n    x = self.geometry.random_points(n, random=random)\n    t = self.timedomain.random_points(n, random=random)\n    t = np.random.permutation(t)\n    return np.hstack((t, x))\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry.sample_initial_interior","title":"<code>sample_initial_interior(n, random='pseudo', criteria=None, evenly=False, compute_sdf_derivatives=False)</code>","text":"<p>Sample random points in the time-geometry and return those meet criteria.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The total number of interior points generated.</p> required <code>random</code> <code>str</code> <p>The method used to specify the initial point of generation. Default is \"pseudo\".</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable]</code> <p>Used to filter the generated interior points, only points that meet certain conditions are retained. Default is None.</p> <code>None</code> <code>evenly</code> <code>bool</code> <p>Indicates whether the initial points are generated evenly. Default is False.</p> <code>False</code> <code>compute_sdf_derivatives</code> <code>bool</code> <p>Indicates whether to calculate the derivative of signed distance function or not. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>np.ndarray: Contains the coordinates of the initial internal point generated, as well as the potentially computed signed distance function and its derivative.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n&gt;&gt;&gt; ts = time_geom.sample_initial_interior(1000)\n&gt;&gt;&gt; for k,v in ts.items():\n...     print(k, v.shape)\nt (1000, 1)\nx (1000, 1)\ny (1000, 1)\nsdf (1000, 1)\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def sample_initial_interior(\n    self,\n    n: int,\n    random: str = \"pseudo\",\n    criteria: Optional[Callable] = None,\n    evenly: bool = False,\n    compute_sdf_derivatives: bool = False,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Sample random points in the time-geometry and return those meet criteria.\n\n    Args:\n        n (int): The total number of interior points generated.\n        random (str): The method used to specify the initial point of generation. Default is \"pseudo\".\n        criteria (Optional[Callable]): Used to filter the generated interior points, only points that meet certain conditions are retained. Default is None.\n        evenly (bool): Indicates whether the initial points are generated evenly. Default is False.\n        compute_sdf_derivatives (bool): Indicates whether to calculate the derivative of signed distance function or not. Default is False.\n\n    Returns:\n        np.ndarray: Contains the coordinates of the initial internal point generated, as well as the potentially computed signed distance function and its derivative.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n        &gt;&gt;&gt; ts = time_geom.sample_initial_interior(1000)\n        &gt;&gt;&gt; for k,v in ts.items():\n        ...     print(k, v.shape)\n        t (1000, 1)\n        x (1000, 1)\n        y (1000, 1)\n        sdf (1000, 1)\n    \"\"\"\n    x = np.empty(shape=(n, self.ndim), dtype=paddle.get_default_dtype())\n    _size, _ntry, _nsuc = 0, 0, 0\n    while _size &lt; n:\n        if evenly:\n            points = self.uniform_initial_points(n)\n        else:\n            points = self.random_initial_points(n, random)\n\n        if criteria is not None:\n            criteria_mask = criteria(*np.split(points, self.ndim, axis=1)).flatten()\n            points = points[criteria_mask]\n\n        if len(points) &gt; n - _size:\n            points = points[: n - _size]\n        x[_size : _size + len(points)] = points\n\n        _size += len(points)\n        _ntry += 1\n        if len(points) &gt; 0:\n            _nsuc += 1\n\n        if _ntry &gt;= 1000 and _nsuc == 0:\n            raise ValueError(\n                \"Sample initial interior points failed, \"\n                \"please check correctness of geometry and given criteria.\"\n            )\n\n    # if sdf_func added, return x_dict and sdf_dict, else, only return the x_dict\n    if hasattr(self.geometry, \"sdf_func\"):\n        # compute sdf excluding time t\n        sdf = -self.geometry.sdf_func(x[..., 1:])\n        sdf_dict = misc.convert_to_dict(sdf, (\"sdf\",))\n        sdf_derives_dict = {}\n        if compute_sdf_derivatives:\n            # compute sdf derivatives excluding time t\n            sdf_derives = -self.geometry.sdf_derivatives(x[..., 1:])\n            sdf_derives_dict = misc.convert_to_dict(\n                sdf_derives, tuple(f\"sdf__{key}\" for key in self.geometry.dim_keys)\n            )\n    else:\n        sdf_dict = {}\n        sdf_derives_dict = {}\n    x_dict = misc.convert_to_dict(x, self.dim_keys)\n\n    return {**x_dict, **sdf_dict, **sdf_derives_dict}\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry.uniform_boundary_points","title":"<code>uniform_boundary_points(n, criteria=None)</code>","text":"<p>Uniform boundary points on the spatial-temporal domain. Geometry surface area ~ bbox. Time surface area ~ diam.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The total number of boundary points on the spatial-temporal domain to be generated that are evenly distributed across geometry boundaries.</p> required <code>criteria</code> <code>Optional[Callable]</code> <p>Used to filter the generated boundary points, only points that meet certain conditions are retained. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A set of  point coordinates evenly distributed across geometry boundaries on the spatial-temporal domain.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n&gt;&gt;&gt; ts = time_geom.uniform_boundary_points(1000)\n&gt;&gt;&gt; print(ts.shape)\n(1000, 3)\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def uniform_boundary_points(\n    self, n: int, criteria: Optional[Callable] = None\n) -&gt; np.ndarray:\n    \"\"\"Uniform boundary points on the spatial-temporal domain.\n    Geometry surface area ~ bbox.\n    Time surface area ~ diam.\n\n    Args:\n        n (int): The total number of boundary points on the spatial-temporal domain to be generated that are evenly distributed across geometry boundaries.\n        criteria (Optional[Callable]): Used to filter the generated boundary points, only points that meet certain conditions are retained. Default is None.\n\n    Returns:\n        np.ndarray: A set of  point coordinates evenly distributed across geometry boundaries on the spatial-temporal domain.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n        &gt;&gt;&gt; ts = time_geom.uniform_boundary_points(1000)\n        &gt;&gt;&gt; print(ts.shape)\n        (1000, 3)\n    \"\"\"\n    if self.geometry.ndim == 1:\n        nx = 2\n    else:\n        s = 2 * sum(\n            map(\n                lambda l: l[0] * l[1],\n                itertools.combinations(\n                    self.geometry.bbox[1] - self.geometry.bbox[0], 2\n                ),\n            )\n        )\n        nx = int((n * s / self.timedomain.diam) ** 0.5)\n    nt = int(np.ceil(n / nx))\n\n    _size, _ntry, _nsuc = 0, 0, 0\n    x = np.empty(shape=(nx, self.geometry.ndim), dtype=paddle.get_default_dtype())\n    while _size &lt; nx:\n        _x = self.geometry.uniform_boundary_points(nx)\n        if criteria is not None:\n            # fix arg 't' to None in criteria there\n            criteria_mask = criteria(\n                None, *np.split(_x, self.geometry.ndim, axis=1)\n            ).flatten()\n            _x = _x[criteria_mask]\n        if len(_x) &gt; nx - _size:\n            _x = _x[: nx - _size]\n        x[_size : _size + len(_x)] = _x\n\n        _size += len(_x)\n        _ntry += 1\n        if len(_x) &gt; 0:\n            _nsuc += 1\n\n        if _ntry &gt;= 1000 and _nsuc == 0:\n            raise ValueError(\n                \"Sample boundary points failed, \"\n                \"please check correctness of geometry and given criteria.\"\n            )\n\n    nx = len(x)\n    t = np.linspace(\n        self.timedomain.t1,\n        self.timedomain.t0,\n        num=nt,\n        endpoint=False,\n        dtype=paddle.get_default_dtype(),\n    )[:, None][::-1]\n    tx = []\n    for ti in t:\n        tx.append(\n            np.hstack((np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x))\n        )\n    tx = np.vstack(tx)\n    if len(tx) &gt; n:\n        tx = tx[:n]\n    return tx\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry.uniform_initial_points","title":"<code>uniform_initial_points(n)</code>","text":"<p>Generate evenly distributed point coordinates on the spatial-temporal domain at the initial moment.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The total number of generated points.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A set of point coordinates evenly distributed on the spatial-temporal domain at the initial moment.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n&gt;&gt;&gt; ts = time_geom.uniform_initial_points(1000)\n&gt;&gt;&gt; print(ts.shape)\n(1000, 3)\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def uniform_initial_points(self, n: int) -&gt; np.ndarray:\n    \"\"\"Generate evenly distributed point coordinates on the spatial-temporal domain at the initial moment.\n\n    Args:\n        n (int): The total number of generated points.\n\n    Returns:\n       np.ndarray: A set of point coordinates evenly distributed on the spatial-temporal domain at the initial moment.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n        &gt;&gt;&gt; ts = time_geom.uniform_initial_points(1000)\n        &gt;&gt;&gt; print(ts.shape)\n        (1000, 3)\n    \"\"\"\n    x = self.geometry.uniform_points(n, True)\n    t = self.timedomain.t0\n    if len(x) &gt; n:\n        x = x[:n]\n    return np.hstack((np.full([n, 1], t, dtype=paddle.get_default_dtype()), x))\n</code></pre>"},{"location":"zh/api/geometry/#ppsci.geometry.TimeXGeometry.uniform_points","title":"<code>uniform_points(n, boundary=True)</code>","text":"<p>Uniform points on the spatial-temporal domain. Geometry volume ~ bbox. Time volume ~ diam.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The total number of sample points to be generated.</p> required <code>boundary</code> <code>bool</code> <p>Indicates whether boundary points are included, default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: a set of spatial-temporal coordinate points 'tx' that represent sample points evenly distributed within the spatial-temporal domain.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.001)\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n&gt;&gt;&gt; ts = time_geom.uniform_points(1000)\n&gt;&gt;&gt; print(ts.shape)\n(1000, 3)\n</code></pre> Source code in <code>ppsci/geometry/timedomain.py</code> <pre><code>def uniform_points(self, n: int, boundary: bool = True) -&gt; np.ndarray:\n    \"\"\"Uniform points on the spatial-temporal domain.\n    Geometry volume ~ bbox.\n    Time volume ~ diam.\n\n    Args:\n        n (int): The total number of sample points to be generated.\n        boundary (bool): Indicates whether boundary points are included, default is True.\n\n    Returns:\n        np.ndarray: a set of spatial-temporal coordinate points 'tx' that represent sample points evenly distributed within the spatial-temporal domain.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; timedomain = ppsci.geometry.TimeDomain(0, 1, 0.001)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; time_geom = ppsci.geometry.TimeXGeometry(timedomain, geom)\n        &gt;&gt;&gt; ts = time_geom.uniform_points(1000)\n        &gt;&gt;&gt; print(ts.shape)\n        (1000, 3)\n    \"\"\"\n    if self.timedomain.time_step is not None:\n        # exclude start time t0\n        nt = int(np.ceil(self.timedomain.diam / self.timedomain.time_step))\n        nx = int(np.ceil(n / nt))\n    elif self.timedomain.timestamps is not None:\n        # exclude start time t0\n        nt = self.timedomain.num_timestamps - 1\n        nx = int(np.ceil(n / nt))\n    else:\n        nx = int(\n            np.ceil(\n                (\n                    n\n                    * np.prod(self.geometry.bbox[1] - self.geometry.bbox[0])\n                    / self.timedomain.diam\n                )\n                ** 0.5\n            )\n        )\n        nt = int(np.ceil(n / nx))\n    x = self.geometry.uniform_points(nx, boundary=boundary)\n    nx = len(x)\n    if boundary and (\n        self.timedomain.time_step is None and self.timedomain.timestamps is None\n    ):\n        t = self.timedomain.uniform_points(nt, boundary=True)\n    else:\n        if self.timedomain.time_step is not None:\n            t = np.linspace(\n                self.timedomain.t1,\n                self.timedomain.t0,\n                num=nt,\n                endpoint=boundary,\n                dtype=paddle.get_default_dtype(),\n            )[:, None][::-1]\n        else:\n            t = self.timedomain.timestamps[1:]\n    tx = []\n    for ti in t:\n        tx.append(\n            np.hstack((np.full([nx, 1], ti, dtype=paddle.get_default_dtype()), x))\n        )\n    tx = np.vstack(tx)\n    if len(tx) &gt; n:\n        tx = tx[:n]\n    return tx\n</code></pre>"},{"location":"zh/api/lr_scheduler/","title":"ppsci.optimizer.lr_scheduler","text":""},{"location":"zh/api/lr_scheduler/#optimizerlr_scheduler","title":"Optimizer.lr_scheduler(\u5b66\u4e60\u7387) \u6a21\u5757","text":""},{"location":"zh/api/lr_scheduler/#ppsci.optimizer.lr_scheduler","title":"<code>ppsci.optimizer.lr_scheduler</code>","text":""},{"location":"zh/api/lr_scheduler/#ppsci.optimizer.lr_scheduler.Linear","title":"<code>Linear</code>","text":"<p>               Bases: <code>LRBase</code></p> <p>Linear learning rate decay.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Total epoch(s).</p> required <code>iters_per_epoch</code> <code>int</code> <p>Number of iterations within an epoch.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> required <code>end_lr</code> <code>float</code> <p>The minimum final learning rate. Defaults to 0.0.</p> <code>0.0</code> <code>power</code> <code>float</code> <p>Power of polynomial. Defaults to 1.0.</p> <code>1.0</code> <code>cycle</code> <code>bool</code> <p>Whether the learning rate rises again. If True, then the learning rate will rise when it decrease to <code>end_lr</code> .  If False, the learning rate is monotone decreasing. Defaults to False.</p> <code>False</code> <code>warmup_epoch</code> <code>int</code> <p>Number of warmup epochs.</p> <code>0</code> <code>warmup_start_lr</code> <code>float</code> <p>Start learning rate within warmup.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>Last epoch.</p> <code>-1</code> <code>by_epoch</code> <code>bool</code> <p>Learning rate decays by epoch when by_epoch is True, else by iter.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.Linear(10, 2, 0.001)()\n</code></pre> Source code in <code>ppsci/optimizer/lr_scheduler.py</code> <pre><code>class Linear(LRBase):\n    \"\"\"Linear learning rate decay.\n\n    Args:\n        epochs (int): Total epoch(s).\n        iters_per_epoch (int): Number of iterations within an epoch.\n        learning_rate (float): Learning rate.\n        end_lr (float, optional): The minimum final learning rate. Defaults to 0.0.\n        power (float, optional): Power of polynomial. Defaults to 1.0.\n        cycle (bool, optional): Whether the learning rate rises again. If True, then the learning rate will rise when it decrease\n            to ``end_lr`` .  If False, the learning rate is monotone decreasing. Defaults to False.\n        warmup_epoch (int): Number of warmup epochs.\n        warmup_start_lr (float): Start learning rate within warmup.\n        last_epoch (int): Last epoch.\n        by_epoch (bool): Learning rate decays by epoch when by_epoch is True, else by iter.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.Linear(10, 2, 0.001)()\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int,\n        iters_per_epoch: int,\n        learning_rate: float,\n        end_lr: float = 0.0,\n        power: float = 1.0,\n        cycle: bool = False,\n        warmup_epoch: int = 0,\n        warmup_start_lr: float = 0.0,\n        last_epoch: int = -1,\n        by_epoch: bool = False,\n    ):\n        super().__init__(\n            epochs,\n            iters_per_epoch,\n            learning_rate,\n            warmup_epoch,\n            warmup_start_lr,\n            last_epoch,\n            by_epoch,\n        )\n        self.decay_steps = (epochs - self.warmup_epoch) * iters_per_epoch\n        self.end_lr = end_lr\n        self.power = power\n        self.cycle = cycle\n        self.warmup_steps = round(self.warmup_epoch * iters_per_epoch)\n        if self.by_epoch:\n            self.decay_steps = self.epochs - self.warmup_epoch\n\n    def __call__(self):\n        learning_rate = (\n            lr.PolynomialDecay(\n                learning_rate=self.learning_rate,\n                decay_steps=self.decay_steps,\n                end_lr=self.end_lr,\n                power=self.power,\n                cycle=self.cycle,\n                last_epoch=self.last_epoch,\n            )\n            if self.decay_steps &gt; 0\n            else Constant(self.learning_rate)\n        )\n\n        if self.warmup_steps &gt; 0:\n            learning_rate = self.linear_warmup(learning_rate)\n\n        setattr(learning_rate, \"by_epoch\", self.by_epoch)\n        return learning_rate\n</code></pre>"},{"location":"zh/api/lr_scheduler/#ppsci.optimizer.lr_scheduler.Cosine","title":"<code>Cosine</code>","text":"<p>               Bases: <code>LRBase</code></p> <p>Cosine learning rate decay.</p> <p>lr = 0.05 * (math.cos(epoch * (math.pi / epochs)) + 1)</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Total epoch(s).</p> required <code>iters_per_epoch</code> <code>int</code> <p>Number of iterations within an epoch.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> required <code>eta_min</code> <code>float</code> <p>Minimum learning rate. Defaults to 0.0.</p> <code>0.0</code> <code>warmup_epoch</code> <code>int</code> <p>The epoch numbers for LinearWarmup. Defaults to 0.</p> <code>0</code> <code>warmup_start_lr</code> <code>float</code> <p>Start learning rate within warmup. Defaults to 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>Last epoch. Defaults to -1.</p> <code>-1</code> <code>by_epoch</code> <code>bool</code> <p>Learning rate decays by epoch when by_epoch is True, else by iter. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.Cosine(10, 2, 1e-3)()\n</code></pre> Source code in <code>ppsci/optimizer/lr_scheduler.py</code> <pre><code>class Cosine(LRBase):\n    \"\"\"Cosine learning rate decay.\n\n    lr = 0.05 * (math.cos(epoch * (math.pi / epochs)) + 1)\n\n    Args:\n        epochs (int): Total epoch(s).\n        iters_per_epoch (int): Number of iterations within an epoch.\n        learning_rate (float): Learning rate.\n        eta_min (float, optional): Minimum learning rate. Defaults to 0.0.\n        warmup_epoch (int, optional): The epoch numbers for LinearWarmup. Defaults to 0.\n        warmup_start_lr (float, optional): Start learning rate within warmup. Defaults to 0.0.\n        last_epoch (int, optional): Last epoch. Defaults to -1.\n        by_epoch (bool, optional): Learning rate decays by epoch when by_epoch is True,\n            else by iter. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.Cosine(10, 2, 1e-3)()\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int,\n        iters_per_epoch: int,\n        learning_rate: float,\n        eta_min: float = 0.0,\n        warmup_epoch: int = 0,\n        warmup_start_lr: float = 0.0,\n        last_epoch: int = -1,\n        by_epoch: bool = False,\n    ):\n        super().__init__(\n            epochs,\n            iters_per_epoch,\n            learning_rate,\n            warmup_epoch,\n            warmup_start_lr,\n            last_epoch,\n            by_epoch,\n        )\n        self.T_max = (self.epochs - self.warmup_epoch) * self.iters_per_epoch\n        self.eta_min = eta_min\n        if self.by_epoch:\n            self.T_max = self.epochs - self.warmup_epoch\n\n    def __call__(self):\n        learning_rate = (\n            lr.CosineAnnealingDecay(\n                learning_rate=self.learning_rate,\n                T_max=self.T_max,\n                eta_min=self.eta_min,\n                last_epoch=self.last_epoch,\n            )\n            if self.T_max &gt; 0\n            else Constant(self.learning_rate)\n        )\n\n        if self.warmup_steps &gt; 0:\n            learning_rate = self.linear_warmup(learning_rate)\n\n        setattr(learning_rate, \"by_epoch\", self.by_epoch)\n        return learning_rate\n</code></pre>"},{"location":"zh/api/lr_scheduler/#ppsci.optimizer.lr_scheduler.Step","title":"<code>Step</code>","text":"<p>               Bases: <code>LRBase</code></p> <p>Step learning rate decay.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Total epoch(s).</p> required <code>iters_per_epoch</code> <code>int</code> <p>Number of iterations within an epoch.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> required <code>step_size</code> <code>int</code> <p>The interval to update.</p> required <code>gamma</code> <code>float</code> <p>The Ratio that the learning rate will be reduced. <code>new_lr = origin_lr * gamma</code>. It should be less than 1.0. Default: 0.1.</p> required <code>warmup_epoch</code> <code>int</code> <p>The epoch numbers for LinearWarmup. Defaults to 0.</p> <code>0</code> <code>warmup_start_lr</code> <code>float</code> <p>Start learning rate within warmup. Defaults to 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>Last epoch. Defaults to -1.</p> <code>-1</code> <code>by_epoch</code> <code>bool</code> <p>Learning rate decays by epoch when by_epoch is True, else by iter. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.Step(10, 1, 1e-3, 2, 0.95)()\n</code></pre> Source code in <code>ppsci/optimizer/lr_scheduler.py</code> <pre><code>class Step(LRBase):\n    \"\"\"Step learning rate decay.\n\n    Args:\n        epochs (int): Total epoch(s).\n        iters_per_epoch (int): Number of iterations within an epoch.\n        learning_rate (float): Learning rate.\n        step_size (int): The interval to update.\n        gamma (float, optional): The Ratio that the learning rate will be reduced.\n            ``new_lr = origin_lr * gamma``. It should be less than 1.0. Default: 0.1.\n        warmup_epoch (int, optional): The epoch numbers for LinearWarmup. Defaults to 0.\n        warmup_start_lr (float, optional): Start learning rate within warmup. Defaults to 0.0.\n        last_epoch (int, optional): Last epoch. Defaults to -1.\n        by_epoch (bool, optional): Learning rate decays by epoch when by_epoch is True,\n            else by iter. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.Step(10, 1, 1e-3, 2, 0.95)()\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int,\n        iters_per_epoch: int,\n        learning_rate: float,\n        step_size: int,\n        gamma: float,\n        warmup_epoch: int = 0,\n        warmup_start_lr: float = 0.0,\n        last_epoch: int = -1,\n        by_epoch: bool = False,\n    ):\n        super().__init__(\n            epochs,\n            iters_per_epoch,\n            learning_rate,\n            warmup_epoch,\n            warmup_start_lr,\n            last_epoch,\n            by_epoch,\n        )\n        self.step_size = step_size * iters_per_epoch\n        self.gamma = gamma\n        if self.by_epoch:\n            self.step_size = step_size\n\n    def __call__(self):\n        learning_rate = lr.StepDecay(\n            learning_rate=self.learning_rate,\n            step_size=self.step_size,\n            gamma=self.gamma,\n            last_epoch=self.last_epoch,\n        )\n\n        if self.warmup_steps &gt; 0:\n            learning_rate = self.linear_warmup(learning_rate)\n\n        setattr(learning_rate, \"by_epoch\", self.by_epoch)\n        return learning_rate\n</code></pre>"},{"location":"zh/api/lr_scheduler/#ppsci.optimizer.lr_scheduler.Piecewise","title":"<code>Piecewise</code>","text":"<p>               Bases: <code>LRBase</code></p> <p>Piecewise learning rate decay</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Total epoch(s)</p> required <code>iters_per_epoch</code> <code>int</code> <p>Number of iterations within an epoch</p> required <code>decay_epochs</code> <code>Tuple[int, ...]</code> <p>A list of steps numbers. The type of element in the list is python int.</p> required <code>values</code> <code>Tuple[float, ...]</code> <p>Tuple of learning rate values that will be picked during different epoch boundaries.</p> required <code>warmup_epoch</code> <code>int</code> <p>The epoch numbers for LinearWarmup. Defaults to 0.</p> <code>0</code> <code>warmup_start_lr</code> <code>float</code> <p>Start learning rate within warmup. Defaults to 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>Last epoch. Defaults to -1.</p> <code>-1</code> <code>by_epoch</code> <code>bool</code> <p>Learning rate decays by epoch when by_epoch is True, else by iter. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.Piecewise(\n...     10, 1, [2, 4], (1e-3, 1e-4, 1e-5)\n... )()\n</code></pre> Source code in <code>ppsci/optimizer/lr_scheduler.py</code> <pre><code>class Piecewise(LRBase):\n    \"\"\"Piecewise learning rate decay\n\n    Args:\n        epochs (int): Total epoch(s)\n        iters_per_epoch (int): Number of iterations within an epoch\n        decay_epochs (Tuple[int, ...]): A list of steps numbers. The type of element in the\n            list is python int.\n        values (Tuple[float, ...]): Tuple of learning rate values that will be picked during\n            different epoch boundaries.\n        warmup_epoch (int, optional): The epoch numbers for LinearWarmup. Defaults to 0.\n        warmup_start_lr (float, optional): Start learning rate within warmup. Defaults to 0.0.\n        last_epoch (int, optional): Last epoch. Defaults to -1.\n        by_epoch (bool, optional): Learning rate decays by epoch when by_epoch is True,\n            else by iter. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.Piecewise(\n        ...     10, 1, [2, 4], (1e-3, 1e-4, 1e-5)\n        ... )()\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int,\n        iters_per_epoch: int,\n        decay_epochs: Tuple[int, ...],\n        values: Tuple[float, ...],\n        warmup_epoch: int = 0,\n        warmup_start_lr: float = 0.0,\n        last_epoch: int = -1,\n        by_epoch: bool = False,\n    ):\n        super().__init__(\n            epochs,\n            iters_per_epoch,\n            values[0],\n            warmup_epoch,\n            warmup_start_lr,\n            last_epoch,\n            by_epoch,\n        )\n        self.values = values\n        self.boundaries_steps = [e * iters_per_epoch for e in decay_epochs]\n        if self.by_epoch is True:\n            self.boundaries_steps = decay_epochs\n\n    def __call__(self):\n        learning_rate = lr.PiecewiseDecay(\n            boundaries=self.boundaries_steps,\n            values=self.values,\n            last_epoch=self.last_epoch,\n        )\n\n        if self.warmup_steps &gt; 0:\n            learning_rate = self.linear_warmup(learning_rate)\n\n        setattr(learning_rate, \"by_epoch\", self.by_epoch)\n        return learning_rate\n</code></pre>"},{"location":"zh/api/lr_scheduler/#ppsci.optimizer.lr_scheduler.MultiStepDecay","title":"<code>MultiStepDecay</code>","text":"<p>               Bases: <code>LRBase</code></p> <p>MultiStepDecay learning rate decay</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Total epoch(s)</p> required <code>iters_per_epoch</code> <code>int</code> <p>Number of iterations within an epoch</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate</p> required <code>milestones</code> <code>Tuple[int, ...]</code> <p>Tuple of each boundaries. should be increasing.</p> required <code>gamma</code> <code>float</code> <p>The Ratio that the learning rate will be reduced. <code>new_lr = origin_lr * gamma</code>. It should be less than 1.0. Defaults to 0.1.</p> <code>0.1</code> <code>warmup_epoch</code> <code>int</code> <p>The epoch numbers for LinearWarmup. Defaults to 0.</p> <code>0</code> <code>warmup_start_lr</code> <code>float</code> <p>Start learning rate within warmup. Defaults to 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>Last epoch. Defaults to -1.</p> <code>-1</code> <code>by_epoch</code> <code>bool</code> <p>Learning rate decays by epoch when by_epoch is True, else by iter. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.MultiStepDecay(10, 1, 1e-3, (4, 5))()\n</code></pre> Source code in <code>ppsci/optimizer/lr_scheduler.py</code> <pre><code>class MultiStepDecay(LRBase):\n    \"\"\"MultiStepDecay learning rate decay\n\n    Args:\n        epochs (int): Total epoch(s)\n        iters_per_epoch (int): Number of iterations within an epoch\n        learning_rate (float): Learning rate\n        milestones (Tuple[int, ...]): Tuple of each boundaries. should be increasing.\n        gamma (float, optional): The Ratio that the learning rate will be reduced.\n            `new_lr = origin_lr * gamma`. It should be less than 1.0. Defaults to 0.1.\n        warmup_epoch (int, optional): The epoch numbers for LinearWarmup. Defaults to 0.\n        warmup_start_lr (float, optional): Start learning rate within warmup. Defaults to 0.0.\n        last_epoch (int, optional): Last epoch. Defaults to -1.\n        by_epoch (bool, optional): Learning rate decays by epoch when by_epoch is True,\n            else by iter. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.MultiStepDecay(10, 1, 1e-3, (4, 5))()\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int,\n        iters_per_epoch: int,\n        learning_rate: float,\n        milestones: Tuple[int, ...],\n        gamma: float = 0.1,\n        warmup_epoch: int = 0,\n        warmup_start_lr: float = 0.0,\n        last_epoch: int = -1,\n        by_epoch: bool = False,\n    ):\n        super().__init__(\n            epochs,\n            iters_per_epoch,\n            learning_rate,\n            warmup_epoch,\n            warmup_start_lr,\n            last_epoch,\n            by_epoch,\n        )\n        self.milestones = [x * iters_per_epoch for x in milestones]\n        self.gamma = gamma\n        if self.by_epoch:\n            self.milestones = milestones\n\n    def __call__(self):\n        learning_rate = lr.MultiStepDecay(\n            learning_rate=self.learning_rate,\n            milestones=self.milestones,\n            gamma=self.gamma,\n            last_epoch=self.last_epoch,\n        )\n\n        if self.warmup_steps &gt; 0:\n            learning_rate = self.linear_warmup(learning_rate)\n\n        setattr(learning_rate, \"by_epoch\", self.by_epoch)\n        return learning_rate\n</code></pre>"},{"location":"zh/api/lr_scheduler/#ppsci.optimizer.lr_scheduler.ExponentialDecay","title":"<code>ExponentialDecay</code>","text":"<p>               Bases: <code>LRBase</code></p> <p>ExponentialDecay learning rate decay.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Total epoch(s).</p> required <code>iters_per_epoch</code> <code>int</code> <p>Number of iterations within an epoch.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> required <code>gamma</code> <code>float</code> <p>The decay rate.</p> required <code>decay_steps</code> <code>int</code> <p>The number of steps to decay.</p> required <code>warmup_epoch</code> <code>int</code> <p>Number of warmup epochs.</p> <code>0</code> <code>warmup_start_lr</code> <code>float</code> <p>Start learning rate within warmup.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>Last epoch.</p> <code>-1</code> <code>by_epoch</code> <code>bool</code> <p>Learning rate decays by epoch when by_epoch is True, else by iter.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.ExponentialDecay(10, 2, 1e-3, 0.95, 3)()\n</code></pre> Source code in <code>ppsci/optimizer/lr_scheduler.py</code> <pre><code>class ExponentialDecay(LRBase):\n    \"\"\"ExponentialDecay learning rate decay.\n\n    Args:\n        epochs (int): Total epoch(s).\n        iters_per_epoch (int): Number of iterations within an epoch.\n        learning_rate (float): Learning rate.\n        gamma (float): The decay rate.\n        decay_steps (int): The number of steps to decay.\n        warmup_epoch (int): Number of warmup epochs.\n        warmup_start_lr (float): Start learning rate within warmup.\n        last_epoch (int): Last epoch.\n        by_epoch (bool): Learning rate decays by epoch when by_epoch is True, else by iter.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.ExponentialDecay(10, 2, 1e-3, 0.95, 3)()\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int,\n        iters_per_epoch: int,\n        learning_rate: float,\n        gamma: float,\n        decay_steps: int,\n        warmup_epoch: int = 0,\n        warmup_start_lr: float = 0.0,\n        last_epoch: int = -1,\n        by_epoch: bool = False,\n    ):\n        super().__init__(\n            epochs,\n            iters_per_epoch,\n            learning_rate,\n            warmup_epoch,\n            warmup_start_lr,\n            last_epoch,\n            by_epoch,\n        )\n        self.decay_steps = decay_steps\n        self.gamma = gamma\n        self.warmup_steps = round(self.warmup_epoch * iters_per_epoch)\n        if self.by_epoch:\n            self.decay_steps /= iters_per_epoch\n\n    def __call__(self):\n        learning_rate = lr.ExponentialDecay(\n            learning_rate=self.learning_rate,\n            gamma=self.gamma ** (1 / self.decay_steps),\n            last_epoch=self.last_epoch,\n        )\n\n        if self.warmup_steps &gt; 0:\n            learning_rate = self.linear_warmup(learning_rate)\n\n        setattr(learning_rate, \"by_epoch\", self.by_epoch)\n        return learning_rate\n</code></pre>"},{"location":"zh/api/lr_scheduler/#ppsci.optimizer.lr_scheduler.CosineWarmRestarts","title":"<code>CosineWarmRestarts</code>","text":"<p>               Bases: <code>LRBase</code></p> <p>Set the learning rate using a cosine annealing schedule with warm restarts.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Total epoch(s)</p> required <code>iters_per_epoch</code> <code>int</code> <p>Number of iterations within an epoch</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate</p> required <code>T_0</code> <code>int</code> <p>Number of iterations for the first restart.</p> required <code>T_mult</code> <code>int</code> <p>A factor increases T_i after a restart</p> required <code>eta_min</code> <code>float</code> <p>Minimum learning rate. Defaults to 0.0.</p> <code>0.0</code> <code>warmup_epoch</code> <code>int</code> <p>The epoch numbers for LinearWarmup. Defaults to 0.</p> <code>0</code> <code>warmup_start_lr</code> <code>float</code> <p>Start learning rate within warmup. Defaults to 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>Last epoch. Defaults to -1.</p> <code>-1</code> <code>by_epoch</code> <code>bool</code> <p>Learning rate decays by epoch when by_epoch is True, else by iter. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.CosineWarmRestarts(20, 1, 1e-3, 14, 2)()\n</code></pre> Source code in <code>ppsci/optimizer/lr_scheduler.py</code> <pre><code>class CosineWarmRestarts(LRBase):\n    \"\"\"Set the learning rate using a cosine annealing schedule with warm restarts.\n\n    Args:\n        epochs (int): Total epoch(s)\n        iters_per_epoch (int): Number of iterations within an epoch\n        learning_rate (float): Learning rate\n        T_0 (int): Number of iterations for the first restart.\n        T_mult (int): A factor increases T_i after a restart\n        eta_min (float, optional): Minimum learning rate. Defaults to 0.0.\n        warmup_epoch (int, optional): The epoch numbers for LinearWarmup. Defaults to 0.\n        warmup_start_lr (float, optional): Start learning rate within warmup. Defaults to 0.0.\n        last_epoch (int, optional): Last epoch. Defaults to -1.\n        by_epoch (bool, optional): Learning rate decays by epoch when by_epoch is True, else by iter. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.CosineWarmRestarts(20, 1, 1e-3, 14, 2)()\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int,\n        iters_per_epoch: int,\n        learning_rate: float,\n        T_0: int,\n        T_mult: int,\n        eta_min: float = 0.0,\n        warmup_epoch: int = 0,\n        warmup_start_lr: float = 0.0,\n        last_epoch: int = -1,\n        by_epoch: bool = False,\n    ):\n        super().__init__(\n            epochs,\n            iters_per_epoch,\n            learning_rate,\n            warmup_epoch,\n            warmup_start_lr,\n            last_epoch,\n            by_epoch,\n        )\n        self.T_0 = T_0\n        self.T_mult = T_mult\n        self.eta_min = eta_min\n        if self.by_epoch is False:\n            self.T_0 = T_0 * iters_per_epoch\n\n    def __call__(self):\n        learning_rate = CosineAnnealingWarmRestarts(\n            learning_rate=self.learning_rate,\n            T_0=self.T_0,\n            T_mult=self.T_mult,\n            eta_min=self.eta_min,\n            last_epoch=self.last_epoch,\n            verbose=self.verbose,\n        )\n\n        if self.warmup_steps &gt; 0:\n            learning_rate = self.linear_warmup(learning_rate)\n\n        setattr(learning_rate, \"by_epoch\", self.by_epoch)\n        return learning_rate\n</code></pre>"},{"location":"zh/api/lr_scheduler/#ppsci.optimizer.lr_scheduler.OneCycleLR","title":"<code>OneCycleLR</code>","text":"<p>               Bases: <code>LRBase</code></p> <p>Sets the learning rate according to the one cycle learning rate scheduler. The scheduler adjusts the learning rate from an initial learning rate to the maximum learning rate and then from that maximum learning rate to the minimum learning rate, which is much less than the initial learning rate.</p> <p>It has been proposed in Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates.</p> <p>Please note that the default behavior of this scheduler follows the fastai implementation of one cycle, which claims that \"unpublished work has shown even better results by using only two phases\". If you want the behavior of this scheduler to be consistent with the paper, please set <code>three_phase=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Total epoch(s).</p> required <code>iters_per_epoch</code> <code>int</code> <p>Number of iterations within an epoch.</p> required <code>max_learning_rate</code> <code>float</code> <p>The maximum learning rate. It is a python float number. Functionally, it defines the initial learning rate by <code>divide_factor</code> .</p> required <code>divide_factor</code> <code>float</code> <p>Initial learning rate will be determined by initial_learning_rate = max_learning_rate / divide_factor. Defaults to 25.0.</p> <code>25.0</code> <code>end_learning_rate</code> <code>float</code> <p>The minimum learning rate during training, it should be much less than initial learning rate. Defaults to 0.0001.</p> <code>0.0001</code> <code>phase_pct</code> <code>float</code> <p>The percentage of total steps which used to increasing learning rate. Defaults to 0.3.</p> <code>0.3</code> <code>anneal_strategy</code> <code>str</code> <p>Strategy of adjusting learning rate. \"cos\" for cosine annealing, \"linear\" for linear annealing. Defaults to \"cos\".</p> <code>'cos'</code> <code>three_phase</code> <code>bool</code> <p>Whether to use three phase. Defaults to False.</p> <code>False</code> <code>warmup_epoch</code> <code>int</code> <p>The epoch numbers for LinearWarmup. Defaults to 0.</p> <code>0</code> <code>warmup_start_lr</code> <code>float</code> <p>Start learning rate within warmup. Defaults to 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>Last epoch. Defaults to -1.</p> <code>-1</code> <code>by_epoch</code> <code>bool</code> <p>Learning rate decays by epoch when by_epoch is True, else by iter. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.OneCycleLR(100, 1, 1e-3)()\n</code></pre> Source code in <code>ppsci/optimizer/lr_scheduler.py</code> <pre><code>class OneCycleLR(LRBase):\n    \"\"\"Sets the learning rate according to the one cycle learning rate scheduler.\n    The scheduler adjusts the learning rate from an initial learning rate to the maximum learning rate and then\n    from that maximum learning rate to the minimum learning rate, which is much less than the initial learning rate.\n\n    It has been proposed in [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120).\n\n    Please note that the default behavior of this scheduler follows the fastai implementation of one cycle,\n    which claims that **\"unpublished work has shown even better results by using only two phases\"**.\n    If you want the behavior of this scheduler to be consistent with the paper, please set `three_phase=True`.\n\n    Args:\n        epochs (int): Total epoch(s).\n        iters_per_epoch (int): Number of iterations within an epoch.\n        max_learning_rate (float): The maximum learning rate. It is a python float number. Functionally, it defines the initial learning rate by `divide_factor` .\n        divide_factor (float, optional): Initial learning rate will be determined by initial_learning_rate = max_learning_rate / divide_factor. Defaults to 25.0.\n        end_learning_rate (float, optional): The minimum learning rate during training, it should be much less than initial learning rate. Defaults to 0.0001.\n        phase_pct (float): The percentage of total steps which used to increasing learning rate. Defaults to 0.3.\n        anneal_strategy (str, optional): Strategy of adjusting learning rate. \"cos\" for cosine annealing, \"linear\" for linear annealing. Defaults to \"cos\".\n        three_phase (bool, optional): Whether to use three phase. Defaults to False.\n        warmup_epoch (int, optional): The epoch numbers for LinearWarmup. Defaults to 0.\n        warmup_start_lr (float, optional): Start learning rate within warmup. Defaults to 0.0.\n        last_epoch (int, optional): Last epoch. Defaults to -1.\n        by_epoch (bool, optional): Learning rate decays by epoch when by_epoch is True, else by iter. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; lr = ppsci.optimizer.lr_scheduler.OneCycleLR(100, 1, 1e-3)()\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int,\n        iters_per_epoch: int,\n        max_learning_rate: float,\n        divide_factor: float = 25.0,\n        end_learning_rate: float = 0.0001,\n        phase_pct: float = 0.3,\n        anneal_strategy: str = \"cos\",\n        three_phase: bool = False,\n        warmup_epoch: int = 0,\n        warmup_start_lr: float = 0.0,\n        last_epoch: int = -1,\n        by_epoch: bool = False,\n    ):\n        super().__init__(\n            epochs,\n            iters_per_epoch,\n            max_learning_rate,\n            warmup_epoch,\n            warmup_start_lr,\n            last_epoch,\n            by_epoch,\n        )\n        self.total_steps = epochs\n        if not by_epoch:\n            self.total_steps *= iters_per_epoch\n        self.divide_factor = divide_factor\n        self.end_learning_rate = end_learning_rate\n        self.phase_pct = phase_pct\n        self.anneal_strategy = anneal_strategy\n        self.three_phase = three_phase\n\n    def __call__(self):\n        learning_rate = lr.OneCycleLR(\n            max_learning_rate=self.learning_rate,\n            total_steps=self.total_steps,\n            divide_factor=self.divide_factor,\n            end_learning_rate=self.end_learning_rate,\n            phase_pct=self.phase_pct,\n            anneal_strategy=self.anneal_strategy,\n            three_phase=self.three_phase,\n            last_epoch=self.last_epoch,\n            verbose=self.verbose,\n        )\n\n        if self.warmup_steps &gt; 0:\n            learning_rate = self.linear_warmup(learning_rate)\n\n        setattr(learning_rate, \"by_epoch\", self.by_epoch)\n        return learning_rate\n</code></pre>"},{"location":"zh/api/metric/","title":"ppsci.metric","text":""},{"location":"zh/api/metric/#metric","title":"Metric(\u8bc4\u4ef7\u6307\u6807) \u6a21\u5757","text":""},{"location":"zh/api/metric/#ppsci.metric","title":"<code>ppsci.metric</code>","text":""},{"location":"zh/api/metric/#ppsci.metric.Metric","title":"<code>Metric</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Base class for metric.</p> Source code in <code>ppsci/metric/base.py</code> <pre><code>class Metric(nn.Layer):\n    \"\"\"Base class for metric.\"\"\"\n\n    def __init__(self, keep_batch: bool = False):\n        super().__init__()\n        self.keep_batch = keep_batch\n</code></pre>"},{"location":"zh/api/metric/#ppsci.metric.FunctionalMetric","title":"<code>FunctionalMetric</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Functional metric class, which allows to use custom metric computing function from given metric_expr for complex computation cases.</p> <p>Parameters:</p> Name Type Description Default <code>metric_expr</code> <code>Callable</code> <p>Expression of metric calculation.</p> required <code>keep_batch</code> <code>bool</code> <p>Whether keep batch axis. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.metric import FunctionalMetric\n&gt;&gt;&gt; def metric_expr(output_dict, *args):\n...     rel_l2 = 0\n...     for key in output_dict:\n...         length = int(len(output_dict[key])/2)\n...         out_dict = output_dict[key][:length]\n...         label_dict = output_dict[key][length:]\n...         rel_l2 += paddle.norm(out_dict - label_dict) / paddle.norm(label_dict)\n...     return {\"rel_l2\": rel_l2}\n&gt;&gt;&gt; metric_dict = FunctionalMetric(metric_expr)\n&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3], [-0.2, 1.5], [-0.1, -0.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3], [-1.8, 1.0], [-0.2, 2.5]])}\n&gt;&gt;&gt; result = metric_dict(output_dict)\n&gt;&gt;&gt; print(result)\n{'rel_l2': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       2.59985542)}\n</code></pre> Source code in <code>ppsci/metric/func.py</code> <pre><code>class FunctionalMetric(base.Metric):\n    r\"\"\"Functional metric class, which allows to use custom metric computing function from given metric_expr for complex computation cases.\n\n    Args:\n        metric_expr (Callable): Expression of metric calculation.\n        keep_batch (bool, optional): Whether keep batch axis. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.metric import FunctionalMetric\n        &gt;&gt;&gt; def metric_expr(output_dict, *args):\n        ...     rel_l2 = 0\n        ...     for key in output_dict:\n        ...         length = int(len(output_dict[key])/2)\n        ...         out_dict = output_dict[key][:length]\n        ...         label_dict = output_dict[key][length:]\n        ...         rel_l2 += paddle.norm(out_dict - label_dict) / paddle.norm(label_dict)\n        ...     return {\"rel_l2\": rel_l2}\n        &gt;&gt;&gt; metric_dict = FunctionalMetric(metric_expr)\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3], [-0.2, 1.5], [-0.1, -0.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3], [-1.8, 1.0], [-0.2, 2.5]])}\n        &gt;&gt;&gt; result = metric_dict(output_dict)\n        &gt;&gt;&gt; print(result)\n        {'rel_l2': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               2.59985542)}\n    \"\"\"\n\n    def __init__(\n        self,\n        metric_expr: Callable,\n        keep_batch: bool = False,\n    ):\n        super().__init__(keep_batch)\n        self.metric_expr = metric_expr\n\n    def forward(self, output_dict, label_dict=None) -&gt; Dict[str, \"paddle.Tensor\"]:\n        return self.metric_expr(output_dict, label_dict)\n</code></pre>"},{"location":"zh/api/metric/#ppsci.metric.MAE","title":"<code>MAE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean absolute error.</p> \\[ metric = \\dfrac{1}{N} \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>Parameters:</p> Name Type Description Default <code>keep_batch</code> <code>bool</code> <p>Whether keep batch axis. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.metric import MAE\n&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; loss = MAE()\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       1.87500000), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.89999998)}\n&gt;&gt;&gt; loss = MAE(keep_batch=True)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [1.20000005, 2.54999995]), 'v': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [0.59999996, 1.20000005])}\n</code></pre> Source code in <code>ppsci/metric/mae.py</code> <pre><code>class MAE(base.Metric):\n    r\"\"\"Mean absolute error.\n\n    $$\n    metric = \\dfrac{1}{N} \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    Args:\n        keep_batch (bool, optional): Whether keep batch axis. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.metric import MAE\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; loss = MAE()\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               1.87500000), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.89999998)}\n        &gt;&gt;&gt; loss = MAE(keep_batch=True)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [1.20000005, 2.54999995]), 'v': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [0.59999996, 1.20000005])}\n    \"\"\"\n\n    def __init__(self, keep_batch: bool = False):\n        super().__init__(keep_batch)\n\n    @paddle.no_grad()\n    def forward(self, output_dict, label_dict) -&gt; Dict[str, \"paddle.Tensor\"]:\n        metric_dict = {}\n        for key in label_dict:\n            mae = F.l1_loss(output_dict[key], label_dict[key], \"none\")\n            if self.keep_batch:\n                metric_dict[key] = mae.mean(axis=tuple(range(1, mae.ndim)))\n            else:\n                metric_dict[key] = mae.mean()\n\n        return metric_dict\n</code></pre>"},{"location":"zh/api/metric/#ppsci.metric.MSE","title":"<code>MSE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean square error</p> \\[ metric = \\dfrac{1}{N} \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2^2 \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>Parameters:</p> Name Type Description Default <code>keep_batch</code> <code>bool</code> <p>Whether keep batch axis. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.metric import MSE\n&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; loss = MSE()\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       5.35750008), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.94000000)}\n&gt;&gt;&gt; loss = MSE(keep_batch=True)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [2.65000010, 8.06499958]), 'v': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [0.39999998, 1.48000002])}\n</code></pre> Source code in <code>ppsci/metric/mse.py</code> <pre><code>class MSE(base.Metric):\n    r\"\"\"Mean square error\n\n    $$\n    metric = \\dfrac{1}{N} \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2^2\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    Args:\n        keep_batch (bool, optional): Whether keep batch axis. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.metric import MSE\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; loss = MSE()\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               5.35750008), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.94000000)}\n        &gt;&gt;&gt; loss = MSE(keep_batch=True)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [2.65000010, 8.06499958]), 'v': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [0.39999998, 1.48000002])}\n    \"\"\"\n\n    def __init__(self, keep_batch: bool = False):\n        super().__init__(keep_batch)\n\n    @paddle.no_grad()\n    def forward(self, output_dict, label_dict) -&gt; Dict[str, \"paddle.Tensor\"]:\n        metric_dict = {}\n        for key in label_dict:\n            mse = F.mse_loss(output_dict[key], label_dict[key], \"none\")\n            if self.keep_batch:\n                metric_dict[key] = mse.mean(axis=tuple(range(1, mse.ndim)))\n            else:\n                metric_dict[key] = mse.mean()\n\n        return metric_dict\n</code></pre>"},{"location":"zh/api/metric/#ppsci.metric.RMSE","title":"<code>RMSE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Root mean square error</p> \\[ metric = \\sqrt{\\dfrac{1}{N} \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2^2} \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>Parameters:</p> Name Type Description Default <code>keep_batch</code> <code>bool</code> <p>Whether keep batch axis. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.metric import RMSE\n&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; loss = RMSE()\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       2.31462741), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.96953595)}\n</code></pre> Source code in <code>ppsci/metric/rmse.py</code> <pre><code>class RMSE(base.Metric):\n    r\"\"\"Root mean square error\n\n    $$\n    metric = \\sqrt{\\dfrac{1}{N} \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2^2}\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    Args:\n        keep_batch (bool, optional): Whether keep batch axis. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.metric import RMSE\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; loss = RMSE()\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               2.31462741), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.96953595)}\n    \"\"\"\n\n    def __init__(self, keep_batch: bool = False):\n        if keep_batch:\n            raise ValueError(f\"keep_batch should be False, but got {keep_batch}.\")\n        super().__init__(keep_batch)\n\n    @paddle.no_grad()\n    def forward(self, output_dict, label_dict) -&gt; Dict[str, \"paddle.Tensor\"]:\n        metric_dict = {}\n        for key in label_dict:\n            rmse = F.mse_loss(output_dict[key], label_dict[key], \"mean\") ** 0.5\n            metric_dict[key] = rmse\n\n        return metric_dict\n</code></pre>"},{"location":"zh/api/metric/#ppsci.metric.L2Rel","title":"<code>L2Rel</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Class for l2 relative error.</p> <p>NOTE: This metric API is slightly different from <code>MeanL2Rel</code>, difference is as below:</p> <ul> <li><code>L2Rel</code> regards the input sample as a whole and calculates the l2 relative error of the whole;</li> <li><code>MeanL2Rel</code> will calculate L2Rel separately for each input sample and return the average of l2 relative error for all samples.</li> </ul> \\[ metric = \\dfrac{\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2}{\\max(\\Vert \\mathbf{y} \\Vert_2, \\epsilon)} \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>Parameters:</p> Name Type Description Default <code>keep_batch</code> <code>bool</code> <p>Whether keep batch axis. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.metric import L2Rel\n&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; loss = L2Rel()\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       1.42658269), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       9.69535923)}\n</code></pre> Source code in <code>ppsci/metric/l2_rel.py</code> <pre><code>class L2Rel(base.Metric):\n    r\"\"\"Class for l2 relative error.\n\n    NOTE: This metric API is slightly different from `MeanL2Rel`, difference is as below:\n\n    - `L2Rel` regards the input sample as a whole and calculates the l2 relative error of the whole;\n    - `MeanL2Rel` will calculate L2Rel separately for each input sample and return the average of l2 relative error for all samples.\n\n    $$\n    metric = \\dfrac{\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2}{\\max(\\Vert \\mathbf{y} \\Vert_2, \\epsilon)}\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    Args:\n        keep_batch (bool, optional): Whether keep batch axis. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.metric import L2Rel\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; loss = L2Rel()\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               1.42658269), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               9.69535923)}\n    \"\"\"\n\n    # NOTE: Avoid divide by zero in result\n    # see https://github.com/scikit-learn/scikit-learn/pull/15007\n    EPS: float = np.finfo(np.float32).eps\n\n    def __init__(self, keep_batch: bool = False):\n        if keep_batch:\n            raise ValueError(f\"keep_batch should be False, but got {keep_batch}.\")\n        super().__init__(keep_batch)\n\n    @paddle.no_grad()\n    def forward(self, output_dict, label_dict) -&gt; Dict[str, \"paddle.Tensor\"]:\n        metric_dict = {}\n        for key in label_dict:\n            rel_l2 = paddle.norm(label_dict[key] - output_dict[key], p=2) / paddle.norm(\n                label_dict[key], p=2\n            ).clip(min=self.EPS)\n            metric_dict[key] = rel_l2\n\n        return metric_dict\n</code></pre>"},{"location":"zh/api/metric/#ppsci.metric.MeanL2Rel","title":"<code>MeanL2Rel</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Class for mean l2 relative error.</p> <p>NOTE: This metric API is slightly different from <code>L2Rel</code>, difference is as below:</p> <ul> <li><code>MeanL2Rel</code> will calculate L2Rel separately for each input sample and return the average of l2 relative error for all samples.</li> <li><code>L2Rel</code> regards the input sample as a whole and calculates the l2 relative error of the whole;</li> </ul> \\[ metric = \\dfrac{1}{M} \\sum_{i=1}^{M}\\dfrac{\\Vert \\mathbf{x_i} - \\mathbf{y_i} \\Vert_2}{\\max(\\Vert \\mathbf{y_i} \\Vert_2, \\epsilon) } \\] \\[ \\mathbf{x_i}, \\mathbf{y_i} \\in \\mathcal{R}^{N} \\] <p>Parameters:</p> Name Type Description Default <code>keep_batch</code> <code>bool</code> <p>Whether keep batch axis. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.metric import MeanL2Rel\n&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; loss = MeanL2Rel()\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       1.35970235), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       9.24504089)}\n&gt;&gt;&gt; loss = MeanL2Rel(keep_batch=True)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [1.11803389, 1.60137081]), 'v': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [6.32455540 , 12.16552544])}\n</code></pre> Source code in <code>ppsci/metric/l2_rel.py</code> <pre><code>class MeanL2Rel(base.Metric):\n    r\"\"\"Class for mean l2 relative error.\n\n    NOTE: This metric API is slightly different from `L2Rel`, difference is as below:\n\n    - `MeanL2Rel` will calculate L2Rel separately for each input sample and return the average of l2 relative error for all samples.\n    - `L2Rel` regards the input sample as a whole and calculates the l2 relative error of the whole;\n\n    $$\n    metric = \\dfrac{1}{M} \\sum_{i=1}^{M}\\dfrac{\\Vert \\mathbf{x_i} - \\mathbf{y_i} \\Vert_2}{\\max(\\Vert \\mathbf{y_i} \\Vert_2, \\epsilon) }\n    $$\n\n    $$\n    \\mathbf{x_i}, \\mathbf{y_i} \\in \\mathcal{R}^{N}\n    $$\n\n    Args:\n        keep_batch (bool, optional): Whether keep batch axis. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.metric import MeanL2Rel\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; loss = MeanL2Rel()\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               1.35970235), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               9.24504089)}\n        &gt;&gt;&gt; loss = MeanL2Rel(keep_batch=True)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [1.11803389, 1.60137081]), 'v': Tensor(shape=[2], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               [6.32455540 , 12.16552544])}\n    \"\"\"\n\n    # NOTE: Avoid divide by zero in result\n    # see https://github.com/scikit-learn/scikit-learn/pull/15007\n    EPS: float = np.finfo(np.float32).eps\n\n    def __init__(self, keep_batch: bool = False):\n        super().__init__(keep_batch)\n\n    @paddle.no_grad()\n    def forward(self, output_dict, label_dict) -&gt; Dict[str, \"paddle.Tensor\"]:\n        metric_dict = {}\n        for key in label_dict:\n            rel_l2 = paddle.norm(\n                label_dict[key] - output_dict[key], p=2, axis=1\n            ) / paddle.norm(label_dict[key], p=2, axis=1).clip(min=self.EPS)\n            if self.keep_batch:\n                metric_dict[key] = rel_l2\n            else:\n                metric_dict[key] = rel_l2.mean()\n\n        return metric_dict\n</code></pre>"},{"location":"zh/api/metric/#ppsci.metric.LatitudeWeightedACC","title":"<code>LatitudeWeightedACC</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Latitude weighted anomaly correlation coefficient.</p> \\[ metric =     \\dfrac{\\sum\\limits_{m,n}{L_mX_{mn}Y_{mn}}}{\\sqrt{\\sum\\limits_{m,n}{L_mX_{mn}^{2}}\\sum\\limits_{m,n}{L_mY_{mn}^{2}}}} \\] \\[ L_m = N_{lat}\\dfrac{\\cos(lat_m)}{\\sum\\limits_{j=1}^{N_{lat}}\\cos(lat_j)} \\] <p>\\(lat_m\\) is the latitude at m. \\(N_{lat}\\) is the number of latitude set by <code>num_lat</code>.</p> <p>Parameters:</p> Name Type Description Default <code>num_lat</code> <code>int</code> <p>Number of latitude.</p> required <code>mean</code> <code>Optional[Union[array, Tuple[float, ...]]]</code> <p>Mean of training data. Defaults to None.</p> required <code>keep_batch</code> <code>bool</code> <p>Whether keep batch axis. Defaults to False.</p> <code>False</code> <code>variable_dict</code> <code>Optional[Dict[str, int]]</code> <p>Variable dictionary, the key is the name of a variable and the value is its index. Defaults to None.</p> <code>None</code> <code>unlog</code> <code>bool</code> <p>Whether calculate expm1 for all elements in the array. Defaults to False.</p> <code>False</code> <code>scale</code> <code>float</code> <p>The scale value used after expm1. Defaults to 1e-5.</p> <code>1e-05</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; mean = np.random.randn(20, 720, 1440)\n&gt;&gt;&gt; metric = ppsci.metric.LatitudeWeightedACC(720, mean=mean)\n</code></pre> Source code in <code>ppsci/metric/anomaly_coef.py</code> <pre><code>class LatitudeWeightedACC(base.Metric):\n    r\"\"\"Latitude weighted anomaly correlation coefficient.\n\n    $$\n    metric =\n        \\dfrac{\\sum\\limits_{m,n}{L_mX_{mn}Y_{mn}}}{\\sqrt{\\sum\\limits_{m,n}{L_mX_{mn}^{2}}\\sum\\limits_{m,n}{L_mY_{mn}^{2}}}}\n    $$\n\n    $$\n    L_m = N_{lat}\\dfrac{\\cos(lat_m)}{\\sum\\limits_{j=1}^{N_{lat}}\\cos(lat_j)}\n    $$\n\n    $lat_m$ is the latitude at m.\n    $N_{lat}$ is the number of latitude set by `num_lat`.\n\n    Args:\n        num_lat (int): Number of latitude.\n        mean (Optional[Union[np.array, Tuple[float, ...]]]): Mean of training data. Defaults to None.\n        keep_batch (bool, optional): Whether keep batch axis. Defaults to False.\n        variable_dict (Optional[Dict[str, int]]): Variable dictionary, the key is the name of a variable and\n            the value is its index. Defaults to None.\n        unlog (bool, optional): Whether calculate expm1 for all elements in the array. Defaults to False.\n        scale (float, optional): The scale value used after expm1. Defaults to 1e-5.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; mean = np.random.randn(20, 720, 1440)\n        &gt;&gt;&gt; metric = ppsci.metric.LatitudeWeightedACC(720, mean=mean)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_lat: int,\n        mean: Optional[Union[np.array, Tuple[float, ...]]],\n        keep_batch: bool = False,\n        variable_dict: Optional[Dict[str, int]] = None,\n        unlog: bool = False,\n        scale: float = 1e-5,\n    ):\n        super().__init__(keep_batch)\n        self.num_lat = num_lat\n        self.mean = (\n            None if mean is None else paddle.to_tensor(mean, paddle.get_default_dtype())\n        )\n        self.variable_dict = variable_dict\n        self.unlog = unlog\n        self.scale = scale\n\n        self.weight = self.get_latitude_weight(num_lat)\n\n    def get_latitude_weight(self, num_lat: int = 720):\n        lat_t = paddle.linspace(start=0, stop=1, num=num_lat)\n        lat_t = paddle.cos(3.1416 * (0.5 - lat_t))\n        weight = num_lat * lat_t / paddle.sum(lat_t)\n        weight = weight.reshape((1, 1, -1, 1))\n        return weight\n\n    def scale_expm1(self, x: paddle.Tensor):\n        return self.scale * paddle.expm1(x)\n\n    @paddle.no_grad()\n    def forward(self, output_dict, label_dict) -&gt; Dict[str, \"paddle.Tensor\"]:\n        metric_dict = {}\n\n        for key in label_dict:\n            output = (\n                self.scale_expm1(output_dict[key]) if self.unlog else output_dict[key]\n            )\n            label = self.scale_expm1(label_dict[key]) if self.unlog else label_dict[key]\n\n            if self.mean is not None:\n                output = output - self.mean\n                label = label - self.mean\n\n            rmse = paddle.sum(\n                self.weight * output * label, axis=(-1, -2)\n            ) / paddle.sqrt(\n                paddle.sum(self.weight * output**2, axis=(-1, -2))\n                * paddle.sum(self.weight * label**2, axis=(-1, -2))\n            )\n\n            if self.variable_dict is not None:\n                for variable_name, idx in self.variable_dict.items():\n                    if self.keep_batch:\n                        metric_dict[f\"{key}.{variable_name}\"] = rmse[:, idx]\n                    else:\n                        metric_dict[f\"{key}.{variable_name}\"] = rmse[:, idx].mean()\n            else:\n                if self.keep_batch:\n                    metric_dict[key] = rmse.mean(axis=1)\n                else:\n                    metric_dict[key] = rmse.mean()\n\n        return metric_dict\n</code></pre>"},{"location":"zh/api/metric/#ppsci.metric.LatitudeWeightedRMSE","title":"<code>LatitudeWeightedRMSE</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Latitude weighted root mean square error.</p> \\[ metric =\\sqrt{\\dfrac{1}{MN}\\sum\\limits_{m=1}^{M}\\sum\\limits_{n=1}^{N}L_m(X_{mn}-Y_{mn})^{2}} \\] \\[ L_m = N_{lat}\\dfrac{\\cos(lat_m)}{\\sum\\limits_{j=1}^{N_{lat}}\\cos(lat_j)} \\] <p>\\(lat_m\\) is the latitude at m. \\(N_{lat}\\) is the number of latitude set by <code>num_lat</code>.</p> <p>Parameters:</p> Name Type Description Default <code>num_lat</code> <code>int</code> <p>Number of latitude.</p> required <code>std</code> <code>Optional[Union[array, Tuple[float, ...]]]</code> <p>Standard Deviation of training dataset. Defaults to None.</p> <code>None</code> <code>keep_batch</code> <code>bool</code> <p>Whether keep batch axis. Defaults to False.</p> <code>False</code> <code>variable_dict</code> <code>Optional[Dict[str, int]]</code> <p>Variable dictionary, the key is the name of a variable and the value is its index. Defaults to None.</p> <code>None</code> <code>unlog</code> <code>bool</code> <p>Whether calculate expm1 for all elements in the array. Defaults to False.</p> <code>False</code> <code>scale</code> <code>float</code> <p>The scale value used after expm1. Defaults to 1e-5.</p> <code>1e-05</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; std = np.random.randn(20, 1, 1)\n&gt;&gt;&gt; metric = ppsci.metric.LatitudeWeightedRMSE(720, std=std)\n</code></pre> Source code in <code>ppsci/metric/rmse.py</code> <pre><code>class LatitudeWeightedRMSE(base.Metric):\n    r\"\"\"Latitude weighted root mean square error.\n\n    $$\n    metric =\\sqrt{\\dfrac{1}{MN}\\sum\\limits_{m=1}^{M}\\sum\\limits_{n=1}^{N}L_m(X_{mn}-Y_{mn})^{2}}\n    $$\n\n    $$\n    L_m = N_{lat}\\dfrac{\\cos(lat_m)}{\\sum\\limits_{j=1}^{N_{lat}}\\cos(lat_j)}\n    $$\n\n    $lat_m$ is the latitude at m.\n    $N_{lat}$ is the number of latitude set by `num_lat`.\n\n    Args:\n        num_lat (int): Number of latitude.\n        std (Optional[Union[np.array, Tuple[float, ...]]]): Standard Deviation of training dataset. Defaults to None.\n        keep_batch (bool, optional): Whether keep batch axis. Defaults to False.\n        variable_dict (Optional[Dict[str, int]]): Variable dictionary, the key is the name of a variable and\n            the value is its index. Defaults to None.\n        unlog (bool, optional): Whether calculate expm1 for all elements in the array. Defaults to False.\n        scale (float, optional): The scale value used after expm1. Defaults to 1e-5.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; std = np.random.randn(20, 1, 1)\n        &gt;&gt;&gt; metric = ppsci.metric.LatitudeWeightedRMSE(720, std=std)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_lat: int,\n        std: Optional[Union[np.array, Tuple[float, ...]]] = None,\n        keep_batch: bool = False,\n        variable_dict: Dict[str, int] = None,\n        unlog: bool = False,\n        scale: float = 1e-5,\n    ):\n        super().__init__(keep_batch)\n        self.num_lat = num_lat\n        self.std = (\n            None\n            if std is None\n            else paddle.to_tensor(std, paddle.get_default_dtype()).reshape((1, -1))\n        )\n        self.variable_dict = variable_dict\n        self.unlog = unlog\n        self.scale = scale\n        self.weight = self.get_latitude_weight(num_lat)\n\n    def get_latitude_weight(self, num_lat: int = 720):\n        lat_t = paddle.linspace(start=0, stop=1, num=num_lat)\n        lat_t = paddle.cos(3.1416 * (0.5 - lat_t))\n        weight = num_lat * lat_t / paddle.sum(lat_t)\n        weight = weight.reshape((1, 1, -1, 1))\n        return weight\n\n    def scale_expm1(self, x: paddle.Tensor):\n        return self.scale * paddle.expm1(x)\n\n    @paddle.no_grad()\n    def forward(self, output_dict, label_dict) -&gt; Dict[str, \"paddle.Tensor\"]:\n        metric_dict = {}\n        for key in label_dict:\n            output = (\n                self.scale_expm1(output_dict[key]) if self.unlog else output_dict[key]\n            )\n            label = self.scale_expm1(label_dict[key]) if self.unlog else label_dict[key]\n\n            mse = F.mse_loss(output, label, \"none\")\n            rmse = (mse * self.weight).mean(axis=(-1, -2)) ** 0.5\n            if self.std is not None:\n                rmse = rmse * self.std\n            if self.variable_dict is not None:\n                for variable_name, idx in self.variable_dict.items():\n                    metric_dict[f\"{key}.{variable_name}\"] = (\n                        rmse[:, idx] if self.keep_batch else rmse[:, idx].mean()\n                    )\n            else:\n                metric_dict[key] = rmse.mean(axis=1) if self.keep_batch else rmse.mean()\n\n        return metric_dict\n</code></pre>"},{"location":"zh/api/optimizer/","title":"ppsci.optimizer.optimizer","text":""},{"location":"zh/api/optimizer/#optimizeroptimizer","title":"Optimizer.optimizer(\u4f18\u5316\u5668) \u6a21\u5757","text":""},{"location":"zh/api/optimizer/#ppsci.optimizer.optimizer","title":"<code>ppsci.optimizer.optimizer</code>","text":""},{"location":"zh/api/optimizer/#ppsci.optimizer.optimizer.SGD","title":"<code>SGD</code>","text":"<p>Stochastic Gradient Descent.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>Union[float, LRScheduler]</code> <p>The learning rate used to update parameter(s). Defaults to 0.001.</p> <code>0.001</code> <code>weight_decay</code> <code>Optional[Union[float, L1Decay, L2Decay]]</code> <p>Regularization strategy. Defaults to None.</p> <code>None</code> <code>grad_clip</code> <code>Optional[Union[ClipGradByNorm, ClipGradByValue, ClipGradByGlobalNorm]]</code> <p>Gradient clipping strategy. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n&gt;&gt;&gt; opt = ppsci.optimizer.SGD(1e-3)(model)\n</code></pre> Source code in <code>ppsci/optimizer/optimizer.py</code> <pre><code>class SGD:\n    \"\"\"Stochastic Gradient Descent.\n\n    Args:\n        learning_rate (Union[float, optim.lr.LRScheduler], optional): The learning rate\n            used to update parameter(s). Defaults to 0.001.\n        weight_decay (Optional[Union[float, regularizer.L1Decay, regularizer.L2Decay]]):\n            Regularization strategy. Defaults to None.\n        grad_clip (Optional[Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]]):\n            Gradient clipping strategy. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n        &gt;&gt;&gt; opt = ppsci.optimizer.SGD(1e-3)(model)\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: Union[float, optim.lr.LRScheduler] = 0.001,\n        weight_decay: Optional[\n            Union[float, regularizer.L1Decay, regularizer.L2Decay]\n        ] = None,\n        grad_clip: Optional[\n            Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]\n        ] = None,\n    ):\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.grad_clip = grad_clip\n\n    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):\n        # model_list is None in static graph\n        if not isinstance(model_list, (tuple, list)):\n            model_list = (model_list,)\n        parameters = (\n            sum([m.parameters() for m in model_list], []) if model_list else None\n        )\n        opt = optim.SGD(\n            learning_rate=self.learning_rate,\n            parameters=parameters,\n            weight_decay=self.weight_decay,\n            grad_clip=self.grad_clip,\n        )\n        return opt\n</code></pre>"},{"location":"zh/api/optimizer/#ppsci.optimizer.optimizer.Momentum","title":"<code>Momentum</code>","text":"<p>Simple Momentum optimizer with velocity state.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>Union[float, LRScheduler]</code> <p>The learning rate used to update parameter(s).</p> required <code>momentum</code> <code>float</code> <p>Momentum factor.</p> required <code>weight_decay</code> <code>Optional[Union[float, L1Decay, L2Decay]]</code> <p>Regularization strategy. Defaults to None.</p> <code>None</code> <code>grad_clip</code> <code>Optional[Union[ClipGradByNorm, ClipGradByValue, ClipGradByGlobalNorm]]</code> <p>Gradient clipping strategy. Defaults to None.</p> <code>None</code> <code>use_nesterov</code> <code>bool</code> <p>Whether to use nesterov momentum. Defaults to False.</p> <code>False</code> <code>no_weight_decay_name</code> <code>Optional[str]</code> <p>List of names of no weight decay parameters split by white space. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n&gt;&gt;&gt; opt = ppsci.optimizer.Momentum(1e-3, 0.9)(model)\n</code></pre> Source code in <code>ppsci/optimizer/optimizer.py</code> <pre><code>class Momentum:\n    \"\"\"Simple Momentum optimizer with velocity state.\n\n    Args:\n        learning_rate (Union[float, optim.lr.LRScheduler]): The learning rate\n            used to update parameter(s).\n        momentum (float): Momentum factor.\n        weight_decay (Optional[Union[float, regularizer.L1Decay, regularizer.L2Decay]]):\n            Regularization strategy. Defaults to None.\n        grad_clip (Optional[Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]]):\n            Gradient clipping strategy. Defaults to None.\n        use_nesterov (bool, optional): Whether to use nesterov momentum. Defaults to False.\n        no_weight_decay_name (Optional[str]): List of names of no weight decay parameters split by white space. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n        &gt;&gt;&gt; opt = ppsci.optimizer.Momentum(1e-3, 0.9)(model)\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: Union[float, optim.lr.LRScheduler],\n        momentum: float,\n        weight_decay: Optional[\n            Union[float, regularizer.L1Decay, regularizer.L2Decay]\n        ] = None,\n        grad_clip: Optional[\n            Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]\n        ] = None,\n        use_nesterov: bool = False,\n        no_weight_decay_name: Optional[str] = None,\n    ):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n        self.grad_clip = grad_clip\n        self.use_nesterov = use_nesterov\n        self.no_weight_decay_name_list = (\n            no_weight_decay_name.split() if no_weight_decay_name else []\n        )\n\n    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):\n        # model_list is None in static graph\n        if not isinstance(model_list, (tuple, list)):\n            model_list = (model_list,)\n        parameters = None\n        if len(self.no_weight_decay_name_list) &gt; 0:\n            params_with_decay = []\n            params_without_decay = []\n            for m in model_list:\n                params = [\n                    p\n                    for n, p in m.named_parameters()\n                    if not any(nd in n for nd in self.no_weight_decay_name_list)\n                ]\n                params_with_decay.extend(params)\n                params = [\n                    p\n                    for n, p in m.named_parameters()\n                    if any(nd in n for nd in self.no_weight_decay_name_list)\n                ]\n                params_without_decay.extend(params)\n            parameters = [\n                {\"params\": params_with_decay, \"weight_decay\": self.weight_decay},\n                {\"params\": params_without_decay, \"weight_decay\": 0.0},\n            ]\n        else:\n            parameters = (\n                sum([m.parameters() for m in model_list], []) if model_list else None\n            )\n        opt = optim.Momentum(\n            learning_rate=self.learning_rate,\n            momentum=self.momentum,\n            weight_decay=self.weight_decay,\n            grad_clip=self.grad_clip,\n            use_nesterov=self.use_nesterov,\n            parameters=parameters,\n        )\n        if hasattr(opt, \"_use_multi_tensor\"):\n            opt = optim.Momentum(\n                learning_rate=self.learning_rate,\n                momentum=self.momentum,\n                weight_decay=self.weight_decay,\n                grad_clip=self.grad_clip,\n                parameters=parameters,\n                use_nesterov=self.use_nesterov,\n                use_multi_tensor=True,\n            )\n        return opt\n</code></pre>"},{"location":"zh/api/optimizer/#ppsci.optimizer.optimizer.Adam","title":"<code>Adam</code>","text":"<p>Adam: A Method for Stochastic Optimization.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>Union[float, LRScheduler]</code> <p>The learning rate used to update parameter(s). Defaults to 0.001.</p> <code>0.001</code> <code>beta1</code> <code>float</code> <p>The exponential decay rate for the 1st moment estimates. Defaults to 0.9.</p> <code>0.9</code> <code>beta2</code> <code>float</code> <p>The exponential decay rate for the 2nd moment estimates. Defaults to 0.999.</p> <code>0.999</code> <code>epsilon</code> <code>float</code> <p>A small float value for numerical stability. Defaults to 1e-08.</p> <code>1e-08</code> <code>weight_decay</code> <code>Optional[Union[float, L1Decay, L2Decay]]</code> <p>Regularization strategy. Defaults to None.</p> <code>None</code> <code>grad_clip</code> <code>Optional[Union[ClipGradByNorm, ClipGradByValue, ClipGradByGlobalNorm]]</code> <p>Gradient clipping strategy. Defaults to None.</p> <code>None</code> <code>lazy_mode</code> <code>bool</code> <p>Whether to enable lazy mode for moving-average. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n&gt;&gt;&gt; opt = ppsci.optimizer.Adam(1e-3)(model)\n</code></pre> Source code in <code>ppsci/optimizer/optimizer.py</code> <pre><code>class Adam:\n    \"\"\"Adam: A Method for Stochastic Optimization.\n\n    Args:\n        learning_rate (Union[float, optim.lr.LRScheduler], optional): The learning rate\n            used to update parameter(s). Defaults to 0.001.\n        beta1 (float, optional): The exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n        beta2 (float, optional): The exponential decay rate for the 2nd moment estimates. Defaults to 0.999.\n        epsilon (float, optional): A small float value for numerical stability. Defaults to 1e-08.\n        weight_decay (Optional[Union[float, regularizer.L1Decay, regularizer.L2Decay]]): Regularization strategy. Defaults to None.\n        grad_clip (Optional[Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]]): Gradient clipping strategy. Defaults to None.\n        lazy_mode (bool, optional): Whether to enable lazy mode for moving-average. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n        &gt;&gt;&gt; opt = ppsci.optimizer.Adam(1e-3)(model)\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: Union[float, optim.lr.LRScheduler] = 0.001,\n        beta1: float = 0.9,\n        beta2: float = 0.999,\n        epsilon: float = 1e-08,\n        weight_decay: Optional[\n            Union[float, regularizer.L1Decay, regularizer.L2Decay]\n        ] = None,\n        grad_clip: Optional[\n            Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]\n        ] = None,\n        lazy_mode: bool = False,\n    ):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.grad_clip = grad_clip\n        self.lazy_mode = lazy_mode\n\n    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):\n        # model_list is None in static graph\n        if not isinstance(model_list, (tuple, list)):\n            model_list = (model_list,)\n        parameters = (\n            sum([m.parameters() for m in model_list], []) if model_list else None\n        )\n        opt = optim.Adam(\n            learning_rate=self.learning_rate,\n            beta1=self.beta1,\n            beta2=self.beta2,\n            epsilon=self.epsilon,\n            weight_decay=self.weight_decay,\n            grad_clip=self.grad_clip,\n            lazy_mode=self.lazy_mode,\n            parameters=parameters,\n        )\n        return opt\n</code></pre>"},{"location":"zh/api/optimizer/#ppsci.optimizer.optimizer.AdamW","title":"<code>AdamW</code>","text":"<p>AdamW is implemented based on DECOUPLED WEIGHT DECAY REGULARIZATION.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>Union[float, LRScheduler]</code> <p>The learning rate used to update parameter(s). Defaults to 0.001.</p> <code>0.001</code> <code>beta1</code> <code>float</code> <p>The exponential decay rate for the 1st moment estimates. Defaults to 0.9.</p> <code>0.9</code> <code>beta2</code> <code>float</code> <p>The exponential decay rate for the 2nd moment estimates. Defaults to 0.999.</p> <code>0.999</code> <code>epsilon</code> <code>float</code> <p>A small float value for numerical stability. Defaults to 1e-8.</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>Regularization coefficient. Defaults to 0.01.</p> <code>0.001</code> <code>grad_clip</code> <code>Optional[Union[ClipGradByNorm, ClipGradByValue, ClipGradByGlobalNorm]]</code> <p>Gradient clipping strategy. Defaults to None.</p> <code>None</code> <code>no_weight_decay_name</code> <code>Optional[str]</code> <p>List of names of no weight decay parameters split by white space. Defaults to None.</p> <code>None</code> <code>one_dim_param_no_weight_decay</code> <code>bool</code> <p>Apply no weight decay on 1-D parameter(s). Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n&gt;&gt;&gt; opt = ppsci.optimizer.AdamW(1e-3)(model)\n</code></pre> Source code in <code>ppsci/optimizer/optimizer.py</code> <pre><code>class AdamW:\n    \"\"\"AdamW is implemented based on DECOUPLED WEIGHT DECAY REGULARIZATION.\n\n    Args:\n        learning_rate (Union[float, optim.lr.LRScheduler], optional): The learning rate\n            used to update parameter(s). Defaults to 0.001.\n        beta1 (float, optional): The exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n        beta2 (float, optional): The exponential decay rate for the 2nd moment estimates. Defaults to 0.999.\n        epsilon (float, optional): A small float value for numerical stability. Defaults to 1e-8.\n        weight_decay (float, optional): Regularization coefficient. Defaults to 0.01.\n        grad_clip (Optional[Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]]): Gradient clipping strategy. Defaults to None.\n        no_weight_decay_name (Optional[str]): List of names of no weight decay parameters split by white space. Defaults to None.\n        one_dim_param_no_weight_decay (bool, optional): Apply no weight decay on 1-D parameter(s). Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n        &gt;&gt;&gt; opt = ppsci.optimizer.AdamW(1e-3)(model)\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: Union[float, optim.lr.LRScheduler] = 0.001,\n        beta1: float = 0.9,\n        beta2: float = 0.999,\n        epsilon: float = 1e-8,\n        weight_decay: float = 0.001,\n        grad_clip: Optional[\n            Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]\n        ] = None,\n        no_weight_decay_name: Optional[str] = None,\n        one_dim_param_no_weight_decay: bool = False,\n    ):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.grad_clip = grad_clip\n        self.weight_decay = weight_decay\n        self.no_weight_decay_name_list = (\n            no_weight_decay_name.split() if no_weight_decay_name else []\n        )\n        self.one_dim_param_no_weight_decay = one_dim_param_no_weight_decay\n\n    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):\n        # model_list is None in static graph\n        if not isinstance(model_list, (tuple, list)):\n            model_list = (model_list,)\n        parameters = (\n            sum([m.parameters() for m in model_list], []) if model_list else None\n        )\n\n        # TODO(gaotingquan): Model_list is None when in static graph, \"no_weight_decay\" not work.\n        if model_list is None:\n            if (\n                self.one_dim_param_no_weight_decay\n                or len(self.no_weight_decay_name_list) != 0\n            ):\n                msg = '\"AdamW\" does not support setting \"no_weight_decay\" in static graph. Please use dynamic graph.'\n                logger.error(Exception(msg))\n                raise Exception(msg)\n\n        self.no_weight_decay_param_name_list = (\n            [\n                p.name\n                for model in model_list\n                for n, p in model.named_parameters()\n                if any(nd in n for nd in self.no_weight_decay_name_list)\n            ]\n            if model_list\n            else []\n        )\n\n        if self.one_dim_param_no_weight_decay:\n            self.no_weight_decay_param_name_list += (\n                [\n                    p.name\n                    for model in model_list\n                    for n, p in model.named_parameters()\n                    if len(p.shape) == 1\n                ]\n                if model_list\n                else []\n            )\n\n        opt = optim.AdamW(\n            learning_rate=self.learning_rate,\n            beta1=self.beta1,\n            beta2=self.beta2,\n            epsilon=self.epsilon,\n            parameters=parameters,\n            weight_decay=self.weight_decay,\n            grad_clip=self.grad_clip,\n            apply_decay_param_fun=self._apply_decay_param_fun,\n        )\n        return opt\n\n    def _apply_decay_param_fun(self, name):\n        return name not in self.no_weight_decay_param_name_list\n</code></pre>"},{"location":"zh/api/optimizer/#ppsci.optimizer.optimizer.RMSProp","title":"<code>RMSProp</code>","text":"<p>Root Mean Squared Propagation (RMSProp) is an unpublished, adaptive learning rate method.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>Union[float, LRScheduler]</code> <p>The learning rate used to update parameter(s)</p> required <code>rho</code> <code>float</code> <p>Factor \u03c1 in equation. Defaults to 0.95.</p> <code>0.95</code> <code>epsilon</code> <code>float</code> <p>Factor \u03f5 in equation as a smoothing term. Defaults to 1e-6.</p> <code>1e-06</code> <code>momentum</code> <code>float</code> <p>\u03b2 in equation is the momentum term. Defaults to 0.0.</p> <code>0.0</code> <code>weight_decay</code> <code>Optional[Union[float, L1Decay, L2Decay]]</code> <p>Regularization strategy. Defaults to None.</p> <code>None</code> <code>grad_clip</code> <code>Optional[Union[ClipGradByNorm, ClipGradByValue, ClipGradByGlobalNorm]]</code> <p>Gradient clipping strategy. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n&gt;&gt;&gt; opt = ppsci.optimizer.RMSProp(1e-3)(model)\n</code></pre> Source code in <code>ppsci/optimizer/optimizer.py</code> <pre><code>class RMSProp:\n    \"\"\"Root Mean Squared Propagation (RMSProp) is an unpublished, adaptive learning rate method.\n\n    Args:\n        learning_rate (Union[float, optim.lr.LRScheduler]): The learning rate\n            used to update parameter(s)\n        rho (float, optional): Factor \u03c1 in equation. Defaults to 0.95.\n        epsilon (float, optional): Factor \u03f5 in equation as a smoothing term. Defaults to 1e-6.\n        momentum (float, optional):\u03b2 in equation is the momentum term. Defaults to 0.0.\n        weight_decay (Optional[Union[float, regularizer.L1Decay, regularizer.L2Decay]]):\n            Regularization strategy. Defaults to None.\n        grad_clip (Optional[Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]]):\n            Gradient clipping strategy. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n        &gt;&gt;&gt; opt = ppsci.optimizer.RMSProp(1e-3)(model)\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: Union[float, optim.lr.LRScheduler],\n        rho: float = 0.95,\n        epsilon: float = 1e-6,\n        momentum: float = 0.0,\n        weight_decay: Optional[\n            Union[float, regularizer.L1Decay, regularizer.L2Decay]\n        ] = None,\n        grad_clip: Optional[\n            Union[nn.ClipGradByNorm, nn.ClipGradByValue, nn.ClipGradByGlobalNorm]\n        ] = None,\n    ):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.rho = rho\n        self.epsilon = epsilon\n        self.weight_decay = weight_decay\n        self.grad_clip = grad_clip\n\n    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):\n        # model_list is None in static graph\n        if not isinstance(model_list, (tuple, list)):\n            model_list = (model_list,)\n        parameters = (\n            sum([m.parameters() for m in model_list], []) if model_list else None\n        )\n        opt = optim.RMSProp(\n            learning_rate=self.learning_rate,\n            momentum=self.momentum,\n            rho=self.rho,\n            epsilon=self.epsilon,\n            weight_decay=self.weight_decay,\n            grad_clip=self.grad_clip,\n            parameters=parameters,\n        )\n        return opt\n</code></pre>"},{"location":"zh/api/optimizer/#ppsci.optimizer.optimizer.LBFGS","title":"<code>LBFGS</code>","text":"<p>The L-BFGS is a quasi-Newton method for solving an unconstrained optimization     problem over a differentiable function. Closely related is the Newton method for minimization.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>The learning rate used to update parameter(s). Defaults to 1.0.</p> <code>1.0</code> <code>max_iter</code> <code>int</code> <p>Maximal number of iterations per optimization step. Defaults to 1.</p> <code>1</code> <code>max_eval</code> <code>Optional[int]</code> <p>Maximal number of function evaluations per optimization step. Defaults to None.</p> <code>None</code> <code>tolerance_grad</code> <code>float</code> <p>Termination tolerance on first order optimality. Defaults to 1e-07.</p> <code>1e-07</code> <code>tolerance_change</code> <code>float</code> <p>Termination tolerance on function value/parameter changes. Defaults to 1e-09.</p> <code>1e-09</code> <code>history_size</code> <code>int</code> <p>Update history size. Defaults to 100.</p> <code>100</code> <code>line_search_fn</code> <code>Optional[Literal['strong_wolfe']]</code> <p>Either 'strong_wolfe' or None. Defaults to \"strong_wolfe\".</p> <code>'strong_wolfe'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n&gt;&gt;&gt; opt = ppsci.optimizer.LBFGS(1e-3)(model)\n</code></pre> Source code in <code>ppsci/optimizer/optimizer.py</code> <pre><code>class LBFGS:\n    \"\"\"The L-BFGS is a quasi-Newton method for solving an unconstrained optimization\n        problem over a differentiable function. Closely related is the Newton method for minimization.\n\n    Args:\n        learning_rate (float, optional): The learning rate\n            used to update parameter(s). Defaults to 1.0.\n        max_iter (int, optional): Maximal number of iterations per optimization step.\n            Defaults to 1.\n        max_eval (Optional[int]): Maximal number of function evaluations per\n            optimization step. Defaults to None.\n        tolerance_grad (float, optional): Termination tolerance on first order optimality.\n            Defaults to 1e-07.\n        tolerance_change (float, optional): Termination tolerance on function\n            value/parameter changes. Defaults to 1e-09.\n        history_size (int, optional): Update history size. Defaults to 100.\n        line_search_fn (Optional[Literal[\"strong_wolfe\"]]): Either 'strong_wolfe' or None.\n            Defaults to \"strong_wolfe\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n        &gt;&gt;&gt; opt = ppsci.optimizer.LBFGS(1e-3)(model)\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: float = 1.0,\n        max_iter: int = 1,\n        max_eval: Optional[int] = None,\n        tolerance_grad: float = 1e-07,\n        tolerance_change: float = 1e-09,\n        history_size: int = 100,\n        line_search_fn: Optional[Literal[\"strong_wolfe\"]] = \"strong_wolfe\",\n    ):\n        self.lr = learning_rate\n        self.max_iter = max_iter\n        self.max_eval = max_eval\n        self.tolerance_grad = tolerance_grad\n        self.tolerance_change = tolerance_change\n        self.history_size = history_size\n        self.line_search_fn = line_search_fn\n\n    def __call__(self, model_list: Union[nn.Layer, Tuple[nn.Layer, ...]]):\n        # model_list is None in static graph\n        if not isinstance(model_list, (tuple, list)):\n            model_list = (model_list,)\n        parameters = (\n            sum([m.parameters() for m in model_list], []) if model_list else None\n        )\n        try:\n            opt = getattr(optim, \"LBFGS\")(\n                learning_rate=self.lr,\n                max_iter=self.max_iter,\n                max_eval=self.max_eval,\n                tolerance_grad=self.tolerance_grad,\n                tolerance_change=self.tolerance_change,\n                history_size=self.history_size,\n                line_search_fn=self.line_search_fn,\n                parameters=parameters,\n            )\n        except AttributeError:\n            opt = getattr(incubate_optim, \"LBFGS\")(\n                learning_rate=self.lr,\n                max_iter=self.max_iter,\n                max_eval=self.max_eval,\n                tolerance_grad=self.tolerance_grad,\n                tolerance_change=self.tolerance_change,\n                history_size=self.history_size,\n                line_search_fn=self.line_search_fn,\n                parameters=parameters,\n            )\n        return opt\n</code></pre>"},{"location":"zh/api/optimizer/#ppsci.optimizer.optimizer.OptimizerList","title":"<code>OptimizerList</code>","text":"<p>OptimizerList which wrap more than one optimizer. NOTE: LBFGS is not supported yet.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_list</code> <code>Tuple[Optimizer, ...]</code> <p>Optimizers listed in a tuple.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model1 = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n&gt;&gt;&gt; opt1 = ppsci.optimizer.Adam(1e-3)(model1)\n&gt;&gt;&gt; model2 = ppsci.arch.MLP((\"y\",), (\"v\",), 5, 20)\n&gt;&gt;&gt; opt2 = ppsci.optimizer.Adam(1e-3)(model2)\n&gt;&gt;&gt; opt = ppsci.optimizer.OptimizerList((opt1, opt2))\n</code></pre> Source code in <code>ppsci/optimizer/optimizer.py</code> <pre><code>class OptimizerList:\n    \"\"\"OptimizerList which wrap more than one optimizer.\n    NOTE: LBFGS is not supported yet.\n\n    Args:\n        optimizer_list (Tuple[optim.Optimizer, ...]): Optimizers listed in a tuple.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model1 = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n        &gt;&gt;&gt; opt1 = ppsci.optimizer.Adam(1e-3)(model1)\n        &gt;&gt;&gt; model2 = ppsci.arch.MLP((\"y\",), (\"v\",), 5, 20)\n        &gt;&gt;&gt; opt2 = ppsci.optimizer.Adam(1e-3)(model2)\n        &gt;&gt;&gt; opt = ppsci.optimizer.OptimizerList((opt1, opt2))\n    \"\"\"\n\n    def __init__(self, optimizer_list: Tuple[optim.Optimizer, ...]):\n        super().__init__()\n        self._opt_list = optimizer_list\n        if \"LBFGS\" in set(misc.typename(opt) for opt in optimizer_list):\n            raise ValueError(\"LBFGS is not supported in OptimizerList yet.\")\n\n    def step(self):\n        for opt in self._opt_list:\n            opt.step()\n\n    def clear_grad(self):\n        for opt in self._opt_list:\n            opt.clear_grad()\n\n    def get_lr(self) -&gt; float:\n        \"\"\"Return learning rate of first optimizer\"\"\"\n        return self._opt_list[0].get_lr()\n\n    def set_state_dict(self, state_dicts: List[Dict[str, \"paddle.Tensor\"]]):\n        for i, opt in enumerate(self._opt_list):\n            opt.set_state_dict(state_dicts[i])\n\n    def state_dict(self) -&gt; List[Dict[str, \"paddle.Tensor\"]]:\n        state_dicts = [opt.state_dict() for opt in self._opt_list]\n        return state_dicts\n\n    def __len__(self) -&gt; int:\n        return len(self._opt_list)\n\n    def __getitem__(self, idx):\n        return self._opt_list[idx]\n\n    def __setitem__(self, idx, opt):\n        raise NotImplementedError(\"Can not modify any item in OptimizerList.\")\n\n    def __iter__(self):\n        yield from iter(self._opt_list)\n</code></pre>"},{"location":"zh/api/optimizer/#ppsci.optimizer.optimizer.OptimizerList.get_lr","title":"<code>get_lr()</code>","text":"<p>Return learning rate of first optimizer</p> Source code in <code>ppsci/optimizer/optimizer.py</code> <pre><code>def get_lr(self) -&gt; float:\n    \"\"\"Return learning rate of first optimizer\"\"\"\n    return self._opt_list[0].get_lr()\n</code></pre>"},{"location":"zh/api/probability/","title":"ppsci.probability","text":""},{"location":"zh/api/probability/#probability","title":"Probability(\u6982\u7387\u7f16\u7a0b) \u6a21\u5757","text":""},{"location":"zh/api/probability/#ppsci.probability","title":"<code>ppsci.probability</code>","text":""},{"location":"zh/api/probability/#ppsci.probability.HamiltonianMonteCarlo","title":"<code>HamiltonianMonteCarlo</code>","text":"<p>Using the HamiltonianMonteCarlo(HMC) to sample from the desired probability distribution. The HMC combine the Hamiltonian Dynamics and Markov Chain Monte Carlo sampling algorithm which is a more efficient way compared to the Metropolis Hasting (MH) method.</p> <p>Parameters:</p> Name Type Description Default <code>distribution_fn</code> <code>Distribution</code> <p>The Log (Posterior) Distribution function that of the parameters needed to be sampled.</p> required <code>path_len</code> <code>float</code> <p>The total path length.</p> <code>1.0</code> <code>step_size</code> <code>float</code> <p>Every step size.</p> <code>0.25</code> <code>num_warmup_steps</code> <code>int</code> <p>The number of warm-up steps for the MCMC run.</p> <code>0</code> <code>random_seed</code> <code>int</code> <p>Random seed number.</p> <code>1024</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.probability.hmc import HamiltonianMonteCarlo\n&gt;&gt;&gt; def log_posterior(**kwargs):\n...     dist = paddle.distribution.Normal(loc=0, scale=1)\n...     return dist.log_prob(kwargs['x'])\n&gt;&gt;&gt; HMC = HamiltonianMonteCarlo(log_posterior, path_len=1.5, step_size=0.25)\n&gt;&gt;&gt; trial = HMC.run_chain(1000, {'x': paddle.to_tensor(0.0)})\n</code></pre> Source code in <code>ppsci/probability/hmc.py</code> <pre><code>class HamiltonianMonteCarlo:\n    \"\"\"\n    Using the HamiltonianMonteCarlo(HMC) to sample from the desired probability distribution. The HMC combine the Hamiltonian Dynamics and Markov Chain Monte Carlo sampling algorithm which is a more efficient way compared to the Metropolis Hasting (MH) method.\n\n    Args:\n        distribution_fn (paddle.distribution.Distribution): The Log (Posterior) Distribution function that of the parameters needed to be sampled.\n        path_len (float): The total path length.\n        step_size (float): Every step size.\n        num_warmup_steps (int): The number of warm-up steps for the MCMC run.\n        random_seed (int): Random seed number.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.probability.hmc import HamiltonianMonteCarlo\n        &gt;&gt;&gt; def log_posterior(**kwargs):\n        ...     dist = paddle.distribution.Normal(loc=0, scale=1)\n        ...     return dist.log_prob(kwargs['x'])\n        &gt;&gt;&gt; HMC = HamiltonianMonteCarlo(log_posterior, path_len=1.5, step_size=0.25)\n        &gt;&gt;&gt; trial = HMC.run_chain(1000, {'x': paddle.to_tensor(0.0)})\n    \"\"\"\n\n    def __init__(\n        self,\n        distribution_fn: Callable,\n        path_len: float = 1.0,\n        step_size: float = 0.25,\n        num_warmup_steps: int = 0,\n        random_seed: int = 1024,\n    ):\n        self.distribution_fn = distribution_fn\n        self.steps = int(path_len / step_size)\n        self.step_size = step_size\n        self.path_len = path_len\n        self.num_warmup_steps = num_warmup_steps\n        utils.set_random_seed(random_seed)\n        self._rv_unif = distribution.Uniform(0, 1)\n\n    def sample(\n        self, last_position: Dict[str, paddle.Tensor]\n    ) -&gt; Dict[str, paddle.Tensor]:\n        \"\"\"\n        Single step for sample\n        \"\"\"\n        q0 = q1 = last_position\n        p0 = p1 = self._sample_r(q0)\n\n        for s in range(self.steps):\n            grad = self._potential_energy_gradient(q1)\n            for site_name in p1.keys():\n                p1[site_name] -= self.step_size * grad[site_name] / 2\n            for site_name in q1.keys():\n                q1[site_name] += self.step_size * p1[site_name]\n\n            grad = self._potential_energy_gradient(q1)\n            for site_name in p1.keys():\n                p1[site_name] -= self.step_size * grad[site_name] / 2\n\n        # set the next state in the Markov chain\n        return q1 if self._check_acceptance(q0, q1, p0, p1) else q0\n\n    def run_chain(\n        self, epochs: int, initial_position: Dict[str, paddle.Tensor]\n    ) -&gt; Dict[str, paddle.Tensor]:\n        sampling_result: Dict[str, paddle.Tensor] = {}\n        for k in initial_position.keys():\n            sampling_result[k] = []\n        pos = initial_position\n\n        # warmup\n        for _ in range(self.num_warmup_steps):\n            pos = self.sample(pos)\n\n        # begin collecting sampling result\n        for e in range(epochs):\n            pos = self.sample(pos)\n            for k in pos.keys():\n                sampling_result[k].append(pos[k].numpy())\n\n        for k in initial_position.keys():\n            sampling_result[k] = paddle.to_tensor(sampling_result[k])\n\n        return sampling_result\n\n    def _potential_energy_gradient(\n        self, pos: Dict[str, paddle.Tensor]\n    ) -&gt; Dict[str, paddle.Tensor]:\n        \"\"\"\n        Calculate the gradient of potential energy\n        \"\"\"\n        grads = {}\n        with EnableGradient(pos):\n            (-self.distribution_fn(**pos)).backward()\n            for k, v in pos.items():\n                grads[k] = v.grad.detach()\n        return grads\n\n    def _k_energy_fn(self, r: Dict[str, paddle.Tensor]) -&gt; paddle.Tensor:\n        energy = 0.0\n        for v in r.values():\n            energy = energy + v.dot(v)\n        return 0.5 * energy\n\n    def _sample_r(\n        self, params_dict: Dict[str, paddle.Tensor]\n    ) -&gt; Dict[str, paddle.Tensor]:\n        # sample r for params\n        r = {}\n        for k, v in params_dict.items():\n            rv_r = distribution.Normal(paddle.zeros_like(v), paddle.ones_like(v))\n            r[k] = rv_r.sample([1])\n            if not (v.shape == [] or v.shape == 1):\n                r[k] = r[k].squeeze()\n        return r\n\n    def _check_acceptance(\n        self,\n        q0: Dict[str, paddle.Tensor],\n        q1: Dict[str, paddle.Tensor],\n        p0: Dict[str, paddle.Tensor],\n        p1: Dict[str, paddle.Tensor],\n    ) -&gt; bool:\n        # calculate the Metropolis acceptance probability\n        energy_current = -self.distribution_fn(**q0) + self._k_energy_fn(p0)\n        energy_proposed = -self.distribution_fn(**q1) + self._k_energy_fn(p1)\n\n        acceptance = paddle.minimum(\n            paddle.to_tensor(1.0), paddle.exp(energy_current - energy_proposed)\n        )\n\n        # whether accept the proposed state position\n        event = self._rv_unif.sample([])\n        return event &lt;= acceptance\n</code></pre>"},{"location":"zh/api/probability/#ppsci.probability.HamiltonianMonteCarlo.sample","title":"<code>sample(last_position)</code>","text":"<p>Single step for sample</p> Source code in <code>ppsci/probability/hmc.py</code> <pre><code>def sample(\n    self, last_position: Dict[str, paddle.Tensor]\n) -&gt; Dict[str, paddle.Tensor]:\n    \"\"\"\n    Single step for sample\n    \"\"\"\n    q0 = q1 = last_position\n    p0 = p1 = self._sample_r(q0)\n\n    for s in range(self.steps):\n        grad = self._potential_energy_gradient(q1)\n        for site_name in p1.keys():\n            p1[site_name] -= self.step_size * grad[site_name] / 2\n        for site_name in q1.keys():\n            q1[site_name] += self.step_size * p1[site_name]\n\n        grad = self._potential_energy_gradient(q1)\n        for site_name in p1.keys():\n            p1[site_name] -= self.step_size * grad[site_name] / 2\n\n    # set the next state in the Markov chain\n    return q1 if self._check_acceptance(q0, q1, p0, p1) else q0\n</code></pre>"},{"location":"zh/api/solver/","title":"ppsci.solver","text":""},{"location":"zh/api/solver/#solver","title":"Solver(\u6c42\u89e3\u5668) \u6a21\u5757","text":""},{"location":"zh/api/solver/#ppsci.solver","title":"<code>ppsci.solver</code>","text":""},{"location":"zh/api/solver/#ppsci.solver.Solver","title":"<code>Solver</code>","text":"<p>Class for solver.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>Model.</p> required <code>constraint</code> <code>Optional[Dict[str, Constraint]]</code> <p>Constraint(s) applied on model. Defaults to None.</p> <code>None</code> <code>output_dir</code> <code>Optional[str]</code> <p>Output directory. Defaults to \"./output/\".</p> <code>'./output/'</code> <code>optimizer</code> <code>Optional[Optimizer]</code> <p>Optimizer object. Defaults to None.</p> <code>None</code> <code>lr_scheduler</code> <code>Optional[LRScheduler]</code> <p>Learning rate scheduler. Defaults to None.</p> <code>None</code> <code>epochs</code> <code>int</code> <p>Training epoch(s). Defaults to 5.</p> <code>5</code> <code>iters_per_epoch</code> <code>int</code> <p>Number of iterations within an epoch. Defaults to 20.</p> <code>20</code> <code>update_freq</code> <code>int</code> <p>Update frequency of parameters. Defaults to 1.</p> <code>1</code> <code>save_freq</code> <code>int</code> <p>Saving frequency for checkpoint. Defaults to 0.</p> <code>0</code> <code>log_freq</code> <code>int</code> <p>Logging frequency. Defaults to 10.</p> <code>10</code> <code>eval_during_train</code> <code>bool</code> <p>Whether evaluate model during training. Defaults to False.</p> <code>False</code> <code>start_eval_epoch</code> <code>int</code> <p>Epoch number evaluation applied begin after. Defaults to 1.</p> <code>1</code> <code>eval_freq</code> <code>int</code> <p>Evaluation frequency. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int</code> <p>Random seed. Defaults to 42.</p> <code>42</code> <code>use_vdl</code> <code>Optional[bool]</code> <p>Whether use VisualDL to log scalars. Defaults to False.</p> <code>False</code> <code>use_wandb</code> <code>Optional[bool]</code> <p>Whether use wandb to log data. Defaults to False.</p> <code>False</code> <code>use_tbd</code> <code>Optional[bool]</code> <p>Whether use tensorboardX to log data. Defaults to False.</p> <code>False</code> <code>wandb_config</code> <code>Optional[Dict[str, str]]</code> <p>Config dict of WandB. Defaults to None.</p> <code>None</code> <code>device</code> <code>Literal['cpu', 'gpu', 'xpu']</code> <p>Runtime device. Defaults to \"gpu\".</p> <code>'gpu'</code> <code>equation</code> <code>Optional[Dict[str, PDE]]</code> <p>Equation dict. Defaults to None.</p> <code>None</code> <code>geom</code> <code>Optional[Dict[str, Geometry]]</code> <p>Geometry dict. Defaults to None.</p> <code>None</code> <code>validator</code> <code>Optional[Dict[str, Validator]]</code> <p>Validator dict. Defaults to None.</p> <code>None</code> <code>visualizer</code> <code>Optional[Dict[str, Visualizer]]</code> <p>Visualizer dict. Defaults to None.</p> <code>None</code> <code>use_amp</code> <code>bool</code> <p>Whether use AMP. Defaults to False.</p> <code>False</code> <code>amp_level</code> <code>Literal['O0', 'O1', 'O2', 'OD']</code> <p>AMP level. Defaults to \"O1\".</p> <code>'O1'</code> <code>pretrained_model_path</code> <code>Optional[str]</code> <p>Pretrained model path. Defaults to None.</p> <code>None</code> <code>checkpoint_path</code> <code>Optional[str]</code> <p>Checkpoint path. Defaults to None.</p> <code>None</code> <code>compute_metric_by_batch</code> <code>bool</code> <p>Whether calculate metrics after each batch during evaluation. Defaults to False.</p> <code>False</code> <code>eval_with_no_grad</code> <code>bool</code> <p>Whether set <code>stop_gradient=True</code> for every Tensor if no differentiation involved during computation, generally for save GPU memory and accelerate computing. Defaults to False.</p> <code>False</code> <code>to_static</code> <code>bool</code> <p>Whether enable to_static for forward pass. Defaults to False.</p> <code>False</code> <code>loss_aggregator</code> <code>Optional[LossAggregator]</code> <p>Loss aggregator, such as a multi-task learning loss aggregator. Defaults to None.</p> <code>None</code> <code>cfg</code> <code>Optional[DictConfig]</code> <p>(Optional[DictConfig]): Running config dict. Defaults to None. NOTE: This will be required in the future.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n&gt;&gt;&gt; opt = ppsci.optimizer.AdamW(1e-3)(model)\n&gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; pde_constraint = ppsci.constraint.InteriorConstraint(\n...     {\"u\": lambda out: out[\"u\"]},\n...     {\"u\": 0},\n...     geom,\n...     {\n...         \"dataset\": \"IterableNamedArrayDataset\",\n...         \"iters_per_epoch\": 1,\n...         \"batch_size\": 16,\n...     },\n...     ppsci.loss.MSELoss(\"mean\"),\n...     name=\"EQ\",\n... )\n&gt;&gt;&gt; solver = ppsci.solver.Solver(\n...     model,\n...     {\"EQ\": pde_constraint},\n...     \"./output\",\n...     opt,\n...     None,\n... )\n</code></pre> Source code in <code>ppsci/solver/solver.py</code> <pre><code>class Solver:\n    \"\"\"Class for solver.\n\n    Args:\n        model (nn.Layer): Model.\n        constraint (Optional[Dict[str, ppsci.constraint.Constraint]]): Constraint(s) applied on model. Defaults to None.\n        output_dir (Optional[str]): Output directory. Defaults to \"./output/\".\n        optimizer (Optional[optimizer.Optimizer]): Optimizer object. Defaults to None.\n        lr_scheduler (Optional[optimizer.lr.LRScheduler]): Learning rate scheduler. Defaults to None.\n        epochs (int, optional): Training epoch(s). Defaults to 5.\n        iters_per_epoch (int, optional): Number of iterations within an epoch. Defaults to 20.\n        update_freq (int, optional): Update frequency of parameters. Defaults to 1.\n        save_freq (int, optional): Saving frequency for checkpoint. Defaults to 0.\n        log_freq (int, optional): Logging frequency. Defaults to 10.\n        eval_during_train (bool, optional): Whether evaluate model during training. Defaults to False.\n        start_eval_epoch (int, optional): Epoch number evaluation applied begin after. Defaults to 1.\n        eval_freq (int, optional): Evaluation frequency. Defaults to 1.\n        seed (int, optional): Random seed. Defaults to 42.\n        use_vdl (Optional[bool]): Whether use VisualDL to log scalars. Defaults to False.\n        use_wandb (Optional[bool]): Whether use wandb to log data. Defaults to False.\n        use_tbd (Optional[bool]): Whether use tensorboardX to log data. Defaults to False.\n        wandb_config (Optional[Dict[str, str]]): Config dict of WandB. Defaults to None.\n        device (Literal[\"cpu\", \"gpu\", \"xpu\"], optional): Runtime device. Defaults to \"gpu\".\n        equation (Optional[Dict[str, ppsci.equation.PDE]]): Equation dict. Defaults to None.\n        geom (Optional[Dict[str, ppsci.geometry.Geometry]]): Geometry dict. Defaults to None.\n        validator (Optional[Dict[str, ppsci.validate.Validator]]): Validator dict. Defaults to None.\n        visualizer (Optional[Dict[str, ppsci.visualize.Visualizer]]): Visualizer dict. Defaults to None.\n        use_amp (bool, optional): Whether use AMP. Defaults to False.\n        amp_level (Literal[\"O0\", \"O1\", \"O2\", \"OD\"], optional): AMP level. Defaults to \"O1\".\n        pretrained_model_path (Optional[str]): Pretrained model path. Defaults to None.\n        checkpoint_path (Optional[str]): Checkpoint path. Defaults to None.\n        compute_metric_by_batch (bool, optional): Whether calculate metrics after each batch during evaluation. Defaults to False.\n        eval_with_no_grad (bool, optional): Whether set `stop_gradient=True` for every Tensor if no differentiation\n            involved during computation, generally for save GPU memory and accelerate computing. Defaults to False.\n        to_static (bool, optional): Whether enable to_static for forward pass. Defaults to False.\n        loss_aggregator (Optional[mtl.LossAggregator]): Loss aggregator, such as a multi-task learning loss aggregator. Defaults to None.\n        cfg: (Optional[DictConfig]): Running config dict. Defaults to None. NOTE: This will be required in the future.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\",), (\"u\",), 5, 20)\n        &gt;&gt;&gt; opt = ppsci.optimizer.AdamW(1e-3)(model)\n        &gt;&gt;&gt; geom = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; pde_constraint = ppsci.constraint.InteriorConstraint(\n        ...     {\"u\": lambda out: out[\"u\"]},\n        ...     {\"u\": 0},\n        ...     geom,\n        ...     {\n        ...         \"dataset\": \"IterableNamedArrayDataset\",\n        ...         \"iters_per_epoch\": 1,\n        ...         \"batch_size\": 16,\n        ...     },\n        ...     ppsci.loss.MSELoss(\"mean\"),\n        ...     name=\"EQ\",\n        ... )  # doctest: +SKIP\n        &gt;&gt;&gt; solver = ppsci.solver.Solver(\n        ...     model,\n        ...     {\"EQ\": pde_constraint},\n        ...     \"./output\",\n        ...     opt,\n        ...     None,\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Layer,\n        constraint: Optional[Dict[str, ppsci.constraint.Constraint]] = None,\n        output_dir: Optional[str] = \"./output/\",\n        optimizer: Optional[optim.Optimizer] = None,\n        lr_scheduler: Optional[optim.lr.LRScheduler] = None,\n        epochs: int = 5,\n        iters_per_epoch: int = 20,\n        update_freq: int = 1,\n        save_freq: int = 0,\n        log_freq: int = 10,\n        eval_during_train: bool = False,\n        start_eval_epoch: int = 1,\n        eval_freq: int = 1,\n        seed: int = 42,\n        use_vdl: bool = False,\n        use_wandb: bool = False,\n        use_tbd: bool = False,\n        wandb_config: Optional[Mapping] = None,\n        device: Literal[\"cpu\", \"gpu\", \"xpu\"] = \"gpu\",\n        equation: Optional[Dict[str, ppsci.equation.PDE]] = None,\n        geom: Optional[Dict[str, ppsci.geometry.Geometry]] = None,\n        validator: Optional[Dict[str, ppsci.validate.Validator]] = None,\n        visualizer: Optional[Dict[str, ppsci.visualize.Visualizer]] = None,\n        use_amp: bool = False,\n        amp_level: Literal[\"O0\", \"O1\", \"O2\", \"OD\"] = \"O1\",\n        pretrained_model_path: Optional[str] = None,\n        checkpoint_path: Optional[str] = None,\n        compute_metric_by_batch: bool = False,\n        eval_with_no_grad: bool = False,\n        to_static: bool = False,\n        loss_aggregator: Optional[mtl.LossAggregator] = None,\n        *,\n        cfg: Optional[DictConfig] = None,\n    ):\n        self.cfg = cfg\n        if isinstance(cfg, DictConfig):\n            # (Recommended)Params can be passed within cfg\n            # rather than passed to 'Solver.__init__' one-by-one.\n            self._parse_params_from_cfg(cfg)\n\n        # set model\n        self.model = model\n        # set constraint\n        self.constraint = constraint\n        # set output directory\n        if not cfg:\n            self.output_dir = output_dir\n\n        # set optimizer\n        self.optimizer = optimizer\n        # set learning rate scheduler\n        if lr_scheduler is not None:\n            logger.warning(\n                \"The argument: 'lr_scheduler' now automatically retrieves from \"\n                \"'optimizer._learning_rate' when 'optimizer' is given, so it is \"\n                \"recommended to remove it from the Solver's initialization arguments.\"\n            )\n        self.lr_scheduler = (\n            optimizer._learning_rate\n            if (\n                isinstance(optimizer, optim.Optimizer)\n                and isinstance(optimizer._learning_rate, optim.lr.LRScheduler)\n            )\n            else None\n        )\n        if isinstance(self.optimizer, ppsci.optimizer.OptimizerList):\n            self.lr_scheduler = ppsci.optimizer.lr_scheduler.SchedulerList(\n                tuple(\n                    opt._learning_rate\n                    for opt in self.optimizer\n                    if isinstance(opt._learning_rate, optim.lr.LRScheduler)\n                )\n            )\n\n        # set training hyper-parameter\n        if not cfg:\n            self.epochs = epochs\n            self.iters_per_epoch = iters_per_epoch\n            # set update_freq for gradient accumulation\n            self.update_freq = update_freq\n            # set checkpoint saving frequency\n            self.save_freq = save_freq\n            # set logging frequency\n            self.log_freq = log_freq\n\n            # set evaluation hyper-parameter\n            self.eval_during_train = eval_during_train\n            self.start_eval_epoch = start_eval_epoch\n            self.eval_freq = eval_freq\n\n        # initialize training log(training loss, time cost, etc.) recorder during one epoch\n        self.train_output_info: Dict[str, misc.AverageMeter] = {}\n        self.train_time_info = {\n            \"batch_cost\": misc.AverageMeter(\"batch_cost\", \".5f\", postfix=\"s\"),\n            \"reader_cost\": misc.AverageMeter(\"reader_cost\", \".5f\", postfix=\"s\"),\n        }\n        self.train_loss_info: Dict[str, misc.AverageMeter] = {}\n\n        # initialize evaluation log(evaluation loss, metric, etc.) recorder.\n        self.eval_output_info: Dict[str, misc.AverageMeter] = {}\n        self.eval_time_info = {\n            \"batch_cost\": misc.AverageMeter(\"batch_cost\", \".5f\", postfix=\"s\"),\n            \"reader_cost\": misc.AverageMeter(\"reader_cost\", \".5f\", postfix=\"s\"),\n        }\n\n        # set running device\n        if not cfg:\n            self.device = device\n        if self.device != \"cpu\" and paddle.device.get_device() == \"cpu\":\n            logger.warning(f\"Set device({device}) to 'cpu' for only cpu available.\")\n            self.device = \"cpu\"\n        self.device = paddle.set_device(self.device)\n\n        # set equations for physics-driven or data-physics hybrid driven task, such as PINN\n        self.equation = equation\n\n        # set validator\n        self.validator = validator\n\n        # set visualizer\n        self.visualizer = visualizer\n\n        # set automatic mixed precision(AMP) configuration\n        if not cfg:\n            self.use_amp = use_amp\n            self.amp_level = amp_level\n        self.scaler = amp.GradScaler(True) if self.use_amp else None\n\n        # whether calculate metrics by each batch during evaluation, mainly for memory efficiency\n        if not cfg:\n            self.compute_metric_by_batch = compute_metric_by_batch\n        if validator is not None:\n            for metric in itertools.chain(\n                *[_v.metric.values() for _v in self.validator.values()]\n            ):\n                if metric.keep_batch ^ self.compute_metric_by_batch:\n                    raise ValueError(\n                        f\"{misc.typename(metric)}.keep_batch should be \"\n                        f\"{self.compute_metric_by_batch} when compute_metric_by_batch=\"\n                        f\"{self.compute_metric_by_batch}.\"\n                    )\n        # whether set `stop_gradient=True` for every Tensor if no differentiation involved during evaluation\n        if not cfg:\n            self.eval_with_no_grad = eval_with_no_grad\n\n        self.rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        # initialize distributed environment\n        if self.world_size &gt; 1:\n            # TODO(sensen): Support different kind of DistributedStrategy\n            fleet.init(is_collective=True)\n            logger.warning(\n                f\"Detected 'world_size'({self.world_size}) &gt; 1, it is recommended to \"\n                \"scale up the learning rate and reduce the 'epochs' or \"\n                \"'iters_per_epoch' according to the 'world_size' both linearly if you \"\n                \"are training model.\"\n            )\n\n        # set moving average model(optional)\n        self.ema_model = None\n        if self.cfg and any(key in self.cfg.TRAIN for key in [\"ema\", \"swa\"]):\n            if \"ema\" in self.cfg.TRAIN and cfg.TRAIN.ema.get(\"use_ema\", False):\n                self.ema_model = ema.ExponentialMovingAverage(\n                    self.model, self.cfg.TRAIN.ema.decay\n                )\n            elif \"swa\" in self.cfg.TRAIN and cfg.TRAIN.swa.get(\"use_swa\", False):\n                self.ema_model = ema.StochasticWeightAverage(self.model)\n\n        # load pretrained model, usually used for transfer learning\n        if not cfg:\n            self.pretrained_model_path = pretrained_model_path\n        if self.pretrained_model_path is not None:\n            save_load.load_pretrain(\n                self.model, self.pretrained_model_path, self.equation\n            )\n\n        # initialize an dict for tracking best metric during training\n        self.best_metric = {\n            \"metric\": float(\"inf\"),\n            \"epoch\": 0,\n        }\n        # load model checkpoint, usually used for resume training\n        if not cfg:\n            self.checkpoint_path = checkpoint_path\n        if self.checkpoint_path is not None:\n            if self.pretrained_model_path is not None:\n                logger.warning(\n                    \"Detected 'pretrained_model_path' is given, weights in which might be\"\n                    \"overridden by weights loaded from given 'checkpoint_path'.\"\n                )\n            loaded_metric = save_load.load_checkpoint(\n                self.checkpoint_path,\n                self.model,\n                self.optimizer,\n                self.scaler,\n                self.equation,\n                self.ema_model,\n            )\n            if isinstance(loaded_metric, dict):\n                self.best_metric.update(loaded_metric)\n\n        # decorate model(s) and optimizer(s) for AMP\n        if self.use_amp:\n            self.model, self.optimizer = amp.decorate(\n                self.model,\n                self.optimizer,\n                self.amp_level,\n                save_dtype=\"float32\",\n            )\n\n        # choosing an appropriate training function for different optimizers\n        if misc.typename(self.optimizer) == \"LBFGS\":\n            if self.use_amp:\n                raise ValueError(\n                    \"Auto Mix Precision is not supported for L-BFGS optimizer.\"\n                )\n            self.train_epoch_func = ppsci.solver.train.train_LBFGS_epoch_func\n            if self.update_freq != 1:\n                self.update_freq = 1\n                logger.warning(\"Set 'update_freq' to to 1 when using L-BFGS optimizer.\")\n        else:\n            self.train_epoch_func = ppsci.solver.train.train_epoch_func\n\n        # wrap model and optimizer to parallel object\n        if self.world_size &gt; 1:\n            if isinstance(self.model, paddle.DataParallel):\n                raise ValueError(\n                    \"Given model is already wrapped by paddle.DataParallel.\"\n                    \"Please do not wrap your model with DataParallel \"\n                    \"before 'Solver.__init__' and keep it's type as 'nn.Layer'.\"\n                )\n\n            def dist_wrapper(model: nn.Layer) -&gt; paddle.DataParallel:\n                dist_model = fleet.distributed_model(model)\n                if hasattr(model, \"input_keys\"):\n                    dist_model.input_keys = dist_model._layers.input_keys\n                if hasattr(model, \"output_keys\"):\n                    dist_model.output_keys = dist_model._layers.output_keys\n                return dist_model\n\n            if isinstance(self.model, ppsci.arch.ModelList):\n                for i in range(len(self.model.model_list)):\n                    # NOTE: Convert each model in model_list to DataParallel\n                    self.model.model_list[i] = dist_wrapper(self.model.model_list[i])\n            else:\n                self.model = dist_wrapper(self.model)\n\n            if self.optimizer is not None:\n                self.optimizer = fleet.distributed_optimizer(self.optimizer)\n\n        # set VisualDL tool\n        self.vdl_writer = None\n        if not cfg:\n            self.use_vdl = use_vdl\n        if self.use_vdl:\n            try:\n                import visualdl as vdl\n            except ModuleNotFoundError:\n                raise ModuleNotFoundError(\n                    \"Please install 'visualdl' with `pip install visualdl` first.\"\n                )\n            with misc.RankZeroOnly(self.rank) as is_master:\n                if is_master:\n                    self.vdl_writer = vdl.LogWriter(osp.join(self.output_dir, \"vdl\"))\n            logger.info(\n                \"VisualDL is enabled for logging, you can view it by \"\n                f\"running:\\nvisualdl --logdir {self.vdl_writer._logdir} --port 8080\"\n            )\n\n        # set WandB tool\n        self.wandb_writer = None\n        if not cfg:\n            self.use_wandb = use_wandb\n        if self.use_wandb:\n            try:\n                import wandb\n            except ModuleNotFoundError:\n                raise ModuleNotFoundError(\n                    \"Please install 'wandb' with `pip install wandb` first.\"\n                )\n            with misc.RankZeroOnly(self.rank) as is_master:\n                if is_master:\n                    self.wandb_writer = wandb.init(**wandb_config)\n\n        # set TensorBoardX tool\n        self.tbd_writer = None\n        if not cfg:\n            self.use_tbd = use_tbd\n        if self.use_tbd:\n            try:\n                import tensorboardX\n            except ModuleNotFoundError:\n                raise ModuleNotFoundError(\n                    \"Please install 'tensorboardX' with `pip install tensorboardX` first.\"\n                )\n            with misc.RankZeroOnly(self.rank) as is_master:\n                if is_master:\n                    self.tbd_writer = tensorboardX.SummaryWriter(\n                        osp.join(self.output_dir, \"tensorboard\")\n                    )\n            logger.message(\n                \"TensorboardX is enabled for logging, you can view it by \"\n                f\"running:\\ntensorboard --logdir {self.tbd_writer.logdir}\"\n            )\n\n        self.global_step = 0\n\n        # log paddlepaddle's version\n        if version.Version(paddle.__version__) != version.Version(\"0.0.0\"):\n            paddle_version = paddle.__version__\n            if version.Version(paddle.__version__) &lt; version.Version(\"2.6.0\"):\n                logger.warning(\n                    f\"Detected paddlepaddle version is '{paddle_version}', \"\n                    \"currently it is recommended to use release 2.6 or develop version.\"\n                )\n        else:\n            paddle_version = f\"develop({paddle.version.commit[:7]})\"\n\n        logger.info(f\"Using paddlepaddle {paddle_version} on device {self.device}\")\n\n        self.forward_helper = expression.ExpressionSolver()\n\n        # whether enable static for forward pass. Defaults to False\n        jit.enable_to_static(to_static)\n        logger.message(f\"Set to_static={to_static} for computational optimization.\")\n\n        # use loss aggregator, use Sum if None\n        if isinstance(loss_aggregator, (mtl.AGDA, mtl.PCGrad)) and self.use_amp:\n            raise ValueError(\n                \"Auto Mix Precision do not support AGDA, PCGrad loss aggregator yet, \"\n                \"please set use_amp=False.\"\n            )\n        self.loss_aggregator = loss_aggregator or mtl.Sum()\n\n        # convert sympy to callable object if exist\n        extra_parameters = []\n        if self.equation:\n            for equation in self.equation.values():\n                extra_parameters += list(equation.learnable_parameters)\n\n        def convert_expr(\n            container_dict: Union[\n                Dict[str, ppsci.constraint.Constraint],\n                Dict[str, ppsci.validate.Validator],\n                Dict[str, ppsci.visualize.Visualizer],\n            ]\n        ) -&gt; None:\n            for container in container_dict.values():\n                exprs = [\n                    expr\n                    for expr in container.output_expr.values()\n                    if isinstance(expr, sp.Basic)\n                ]\n                if len(exprs) &gt; 0:\n                    funcs = ppsci.lambdify(\n                        exprs,\n                        self.model,\n                        extra_parameters=extra_parameters,\n                        # graph_filename=osp.join(self.output_dir, \"symbolic_graph_visual\"),  # HACK: Activate it for DEBUG.\n                        fuse_derivative=True,\n                    )\n                    ind = 0\n                    for name in container.output_expr:\n                        if isinstance(container.output_expr[name], sp.Basic):\n                            container.output_expr[name] = funcs[ind]\n                            if self.world_size &gt; 1:\n                                container.output_expr[name] = dist_wrapper(\n                                    container.output_expr[name]\n                                )\n                            ind += 1\n\n        if self.constraint:\n            convert_expr(self.constraint)\n\n        if self.validator:\n            convert_expr(self.validator)\n\n        if self.visualizer:\n            convert_expr(self.visualizer)\n\n        # set up benchmark flag, will print memory stat if enabled\n        self.benchmark_flag: bool = os.getenv(\"BENCHMARK_ROOT\", None) is not None\n\n        # set up nvtx flag for nsight analysis\n        self.nvtx_flag: bool = os.getenv(\"NVTX\", None) is not None\n        self.forward_helper.nvtx_flag = self.nvtx_flag\n\n    def train(self) -&gt; None:\n        \"\"\"Training.\"\"\"\n        self.global_step = self.best_metric[\"epoch\"] * self.iters_per_epoch\n        self.max_steps = self.epochs * self.iters_per_epoch\n\n        start_epoch = self.best_metric[\"epoch\"] + 1\n\n        if self.use_tbd and isinstance(self.cfg, DictConfig):\n            self.tbd_writer.add_text(\n                \"config\", f\"&lt;pre&gt;{str(OmegaConf.to_yaml(self.cfg))}&lt;/pre&gt;\"\n            )\n\n        if self.nvtx_flag:\n            core.nvprof_start()\n            core.nvprof_enable_record_event()\n\n        for epoch_id in range(start_epoch, self.epochs + 1):\n            self.train_epoch_func(self, epoch_id, self.log_freq)\n            self.train_output_info.clear()\n\n            # update average model if exist\n            if self.ema_model and epoch_id % self.avg_freq == 0:\n                self.ema_model.update()\n\n            cur_metric = float(\"inf\")\n            # evaluate during training\n            if (\n                self.eval_during_train\n                and epoch_id % self.eval_freq == 0\n                and epoch_id &gt;= self.start_eval_epoch\n            ):\n                cur_metric, metric_dict_group = self.eval(epoch_id)\n                if cur_metric &lt; self.best_metric[\"metric\"]:\n                    self.best_metric[\"metric\"] = cur_metric\n                    self.best_metric[\"epoch\"] = epoch_id\n                    save_load.save_checkpoint(\n                        self.model,\n                        self.optimizer,\n                        self.best_metric,\n                        self.scaler,\n                        self.output_dir,\n                        \"best_model\",\n                        self.equation,\n                    )\n                logger.info(\n                    f\"[Eval][Epoch {epoch_id}]\"\n                    f\"[best metric: {self.best_metric['metric']}]\"\n                )\n                for metric_name, metric_dict in metric_dict_group.items():\n                    logger.scalar(\n                        {f\"eval/{metric_name}/{k}\": v for k, v in metric_dict.items()},\n                        epoch_id,\n                        self.vdl_writer,\n                        self.wandb_writer,\n                        self.tbd_writer,\n                    )\n\n                # visualize after evaluation\n                if self.visualizer is not None:\n                    self.visualize(epoch_id)\n\n                # evaluation for moving average evaluation(almost same procedure)\n                if self.ema_model and epoch_id % self.avg_freq == 0:\n                    self.ema_model.apply_shadow()\n                    logger.info(\"Evaluating metric of averaging model...\")\n                    cur_metric_ema, metric_dict_group_ema = self.eval(epoch_id)\n                    self.ema_model.restore()\n\n                    if cur_metric_ema &lt; self.best_metric[\"metric\"]:\n                        self.best_metric[\"metric\"] = cur_metric_ema\n                        self.best_metric[\"epoch\"] = epoch_id\n                        save_load.save_checkpoint(\n                            self.ema_model,\n                            None,\n                            metric=self.best_metric,\n                            output_dir=self.output_dir,\n                            prefix=\"best_model_ema\",\n                        )\n                    logger.info(\n                        f\"[Eval][Epoch {epoch_id}]\"\n                        f\"[best metric: {self.best_metric['metric']}]\"\n                    )\n                    for metric_name, metric_dict in metric_dict_group_ema.items():\n                        logger.scalar(\n                            {\n                                f\"eval_ema/{metric_name}/{k}\": v\n                                for k, v in metric_dict.items()\n                            },\n                            epoch_id,\n                            self.vdl_writer,\n                            self.wandb_writer,\n                            self.tbd_writer,\n                        )\n\n            # update learning rate by epoch\n            if self.lr_scheduler is not None and self.lr_scheduler.by_epoch:\n                self.lr_scheduler.step()\n\n            # save epoch model every save_freq epochs\n            if self.save_freq &gt; 0 and epoch_id % self.save_freq == 0:\n                save_load.save_checkpoint(\n                    self.model,\n                    self.optimizer,\n                    {\"metric\": cur_metric, \"epoch\": epoch_id},\n                    self.scaler,\n                    self.output_dir,\n                    f\"epoch_{epoch_id}\",\n                    self.equation,\n                    ema_model=self.ema_model,\n                )\n\n            # save the latest model for convenient resume training\n            save_load.save_checkpoint(\n                self.model,\n                self.optimizer,\n                {\"metric\": cur_metric, \"epoch\": epoch_id},\n                self.scaler,\n                self.output_dir,\n                \"latest\",\n                self.equation,\n                print_log=(epoch_id == start_epoch),\n                ema_model=self.ema_model,\n            )\n\n    def finetune(self, pretrained_model_path: str) -&gt; None:\n        \"\"\"Finetune model based on given pretrained model path.\n\n        Args:\n            pretrained_model_path (str): Pretrained model path or url.\n        \"\"\"\n        # load pretrained model\n        save_load.load_pretrain(self.model, pretrained_model_path, self.equation)\n\n        # call train program\n        self.train()\n\n    @misc.run_on_eval_mode\n    def eval(\n        self, epoch_id: Optional[int] = None\n    ) -&gt; Tuple[float, Dict[str, Dict[str, float]]]:\n        \"\"\"Evaluation.\n\n        Args:\n            epoch_id (Optional[int]): Epoch id. Defaults to None.\n\n        Returns:\n            Tuple[float, Dict[str, Dict[str, float]]]: A targe metric value(float) and\n                all metric(s)(dict) of evaluation, used to judge the quality of the model.\n        \"\"\"\n        # set eval func\n        self.eval_func = ppsci.solver.eval.eval_func\n\n        result = self.eval_func(self, epoch_id, self.log_freq)\n        metric_msg = \", \".join(\n            [self.eval_output_info[key].avg_info for key in self.eval_output_info]\n        )\n\n        if isinstance(epoch_id, int):\n            logger.info(f\"[Eval][Epoch {epoch_id}][Avg] {metric_msg}\")\n        else:\n            logger.info(f\"[Eval][Avg] {metric_msg}\")\n        self.eval_output_info.clear()\n\n        return result\n\n    @misc.run_on_eval_mode\n    def visualize(self, epoch_id: Optional[int] = None):\n        \"\"\"Visualization.\n\n        Args:\n            epoch_id (Optional[int]): Epoch id. Defaults to None.\n        \"\"\"\n        # set visualize func\n        self.visu_func = ppsci.solver.visu.visualize_func\n\n        self.visu_func(self, epoch_id)\n        if isinstance(epoch_id, int):\n            logger.info(f\"[Visualize][Epoch {epoch_id}] Finish visualization\")\n        else:\n            logger.info(\"[Visualize] Finish visualization\")\n\n    @misc.run_on_eval_mode\n    def predict(\n        self,\n        input_dict: Dict[str, Union[np.ndarray, paddle.Tensor]],\n        expr_dict: Optional[Dict[str, Callable]] = None,\n        batch_size: int = 64,\n        no_grad: bool = True,\n        return_numpy: bool = False,\n    ) -&gt; Dict[str, Union[paddle.Tensor, np.ndarray]]:\n        \"\"\"Pure prediction using model.forward(...) and expression(optional, if given).\n\n        Args:\n            input_dict (Dict[str, Union[np.ndarray, paddle.Tensor]]): Input data in dict.\n            expr_dict (Optional[Dict[str, Callable]]): Expression dict, which guide to\n                compute equation variable with callable function. Defaults to None.\n            batch_size (int, optional): Predicting by batch size. Defaults to 64.\n            no_grad (bool): Whether set stop_gradient=True for entire prediction, mainly\n                for memory-efficiency. Defaults to True.\n            return_numpy (bool): Whether convert result from Tensor to numpy ndarray.\n                Defaults to False.\n\n        Returns:\n            Dict[str, Union[paddle.Tensor, np.ndarray]]: Prediction in dict.\n\n        Examples:\n            &gt;&gt;&gt; import paddle\n            &gt;&gt;&gt; import ppsci\n            &gt;&gt;&gt; model = ppsci.arch.MLP(('x', 'y'), ('u', 'v'), num_layers=None, hidden_size=[32, 8])\n            &gt;&gt;&gt; solver = ppsci.solver.Solver(model) # doctest: +SKIP\n            &gt;&gt;&gt; input_dict = {'x': paddle.rand([32, 1]),\n            ...               'y': paddle.rand([32, 1])}\n            &gt;&gt;&gt; pred = solver.predict(input_dict) # doctest: +SKIP\n            &gt;&gt;&gt; for k, v in pred.items(): # doctest: +SKIP\n            ...     print(k, v.shape) # doctest: +SKIP\n            u [32, 1]\n            v [32, 1]\n        \"\"\"\n        num_samples = len(next(iter(input_dict.values())))\n        num_pad = (self.world_size - num_samples % self.world_size) % self.world_size\n        # pad with last element if `num_samples` is not divisible by `world_size`\n        # ensuring every device get same number of data.\n        if num_pad &gt; 0:\n            for k, v in input_dict.items():\n                repeat_times = (num_pad, *(1 for _ in range(v.ndim - 1)))\n                if isinstance(v, np.ndarray):\n                    input_dict[k] = np.concatenate(\n                        (\n                            v,\n                            np.tile(v[num_samples - 1 : num_samples], repeat_times),\n                        ),\n                    )\n                elif isinstance(v, paddle.Tensor):\n                    input_dict[k] = paddle.concat(\n                        (\n                            v,\n                            paddle.tile(v[num_samples - 1 : num_samples], repeat_times),\n                        ),\n                    )\n                else:\n                    raise ValueError(f\"Unsupported data type {type(v)}.\")\n\n        num_samples_pad = num_samples + num_pad\n        local_num_samples_pad = num_samples_pad // self.world_size\n        local_input_dict = (\n            {k: v[self.rank :: self.world_size] for k, v in input_dict.items()}\n            if self.world_size &gt; 1\n            else input_dict\n        )\n        local_batch_num = (local_num_samples_pad + (batch_size - 1)) // batch_size\n\n        pred_dict = misc.Prettydefaultdict(list)\n        with self.no_grad_context_manager(no_grad), self.no_sync_context_manager(\n            self.world_size &gt; 1, self.model\n        ):\n            for batch_id in range(local_batch_num):\n                batch_input_dict = {}\n                st = batch_id * batch_size\n                ed = min(local_num_samples_pad, (batch_id + 1) * batch_size)\n\n                # prepare batch input dict\n                for key in local_input_dict:\n                    if not paddle.is_tensor(local_input_dict[key]):\n                        batch_input_dict[key] = paddle.to_tensor(\n                            local_input_dict[key][st:ed], paddle.get_default_dtype()\n                        )\n                    else:\n                        batch_input_dict[key] = local_input_dict[key][st:ed]\n                    batch_input_dict[key].stop_gradient = no_grad\n\n                # forward\n                with self.autocast_context_manager(self.use_amp, self.amp_level):\n                    batch_output_dict = self.forward_helper.visu_forward(\n                        expr_dict, batch_input_dict, self.model\n                    )\n\n                # collect batch data\n                for key, batch_output in batch_output_dict.items():\n                    pred_dict[key].append(\n                        batch_output.detach() if no_grad else batch_output\n                    )\n\n            # concatenate local predictions\n            pred_dict = {key: paddle.concat(value) for key, value in pred_dict.items()}\n\n            if self.world_size &gt; 1:\n                # gather global predictions from all devices if world_size &gt; 1\n                pred_dict = {\n                    key: misc.all_gather(value) for key, value in pred_dict.items()\n                }\n                # rearrange predictions as the same order of input_dict according\n                # to inverse permutation\n                perm = np.arange(num_samples_pad, dtype=\"int64\")\n                perm = np.concatenate(\n                    [perm[rank :: self.world_size] for rank in range(self.world_size)],\n                    axis=0,\n                )\n                perm_inv = np.empty_like(perm)\n                perm_inv[perm] = np.arange(num_samples_pad, dtype=\"int64\")\n                perm_inv = paddle.to_tensor(perm_inv)\n                pred_dict = {key: value[perm_inv] for key, value in pred_dict.items()}\n                # then discard predictions of padding data at the end if num_pad &gt; 0\n                if num_pad &gt; 0:\n                    pred_dict = {\n                        key: value[:num_samples] for key, value in pred_dict.items()\n                    }\n                    # NOTE: Discard padding data in input_dict for consistency\n                    for k in input_dict:\n                        input_dict[k] = input_dict[k][:num_samples]\n\n        # convert to numpy ndarray if specified\n        if return_numpy:\n            pred_dict = {\n                k: (v.numpy() if paddle.is_tensor(v) else v)\n                for k, v in pred_dict.items()\n            }\n\n        return pred_dict\n\n    @misc.run_on_eval_mode\n    def export(\n        self,\n        input_spec: List[\"InputSpec\"],\n        export_path: str,\n        with_onnx: bool = False,\n        skip_prune_program: bool = False,\n    ):\n        \"\"\"\n        Convert model to static graph model and export to files.\n\n        Args:\n            input_spec (List[InputSpec]): InputSpec describes the signature information\n                of the model input.\n            export_path (str): The path prefix to save model.\n            with_onnx (bool, optional): Whether to export model into onnx after\n                paddle inference models are exported. Defaults to False.\n            skip_prune_program (bool, optional): Whether prune program, pruning program\n                may cause unexpectable result, e.g. llm-inference. Defaults to False.\n        \"\"\"\n        jit.enable_to_static(True)\n\n        if self.pretrained_model_path is None:\n            logger.warning(\n                \"'pretrained_model_path' is not given, so the weights of exported \"\n                \"model will be random initialized.\"\n            )\n\n        # convert model to static graph model\n        static_model = jit.to_static(\n            self.model,\n            input_spec=input_spec,\n            full_graph=True,\n        )\n\n        # save static graph model to disk\n        if len(osp.dirname(export_path)):\n            os.makedirs(osp.dirname(export_path), exist_ok=True)\n        try:\n            jit.save(static_model, export_path, skip_prune_program=skip_prune_program)\n        except Exception as e:\n            raise e\n        logger.message(\n            f\"Inference model has been exported to: {export_path}, including \"\n            \"*.pdmodel, *.pdiparams and *.pdiparams.info files.\"\n        )\n        jit.enable_to_static(False)\n\n        if with_onnx:\n            if not importlib.util.find_spec(\"paddle2onnx\"):\n                raise ModuleNotFoundError(\n                    \"Please install paddle2onnx with `pip install paddle2onnx`\"\n                    \" before exporting onnx model.\"\n                )\n            import paddle2onnx\n\n            DEFAULT_OPSET_VERSION = 13\n\n            paddle2onnx.export(\n                model_file=export_path + \".pdmodel\",\n                params_file=export_path + \".pdiparams\",\n                save_file=export_path + \".onnx\",\n                opset_version=DEFAULT_OPSET_VERSION,\n                enable_onnx_checker=True,\n            )\n            logger.message(f\"ONNX model has been exported to: {export_path}.onnx\")\n\n    def autocast_context_manager(\n        self, enable: bool, level: Literal[\"O0\", \"O1\", \"O2\", \"OD\"] = \"O1\"\n    ) -&gt; contextlib.AbstractContextManager:\n        \"\"\"Smart autocast context manager for Auto Mix Precision.\n\n        Args:\n            enable (bool): Enable autocast.\n            level (Literal[\"O0\", \"O1\", \"O2\", \"OD\"]): Autocast level.\n\n        Returns:\n            contextlib.AbstractContextManager: Smart autocast context manager.\n        \"\"\"\n        if enable:\n            ctx_manager = amp.auto_cast(level=level)\n        else:\n            ctx_manager = (\n                contextlib.nullcontext()\n                if sys.version_info &gt;= (3, 7)\n                else contextlib.suppress()\n            )\n        return ctx_manager\n\n    @functools.lru_cache()\n    def no_grad_context_manager(\n        self, enable: bool\n    ) -&gt; contextlib.AbstractContextManager:\n        \"\"\"Smart no_grad context manager.\n\n        Args:\n            enable (bool): Enable no_grad.\n\n        Returns:\n            contextlib.AbstractContextManager: Smart no_grad context manager.\n        \"\"\"\n        if enable:\n            ctx_manager = paddle.no_grad()\n        else:\n            ctx_manager = (\n                contextlib.nullcontext()\n                if sys.version_info &gt;= (3, 7)\n                else contextlib.suppress()\n            )\n        return ctx_manager\n\n    def no_sync_context_manager(\n        self,\n        enable: bool,\n        ddp_model: paddle.DataParallel,\n    ) -&gt; contextlib.AbstractContextManager:\n        \"\"\"Smart no_sync context manager for given model.\n        NOTE: Only `paddle.DataParallel` object has `no_sync` interface.\n\n        Args:\n            enable (bool): Enable no_sync.\n\n        Returns:\n            contextlib.AbstractContextManager: Smart no_sync context manager.\n        \"\"\"\n        if enable:\n            if isinstance(self.model, ppsci.arch.ModelList):\n                for model in self.model.model_list:\n                    if not isinstance(model, paddle.DataParallel):\n                        raise TypeError(\n                            \"no_sync interface is only for model with type \"\n                            \"paddle.DataParallel, but got type \"\n                            f\"{misc.typename(model)}\"\n                        )\n                ctx_manager = contextlib.ExitStack()\n                for model in self.model.model_list:\n                    ctx_manager.enter_context(model.no_sync())\n            else:\n                if not isinstance(self.model, paddle.DataParallel):\n                    raise TypeError(\n                        \"no_sync interface is only for model with type \"\n                        f\"paddle.DataParallel, but got type {misc.typename(ddp_model)}\"\n                    )\n                ctx_manager = ddp_model.no_sync()\n        else:\n            ctx_manager = (\n                contextlib.nullcontext()\n                if sys.version_info &gt;= (3, 7)\n                else contextlib.suppress()\n            )\n        return ctx_manager\n\n    def plot_loss_history(\n        self,\n        by_epoch: bool = False,\n        smooth_step: int = 1,\n        use_semilogy: bool = True,\n    ) -&gt; None:\n        \"\"\"Plotting iteration/epoch-loss curve.\n\n        Args:\n            by_epoch (bool, optional): Whether the abscissa axis of the curve is epoch or iteration. Defaults to False.\n            smooth_step (int, optional): How many steps of loss are squeezed to one point to smooth the curve. Defaults to 1.\n            use_semilogy (bool, optional): Whether to set non-uniform coordinates for the y-axis. Defaults to True.\n        \"\"\"\n        loss_dict = {}\n        for key in self.train_loss_info:\n            loss_arr = np.asarray(self.train_loss_info[key].history)\n            if by_epoch:\n                loss_arr = np.mean(\n                    np.reshape(loss_arr, (-1, self.iters_per_epoch)),\n                    axis=1,\n                )\n            loss_dict[key] = list(loss_arr)\n\n        misc.plot_curve(\n            data=loss_dict,\n            xlabel=\"Epoch\" if by_epoch else \"Iteration\",\n            ylabel=\"Loss\",\n            output_dir=self.output_dir,\n            smooth_step=smooth_step,\n            use_semilogy=use_semilogy,\n        )\n\n    def _parse_params_from_cfg(self, cfg: DictConfig):\n        \"\"\"\n        Parse hyper-parameters from DictConfig.\n        \"\"\"\n        self.output_dir = cfg.output_dir\n        self.log_freq = cfg.log_freq\n        self.use_tbd = cfg.use_tbd\n        self.use_vdl = cfg.use_vdl\n        self.wandb_config = cfg.wandb_config\n        self.use_wandb = cfg.use_wandb\n        self.device = cfg.device\n        self.to_static = cfg.to_static\n\n        self.use_amp = cfg.use_amp\n        self.amp_level = cfg.amp_level\n\n        self.epochs = cfg.TRAIN.epochs\n        self.iters_per_epoch = cfg.TRAIN.iters_per_epoch\n        self.update_freq = cfg.TRAIN.update_freq\n        self.save_freq = cfg.TRAIN.save_freq\n        self.eval_during_train = cfg.TRAIN.eval_during_train\n        self.start_eval_epoch = cfg.TRAIN.start_eval_epoch\n        self.eval_freq = cfg.TRAIN.eval_freq\n        self.checkpoint_path = cfg.TRAIN.checkpoint_path\n\n        if \"ema\" in cfg.TRAIN and cfg.TRAIN.ema.get(\"use_ema\", False):\n            self.avg_freq = cfg.TRAIN.ema.avg_freq\n        elif \"swa\" in cfg.TRAIN and cfg.TRAIN.swa.get(\"use_swa\", False):\n            self.avg_freq = cfg.TRAIN.swa.avg_freq\n\n        self.compute_metric_by_batch = cfg.EVAL.compute_metric_by_batch\n        self.eval_with_no_grad = cfg.EVAL.eval_with_no_grad\n\n        if cfg.mode == \"train\":\n            self.pretrained_model_path = cfg.TRAIN.pretrained_model_path\n        elif cfg.mode == \"eval\":\n            self.pretrained_model_path = cfg.EVAL.pretrained_model_path\n        elif cfg.mode in [\"export\", \"infer\"]:\n            self.pretrained_model_path = cfg.INFER.pretrained_model_path\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.autocast_context_manager","title":"<code>autocast_context_manager(enable, level='O1')</code>","text":"<p>Smart autocast context manager for Auto Mix Precision.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Enable autocast.</p> required <code>level</code> <code>Literal['O0', 'O1', 'O2', 'OD']</code> <p>Autocast level.</p> <code>'O1'</code> <p>Returns:</p> Type Description <code>AbstractContextManager</code> <p>contextlib.AbstractContextManager: Smart autocast context manager.</p> Source code in <code>ppsci/solver/solver.py</code> <pre><code>def autocast_context_manager(\n    self, enable: bool, level: Literal[\"O0\", \"O1\", \"O2\", \"OD\"] = \"O1\"\n) -&gt; contextlib.AbstractContextManager:\n    \"\"\"Smart autocast context manager for Auto Mix Precision.\n\n    Args:\n        enable (bool): Enable autocast.\n        level (Literal[\"O0\", \"O1\", \"O2\", \"OD\"]): Autocast level.\n\n    Returns:\n        contextlib.AbstractContextManager: Smart autocast context manager.\n    \"\"\"\n    if enable:\n        ctx_manager = amp.auto_cast(level=level)\n    else:\n        ctx_manager = (\n            contextlib.nullcontext()\n            if sys.version_info &gt;= (3, 7)\n            else contextlib.suppress()\n        )\n    return ctx_manager\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.eval","title":"<code>eval(epoch_id=None)</code>","text":"<p>Evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_id</code> <code>Optional[int]</code> <p>Epoch id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[float, Dict[str, Dict[str, float]]]</code> <p>Tuple[float, Dict[str, Dict[str, float]]]: A targe metric value(float) and all metric(s)(dict) of evaluation, used to judge the quality of the model.</p> Source code in <code>ppsci/solver/solver.py</code> <pre><code>@misc.run_on_eval_mode\ndef eval(\n    self, epoch_id: Optional[int] = None\n) -&gt; Tuple[float, Dict[str, Dict[str, float]]]:\n    \"\"\"Evaluation.\n\n    Args:\n        epoch_id (Optional[int]): Epoch id. Defaults to None.\n\n    Returns:\n        Tuple[float, Dict[str, Dict[str, float]]]: A targe metric value(float) and\n            all metric(s)(dict) of evaluation, used to judge the quality of the model.\n    \"\"\"\n    # set eval func\n    self.eval_func = ppsci.solver.eval.eval_func\n\n    result = self.eval_func(self, epoch_id, self.log_freq)\n    metric_msg = \", \".join(\n        [self.eval_output_info[key].avg_info for key in self.eval_output_info]\n    )\n\n    if isinstance(epoch_id, int):\n        logger.info(f\"[Eval][Epoch {epoch_id}][Avg] {metric_msg}\")\n    else:\n        logger.info(f\"[Eval][Avg] {metric_msg}\")\n    self.eval_output_info.clear()\n\n    return result\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.export","title":"<code>export(input_spec, export_path, with_onnx=False, skip_prune_program=False)</code>","text":"<p>Convert model to static graph model and export to files.</p> <p>Parameters:</p> Name Type Description Default <code>input_spec</code> <code>List[InputSpec]</code> <p>InputSpec describes the signature information of the model input.</p> required <code>export_path</code> <code>str</code> <p>The path prefix to save model.</p> required <code>with_onnx</code> <code>bool</code> <p>Whether to export model into onnx after paddle inference models are exported. Defaults to False.</p> <code>False</code> <code>skip_prune_program</code> <code>bool</code> <p>Whether prune program, pruning program may cause unexpectable result, e.g. llm-inference. Defaults to False.</p> <code>False</code> Source code in <code>ppsci/solver/solver.py</code> <pre><code>@misc.run_on_eval_mode\ndef export(\n    self,\n    input_spec: List[\"InputSpec\"],\n    export_path: str,\n    with_onnx: bool = False,\n    skip_prune_program: bool = False,\n):\n    \"\"\"\n    Convert model to static graph model and export to files.\n\n    Args:\n        input_spec (List[InputSpec]): InputSpec describes the signature information\n            of the model input.\n        export_path (str): The path prefix to save model.\n        with_onnx (bool, optional): Whether to export model into onnx after\n            paddle inference models are exported. Defaults to False.\n        skip_prune_program (bool, optional): Whether prune program, pruning program\n            may cause unexpectable result, e.g. llm-inference. Defaults to False.\n    \"\"\"\n    jit.enable_to_static(True)\n\n    if self.pretrained_model_path is None:\n        logger.warning(\n            \"'pretrained_model_path' is not given, so the weights of exported \"\n            \"model will be random initialized.\"\n        )\n\n    # convert model to static graph model\n    static_model = jit.to_static(\n        self.model,\n        input_spec=input_spec,\n        full_graph=True,\n    )\n\n    # save static graph model to disk\n    if len(osp.dirname(export_path)):\n        os.makedirs(osp.dirname(export_path), exist_ok=True)\n    try:\n        jit.save(static_model, export_path, skip_prune_program=skip_prune_program)\n    except Exception as e:\n        raise e\n    logger.message(\n        f\"Inference model has been exported to: {export_path}, including \"\n        \"*.pdmodel, *.pdiparams and *.pdiparams.info files.\"\n    )\n    jit.enable_to_static(False)\n\n    if with_onnx:\n        if not importlib.util.find_spec(\"paddle2onnx\"):\n            raise ModuleNotFoundError(\n                \"Please install paddle2onnx with `pip install paddle2onnx`\"\n                \" before exporting onnx model.\"\n            )\n        import paddle2onnx\n\n        DEFAULT_OPSET_VERSION = 13\n\n        paddle2onnx.export(\n            model_file=export_path + \".pdmodel\",\n            params_file=export_path + \".pdiparams\",\n            save_file=export_path + \".onnx\",\n            opset_version=DEFAULT_OPSET_VERSION,\n            enable_onnx_checker=True,\n        )\n        logger.message(f\"ONNX model has been exported to: {export_path}.onnx\")\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.finetune","title":"<code>finetune(pretrained_model_path)</code>","text":"<p>Finetune model based on given pretrained model path.</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_model_path</code> <code>str</code> <p>Pretrained model path or url.</p> required Source code in <code>ppsci/solver/solver.py</code> <pre><code>def finetune(self, pretrained_model_path: str) -&gt; None:\n    \"\"\"Finetune model based on given pretrained model path.\n\n    Args:\n        pretrained_model_path (str): Pretrained model path or url.\n    \"\"\"\n    # load pretrained model\n    save_load.load_pretrain(self.model, pretrained_model_path, self.equation)\n\n    # call train program\n    self.train()\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.no_grad_context_manager","title":"<code>no_grad_context_manager(enable)</code>  <code>cached</code>","text":"<p>Smart no_grad context manager.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Enable no_grad.</p> required <p>Returns:</p> Type Description <code>AbstractContextManager</code> <p>contextlib.AbstractContextManager: Smart no_grad context manager.</p> Source code in <code>ppsci/solver/solver.py</code> <pre><code>@functools.lru_cache()\ndef no_grad_context_manager(\n    self, enable: bool\n) -&gt; contextlib.AbstractContextManager:\n    \"\"\"Smart no_grad context manager.\n\n    Args:\n        enable (bool): Enable no_grad.\n\n    Returns:\n        contextlib.AbstractContextManager: Smart no_grad context manager.\n    \"\"\"\n    if enable:\n        ctx_manager = paddle.no_grad()\n    else:\n        ctx_manager = (\n            contextlib.nullcontext()\n            if sys.version_info &gt;= (3, 7)\n            else contextlib.suppress()\n        )\n    return ctx_manager\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.no_sync_context_manager","title":"<code>no_sync_context_manager(enable, ddp_model)</code>","text":"<p>Smart no_sync context manager for given model. NOTE: Only <code>paddle.DataParallel</code> object has <code>no_sync</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Enable no_sync.</p> required <p>Returns:</p> Type Description <code>AbstractContextManager</code> <p>contextlib.AbstractContextManager: Smart no_sync context manager.</p> Source code in <code>ppsci/solver/solver.py</code> <pre><code>def no_sync_context_manager(\n    self,\n    enable: bool,\n    ddp_model: paddle.DataParallel,\n) -&gt; contextlib.AbstractContextManager:\n    \"\"\"Smart no_sync context manager for given model.\n    NOTE: Only `paddle.DataParallel` object has `no_sync` interface.\n\n    Args:\n        enable (bool): Enable no_sync.\n\n    Returns:\n        contextlib.AbstractContextManager: Smart no_sync context manager.\n    \"\"\"\n    if enable:\n        if isinstance(self.model, ppsci.arch.ModelList):\n            for model in self.model.model_list:\n                if not isinstance(model, paddle.DataParallel):\n                    raise TypeError(\n                        \"no_sync interface is only for model with type \"\n                        \"paddle.DataParallel, but got type \"\n                        f\"{misc.typename(model)}\"\n                    )\n            ctx_manager = contextlib.ExitStack()\n            for model in self.model.model_list:\n                ctx_manager.enter_context(model.no_sync())\n        else:\n            if not isinstance(self.model, paddle.DataParallel):\n                raise TypeError(\n                    \"no_sync interface is only for model with type \"\n                    f\"paddle.DataParallel, but got type {misc.typename(ddp_model)}\"\n                )\n            ctx_manager = ddp_model.no_sync()\n    else:\n        ctx_manager = (\n            contextlib.nullcontext()\n            if sys.version_info &gt;= (3, 7)\n            else contextlib.suppress()\n        )\n    return ctx_manager\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.plot_loss_history","title":"<code>plot_loss_history(by_epoch=False, smooth_step=1, use_semilogy=True)</code>","text":"<p>Plotting iteration/epoch-loss curve.</p> <p>Parameters:</p> Name Type Description Default <code>by_epoch</code> <code>bool</code> <p>Whether the abscissa axis of the curve is epoch or iteration. Defaults to False.</p> <code>False</code> <code>smooth_step</code> <code>int</code> <p>How many steps of loss are squeezed to one point to smooth the curve. Defaults to 1.</p> <code>1</code> <code>use_semilogy</code> <code>bool</code> <p>Whether to set non-uniform coordinates for the y-axis. Defaults to True.</p> <code>True</code> Source code in <code>ppsci/solver/solver.py</code> <pre><code>def plot_loss_history(\n    self,\n    by_epoch: bool = False,\n    smooth_step: int = 1,\n    use_semilogy: bool = True,\n) -&gt; None:\n    \"\"\"Plotting iteration/epoch-loss curve.\n\n    Args:\n        by_epoch (bool, optional): Whether the abscissa axis of the curve is epoch or iteration. Defaults to False.\n        smooth_step (int, optional): How many steps of loss are squeezed to one point to smooth the curve. Defaults to 1.\n        use_semilogy (bool, optional): Whether to set non-uniform coordinates for the y-axis. Defaults to True.\n    \"\"\"\n    loss_dict = {}\n    for key in self.train_loss_info:\n        loss_arr = np.asarray(self.train_loss_info[key].history)\n        if by_epoch:\n            loss_arr = np.mean(\n                np.reshape(loss_arr, (-1, self.iters_per_epoch)),\n                axis=1,\n            )\n        loss_dict[key] = list(loss_arr)\n\n    misc.plot_curve(\n        data=loss_dict,\n        xlabel=\"Epoch\" if by_epoch else \"Iteration\",\n        ylabel=\"Loss\",\n        output_dir=self.output_dir,\n        smooth_step=smooth_step,\n        use_semilogy=use_semilogy,\n    )\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.predict","title":"<code>predict(input_dict, expr_dict=None, batch_size=64, no_grad=True, return_numpy=False)</code>","text":"<p>Pure prediction using model.forward(...) and expression(optional, if given).</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, Union[ndarray, Tensor]]</code> <p>Input data in dict.</p> required <code>expr_dict</code> <code>Optional[Dict[str, Callable]]</code> <p>Expression dict, which guide to compute equation variable with callable function. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Predicting by batch size. Defaults to 64.</p> <code>64</code> <code>no_grad</code> <code>bool</code> <p>Whether set stop_gradient=True for entire prediction, mainly for memory-efficiency. Defaults to True.</p> <code>True</code> <code>return_numpy</code> <code>bool</code> <p>Whether convert result from Tensor to numpy ndarray. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, ndarray]]</code> <p>Dict[str, Union[paddle.Tensor, np.ndarray]]: Prediction in dict.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP(('x', 'y'), ('u', 'v'), num_layers=None, hidden_size=[32, 8])\n&gt;&gt;&gt; solver = ppsci.solver.Solver(model)\n&gt;&gt;&gt; input_dict = {'x': paddle.rand([32, 1]),\n...               'y': paddle.rand([32, 1])}\n&gt;&gt;&gt; pred = solver.predict(input_dict)\n&gt;&gt;&gt; for k, v in pred.items():\n...     print(k, v.shape)\nu [32, 1]\nv [32, 1]\n</code></pre> Source code in <code>ppsci/solver/solver.py</code> <pre><code>@misc.run_on_eval_mode\ndef predict(\n    self,\n    input_dict: Dict[str, Union[np.ndarray, paddle.Tensor]],\n    expr_dict: Optional[Dict[str, Callable]] = None,\n    batch_size: int = 64,\n    no_grad: bool = True,\n    return_numpy: bool = False,\n) -&gt; Dict[str, Union[paddle.Tensor, np.ndarray]]:\n    \"\"\"Pure prediction using model.forward(...) and expression(optional, if given).\n\n    Args:\n        input_dict (Dict[str, Union[np.ndarray, paddle.Tensor]]): Input data in dict.\n        expr_dict (Optional[Dict[str, Callable]]): Expression dict, which guide to\n            compute equation variable with callable function. Defaults to None.\n        batch_size (int, optional): Predicting by batch size. Defaults to 64.\n        no_grad (bool): Whether set stop_gradient=True for entire prediction, mainly\n            for memory-efficiency. Defaults to True.\n        return_numpy (bool): Whether convert result from Tensor to numpy ndarray.\n            Defaults to False.\n\n    Returns:\n        Dict[str, Union[paddle.Tensor, np.ndarray]]: Prediction in dict.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP(('x', 'y'), ('u', 'v'), num_layers=None, hidden_size=[32, 8])\n        &gt;&gt;&gt; solver = ppsci.solver.Solver(model) # doctest: +SKIP\n        &gt;&gt;&gt; input_dict = {'x': paddle.rand([32, 1]),\n        ...               'y': paddle.rand([32, 1])}\n        &gt;&gt;&gt; pred = solver.predict(input_dict) # doctest: +SKIP\n        &gt;&gt;&gt; for k, v in pred.items(): # doctest: +SKIP\n        ...     print(k, v.shape) # doctest: +SKIP\n        u [32, 1]\n        v [32, 1]\n    \"\"\"\n    num_samples = len(next(iter(input_dict.values())))\n    num_pad = (self.world_size - num_samples % self.world_size) % self.world_size\n    # pad with last element if `num_samples` is not divisible by `world_size`\n    # ensuring every device get same number of data.\n    if num_pad &gt; 0:\n        for k, v in input_dict.items():\n            repeat_times = (num_pad, *(1 for _ in range(v.ndim - 1)))\n            if isinstance(v, np.ndarray):\n                input_dict[k] = np.concatenate(\n                    (\n                        v,\n                        np.tile(v[num_samples - 1 : num_samples], repeat_times),\n                    ),\n                )\n            elif isinstance(v, paddle.Tensor):\n                input_dict[k] = paddle.concat(\n                    (\n                        v,\n                        paddle.tile(v[num_samples - 1 : num_samples], repeat_times),\n                    ),\n                )\n            else:\n                raise ValueError(f\"Unsupported data type {type(v)}.\")\n\n    num_samples_pad = num_samples + num_pad\n    local_num_samples_pad = num_samples_pad // self.world_size\n    local_input_dict = (\n        {k: v[self.rank :: self.world_size] for k, v in input_dict.items()}\n        if self.world_size &gt; 1\n        else input_dict\n    )\n    local_batch_num = (local_num_samples_pad + (batch_size - 1)) // batch_size\n\n    pred_dict = misc.Prettydefaultdict(list)\n    with self.no_grad_context_manager(no_grad), self.no_sync_context_manager(\n        self.world_size &gt; 1, self.model\n    ):\n        for batch_id in range(local_batch_num):\n            batch_input_dict = {}\n            st = batch_id * batch_size\n            ed = min(local_num_samples_pad, (batch_id + 1) * batch_size)\n\n            # prepare batch input dict\n            for key in local_input_dict:\n                if not paddle.is_tensor(local_input_dict[key]):\n                    batch_input_dict[key] = paddle.to_tensor(\n                        local_input_dict[key][st:ed], paddle.get_default_dtype()\n                    )\n                else:\n                    batch_input_dict[key] = local_input_dict[key][st:ed]\n                batch_input_dict[key].stop_gradient = no_grad\n\n            # forward\n            with self.autocast_context_manager(self.use_amp, self.amp_level):\n                batch_output_dict = self.forward_helper.visu_forward(\n                    expr_dict, batch_input_dict, self.model\n                )\n\n            # collect batch data\n            for key, batch_output in batch_output_dict.items():\n                pred_dict[key].append(\n                    batch_output.detach() if no_grad else batch_output\n                )\n\n        # concatenate local predictions\n        pred_dict = {key: paddle.concat(value) for key, value in pred_dict.items()}\n\n        if self.world_size &gt; 1:\n            # gather global predictions from all devices if world_size &gt; 1\n            pred_dict = {\n                key: misc.all_gather(value) for key, value in pred_dict.items()\n            }\n            # rearrange predictions as the same order of input_dict according\n            # to inverse permutation\n            perm = np.arange(num_samples_pad, dtype=\"int64\")\n            perm = np.concatenate(\n                [perm[rank :: self.world_size] for rank in range(self.world_size)],\n                axis=0,\n            )\n            perm_inv = np.empty_like(perm)\n            perm_inv[perm] = np.arange(num_samples_pad, dtype=\"int64\")\n            perm_inv = paddle.to_tensor(perm_inv)\n            pred_dict = {key: value[perm_inv] for key, value in pred_dict.items()}\n            # then discard predictions of padding data at the end if num_pad &gt; 0\n            if num_pad &gt; 0:\n                pred_dict = {\n                    key: value[:num_samples] for key, value in pred_dict.items()\n                }\n                # NOTE: Discard padding data in input_dict for consistency\n                for k in input_dict:\n                    input_dict[k] = input_dict[k][:num_samples]\n\n    # convert to numpy ndarray if specified\n    if return_numpy:\n        pred_dict = {\n            k: (v.numpy() if paddle.is_tensor(v) else v)\n            for k, v in pred_dict.items()\n        }\n\n    return pred_dict\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.train","title":"<code>train()</code>","text":"<p>Training.</p> Source code in <code>ppsci/solver/solver.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Training.\"\"\"\n    self.global_step = self.best_metric[\"epoch\"] * self.iters_per_epoch\n    self.max_steps = self.epochs * self.iters_per_epoch\n\n    start_epoch = self.best_metric[\"epoch\"] + 1\n\n    if self.use_tbd and isinstance(self.cfg, DictConfig):\n        self.tbd_writer.add_text(\n            \"config\", f\"&lt;pre&gt;{str(OmegaConf.to_yaml(self.cfg))}&lt;/pre&gt;\"\n        )\n\n    if self.nvtx_flag:\n        core.nvprof_start()\n        core.nvprof_enable_record_event()\n\n    for epoch_id in range(start_epoch, self.epochs + 1):\n        self.train_epoch_func(self, epoch_id, self.log_freq)\n        self.train_output_info.clear()\n\n        # update average model if exist\n        if self.ema_model and epoch_id % self.avg_freq == 0:\n            self.ema_model.update()\n\n        cur_metric = float(\"inf\")\n        # evaluate during training\n        if (\n            self.eval_during_train\n            and epoch_id % self.eval_freq == 0\n            and epoch_id &gt;= self.start_eval_epoch\n        ):\n            cur_metric, metric_dict_group = self.eval(epoch_id)\n            if cur_metric &lt; self.best_metric[\"metric\"]:\n                self.best_metric[\"metric\"] = cur_metric\n                self.best_metric[\"epoch\"] = epoch_id\n                save_load.save_checkpoint(\n                    self.model,\n                    self.optimizer,\n                    self.best_metric,\n                    self.scaler,\n                    self.output_dir,\n                    \"best_model\",\n                    self.equation,\n                )\n            logger.info(\n                f\"[Eval][Epoch {epoch_id}]\"\n                f\"[best metric: {self.best_metric['metric']}]\"\n            )\n            for metric_name, metric_dict in metric_dict_group.items():\n                logger.scalar(\n                    {f\"eval/{metric_name}/{k}\": v for k, v in metric_dict.items()},\n                    epoch_id,\n                    self.vdl_writer,\n                    self.wandb_writer,\n                    self.tbd_writer,\n                )\n\n            # visualize after evaluation\n            if self.visualizer is not None:\n                self.visualize(epoch_id)\n\n            # evaluation for moving average evaluation(almost same procedure)\n            if self.ema_model and epoch_id % self.avg_freq == 0:\n                self.ema_model.apply_shadow()\n                logger.info(\"Evaluating metric of averaging model...\")\n                cur_metric_ema, metric_dict_group_ema = self.eval(epoch_id)\n                self.ema_model.restore()\n\n                if cur_metric_ema &lt; self.best_metric[\"metric\"]:\n                    self.best_metric[\"metric\"] = cur_metric_ema\n                    self.best_metric[\"epoch\"] = epoch_id\n                    save_load.save_checkpoint(\n                        self.ema_model,\n                        None,\n                        metric=self.best_metric,\n                        output_dir=self.output_dir,\n                        prefix=\"best_model_ema\",\n                    )\n                logger.info(\n                    f\"[Eval][Epoch {epoch_id}]\"\n                    f\"[best metric: {self.best_metric['metric']}]\"\n                )\n                for metric_name, metric_dict in metric_dict_group_ema.items():\n                    logger.scalar(\n                        {\n                            f\"eval_ema/{metric_name}/{k}\": v\n                            for k, v in metric_dict.items()\n                        },\n                        epoch_id,\n                        self.vdl_writer,\n                        self.wandb_writer,\n                        self.tbd_writer,\n                    )\n\n        # update learning rate by epoch\n        if self.lr_scheduler is not None and self.lr_scheduler.by_epoch:\n            self.lr_scheduler.step()\n\n        # save epoch model every save_freq epochs\n        if self.save_freq &gt; 0 and epoch_id % self.save_freq == 0:\n            save_load.save_checkpoint(\n                self.model,\n                self.optimizer,\n                {\"metric\": cur_metric, \"epoch\": epoch_id},\n                self.scaler,\n                self.output_dir,\n                f\"epoch_{epoch_id}\",\n                self.equation,\n                ema_model=self.ema_model,\n            )\n\n        # save the latest model for convenient resume training\n        save_load.save_checkpoint(\n            self.model,\n            self.optimizer,\n            {\"metric\": cur_metric, \"epoch\": epoch_id},\n            self.scaler,\n            self.output_dir,\n            \"latest\",\n            self.equation,\n            print_log=(epoch_id == start_epoch),\n            ema_model=self.ema_model,\n        )\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.Solver.visualize","title":"<code>visualize(epoch_id=None)</code>","text":"<p>Visualization.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_id</code> <code>Optional[int]</code> <p>Epoch id. Defaults to None.</p> <code>None</code> Source code in <code>ppsci/solver/solver.py</code> <pre><code>@misc.run_on_eval_mode\ndef visualize(self, epoch_id: Optional[int] = None):\n    \"\"\"Visualization.\n\n    Args:\n        epoch_id (Optional[int]): Epoch id. Defaults to None.\n    \"\"\"\n    # set visualize func\n    self.visu_func = ppsci.solver.visu.visualize_func\n\n    self.visu_func(self, epoch_id)\n    if isinstance(epoch_id, int):\n        logger.info(f\"[Visualize][Epoch {epoch_id}] Finish visualization\")\n    else:\n        logger.info(\"[Visualize] Finish visualization\")\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.train","title":"<code>ppsci.solver.train</code>","text":""},{"location":"zh/api/solver/#ppsci.solver.train.train_epoch_func","title":"<code>train_epoch_func(solver, epoch_id, log_freq)</code>","text":"<p>Train program for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <code>Solver</code> <p>Main solver.</p> required <code>epoch_id</code> <code>int</code> <p>Epoch id.</p> required <code>log_freq</code> <code>int</code> <p>Log training information every <code>log_freq</code> steps.</p> required Source code in <code>ppsci/solver/train.py</code> <pre><code>def train_epoch_func(solver: \"solver.Solver\", epoch_id: int, log_freq: int):\n    \"\"\"Train program for one epoch.\n\n    Args:\n        solver (solver.Solver): Main solver.\n        epoch_id (int): Epoch id.\n        log_freq (int): Log training information every `log_freq` steps.\n    \"\"\"\n    batch_tic = time.perf_counter()\n\n    for iter_id in range(1, solver.iters_per_epoch + 1):\n        if solver.nvtx_flag:  # only for nsight analysis\n            core.nvprof_nvtx_push(\n                f\"Training iteration {solver.global_step + 1}\"\n            )  # Training iteration\n\n        total_batch_size = 0\n        reader_cost = 0.0\n        batch_cost = 0.0\n        reader_tic = time.perf_counter()\n\n        input_dicts = []\n        label_dicts = []\n        weight_dicts = []\n        for _, _constraint in solver.constraint.items():\n            # fetch data from data loader\n            try:\n                input_dict, label_dict, weight_dict = next(_constraint.data_iter)\n            except StopIteration:\n                _constraint.data_iter = iter(_constraint.data_loader)\n                input_dict, label_dict, weight_dict = next(_constraint.data_iter)\n            reader_cost += time.perf_counter() - reader_tic\n\n            # NOTE: eliminate first 5 step for warmup\n            if iter_id == 5:\n                for key in solver.train_time_info:\n                    solver.train_time_info[key].reset()\n\n            for v in input_dict.values():\n                if hasattr(v, \"stop_gradient\"):\n                    v.stop_gradient = False\n\n            # gather each constraint's input, label, weight to a list\n            input_dicts.append(input_dict)\n            label_dicts.append(label_dict)\n            weight_dicts.append(weight_dict)\n            total_batch_size += next(iter(input_dict.values())).shape[0]\n            reader_tic = time.perf_counter()\n\n        loss_dict = misc.Prettydefaultdict(float)\n        loss_dict[\"loss\"] = 0.0\n        # forward for every constraint, including model and equation expression\n        with solver.no_sync_context_manager(solver.world_size &gt; 1, solver.model):\n            with solver.autocast_context_manager(solver.use_amp, solver.amp_level):\n                if solver.nvtx_flag:  # only for nsight analysis\n                    core.nvprof_nvtx_push(\"Loss computation\")\n\n                losses_all, losses_constraint = solver.forward_helper.train_forward(\n                    tuple(\n                        _constraint.output_expr\n                        for _constraint in solver.constraint.values()\n                    ),\n                    input_dicts,\n                    solver.model,\n                    solver.constraint,\n                    label_dicts,\n                    weight_dicts,\n                )\n                assert \"loss\" not in losses_all, (\n                    \"Key 'loss' is not allowed in loss_dict for it is an preserved key\"\n                    \" representing total loss, please use other name instead.\"\n                )\n\n                if solver.nvtx_flag:  # only for nsight analysis\n                    core.nvprof_nvtx_pop()  # Loss computation\n\n                # accumulate all losses\n                if solver.nvtx_flag:  # only for nsight analysis\n                    core.nvprof_nvtx_push(\"Loss aggregator\")\n\n                total_loss = solver.loss_aggregator(losses_all, solver.global_step)\n                if solver.update_freq &gt; 1:\n                    total_loss = total_loss / solver.update_freq\n\n                loss_dict.update(losses_constraint)\n                loss_dict[\"loss\"] = float(total_loss)\n\n                if solver.nvtx_flag:  # only for nsight analysis\n                    core.nvprof_nvtx_pop()  # Loss aggregator\n\n            # backward\n            if solver.nvtx_flag:  # only for nsight analysis\n                core.nvprof_nvtx_push(\"Loss backward\")\n\n            if solver.use_amp:\n                total_loss_scaled = solver.scaler.scale(total_loss)\n                total_loss_scaled.backward()\n            else:\n                total_loss.backward()\n\n            if solver.nvtx_flag:  # only for nsight analysis\n                core.nvprof_nvtx_pop()  # Loss backward\n\n        # update parameters\n        if iter_id % solver.update_freq == 0 or iter_id == solver.iters_per_epoch:\n            if solver.nvtx_flag:  # only for nsight analysis\n                core.nvprof_nvtx_push(\"Optimizer update\")\n\n            if solver.world_size &gt; 1:\n                # fuse + allreduce manually before optimization if use DDP + no_sync\n                # details in https://github.com/PaddlePaddle/Paddle/issues/48898#issuecomment-1343838622\n                hpu.fused_allreduce_gradients(list(solver.model.parameters()), None)\n            if solver.use_amp:\n                solver.scaler.minimize(solver.optimizer, total_loss_scaled)\n            else:\n                solver.optimizer.step()\n\n            if solver.nvtx_flag:  # only for nsight analysis\n                core.nvprof_nvtx_pop()  # Optimizer update\n\n            solver.optimizer.clear_grad()\n\n        # update learning rate by step\n        if solver.lr_scheduler is not None and not solver.lr_scheduler.by_epoch:\n            solver.lr_scheduler.step()\n\n        if solver.benchmark_flag:\n            paddle.device.synchronize()\n        batch_cost += time.perf_counter() - batch_tic\n\n        # update and log training information\n        solver.global_step += 1\n        solver.train_time_info[\"reader_cost\"].update(reader_cost)\n        solver.train_time_info[\"batch_cost\"].update(batch_cost)\n        printer.update_train_loss(solver, loss_dict, total_batch_size)\n        if (\n            solver.global_step % log_freq == 0\n            or solver.global_step == 1\n            or solver.global_step == solver.max_steps\n        ):\n            printer.log_train_info(solver, total_batch_size, epoch_id, iter_id)\n\n        batch_tic = time.perf_counter()\n\n        if solver.nvtx_flag:  # only for nsight analysis\n            core.nvprof_nvtx_pop()  # Training iteration\n            NVTX_STOP_ITER = 25\n            if solver.global_step &gt;= NVTX_STOP_ITER:\n                print(\n                    f\"Only run {NVTX_STOP_ITER} steps when 'NVTX' is set in environment\"\n                    \" for nsight analysis. Exit now ......\\n\"\n                )\n                core.nvprof_stop()\n                sys.exit(0)\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.train.train_LBFGS_epoch_func","title":"<code>train_LBFGS_epoch_func(solver, epoch_id, log_freq)</code>","text":"<p>Train function for one epoch with L-BFGS optimizer.</p> <p>NOTE: L-BFGS training program do not support AMP now.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <code>Solver</code> <p>Main solver.</p> required <code>epoch_id</code> <code>int</code> <p>Epoch id.</p> required <code>log_freq</code> <code>int</code> <p>Log training information every <code>log_freq</code> steps.</p> required Source code in <code>ppsci/solver/train.py</code> <pre><code>def train_LBFGS_epoch_func(solver: \"solver.Solver\", epoch_id: int, log_freq: int):\n    \"\"\"Train function for one epoch with L-BFGS optimizer.\n\n    NOTE: L-BFGS training program do not support AMP now.\n\n    Args:\n        solver (solver.Solver): Main solver.\n        epoch_id (int): Epoch id.\n        log_freq (int): Log training information every `log_freq` steps.\n    \"\"\"\n    batch_tic = time.perf_counter()\n\n    for iter_id in range(1, solver.iters_per_epoch + 1):\n        loss_dict = misc.Prettydefaultdict(float)\n        loss_dict[\"loss\"] = 0.0\n        total_batch_size = 0\n        reader_cost = 0.0\n        batch_cost = 0.0\n        reader_tic = time.perf_counter()\n\n        input_dicts = []\n        label_dicts = []\n        weight_dicts = []\n        for _, _constraint in solver.constraint.items():\n            # fetch data from data loader\n            try:\n                input_dict, label_dict, weight_dict = next(_constraint.data_iter)\n            except StopIteration:\n                _constraint.data_iter = iter(_constraint.data_loader)\n                input_dict, label_dict, weight_dict = next(_constraint.data_iter)\n            reader_cost += time.perf_counter() - reader_tic\n\n            for v in input_dict.values():\n                if hasattr(v, \"stop_gradient\"):\n                    v.stop_gradient = False\n\n            # gather each constraint's input, label, weight to a list\n            input_dicts.append(input_dict)\n            label_dicts.append(label_dict)\n            weight_dicts.append(weight_dict)\n            total_batch_size += next(iter(input_dict.values())).shape[0]\n            reader_tic = time.perf_counter()\n\n        def closure() -&gt; paddle.Tensor:\n            \"\"\"Forward-backward closure function for LBFGS optimizer.\n\n            Returns:\n                paddle.Tensor: Computed loss scalar.\n            \"\"\"\n            with solver.no_sync_context_manager(solver.world_size &gt; 1, solver.model):\n                with solver.autocast_context_manager(solver.use_amp, solver.amp_level):\n                    # forward for every constraint, including model and equation expression\n                    losses_all, losses_constraint = solver.forward_helper.train_forward(\n                        tuple(\n                            _constraint.output_expr\n                            for _constraint in solver.constraint.values()\n                        ),\n                        input_dicts,\n                        solver.model,\n                        solver.constraint,\n                        label_dicts,\n                        weight_dicts,\n                    )\n\n                    # accumulate all losses\n                    total_loss = solver.loss_aggregator(losses_all, solver.global_step)\n                    loss_dict.update(losses_constraint)\n                    loss_dict[\"loss\"] = float(total_loss)\n\n                # backward\n                solver.optimizer.clear_grad()\n                total_loss.backward()\n\n            if solver.world_size &gt; 1:\n                # fuse + allreduce manually before optimization if use DDP model\n                # details in https://github.com/PaddlePaddle/Paddle/issues/48898#issuecomment-1343838622\n                hpu.fused_allreduce_gradients(list(solver.model.parameters()), None)\n\n            return total_loss\n\n        # update parameters\n        solver.optimizer.step(closure)\n\n        # update learning rate by step\n        if solver.lr_scheduler is not None and not solver.lr_scheduler.by_epoch:\n            solver.lr_scheduler.step()\n\n        if solver.benchmark_flag:\n            paddle.device.synchronize()\n        batch_cost += time.perf_counter() - batch_tic\n\n        # update and log training information\n        solver.global_step += 1\n        solver.train_time_info[\"reader_cost\"].update(reader_cost)\n        solver.train_time_info[\"batch_cost\"].update(batch_cost)\n        printer.update_train_loss(solver, loss_dict, total_batch_size)\n        if (\n            solver.global_step % log_freq == 0\n            or solver.global_step == 1\n            or solver.global_step == solver.max_steps\n        ):\n            printer.log_train_info(solver, total_batch_size, epoch_id, iter_id)\n\n        batch_tic = time.perf_counter()\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.eval","title":"<code>ppsci.solver.eval</code>","text":""},{"location":"zh/api/solver/#ppsci.solver.eval.eval_func","title":"<code>eval_func(solver, epoch_id, log_freq)</code>","text":"<p>Evaluation function.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <code>Solver</code> <p>Main Solver.</p> required <code>epoch_id</code> <code>Optional[int]</code> <p>Epoch id.</p> required <code>log_freq</code> <code>int</code> <p>Log evaluation information every <code>log_freq</code> steps.</p> required <p>Returns:</p> Type Description <code>Tuple[float, Dict[str, Dict[str, float]]]</code> <p>Tuple[float, Dict[str, Dict[str, float]]]: Target metric and all metric dicts computed during evaluation.</p> Source code in <code>ppsci/solver/eval.py</code> <pre><code>def eval_func(\n    solver: \"solver.Solver\", epoch_id: Optional[int], log_freq: int\n) -&gt; Tuple[float, Dict[str, Dict[str, float]]]:\n    \"\"\"Evaluation function.\n\n    Args:\n        solver (solver.Solver): Main Solver.\n        epoch_id (Optional[int]): Epoch id.\n        log_freq (int): Log evaluation information every `log_freq` steps.\n\n    Returns:\n        Tuple[float, Dict[str, Dict[str, float]]]: Target metric and all metric dicts\n            computed during evaluation.\n    \"\"\"\n    if solver.compute_metric_by_batch:\n        return _eval_by_batch(solver, epoch_id, log_freq)\n    return _eval_by_dataset(solver, epoch_id, log_freq)\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.visu","title":"<code>ppsci.solver.visu</code>","text":""},{"location":"zh/api/solver/#ppsci.solver.visu.visualize_func","title":"<code>visualize_func(solver, epoch_id)</code>","text":"<p>Visualization program.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <code>Solver</code> <p>Main Solver.</p> required <code>epoch_id</code> <code>Optional[int]</code> <p>Epoch id.</p> required Source code in <code>ppsci/solver/visu.py</code> <pre><code>def visualize_func(solver: \"solver.Solver\", epoch_id: Optional[int]):\n    \"\"\"Visualization program.\n\n    Args:\n        solver (solver.Solver): Main Solver.\n        epoch_id (Optional[int]): Epoch id.\n    \"\"\"\n    for _, _visualizer in solver.visualizer.items():\n        all_input = misc.Prettydefaultdict(list)\n        all_output = misc.Prettydefaultdict(list)\n\n        # NOTE: 'visualize_func' now do not apply data sharding(different from 'Solver.predict'),\n        # where every rank receive same input data and compute same output data\n        # (which will cause computational redundancy),\n        # but only the 0-rank(master) device save the visualization result into disk.\n        # TODO(HydrogenSulfate): This will be optimized in the future.\n\n        input_dict = _visualizer.input_dict\n        batch_size = _visualizer.batch_size\n        num_samples = len(next(iter(input_dict.values())))\n        batch_num = (num_samples + (batch_size - 1)) // batch_size\n\n        for batch_id in range(batch_num):\n            batch_input_dict = {}\n            st = batch_id * batch_size\n            ed = min(num_samples, (batch_id + 1) * batch_size)\n\n            # prepare batch input dict\n            for key in input_dict:\n                if not paddle.is_tensor(input_dict[key]):\n                    batch_input_dict[key] = paddle.to_tensor(\n                        input_dict[key][st:ed], paddle.get_default_dtype()\n                    )\n                else:\n                    batch_input_dict[key] = input_dict[key][st:ed]\n                batch_input_dict[key].stop_gradient = False\n\n            # forward\n            with solver.autocast_context_manager(\n                solver.use_amp, solver.amp_level\n            ), solver.no_grad_context_manager(solver.eval_with_no_grad):\n                batch_output_dict = solver.forward_helper.visu_forward(\n                    _visualizer.output_expr, batch_input_dict, solver.model\n                )\n\n            # collect batch data with dtype fixed to float32 regardless of the dtypes of\n            # paddle runtime, which is most compatible with almost visualization tools.\n            for key, batch_input in batch_input_dict.items():\n                all_input[key].append(batch_input.detach().astype(\"float32\"))\n            for key, batch_output in batch_output_dict.items():\n                all_output[key].append(batch_output.detach().astype(\"float32\"))\n\n        # concatenate all data\n        for key in all_input:\n            all_input[key] = paddle.concat(all_input[key])\n        for key in all_output:\n            all_output[key] = paddle.concat(all_output[key])\n\n        # save visualization\n        with misc.RankZeroOnly(solver.rank) as is_master:\n            if is_master:\n                visual_dir = osp.join(solver.output_dir, \"visual\")\n                if epoch_id:\n                    visual_dir = osp.join(visual_dir, f\"epoch_{epoch_id}\")\n                os.makedirs(visual_dir, exist_ok=True)\n                _visualizer.save(\n                    osp.join(visual_dir, _visualizer.prefix),\n                    {**all_input, **all_output},\n                )\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.printer","title":"<code>ppsci.solver.printer</code>","text":""},{"location":"zh/api/solver/#ppsci.solver.printer.update_train_loss","title":"<code>update_train_loss(solver, loss_dict, batch_size)</code>","text":"Source code in <code>ppsci/solver/printer.py</code> <pre><code>def update_train_loss(\n    solver: \"solver.Solver\", loss_dict: Dict[str, float], batch_size: int\n):\n    for key in loss_dict:\n        if key not in solver.train_output_info:\n            solver.train_output_info[key] = misc.AverageMeter(key, \"7.5f\")\n        solver.train_output_info[key].update(float(loss_dict[key]), batch_size)\n        if key not in solver.train_loss_info:\n            solver.train_loss_info[key] = misc.AverageMeter(key, \".5f\")\n        solver.train_loss_info[key].update(float(loss_dict[key]))\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.printer.update_eval_loss","title":"<code>update_eval_loss(solver, loss_dict, batch_size)</code>","text":"Source code in <code>ppsci/solver/printer.py</code> <pre><code>def update_eval_loss(\n    solver: \"solver.Solver\", loss_dict: Dict[str, float], batch_size: int\n):\n    for key in loss_dict:\n        if key not in solver.eval_output_info:\n            solver.eval_output_info[key] = misc.AverageMeter(key, \"7.5f\")\n        solver.eval_output_info[key].update(float(loss_dict[key]), batch_size)\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.printer.log_train_info","title":"<code>log_train_info(solver, batch_size, epoch_id, iter_id)</code>","text":"Source code in <code>ppsci/solver/printer.py</code> <pre><code>def log_train_info(\n    solver: \"solver.Solver\", batch_size: int, epoch_id: int, iter_id: int\n):\n    lr_msg = f\"lr: {solver.optimizer.get_lr():.5f}\"\n\n    metric_msg = \", \".join(\n        [\n            f\"{key}: {solver.train_output_info[key].avg:.5f}\"\n            for key in solver.train_output_info\n        ]\n    )\n\n    time_msg = \", \".join(\n        [solver.train_time_info[key].mean for key in solver.train_time_info]\n    )\n\n    ips_msg = f\"ips: {batch_size / solver.train_time_info['batch_cost'].avg:.2f}\"\n    if solver.benchmark_flag:\n        ips_msg += \" samples/s\"\n\n    eta_sec = (\n        (solver.epochs - epoch_id + 1) * solver.iters_per_epoch - iter_id\n    ) * solver.train_time_info[\"batch_cost\"].avg\n    eta_msg = f\"eta: {str(datetime.timedelta(seconds=int(eta_sec)))}\"\n\n    epoch_width = len(str(solver.epochs))\n    iters_width = len(str(solver.iters_per_epoch))\n    log_str = (\n        f\"[Train][Epoch {epoch_id:&gt;{epoch_width}}/{solver.epochs}]\"\n        f\"[Iter {iter_id:&gt;{iters_width}}/{solver.iters_per_epoch}] {lr_msg}, \"\n        f\"{metric_msg}, {time_msg}, {ips_msg}, {eta_msg}\"\n    )\n    if solver.benchmark_flag:\n        max_mem_reserved_msg = (\n            f\"max_mem_reserved: {device.cuda.max_memory_reserved() // (1 &lt;&lt; 20)} MB\"\n        )\n        max_mem_allocated_msg = (\n            f\"max_mem_allocated: {device.cuda.max_memory_allocated() // (1 &lt;&lt; 20)} MB\"\n        )\n        log_str += f\", {max_mem_reserved_msg}, {max_mem_allocated_msg}\"\n    logger.info(log_str)\n\n    logger.scalar(\n        {\n            \"train/lr\": solver.optimizer.get_lr(),\n            **{\n                f\"train/{key}\": solver.train_output_info[key].avg\n                for key in solver.train_output_info\n            },\n        },\n        step=solver.global_step,\n        vdl_writer=solver.vdl_writer,\n        wandb_writer=solver.wandb_writer,\n        tbd_writer=solver.tbd_writer,\n    )\n</code></pre>"},{"location":"zh/api/solver/#ppsci.solver.printer.log_eval_info","title":"<code>log_eval_info(solver, batch_size, epoch_id, iters_per_epoch, iter_id)</code>","text":"Source code in <code>ppsci/solver/printer.py</code> <pre><code>def log_eval_info(\n    solver: \"solver.Solver\",\n    batch_size: int,\n    epoch_id: int,\n    iters_per_epoch: int,\n    iter_id: int,\n):\n    metric_msg = \", \".join(\n        [\n            f\"{key}: {solver.eval_output_info[key].avg:.5f}\"\n            for key in solver.eval_output_info\n        ]\n    )\n\n    time_msg = \", \".join(\n        [solver.eval_time_info[key].mean for key in solver.eval_time_info]\n    )\n\n    ips_msg = f\"ips: {batch_size / solver.eval_time_info['batch_cost'].avg:.2f}\"\n\n    eta_sec = (iters_per_epoch - iter_id) * solver.eval_time_info[\"batch_cost\"].avg\n    eta_msg = f\"eta: {str(datetime.timedelta(seconds=int(eta_sec)))}\"\n\n    epoch_width = len(str(solver.epochs))\n    iters_width = len(str(iters_per_epoch))\n    if isinstance(epoch_id, int):\n        logger.info(\n            f\"[Eval][Epoch {epoch_id:&gt;{epoch_width}}/{solver.epochs}]\"\n            f\"[Iter {iter_id:&gt;{iters_width}}/{iters_per_epoch}] \"\n            f\"{metric_msg}, {time_msg}, {ips_msg}, {eta_msg}\"\n        )\n    else:\n        logger.info(\n            f\"[Eval][Iter {iter_id:&gt;{iters_width}}/{iters_per_epoch}] \"\n            f\"{metric_msg}, {time_msg}, {ips_msg}, {eta_msg}\"\n        )\n</code></pre>"},{"location":"zh/api/validate/","title":"ppsci.validate","text":""},{"location":"zh/api/validate/#validate","title":"Validate(\u8bc4\u4f30) \u6a21\u5757","text":""},{"location":"zh/api/validate/#ppsci.validate","title":"<code>ppsci.validate</code>","text":""},{"location":"zh/api/validate/#ppsci.validate.Validator","title":"<code>Validator</code>","text":"<p>Base class for validators.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset for validator.</p> required <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Dataloader config.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>metric</code> <code>Optional[Dict[str, Metric]]</code> <p>Named metric functors in dict.</p> required <code>name</code> <code>str</code> <p>Name of validator.</p> required Source code in <code>ppsci/validate/base.py</code> <pre><code>class Validator:\n    \"\"\"Base class for validators.\n\n    Args:\n        dataset (io.Dataset): Dataset for validator.\n        dataloader_cfg (Dict[str, Any]): Dataloader config.\n        loss (loss.Loss): Loss functor.\n        metric (Optional[Dict[str, metric.Metric]]): Named metric functors in dict.\n        name (str): Name of validator.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: io.Dataset,\n        dataloader_cfg: Dict[str, Any],\n        loss: \"loss.Loss\",\n        metric: Optional[Dict[str, \"metric.Metric\"]],\n        name: str,\n    ):\n        self.data_loader = data.build_dataloader(dataset, dataloader_cfg)\n        self.data_iter = iter(self.data_loader)\n        self.loss = loss\n        self.metric = metric\n        self.name = name\n\n    def __str__(self):\n        return \", \".join(\n            [\n                self.__class__.__name__,\n                f\"name = {self.name}\",\n                f\"input_keys = {self.input_keys}\",\n                f\"output_keys = {self.output_keys}\",\n                f\"output_expr = {self.output_expr}\",\n                f\"label_dict = {self.label_dict}\",\n                f\"len(dataloader) = {len(self.data_loader)}\",\n                f\"loss = {self.loss}\",\n                f\"metric = {list(self.metric.keys())}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/validate/#ppsci.validate.GeometryValidator","title":"<code>GeometryValidator</code>","text":"<p>               Bases: <code>Validator</code></p> <p>Validator for geometry.</p> <p>Parameters:</p> Name Type Description Default <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Function in dict for computing output. e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u will be multiplied by model output v and the result will be named \"u_mul_v\".</p> required <code>label_dict</code> <code>Dict[str, Union[float, Callable]]</code> <p>Function in dict for computing label, which will be a reference value to participate in the loss calculation.</p> required <code>geom</code> <code>Geometry</code> <p>Geometry where data sampled from.</p> required <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Dataloader config.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>random</code> <code>Literal['pseudo', 'Halton', 'LHS']</code> <p>Random method for sampling data in geometry. Defaults to \"pseudo\".</p> <code>'pseudo'</code> <code>criteria</code> <code>Optional[Callable]</code> <p>Criteria for refining specified domain. Defaults to None.</p> <code>None</code> <code>evenly</code> <code>bool</code> <p>Whether to use evenly distribution sampling. Defaults to False.</p> <code>False</code> <code>metric</code> <code>Optional[Dict[str, Metric]]</code> <p>Named metric functors in dict. Defaults to None.</p> <code>None</code> <code>with_initial</code> <code>bool</code> <p>Whether the data contains time t0. Defaults to False.</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of validator. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; rect = ppsci.geometry.Rectangle((0, 0), (1, 1))\n&gt;&gt;&gt; geom_validator = ppsci.validate.GeometryValidator(\n...     {\"u\": lambda out: out[\"u\"]},\n...     {\"u\": 0},\n...     rect,\n...     {\n...         \"dataset\": \"IterableNamedArrayDataset\",\n...         \"iters_per_epoch\": 1,\n...         \"total_size\": 32,\n...         \"batch_size\": 16,\n...     },\n...     ppsci.loss.MSELoss(\"mean\"),\n... )\n</code></pre> Source code in <code>ppsci/validate/geo_validator.py</code> <pre><code>class GeometryValidator(base.Validator):\n    \"\"\"Validator for geometry.\n\n    Args:\n        output_expr (Dict[str, Callable]): Function in dict for computing output.\n            e.g. {\"u_mul_v\": lambda out: out[\"u\"] * out[\"v\"]} means the model output u\n            will be multiplied by model output v and the result will be named \"u_mul_v\".\n        label_dict (Dict[str, Union[float, Callable]]): Function in dict for computing\n            label, which will be a reference value to participate in the loss calculation.\n        geom (geometry.Geometry): Geometry where data sampled from.\n        dataloader_cfg (Dict[str, Any]): Dataloader config.\n        loss (loss.Loss): Loss functor.\n        random (Literal[\"pseudo\", \"Halton\", \"LHS\"], optional): Random method for sampling data in\n            geometry. Defaults to \"pseudo\".\n        criteria (Optional[Callable]): Criteria for refining specified domain. Defaults to None.\n        evenly (bool, optional): Whether to use evenly distribution sampling. Defaults to False.\n        metric (Optional[Dict[str, metric.Metric]]): Named metric functors in dict. Defaults to None.\n        with_initial (bool, optional): Whether the data contains time t0. Defaults to False.\n        name (Optional[str]): Name of validator. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; rect = ppsci.geometry.Rectangle((0, 0), (1, 1))\n        &gt;&gt;&gt; geom_validator = ppsci.validate.GeometryValidator(\n        ...     {\"u\": lambda out: out[\"u\"]},\n        ...     {\"u\": 0},\n        ...     rect,\n        ...     {\n        ...         \"dataset\": \"IterableNamedArrayDataset\",\n        ...         \"iters_per_epoch\": 1,\n        ...         \"total_size\": 32,\n        ...         \"batch_size\": 16,\n        ...     },\n        ...     ppsci.loss.MSELoss(\"mean\"),\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        output_expr: Dict[str, Callable],\n        label_dict: Dict[str, Union[float, Callable]],\n        geom: geometry.Geometry,\n        dataloader_cfg: Dict[str, Any],\n        loss: loss.Loss,\n        random: Literal[\"pseudo\", \"Halton\", \"LHS\"] = \"pseudo\",\n        criteria: Optional[Callable] = None,\n        evenly: bool = False,\n        metric: Optional[Dict[str, metric.Metric]] = None,\n        with_initial: bool = False,\n        name: Optional[str] = None,\n    ):\n        self.output_expr = output_expr\n        self.label_dict = label_dict\n        self.input_keys = geom.dim_keys\n        self.output_keys = tuple(label_dict.keys())\n\n        nx = dataloader_cfg[\"total_size\"]\n        self.num_timestamps = 1\n        # TODO(sensen): Simplify code below\n        if isinstance(geom, geometry.TimeXGeometry):\n            if geom.timedomain.num_timestamps is not None:\n                if with_initial:\n                    # include t0\n                    self.num_timestamps = geom.timedomain.num_timestamps\n                    assert (\n                        nx % self.num_timestamps == 0\n                    ), f\"{nx} % {self.num_timestamps} != 0\"\n                    nx //= self.num_timestamps\n                    input = geom.sample_interior(\n                        nx * (geom.timedomain.num_timestamps - 1),\n                        random,\n                        criteria,\n                        evenly,\n                    )\n                    initial = geom.sample_initial_interior(nx, random, criteria, evenly)\n                    input = {\n                        key: np.vstack((initial[key], input[key])) for key in input\n                    }\n                else:\n                    # exclude t0\n                    self.num_timestamps = geom.timedomain.num_timestamps - 1\n                    assert (\n                        nx % self.num_timestamps == 0\n                    ), f\"{nx} % {self.num_timestamps} != 0\"\n                    nx //= self.num_timestamps\n                    input = geom.sample_interior(\n                        nx * (geom.timedomain.num_timestamps - 1),\n                        random,\n                        criteria,\n                        evenly,\n                    )\n            else:\n                raise NotImplementedError(\n                    \"TimeXGeometry with random timestamp not implemented yet.\"\n                )\n        else:\n            input = geom.sample_interior(nx, random, criteria, evenly)\n\n        label = {}\n        for key, value in label_dict.items():\n            if isinstance(value, (int, float)):\n                label[key] = np.full_like(next(iter(input.values())), value)\n            elif isinstance(value, sympy.Basic):\n                func = sympy.lambdify(\n                    sympy.symbols(geom.dim_keys),\n                    value,\n                    [{\"amax\": lambda xy, _: np.maximum(xy[0], xy[1])}, \"numpy\"],\n                )\n                label[key] = func(\n                    **{k: v for k, v in input.items() if k in geom.dim_keys}\n                )\n            elif callable(value):\n                func = value\n                label[key] = func(input)\n                if isinstance(label[key], (int, float)):\n                    label[key] = np.full(\n                        (next(iter(input.values())).shape[0], 1),\n                        label[key],\n                        paddle.get_default_dtype(),\n                    )\n            else:\n                raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        weight = {key: np.ones_like(next(iter(label.values()))) for key in label}\n\n        _dataset = getattr(dataset, dataloader_cfg[\"dataset\"])(input, label, weight)\n        super().__init__(_dataset, dataloader_cfg, loss, metric, name)\n</code></pre>"},{"location":"zh/api/validate/#ppsci.validate.SupervisedValidator","title":"<code>SupervisedValidator</code>","text":"<p>               Bases: <code>Validator</code></p> <p>Validator for supervised models.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader_cfg</code> <code>Dict[str, Any]</code> <p>Config of building a dataloader.</p> required <code>loss</code> <code>Loss</code> <p>Loss functor.</p> required <code>output_expr</code> <code>Optional[Dict[str, Callable]]</code> <p>List of label expression.</p> <code>None</code> <code>metric</code> <code>Optional[Dict[str, Metric]]</code> <p>Named metric functors in dict. Defaults to None.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Name of validator. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; valid_dataloader_cfg = {\n...     \"dataset\": {\n...         \"name\": \"MatDataset\",\n...         \"file_path\": \"/path/to/file.mat\",\n...         \"input_keys\": (\"t_f\",),\n...         \"label_keys\": (\"eta\", \"f\"),\n...     },\n...     \"batch_size\": 32,\n...     \"sampler\": {\n...         \"name\": \"BatchSampler\",\n...         \"drop_last\": False,\n...         \"shuffle\": False,\n...     },\n... }\n&gt;&gt;&gt; eta_mse_validator = ppsci.validate.SupervisedValidator(\n...     valid_dataloader_cfg,\n...     ppsci.loss.MSELoss(\"mean\"),\n...     {\"eta\": lambda out: out[\"eta\"]},\n...     metric={\"MSE\": ppsci.metric.MSE()},\n...     name=\"eta_mse\",\n... )\n</code></pre> Source code in <code>ppsci/validate/sup_validator.py</code> <pre><code>class SupervisedValidator(base.Validator):\n    \"\"\"Validator for supervised models.\n\n    Args:\n        dataloader_cfg (Dict[str, Any]): Config of building a dataloader.\n        loss (loss.Loss): Loss functor.\n        output_expr (Optional[Dict[str, Callable]]): List of label expression.\n        metric (Optional[Dict[str, metric.Metric]]): Named metric functors in dict. Defaults to None.\n        name (Optional[str]): Name of validator. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; valid_dataloader_cfg = {\n        ...     \"dataset\": {\n        ...         \"name\": \"MatDataset\",\n        ...         \"file_path\": \"/path/to/file.mat\",\n        ...         \"input_keys\": (\"t_f\",),\n        ...         \"label_keys\": (\"eta\", \"f\"),\n        ...     },\n        ...     \"batch_size\": 32,\n        ...     \"sampler\": {\n        ...         \"name\": \"BatchSampler\",\n        ...         \"drop_last\": False,\n        ...         \"shuffle\": False,\n        ...     },\n        ... }  # doctest: +SKIP\n        &gt;&gt;&gt; eta_mse_validator = ppsci.validate.SupervisedValidator(\n        ...     valid_dataloader_cfg,\n        ...     ppsci.loss.MSELoss(\"mean\"),\n        ...     {\"eta\": lambda out: out[\"eta\"]},\n        ...     metric={\"MSE\": ppsci.metric.MSE()},\n        ...     name=\"eta_mse\",\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(\n        self,\n        dataloader_cfg: Dict[str, Any],\n        loss: loss.Loss,\n        output_expr: Optional[Dict[str, Callable]] = None,\n        metric: Optional[Dict[str, metric.Metric]] = None,\n        name: Optional[str] = None,\n    ):\n        self.output_expr = output_expr\n\n        # build dataset\n        _dataset = dataset.build_dataset(dataloader_cfg[\"dataset\"])\n\n        self.input_keys = _dataset.input_keys\n        self.output_keys = (\n            tuple(output_expr.keys())\n            if output_expr is not None\n            else _dataset.label_keys\n        )\n\n        if self.output_expr is None:\n            self.output_expr = {\n                key: lambda out, k=key: out[k] for key in self.output_keys\n            }\n\n        # construct dataloader with dataset and dataloader_cfg\n        super().__init__(_dataset, dataloader_cfg, loss, metric, name)\n\n    def __str__(self):\n        return \", \".join(\n            [\n                self.__class__.__name__,\n                f\"name = {self.name}\",\n                f\"input_keys = {self.input_keys}\",\n                f\"output_keys = {self.output_keys}\",\n                f\"output_expr = {self.output_expr}\",\n                f\"len(dataloader) = {len(self.data_loader)}\",\n                f\"loss = {self.loss}\",\n                f\"metric = {list(self.metric.keys())}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/visualize/","title":"ppsci.visualize","text":""},{"location":"zh/api/visualize/#visualize","title":"Visualize(\u53ef\u89c6\u5316) \u6a21\u5757","text":""},{"location":"zh/api/visualize/#ppsci.visualize","title":"<code>ppsci.visualize</code>","text":""},{"location":"zh/api/visualize/#ppsci.visualize.Visualizer","title":"<code>Visualizer</code>","text":"<p>Base class for visualizer.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Output expression.</p> required <code>batch_size</code> <code>int</code> <p>Batch size of data when computing result in visu.py.</p> required <code>num_timestamps</code> <code>int</code> <p>Number of timestamps.</p> required <code>prefix</code> <code>str</code> <p>Prefix for output file.</p> required Source code in <code>ppsci/visualize/base.py</code> <pre><code>class Visualizer:\n    \"\"\"Base class for visualizer.\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): Input dict.\n        output_expr (Dict[str, Callable]): Output expression.\n        batch_size (int): Batch size of data when computing result in visu.py.\n        num_timestamps (int): Number of timestamps.\n        prefix (str): Prefix for output file.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dict: Dict[str, np.ndarray],\n        output_expr: Dict[str, Callable],\n        batch_size: int,\n        num_timestamps: int,\n        prefix: str,\n    ):\n        self.input_dict = input_dict\n        self.input_keys = tuple(input_dict.keys())\n        self.output_expr = output_expr\n        self.output_keys = tuple(output_expr.keys())\n        self.batch_size = batch_size\n        self.num_timestamps = num_timestamps\n        self.prefix = prefix\n\n    @abc.abstractmethod\n    def save(self, data_dict):\n        \"\"\"Visualize result from data_dict and save as files\"\"\"\n\n    def __str__(self):\n        return \", \".join(\n            [\n                f\"input_keys: {self.input_keys}\",\n                f\"output_keys: {self.output_keys}\",\n                f\"output_expr: {self.output_expr}\",\n                f\"batch_size: {self.batch_size}\",\n                f\"num_timestamps: {self.num_timestamps}\",\n                f\"output file prefix: {self.prefix}\",\n            ]\n        )\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.Visualizer.save","title":"<code>save(data_dict)</code>  <code>abstractmethod</code>","text":"<p>Visualize result from data_dict and save as files</p> Source code in <code>ppsci/visualize/base.py</code> <pre><code>@abc.abstractmethod\ndef save(self, data_dict):\n    \"\"\"Visualize result from data_dict and save as files\"\"\"\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.VisualizerScatter1D","title":"<code>VisualizerScatter1D</code>","text":"<p>               Bases: <code>Visualizer</code></p> <p>Visualizer for 1d scatter data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>coord_keys</code> <code>Tuple[str, ...]</code> <p>Coordinate keys, such as (\"x\", \"y\").</p> required <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Output expression.</p> required <code>batch_size</code> <code>int</code> <p>Batch size of data when computing result in visu.py. Defaults to 64.</p> <code>64</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamps. Defaults to 1.</p> <code>1</code> <code>prefix</code> <code>str</code> <p>Prefix for output file. Defaults to \"plot\".</p> <code>'plot'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; visu_mat = {\"t_f\": np.random.randn(16, 1), \"eta\": np.random.randn(16, 1)}\n&gt;&gt;&gt; visualizer_eta = ppsci.visualize.VisualizerScatter1D(\n...     visu_mat,\n...     (\"t_f\",),\n...     {\"eta\": lambda d: d[\"eta\"]},\n...     num_timestamps=1,\n...     prefix=\"viv_pred\",\n... )\n</code></pre> Source code in <code>ppsci/visualize/visualizer.py</code> <pre><code>class VisualizerScatter1D(base.Visualizer):\n    \"\"\"Visualizer for 1d scatter data.\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): Input dict.\n        coord_keys (Tuple[str, ...]): Coordinate keys, such as (\"x\", \"y\").\n        output_expr (Dict[str, Callable]): Output expression.\n        batch_size (int, optional): Batch size of data when computing result in visu.py. Defaults to 64.\n        num_timestamps (int, optional): Number of timestamps. Defaults to 1.\n        prefix (str, optional): Prefix for output file. Defaults to \"plot\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; visu_mat = {\"t_f\": np.random.randn(16, 1), \"eta\": np.random.randn(16, 1)}\n        &gt;&gt;&gt; visualizer_eta = ppsci.visualize.VisualizerScatter1D(\n        ...     visu_mat,\n        ...     (\"t_f\",),\n        ...     {\"eta\": lambda d: d[\"eta\"]},\n        ...     num_timestamps=1,\n        ...     prefix=\"viv_pred\",\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dict: Dict[str, np.ndarray],\n        coord_keys: Tuple[str, ...],\n        output_expr: Dict[str, Callable],\n        batch_size: int = 64,\n        num_timestamps: int = 1,\n        prefix: str = \"plot\",\n    ):\n        super().__init__(input_dict, output_expr, batch_size, num_timestamps, prefix)\n        self.coord_keys = coord_keys\n\n    def save(self, filename, data_dict):\n        plot.save_plot_from_1d_dict(\n            filename, data_dict, self.coord_keys, self.output_keys, self.num_timestamps\n        )\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.VisualizerScatter3D","title":"<code>VisualizerScatter3D</code>","text":"<p>               Bases: <code>Visualizer</code></p> <p>Visualizer for 3d scatter data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Output expression.</p> required <code>batch_size</code> <code>int</code> <p>Batch size of data when computing result in visu.py. Defaults to 64.</p> <code>64</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamps. Defaults to 1.</p> <code>1</code> <code>prefix</code> <code>str</code> <p>Prefix for output file. Defaults to \"plot3d_scatter\".</p> <code>'plot3d_scatter'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; vis_data = {\"states\": np.random.randn(16, 1)}\n&gt;&gt;&gt; visualizer = ppsci.visualize.VisualizerScatter3D(\n...     vis_data,\n...     {\"states\": lambda d: d[\"states\"]},\n...     num_timestamps=1,\n...     prefix=\"result_states\",\n... )\n</code></pre> Source code in <code>ppsci/visualize/visualizer.py</code> <pre><code>class VisualizerScatter3D(base.Visualizer):\n    \"\"\"Visualizer for 3d scatter data.\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): Input dict.\n        output_expr (Dict[str, Callable]): Output expression.\n        batch_size (int, optional): Batch size of data when computing result in visu.py. Defaults to 64.\n        num_timestamps (int, optional): Number of timestamps. Defaults to 1.\n        prefix (str, optional): Prefix for output file. Defaults to \"plot3d_scatter\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; vis_data = {\"states\": np.random.randn(16, 1)}\n        &gt;&gt;&gt; visualizer = ppsci.visualize.VisualizerScatter3D(\n        ...     vis_data,\n        ...     {\"states\": lambda d: d[\"states\"]},\n        ...     num_timestamps=1,\n        ...     prefix=\"result_states\",\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dict: Dict[str, np.ndarray],\n        output_expr: Dict[str, Callable],\n        batch_size: int = 64,\n        num_timestamps: int = 1,\n        prefix: str = \"plot3d_scatter\",\n    ):\n        super().__init__(input_dict, output_expr, batch_size, num_timestamps, prefix)\n\n    def save(self, filename, data_dict):\n        data_dict = {\n            key: value for key, value in data_dict.items() if key in self.output_keys\n        }\n        value = data_dict[self.output_keys[0]]\n        dim = len(value.shape)\n        if dim == 3:\n            # value.shape=(B, T, 3)\n            for i in range(value.shape[0]):\n                cur_data_dict = {key: value[i] for key, value in data_dict.items()}\n                plot.save_plot_from_3d_dict(\n                    filename + str(i),\n                    cur_data_dict,\n                    self.output_keys,\n                    self.num_timestamps,\n                )\n        else:\n            # value.shape=(T, 3)\n            plot.save_plot_from_3d_dict(\n                filename, data_dict, self.output_keys, self.num_timestamps\n            )\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.VisualizerVtu","title":"<code>VisualizerVtu</code>","text":"<p>               Bases: <code>Visualizer</code></p> <p>Visualizer for 2D points data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Output expression.</p> required <code>batch_size</code> <code>int</code> <p>Batch size of data when computing result in visu.py. Defaults to 64.</p> <code>64</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamps</p> <code>1</code> <code>prefix</code> <code>str</code> <p>Prefix for output file.</p> <code>'vtu'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; vis_points = {\n...     \"x\": np.random.randn(128, 1),\n...     \"y\": np.random.randn(128, 1),\n...     \"u\": np.random.randn(128, 1),\n...     \"v\": np.random.randn(128, 1),\n... }\n&gt;&gt;&gt; visualizer_u_v =  ppsci.visualize.VisualizerVtu(\n...     vis_points,\n...     {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"]},\n...     num_timestamps=1,\n...     prefix=\"result_u_v\",\n... )\n</code></pre> Source code in <code>ppsci/visualize/visualizer.py</code> <pre><code>class VisualizerVtu(base.Visualizer):\n    \"\"\"Visualizer for 2D points data.\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): Input dict.\n        output_expr (Dict[str, Callable]): Output expression.\n        batch_size (int, optional): Batch size of data when computing result in visu.py. Defaults to 64.\n        num_timestamps (int, optional): Number of timestamps\n        prefix (str, optional): Prefix for output file.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; vis_points = {\n        ...     \"x\": np.random.randn(128, 1),\n        ...     \"y\": np.random.randn(128, 1),\n        ...     \"u\": np.random.randn(128, 1),\n        ...     \"v\": np.random.randn(128, 1),\n        ... }\n        &gt;&gt;&gt; visualizer_u_v =  ppsci.visualize.VisualizerVtu(\n        ...     vis_points,\n        ...     {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"]},\n        ...     num_timestamps=1,\n        ...     prefix=\"result_u_v\",\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dict: Dict[str, np.ndarray],\n        output_expr: Dict[str, Callable],\n        batch_size: int = 64,\n        num_timestamps: int = 1,\n        prefix: str = \"vtu\",\n    ):\n        super().__init__(input_dict, output_expr, batch_size, num_timestamps, prefix)\n\n    def save(self, filename, data_dict):\n        vtu.save_vtu_from_dict(\n            filename, data_dict, self.input_keys, self.output_keys, self.num_timestamps\n        )\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.Visualizer2D","title":"<code>Visualizer2D</code>","text":"<p>               Bases: <code>Visualizer</code></p> <p>Visualizer for 2D data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Output expression.</p> required <code>batch_size</code> <code>int</code> <p>Batch size of data when computing result in visu.py. Defaults to 64.</p> <code>64</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamps. Defaults to 1.</p> <code>1</code> <code>prefix</code> <code>str</code> <p>Prefix for output file. Defaults to \"plot2d\".</p> <code>'plot2d'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; vis_points = {\n...     \"x\": np.random.randn(128, 1),\n...     \"y\": np.random.randn(128, 1),\n...     \"u\": np.random.randn(128, 1),\n...     \"v\": np.random.randn(128, 1),\n... }\n&gt;&gt;&gt; visualizer_u_v = ppsci.visualize.Visualizer2D(\n...     vis_points,\n...     {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"]},\n...     num_timestamps=1,\n...     prefix=\"result_u_v\",\n... )\n</code></pre> Source code in <code>ppsci/visualize/visualizer.py</code> <pre><code>class Visualizer2D(base.Visualizer):\n    \"\"\"Visualizer for 2D data.\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): Input dict.\n        output_expr (Dict[str, Callable]): Output expression.\n        batch_size (int, optional): Batch size of data when computing result in visu.py. Defaults to 64.\n        num_timestamps (int, optional): Number of timestamps. Defaults to 1.\n        prefix (str, optional): Prefix for output file. Defaults to \"plot2d\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; vis_points = {\n        ...     \"x\": np.random.randn(128, 1),\n        ...     \"y\": np.random.randn(128, 1),\n        ...     \"u\": np.random.randn(128, 1),\n        ...     \"v\": np.random.randn(128, 1),\n        ... }\n        &gt;&gt;&gt; visualizer_u_v = ppsci.visualize.Visualizer2D(\n        ...     vis_points,\n        ...     {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"]},\n        ...     num_timestamps=1,\n        ...     prefix=\"result_u_v\",\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dict: Dict[str, np.ndarray],\n        output_expr: Dict[str, Callable],\n        batch_size: int = 64,\n        num_timestamps: int = 1,\n        prefix: str = \"plot2d\",\n    ):\n        super().__init__(input_dict, output_expr, batch_size, num_timestamps, prefix)\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.Visualizer2DPlot","title":"<code>Visualizer2DPlot</code>","text":"<p>               Bases: <code>Visualizer2D</code></p> <p>Visualizer for 2D data use matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Output expression.</p> required <code>batch_size</code> <code>int</code> <p>Batch size of data when computing result in visu.py. Defaults to 64.</p> <code>64</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamps.</p> <code>1</code> <code>stride</code> <code>int</code> <p>The time stride of visualization. Defaults to 1.</p> <code>1</code> <code>xticks</code> <code>Optional[Tuple[float, ...]]</code> <p>The list of xtick locations. Defaults to None.</p> <code>None</code> <code>yticks</code> <code>Optional[Tuple[float, ...]]</code> <p>The list of ytick locations. Defaults to None.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for output file. Defaults to \"plot2d\".</p> <code>'plot2d'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; vis_data = {\n...     \"target_ux\": np.random.randn(128, 20, 1),\n...     \"pred_ux\": np.random.randn(128, 20, 1),\n... }\n&gt;&gt;&gt; visualizer_states = ppsci.visualize.Visualizer2DPlot(\n...     vis_data,\n...     {\n...         \"target_ux\": lambda d: d[\"states\"][:, :, 0],\n...         \"pred_ux\": lambda d: output_transform(d)[:, :, 0],\n...     },\n...     batch_size=1,\n...     num_timestamps=10,\n...     stride=20,\n...     xticks=np.linspace(-2, 14, 9),\n...     yticks=np.linspace(-4, 4, 5),\n...     prefix=\"result_states\",\n... )\n</code></pre> Source code in <code>ppsci/visualize/visualizer.py</code> <pre><code>class Visualizer2DPlot(Visualizer2D):\n    \"\"\"Visualizer for 2D data use matplotlib.\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): Input dict.\n        output_expr (Dict[str, Callable]): Output expression.\n        batch_size (int, optional): Batch size of data when computing result in visu.py. Defaults to 64.\n        num_timestamps (int, optional): Number of timestamps.\n        stride (int, optional): The time stride of visualization. Defaults to 1.\n        xticks (Optional[Tuple[float,...]]): The list of xtick locations. Defaults to None.\n        yticks (Optional[Tuple[float,...]]): The list of ytick locations. Defaults to None.\n        prefix (str, optional): Prefix for output file. Defaults to \"plot2d\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; vis_data = {\n        ...     \"target_ux\": np.random.randn(128, 20, 1),\n        ...     \"pred_ux\": np.random.randn(128, 20, 1),\n        ... }\n        &gt;&gt;&gt; visualizer_states = ppsci.visualize.Visualizer2DPlot(\n        ...     vis_data,\n        ...     {\n        ...         \"target_ux\": lambda d: d[\"states\"][:, :, 0],\n        ...         \"pred_ux\": lambda d: output_transform(d)[:, :, 0],\n        ...     },\n        ...     batch_size=1,\n        ...     num_timestamps=10,\n        ...     stride=20,\n        ...     xticks=np.linspace(-2, 14, 9),\n        ...     yticks=np.linspace(-4, 4, 5),\n        ...     prefix=\"result_states\",\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dict: Dict[str, np.ndarray],\n        output_expr: Dict[str, Callable],\n        batch_size: int = 64,\n        num_timestamps: int = 1,\n        stride: int = 1,\n        xticks: Optional[Tuple[float, ...]] = None,\n        yticks: Optional[Tuple[float, ...]] = None,\n        prefix: str = \"plot2d\",\n    ):\n        super().__init__(input_dict, output_expr, batch_size, num_timestamps, prefix)\n        self.stride = stride\n        self.xticks = xticks\n        self.yticks = yticks\n\n    def save(self, filename, data_dict):\n        data_dict = {\n            key: value for key, value in data_dict.items() if key in self.output_keys\n        }\n        value = data_dict[self.output_keys[0]]\n        dim = len(value.shape)\n        if dim == 4:\n            # value.shape=(B, T, H, W)\n            for i in range(value.shape[0]):\n                cur_data_dict = {key: value[i] for key, value in data_dict.items()}\n                plot.save_plot_from_2d_dict(\n                    filename + str(i),\n                    cur_data_dict,\n                    self.output_keys,\n                    self.num_timestamps,\n                    self.stride,\n                    self.xticks,\n                    self.yticks,\n                )\n        else:\n            # value.shape=(T, H, W)\n            plot.save_plot_from_2d_dict(\n                filename,\n                data_dict,\n                self.output_keys,\n                self.num_timestamps,\n                self.stride,\n                self.xticks,\n                self.yticks,\n            )\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.Visualizer3D","title":"<code>Visualizer3D</code>","text":"<p>               Bases: <code>Visualizer</code></p> <p>Visualizer for 3D plot data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Output expression.</p> required <code>batch_size</code> <code>int</code> <p>Batch size of data when computing result in visu.py. Defaults to 64.</p> <code>64</code> <code>label_dict</code> <code>Dict[str, ndarray]</code> <p>Label dict.</p> <code>None</code> <code>time_list</code> <code>Optional[Tuple[float, ...]]</code> <p>Time list.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for output file.</p> <code>'vtu'</code> Source code in <code>ppsci/visualize/visualizer.py</code> <pre><code>class Visualizer3D(base.Visualizer):\n    \"\"\"Visualizer for 3D plot data.\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): Input dict.\n        output_expr (Dict[str, Callable]): Output expression.\n        batch_size (int, optional): Batch size of data when computing result in visu.py. Defaults to 64.\n        label_dict (Dict[str, np.ndarray]): Label dict.\n        time_list (Optional[Tuple[float, ...]]): Time list.\n        prefix (str, optional): Prefix for output file.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dict: Dict[str, np.ndarray],\n        output_expr: Dict[str, Callable],\n        batch_size: int = 64,\n        label_dict: Optional[Dict[str, np.ndarray]] = None,\n        time_list: Optional[Tuple[float, ...]] = None,\n        prefix: str = \"vtu\",\n    ):\n        self.label = label_dict\n        self.time_list = time_list\n        super().__init__(input_dict, output_expr, batch_size, len(time_list), prefix)\n\n    def save(self, filename: str, data_dict: Dict[str, np.ndarray]):\n        n = int((next(iter(data_dict.values()))).shape[0] / self.num_timestamps)\n        coord_keys = [x for x in self.input_dict if x != \"t\"]\n        for i in range(len(self.time_list)):\n            vtu.save_vtu_to_mesh(\n                osp.join(filename, f\"predict_{i+1}.vtu\"),\n                {key: (data_dict[key][i * n : (i + 1) * n]) for key in data_dict},\n                coord_keys,\n                self.output_keys,\n            )\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.VisualizerWeather","title":"<code>VisualizerWeather</code>","text":"<p>               Bases: <code>Visualizer</code></p> <p>Visualizer for weather data use matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Output expression.</p> required <code>xticks</code> <code>Tuple[float, ...]</code> <p>The list of xtick locations.</p> required <code>xticklabels</code> <code>Tuple[str, ...]</code> <p>The x-axis' tick labels.</p> required <code>yticks</code> <code>Tuple[float, ...]</code> <p>The list of ytick locations.</p> required <code>yticklabels</code> <code>Tuple[str, ...]</code> <p>The y-axis' tick labels.</p> required <code>vmin</code> <code>float</code> <p>Minimum value that the colormap covers.</p> required <code>vmax</code> <code>float</code> <p>Maximal value that the colormap covers.</p> required <code>colorbar_label</code> <code>str</code> <p>The color-bar label. Defaults to \"\".</p> <code>''</code> <code>log_norm</code> <code>bool</code> <p>Whether use log norm. Defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>: Batch size of data when computing result in visu.py. Defaults to 1.</p> <code>1</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamps. Defaults to 1.</p> <code>1</code> <code>prefix</code> <code>str</code> <p>Prefix for output file. Defaults to \"plot_weather\".</p> <code>'plot_weather'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; vis_data = {\n...     \"output_6h\": np.random.randn(1, 720, 1440),\n...     \"target_6h\": np.random.randn(1, 720, 1440),\n... }\n&gt;&gt;&gt; visualizer_weather = ppsci.visualize.VisualizerWeather(\n...     vis_data,\n...     {\n...         \"output_6h\": lambda d: d[\"output_6h\"],\n...         \"target_6h\": lambda d: d[\"target_6h\"],\n...     },\n...     xticks=np.linspace(0, 1439, 13),\n...     xticklabels=[str(i) for i in range(360, -1, -30)],\n...     yticks=np.linspace(0, 719, 7),\n...     yticklabels=[str(i) for i in range(90, -91, -30)],\n...     vmin=0,\n...     vmax=25,\n...     prefix=\"result_states\",\n... )\n</code></pre> Source code in <code>ppsci/visualize/visualizer.py</code> <pre><code>class VisualizerWeather(base.Visualizer):\n    \"\"\"Visualizer for weather data use matplotlib.\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): Input dict.\n        output_expr (Dict[str, Callable]): Output expression.\n        xticks (Tuple[float, ...]): The list of xtick locations.\n        xticklabels (Tuple[str, ...]): The x-axis' tick labels.\n        yticks (Tuple[float, ...]): The list of ytick locations.\n        yticklabels (Tuple[str, ...]): The y-axis' tick labels.\n        vmin (float): Minimum value that the colormap covers.\n        vmax (float): Maximal value that the colormap covers.\n        colorbar_label (str, optional): The color-bar label. Defaults to \"\".\n        log_norm (bool, optional): Whether use log norm. Defaults to False.\n        batch_size (int, optional): : Batch size of data when computing result in visu.py. Defaults to 1.\n        num_timestamps (int, optional): Number of timestamps. Defaults to 1.\n        prefix (str, optional): Prefix for output file. Defaults to \"plot_weather\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; vis_data = {\n        ...     \"output_6h\": np.random.randn(1, 720, 1440),\n        ...     \"target_6h\": np.random.randn(1, 720, 1440),\n        ... }\n        &gt;&gt;&gt; visualizer_weather = ppsci.visualize.VisualizerWeather(\n        ...     vis_data,\n        ...     {\n        ...         \"output_6h\": lambda d: d[\"output_6h\"],\n        ...         \"target_6h\": lambda d: d[\"target_6h\"],\n        ...     },\n        ...     xticks=np.linspace(0, 1439, 13),\n        ...     xticklabels=[str(i) for i in range(360, -1, -30)],\n        ...     yticks=np.linspace(0, 719, 7),\n        ...     yticklabels=[str(i) for i in range(90, -91, -30)],\n        ...     vmin=0,\n        ...     vmax=25,\n        ...     prefix=\"result_states\",\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dict: Dict[str, np.ndarray],\n        output_expr: Dict[str, Callable],\n        xticks: Tuple[float, ...],\n        xticklabels: Tuple[str, ...],\n        yticks: Tuple[float, ...],\n        yticklabels: Tuple[str, ...],\n        vmin: float,\n        vmax: float,\n        colorbar_label: str = \"\",\n        log_norm: bool = False,\n        batch_size: int = 1,\n        num_timestamps: int = 1,\n        prefix: str = \"plot_weather\",\n    ):\n        super().__init__(input_dict, output_expr, batch_size, num_timestamps, prefix)\n        self.xticks = xticks\n        self.xticklabels = xticklabels\n        self.yticks = yticks\n        self.yticklabels = yticklabels\n        self.vmin = vmin\n        self.vmax = vmax\n        self.colorbar_label = colorbar_label\n        self.log_norm = log_norm\n\n    def save(self, filename, data_dict):\n        data_dict = {key: data_dict[key] for key in self.output_keys}\n        value = data_dict[self.output_keys[0]]\n        # value.shape=(B, H, W)\n        for i in range(value.shape[0]):\n            cur_data_dict = {key: value[i] for key, value in data_dict.items()}\n            plot.save_plot_weather_from_dict(\n                filename + str(i),\n                cur_data_dict,\n                self.output_keys,\n                self.xticks,\n                self.xticklabels,\n                self.yticks,\n                self.yticklabels,\n                self.vmin,\n                self.vmax,\n                self.colorbar_label,\n                self.log_norm,\n                self.num_timestamps,\n            )\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.VisualizerRadar","title":"<code>VisualizerRadar</code>","text":"<p>               Bases: <code>Visualizer</code></p> <p>Visualizer for NowcastNet Radar Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>output_expr</code> <code>Dict[str, Callable]</code> <p>Output expression.</p> required <code>batch_size</code> <code>int</code> <p>Batch size of data when computing result in visu.py. Defaults to 64.</p> <code>64</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamps</p> <code>1</code> <code>prefix</code> <code>str</code> <p>Prefix for output file.</p> <code>'vtu'</code> <code>case_type</code> <code>str</code> <p>Case type.</p> <code>'normal'</code> <code>total_length</code> <code>str</code> <p>Total length.</p> <code>29</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; frames_tensor = paddle.randn([1, 29, 512, 512, 2])\n&gt;&gt;&gt; visualizer =  ppsci.visualize.VisualizerRadar(\n...     {\"input\": frames_tensor},\n...     {\"output\": lambda out: out[\"output\"]},\n...     num_timestamps=1,\n...     prefix=\"v_nowcastnet\",\n... )\n</code></pre> Source code in <code>ppsci/visualize/radar.py</code> <pre><code>class VisualizerRadar(base.Visualizer):\n    \"\"\"Visualizer for NowcastNet Radar Dataset.\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): Input dict.\n        output_expr (Dict[str, Callable]): Output expression.\n        batch_size (int, optional): Batch size of data when computing result in visu.py. Defaults to 64.\n        num_timestamps (int, optional): Number of timestamps\n        prefix (str, optional): Prefix for output file.\n        case_type (str, optional): Case type.\n        total_length (str, optional): Total length.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; frames_tensor = paddle.randn([1, 29, 512, 512, 2])\n        &gt;&gt;&gt; visualizer =  ppsci.visualize.VisualizerRadar(\n        ...     {\"input\": frames_tensor},\n        ...     {\"output\": lambda out: out[\"output\"]},\n        ...     num_timestamps=1,\n        ...     prefix=\"v_nowcastnet\",\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dict: Dict[str, np.ndarray],\n        output_expr: Dict[str, Callable],\n        batch_size: int = 64,\n        num_timestamps: int = 1,\n        prefix: str = \"vtu\",\n        case_type: str = \"normal\",\n        total_length: int = 29,\n    ):\n        super().__init__(input_dict, output_expr, batch_size, num_timestamps, prefix)\n        self.case_type = case_type\n        self.total_length = total_length\n        self.input_dict = input_dict\n\n    def save(self, path, data_dict):\n        if not os.path.exists(path):\n            os.makedirs(path)\n        test_ims = self.input_dict[list(self.input_dict.keys())[0]]\n        # keys: {\"input\", \"output\"}\n        img_gen = data_dict[list(data_dict.keys())[1]]\n        vis_info = {\"vmin\": 1, \"vmax\": 40}\n        if self.case_type == \"normal\":\n            test_ims_plot = test_ims[0][\n                :-2, 256 - 192 : 256 + 192, 256 - 192 : 256 + 192\n            ]\n            img_gen_plot = img_gen[0][:-2, 256 - 192 : 256 + 192, 256 - 192 : 256 + 192]\n        else:\n            test_ims_plot = test_ims[0][:-2]\n            img_gen_plot = img_gen[0][:-2]\n        save_plots(\n            test_ims_plot,\n            labels=[f\"gt{i + 1}\" for i in range(self.total_length)],\n            res_path=path,\n            vmin=vis_info[\"vmin\"],\n            vmax=vis_info[\"vmax\"],\n        )\n        save_plots(\n            img_gen_plot,\n            labels=[f\"pd{i + 1}\" for i in range(9, self.total_length)],\n            res_path=path,\n            vmin=vis_info[\"vmin\"],\n            vmax=vis_info[\"vmax\"],\n        )\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.save_vtu_from_dict","title":"<code>save_vtu_from_dict(filename, data_dict, coord_keys, value_keys, num_timestamps=1)</code>","text":"<p>Save dict data to '*.vtu' file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Output filename.</p> required <code>data_dict</code> <code>Dict[str, ndarray]</code> <p>Data in dict.</p> required <code>coord_keys</code> <code>Tuple[str, ...]</code> <p>Tuple of coord key. such as (\"x\", \"y\").</p> required <code>value_keys</code> <code>Tuple[str, ...]</code> <p>Tuple of value key. such as (\"u\", \"v\").</p> required <code>num_timestamps</code> <code>int</code> <p>Number of timestamp in data_dict. Defaults to 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; filename = \"path/to/file.vtu\"\n&gt;&gt;&gt; data_dict = {\n...     \"x\": np.array([[1], [2], [3],[4]]),\n...     \"y\": np.array([[2], [3], [4],[4]]),\n...     \"z\": np.array([[3], [4], [5],[4]]),\n...     \"u\": np.array([[4], [5], [6],[4]]),\n...     \"v\": np.array([[5], [6], [7],[4]]),\n... }\n&gt;&gt;&gt; coord_keys = (\"x\",\"y\",\"z\")\n&gt;&gt;&gt; value_keys = (\"u\",\"v\")\n&gt;&gt;&gt; ppsci.visualize.save_vtu_from_dict(filename, data_dict, coord_keys, value_keys)\n</code></pre> Source code in <code>ppsci/visualize/vtu.py</code> <pre><code>def save_vtu_from_dict(\n    filename: str,\n    data_dict: Dict[str, np.ndarray],\n    coord_keys: Tuple[str, ...],\n    value_keys: Tuple[str, ...],\n    num_timestamps: int = 1,\n):\n    \"\"\"Save dict data to '*.vtu' file.\n\n    Args:\n        filename (str): Output filename.\n        data_dict (Dict[str, np.ndarray]): Data in dict.\n        coord_keys (Tuple[str, ...]): Tuple of coord key. such as (\"x\", \"y\").\n        value_keys (Tuple[str, ...]): Tuple of value key. such as (\"u\", \"v\").\n        num_timestamps (int, optional): Number of timestamp in data_dict. Defaults to 1.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; filename = \"path/to/file.vtu\"\n        &gt;&gt;&gt; data_dict = {\n        ...     \"x\": np.array([[1], [2], [3],[4]]),\n        ...     \"y\": np.array([[2], [3], [4],[4]]),\n        ...     \"z\": np.array([[3], [4], [5],[4]]),\n        ...     \"u\": np.array([[4], [5], [6],[4]]),\n        ...     \"v\": np.array([[5], [6], [7],[4]]),\n        ... }\n        &gt;&gt;&gt; coord_keys = (\"x\",\"y\",\"z\")\n        &gt;&gt;&gt; value_keys = (\"u\",\"v\")\n        &gt;&gt;&gt; ppsci.visualize.save_vtu_from_dict(filename, data_dict, coord_keys, value_keys) # doctest: +SKIP\n    \"\"\"\n    if len(coord_keys) not in [2, 3, 4]:\n        raise ValueError(f\"ndim of coord ({len(coord_keys)}) should be 2, 3 or 4\")\n\n    coord = [data_dict[k] for k in coord_keys if k not in (\"t\", \"sdf\")]\n    value = [data_dict[k] for k in value_keys] if value_keys else None\n\n    coord = np.concatenate(coord, axis=1)\n\n    if value is not None:\n        value = np.concatenate(value, axis=1)\n\n    _save_vtu_from_array(filename, coord, value, value_keys, num_timestamps)\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.save_vtu_to_mesh","title":"<code>save_vtu_to_mesh(filename, data_dict, coord_keys, value_keys)</code>","text":"<p>Save data into .vtu format by meshio.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>File name.</p> required <code>data_dict</code> <code>Dict[str, ndarray]</code> <p>Data in dict.</p> required <code>coord_keys</code> <code>Tuple[str, ...]</code> <p>Tuple of coord key. such as (\"x\", \"y\").</p> required <code>value_keys</code> <code>Tuple[str, ...]</code> <p>Tuple of value key. such as (\"u\", \"v\").</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; filename = \"path/to/file.vtu\"\n&gt;&gt;&gt; data_dict = {\n...     \"x\": np.array([[1], [2], [3],[4]]),\n...     \"y\": np.array([[2], [3], [4],[4]]),\n...     \"z\": np.array([[3], [4], [5],[4]]),\n...     \"u\": np.array([[4], [5], [6],[4]]),\n...     \"v\": np.array([[5], [6], [7],[4]]),\n... }\n&gt;&gt;&gt; coord_keys = (\"x\",\"y\",\"z\")\n&gt;&gt;&gt; value_keys = (\"u\",\"v\")\n&gt;&gt;&gt; ppsci.visualize.save_vtu_to_mesh(filename, data_dict, coord_keys, value_keys)\n</code></pre> Source code in <code>ppsci/visualize/vtu.py</code> <pre><code>def save_vtu_to_mesh(\n    filename: str,\n    data_dict: Dict[str, np.ndarray],\n    coord_keys: Tuple[str, ...],\n    value_keys: Tuple[str, ...],\n):\n    \"\"\"Save data into .vtu format by meshio.\n\n    Args:\n        filename (str): File name.\n        data_dict (Dict[str, np.ndarray]): Data in dict.\n        coord_keys (Tuple[str, ...]): Tuple of coord key. such as (\"x\", \"y\").\n        value_keys (Tuple[str, ...]): Tuple of value key. such as (\"u\", \"v\").\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; filename = \"path/to/file.vtu\"\n        &gt;&gt;&gt; data_dict = {\n        ...     \"x\": np.array([[1], [2], [3],[4]]),\n        ...     \"y\": np.array([[2], [3], [4],[4]]),\n        ...     \"z\": np.array([[3], [4], [5],[4]]),\n        ...     \"u\": np.array([[4], [5], [6],[4]]),\n        ...     \"v\": np.array([[5], [6], [7],[4]]),\n        ... }\n        &gt;&gt;&gt; coord_keys = (\"x\",\"y\",\"z\")\n        &gt;&gt;&gt; value_keys = (\"u\",\"v\")\n        &gt;&gt;&gt; ppsci.visualize.save_vtu_to_mesh(filename, data_dict, coord_keys, value_keys) # doctest: +SKIP\n    \"\"\"\n    npoint = len(next(iter(data_dict.values())))\n    coord_ndim = len(coord_keys)\n\n    # get the list variable transposed\n    points = np.stack(tuple(data_dict[key] for key in coord_keys)).reshape(\n        coord_ndim, npoint\n    )\n    mesh = meshio.Mesh(\n        points=points.T, cells=[(\"vertex\", np.arange(npoint).reshape(npoint, 1))]\n    )\n    mesh.point_data = {key: data_dict[key] for key in value_keys}\n    if len(os.path.dirname(filename)):\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\n    mesh.write(filename)\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.save_plot_from_1d_dict","title":"<code>save_plot_from_1d_dict(filename, data_dict, coord_keys, value_keys, num_timestamps=1)</code>","text":"<p>Plot dict data as file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Output filename.</p> required <code>data_dict</code> <code>Dict[str, Union[ndarray, Tensor]]</code> <p>Data in dict.</p> required <code>coord_keys</code> <code>Tuple[str, ...]</code> <p>Tuple of coord key. such as (\"x\", \"y\").</p> required <code>value_keys</code> <code>Tuple[str, ...]</code> <p>Tuple of value key. such as (\"u\", \"v\").</p> required <code>num_timestamps</code> <code>int</code> <p>Number of timestamp in data_dict. Defaults to 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; filename = \"path/to/file\"\n&gt;&gt;&gt; data_dict = {\n...     \"x\": np.array([[1], [2], [3],[4]]),\n...     \"u\": np.array([[4], [5], [6],[4]]),\n... }\n&gt;&gt;&gt; coord_keys = (\"x\",)\n&gt;&gt;&gt; value_keys = (\"u\",)\n&gt;&gt;&gt; ppsci.visualize.save_plot_from_1d_dict(filename, data_dict, coord_keys, value_keys)\n</code></pre> Source code in <code>ppsci/visualize/plot.py</code> <pre><code>def save_plot_from_1d_dict(\n    filename, data_dict, coord_keys, value_keys, num_timestamps=1\n):\n    \"\"\"Plot dict data as file.\n\n    Args:\n        filename (str): Output filename.\n        data_dict (Dict[str, Union[np.ndarray, paddle.Tensor]]): Data in dict.\n        coord_keys (Tuple[str, ...]): Tuple of coord key. such as (\"x\", \"y\").\n        value_keys (Tuple[str, ...]): Tuple of value key. such as (\"u\", \"v\").\n        num_timestamps (int, optional): Number of timestamp in data_dict. Defaults to 1.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; filename = \"path/to/file\"\n        &gt;&gt;&gt; data_dict = {\n        ...     \"x\": np.array([[1], [2], [3],[4]]),\n        ...     \"u\": np.array([[4], [5], [6],[4]]),\n        ... }\n        &gt;&gt;&gt; coord_keys = (\"x\",)\n        &gt;&gt;&gt; value_keys = (\"u\",)\n        &gt;&gt;&gt; ppsci.visualize.save_plot_from_1d_dict(filename, data_dict, coord_keys, value_keys) # doctest: +SKIP\n    \"\"\"\n    space_ndim = len(coord_keys) - int(\"t\" in coord_keys)\n    if space_ndim not in [1, 2, 3]:\n        raise ValueError(f\"ndim of space coord ({space_ndim}) should be 1, 2 or 3\")\n\n    coord = [data_dict[k] for k in coord_keys if k != \"t\"]\n    value = [data_dict[k] for k in value_keys] if value_keys else None\n\n    if isinstance(coord[0], paddle.Tensor):\n        coord = [x.numpy() for x in coord]\n    else:\n        coord = [x for x in coord]\n    coord = np.concatenate(coord, axis=1)\n\n    if value is not None:\n        if isinstance(value[0], paddle.Tensor):\n            value = [x.numpy() for x in value]\n        else:\n            value = [x for x in value]\n        value = np.concatenate(value, axis=1)\n\n    _save_plot_from_1d_array(filename, coord, value, value_keys, num_timestamps)\n</code></pre>"},{"location":"zh/api/visualize/#ppsci.visualize.save_plot_from_3d_dict","title":"<code>save_plot_from_3d_dict(filename, data_dict, visu_keys, num_timestamps=1)</code>","text":"<p>Plot dict data as file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Output filename.</p> required <code>data_dict</code> <code>Dict[str, Union[ndarray, Tensor]]</code> <p>Data in dict.</p> required <code>visu_keys</code> <code>Tuple[str, ...]</code> <p>Keys for visualizing data. such as (\"u\", \"v\").</p> required <code>num_timestamps</code> <code>int</code> <p>Number of timestamp in data_dict. Defaults to 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n</code></pre> <pre><code>&gt;&gt;&gt; data_dict = {\n...     \"u\": np.array([[[10], [20], [30], [40], [50]]]),\n...     \"v\": np.array([[[5], [15], [25], [35], [45]]]),\n... }\n</code></pre> <pre><code>&gt;&gt;&gt; ppsci.visualize.save_plot_from_3d_dict(\n...     \"path/to/file\",\n...     data_dict,\n...     (\"u\", \"v\"),\n...     1,\n... )\n</code></pre> Source code in <code>ppsci/visualize/plot.py</code> <pre><code>def save_plot_from_3d_dict(\n    filename: str,\n    data_dict: Dict[str, Union[np.ndarray, paddle.Tensor]],\n    visu_keys: Tuple[str, ...],\n    num_timestamps: int = 1,\n):\n    \"\"\"Plot dict data as file.\n\n    Args:\n        filename (str): Output filename.\n        data_dict (Dict[str, Union[np.ndarray, paddle.Tensor]]): Data in dict.\n        visu_keys (Tuple[str, ...]): Keys for visualizing data. such as (\"u\", \"v\").\n        num_timestamps (int, optional): Number of timestamp in data_dict. Defaults to 1.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n\n        &gt;&gt;&gt; data_dict = {\n        ...     \"u\": np.array([[[10], [20], [30], [40], [50]]]),\n        ...     \"v\": np.array([[[5], [15], [25], [35], [45]]]),\n        ... }\n\n        &gt;&gt;&gt; ppsci.visualize.save_plot_from_3d_dict(\n        ...     \"path/to/file\",\n        ...     data_dict,\n        ...     (\"u\", \"v\"),\n        ...     1,\n        ... ) # doctest: +SKIP\n    \"\"\"\n    visu_data = [data_dict[k] for k in visu_keys]\n    if isinstance(visu_data[0], paddle.Tensor):\n        visu_data = [x.numpy() for x in visu_data]\n\n    _save_plot_from_3d_array(filename, visu_data, visu_keys, num_timestamps)\n</code></pre>"},{"location":"zh/api/data/dataset/","title":"ppsci.data.dataset","text":""},{"location":"zh/api/data/dataset/#datadataset","title":"Data.dataset(\u6570\u636e\u96c6) \u6a21\u5757","text":""},{"location":"zh/api/data/dataset/#ppsci.data.dataset","title":"<code>ppsci.data.dataset</code>","text":""},{"location":"zh/api/data/dataset/#ppsci.data.dataset.IterableNamedArrayDataset","title":"<code>IterableNamedArrayDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>IterableNamedArrayDataset for full-data loading.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>label</code> <code>Optional[Dict[str, ndarray]]</code> <p>Label dict. Defaults to None.</p> <code>None</code> <code>weight</code> <code>Optional[Dict[str, ndarray]]</code> <p>Weight dict. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; input = {\"x\": np.random.randn(100, 1)}\n&gt;&gt;&gt; label = {\"u\": np.random.randn(100, 1)}\n&gt;&gt;&gt; weight = {\"u\": np.random.randn(100, 1)}\n&gt;&gt;&gt; dataset = ppsci.data.dataset.IterableNamedArrayDataset(input, label, weight)\n</code></pre> Source code in <code>ppsci/data/dataset/array_dataset.py</code> <pre><code>class IterableNamedArrayDataset(io.IterableDataset):\n    \"\"\"IterableNamedArrayDataset for full-data loading.\n\n    Args:\n        input (Dict[str, np.ndarray]): Input dict.\n        label (Optional[Dict[str, np.ndarray]]): Label dict. Defaults to None.\n        weight (Optional[Dict[str, np.ndarray]]): Weight dict. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; input = {\"x\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; label = {\"u\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; weight = {\"u\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.IterableNamedArrayDataset(input, label, weight)\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        input: Dict[str, np.ndarray],\n        label: Optional[Dict[str, np.ndarray]] = None,\n        weight: Optional[Dict[str, np.ndarray]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input = {key: paddle.to_tensor(value) for key, value in input.items()}\n        self.label = (\n            {key: paddle.to_tensor(value) for key, value in label.items()}\n            if label is not None\n            else {}\n        )\n        self.input_keys = tuple(input.keys())\n        self.label_keys = tuple(self.label.keys())\n        self.weight = (\n            {\n                key: paddle.to_tensor(value, paddle.get_default_dtype())\n                for key, value in weight.items()\n            }\n            if weight is not None\n            else None\n        )\n        self._len = len(next(iter(self.input.values())))\n        self.transforms = transforms\n\n    @property\n    def num_samples(self):\n        \"\"\"Number of samples within current dataset.\"\"\"\n        return self._len\n\n    def __iter__(self):\n        if callable(self.transforms):\n            input_, label_, weight_ = self.transforms(\n                self.input, self.label, self.weight\n            )\n            yield input_, label_, weight_\n        else:\n            yield self.input, self.label, self.weight\n\n    def __len__(self):\n        return 1\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.IterableNamedArrayDataset.num_samples","title":"<code>num_samples</code>  <code>property</code>","text":"<p>Number of samples within current dataset.</p>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.NamedArrayDataset","title":"<code>NamedArrayDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Class for Named Array Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>label</code> <code>Optional[Dict[str, ndarray]]</code> <p>Label dict. Defaults to None.</p> <code>None</code> <code>weight</code> <code>Optional[Dict[str, ndarray]]</code> <p>Weight dict. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; input = {\"x\": np.random.randn(100, 1)}\n&gt;&gt;&gt; output = {\"u\": np.random.randn(100, 1)}\n&gt;&gt;&gt; weight = {\"u\": np.random.randn(100, 1)}\n&gt;&gt;&gt; dataset = ppsci.data.dataset.NamedArrayDataset(input, output, weight)\n</code></pre> Source code in <code>ppsci/data/dataset/array_dataset.py</code> <pre><code>class NamedArrayDataset(io.Dataset):\n    \"\"\"Class for Named Array Dataset.\n\n    Args:\n        input (Dict[str, np.ndarray]): Input dict.\n        label (Optional[Dict[str, np.ndarray]]): Label dict. Defaults to None.\n        weight (Optional[Dict[str, np.ndarray]]): Weight dict. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; input = {\"x\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; output = {\"u\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; weight = {\"u\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.NamedArrayDataset(input, output, weight)\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = True\n\n    def __init__(\n        self,\n        input: Dict[str, np.ndarray],\n        label: Optional[Dict[str, np.ndarray]] = None,\n        weight: Optional[Dict[str, np.ndarray]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input = input\n        self.label = {} if label is None else label\n        self.input_keys = tuple(input.keys())\n        self.label_keys = tuple(self.label.keys())\n        self.weight = {} if weight is None else weight\n        self.transforms = transforms\n        self._len = len(next(iter(input.values())))\n        for key in input:\n            if key in self.label and len(input[key]) != len(self.label[key]):\n                logger.warning(\n                    f\"The length of input {key}({len(input[key])}) is not equal to \"\n                    f\"the length of label {key}({len(self.label[key])}).\"\n                )\n\n    def __getitem__(self, idx):\n        input_item = {key: value[idx] for key, value in self.input.items()}\n        label_item = {key: value[idx] for key, value in self.label.items()}\n        weight_item = {key: value[idx] for key, value in self.weight.items()}\n\n        if self.transforms is not None:\n            input_item, label_item, weight_item = self.transforms(\n                input_item, label_item, weight_item\n            )\n\n        return (input_item, label_item, weight_item)\n\n    def __len__(self):\n        return self._len\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.ChipHeatDataset","title":"<code>ChipHeatDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>ChipHeatDataset for data loading of multi-branch DeepONet model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dict[str, ndarray]</code> <p>Input dict.</p> required <code>label</code> <code>Optional[Dict[str, ndarray]]</code> <p>Label dict. Defaults to None.</p> required <code>index</code> <code>tuple[str, ...]</code> <p>Key of input dict.</p> required <code>data_type</code> <code>str</code> <p>One of key of input dict.</p> required <code>weight</code> <code>Optional[Dict[str, ndarray]]</code> <p>Weight dict. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; input = {\"x\": np.random.randn(100, 1)}\n&gt;&gt;&gt; label = {\"u\": np.random.randn(100, 1)}\n&gt;&gt;&gt; index = ('x', 'u', 'bc', 'bc_data')\n&gt;&gt;&gt; data_type = 'u'\n&gt;&gt;&gt; weight = {\"u\": np.random.randn(100, 1)}\n&gt;&gt;&gt; dataset = ppsci.data.dataset.ChipHeatDataset(input, label, index, data_type, weight)\n</code></pre> Source code in <code>ppsci/data/dataset/array_dataset.py</code> <pre><code>class ChipHeatDataset(io.Dataset):\n    \"\"\"ChipHeatDataset for data loading of multi-branch DeepONet model.\n\n    Args:\n        input (Dict[str, np.ndarray]): Input dict.\n        label (Optional[Dict[str, np.ndarray]]): Label dict. Defaults to None.\n        index (tuple[str, ...]): Key of input dict.\n        data_type (str): One of key of input dict.\n        weight (Optional[Dict[str, np.ndarray]]): Weight dict. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; input = {\"x\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; label = {\"u\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; index = ('x', 'u', 'bc', 'bc_data')\n        &gt;&gt;&gt; data_type = 'u'\n        &gt;&gt;&gt; weight = {\"u\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.ChipHeatDataset(input, label, index, data_type, weight)\n    \"\"\"\n\n    def __init__(\n        self,\n        input: Dict[str, np.ndarray],\n        label: Dict[str, np.ndarray],\n        index: tuple[str, ...],\n        data_type: str,\n        weight: Optional[Dict[str, float]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input = input\n        self.label = label\n        self.input_keys = tuple(input.keys())\n        self.label_keys = tuple(label.keys())\n        self.index = index\n        self.data_type = data_type\n        self.weight = {} if weight is None else weight\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        quotient = idx\n        index_ir = dict()\n        for i in self.index:\n            index_ir[i] = 0\n\n        for i in index_ir:\n            num = len(self.input[i])\n            index_ir[i] = quotient % num\n            quotient = quotient // num\n\n        input_item = {}\n        for key in self.input:\n            if key == \"y\":\n                input_item[key] = self.input[key][index_ir[\"x\"]]\n            elif key == \"u_one\":\n                input_item[key] = self.input[key][\n                    len(self.input[self.data_type]) * index_ir[\"x\"]\n                    + index_ir[self.data_type]\n                ]\n            else:\n                input_item[key] = self.input[key][index_ir[key]]\n\n        label_item = {key: value for key, value in self.label.items()}\n        weight_item = {key: value for key, value in self.weight.items()}\n\n        if self.transforms is not None:\n            input_item, label_item, weight_item = self.transforms(\n                (input_item, label_item, weight_item)\n            )\n\n        return (input_item, label_item, weight_item)\n\n    def __len__(self):\n        _len = 1\n        for i in self.index:\n            _len *= len(self.input[i])\n        return _len\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.CSVDataset","title":"<code>CSVDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for .csv file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>CSV file path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>List of input keys.</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>List of label keys.</p> required <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Dict of alias(es) for input and label keys. i.e. {inner_key: outer_key}. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, Union[Callable, float]]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>Optional[Tuple[float, ...]]</code> <p>The number of repetitions of the data in the time dimension. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.CSVDataset(\n...     \"/path/to/file.csv\",\n...     (\"x\",),\n...     (\"u\",),\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/csv_dataset.py</code> <pre><code>class CSVDataset(io.Dataset):\n    \"\"\"Dataset class for .csv file.\n\n    Args:\n        file_path (str): CSV file path.\n        input_keys (Tuple[str, ...]): List of input keys.\n        label_keys (Tuple[str, ...]): List of label keys.\n        alias_dict (Optional[Dict[str, str]]): Dict of alias(es) for input and label keys.\n            i.e. {inner_key: outer_key}. Defaults to None.\n        weight_dict (Optional[Dict[str, Union[Callable, float]]]): Define the weight of\n            each constraint variable. Defaults to None.\n        timestamps (Optional[Tuple[float, ...]]): The number of repetitions of the data\n            in the time dimension. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.CSVDataset(\n        ...     \"/path/to/file.csv\",\n        ...     (\"x\",),\n        ...     (\"u\",),\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = True\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        alias_dict: Optional[Dict[str, str]] = None,\n        weight_dict: Optional[Dict[str, Union[Callable, float]]] = None,\n        timestamps: Optional[Tuple[float, ...]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n\n        # read raw data from file\n        raw_data = reader.load_csv_file(\n            file_path,\n            input_keys + label_keys,\n            alias_dict,\n        )\n        # filter raw data by given timestamps if specified\n        if timestamps is not None:\n            if \"t\" in raw_data:\n                # filter data according to given timestamps\n                raw_time_array = raw_data[\"t\"]\n                mask = []\n                for ti in timestamps:\n                    mask.append(np.nonzero(np.isclose(raw_time_array, ti).flatten())[0])\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                mask = np.concatenate(mask, 0)\n                raw_data = raw_data[mask]\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n            else:\n                # repeat data according to given timestamps\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                raw_data = misc.combine_array_with_time(raw_data, timestamps)\n                self.input_keys = (\"t\",) + tuple(self.input_keys)\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n\n        # fetch input data\n        self.input = {\n            key: value for key, value in raw_data.items() if key in self.input_keys\n        }\n        # fetch label data\n        self.label = {\n            key: value for key, value in raw_data.items() if key in self.label_keys\n        }\n\n        # prepare weights\n        self.weight = (\n            {key: np.ones_like(next(iter(self.label.values()))) for key in self.label}\n            if weight_dict is not None\n            else {}\n        )\n        if weight_dict is not None:\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    self.weight[key] = np.full_like(\n                        next(iter(self.label.values())), value\n                    )\n                elif callable(value):\n                    func = value\n                    self.weight[key] = func(self.input)\n                    if isinstance(self.weight[key], (int, float)):\n                        self.weight[key] = np.full_like(\n                            next(iter(self.label.values())), self.weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        self.transforms = transforms\n        self._len = len(next(iter(self.input.values())))\n\n    def __getitem__(self, idx):\n        input_item = {key: value[idx] for key, value in self.input.items()}\n        label_item = {key: value[idx] for key, value in self.label.items()}\n        weight_item = {key: value[idx] for key, value in self.weight.items()}\n\n        if self.transforms is not None:\n            input_item, label_item, weight_item = self.transforms(\n                input_item, label_item, weight_item\n            )\n\n        return (input_item, label_item, weight_item)\n\n    def __len__(self):\n        return self._len\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.IterableCSVDataset","title":"<code>IterableCSVDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>IterableCSVDataset for full-data loading.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>CSV file path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>List of input keys.</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>List of label keys.</p> required <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Dict of alias(es) for input and label keys. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, Union[Callable, float]]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>Optional[Tuple[float, ...]]</code> <p>The number of repetitions of the data in the time dimension. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.IterableCSVDataset(\n...     \"/path/to/file.csv\"\n...     (\"x\",),\n...     (\"u\",),\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/csv_dataset.py</code> <pre><code>class IterableCSVDataset(io.IterableDataset):\n    \"\"\"IterableCSVDataset for full-data loading.\n\n    Args:\n        file_path (str): CSV file path.\n        input_keys (Tuple[str, ...]): List of input keys.\n        label_keys (Tuple[str, ...]): List of label keys.\n        alias_dict (Optional[Dict[str, str]]): Dict of alias(es) for input and label keys.\n            Defaults to None.\n        weight_dict (Optional[Dict[str, Union[Callable, float]]]): Define the weight of\n            each constraint variable. Defaults to None.\n        timestamps (Optional[Tuple[float, ...]]): The number of repetitions of the data\n            in the time dimension. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.IterableCSVDataset(\n        ...     \"/path/to/file.csv\"\n        ...     (\"x\",),\n        ...     (\"u\",),\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        alias_dict: Optional[Dict[str, str]] = None,\n        weight_dict: Optional[Dict[str, Union[Callable, float]]] = None,\n        timestamps: Optional[Tuple[float, ...]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n\n        # read raw data from file\n        raw_data = reader.load_csv_file(\n            file_path,\n            input_keys + label_keys,\n            alias_dict,\n        )\n        # filter raw data by given timestamps if specified\n        if timestamps is not None:\n            if \"t\" in raw_data:\n                # filter data according to given timestamps\n                raw_time_array = raw_data[\"t\"]\n                mask = []\n                for ti in timestamps:\n                    mask.append(np.nonzero(np.isclose(raw_time_array, ti).flatten())[0])\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                mask = np.concatenate(mask, 0)\n                raw_data = raw_data[mask]\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n            else:\n                # repeat data according to given timestamps\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                raw_data = misc.combine_array_with_time(raw_data, timestamps)\n                self.input_keys = (\"t\",) + tuple(self.input_keys)\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n\n        # fetch input data\n        self.input = {\n            key: value for key, value in raw_data.items() if key in self.input_keys\n        }\n        # fetch label data\n        self.label = {\n            key: value for key, value in raw_data.items() if key in self.label_keys\n        }\n\n        # prepare weights\n        self.weight = (\n            {key: np.ones_like(next(iter(self.label.values()))) for key in self.label}\n            if weight_dict is not None\n            else {}\n        )\n        if weight_dict is not None:\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    self.weight[key] = np.full_like(\n                        next(iter(self.label.values())), value\n                    )\n                elif callable(value):\n                    func = value\n                    self.weight[key] = func(self.input)\n                    if isinstance(self.weight[key], (int, float)):\n                        self.weight[key] = np.full_like(\n                            next(iter(self.label.values())), self.weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        self.input = {key: paddle.to_tensor(value) for key, value in self.input.items()}\n        self.label = {key: paddle.to_tensor(value) for key, value in self.label.items()}\n        self.weight = {\n            key: paddle.to_tensor(value) for key, value in self.weight.items()\n        }\n\n        self.transforms = transforms\n        self._len = len(next(iter(self.input.values())))\n\n    @property\n    def num_samples(self):\n        \"\"\"Number of samples within current dataset.\"\"\"\n        return self._len\n\n    def __iter__(self):\n        if callable(self.transforms):\n            input_, label_, weight_ = self.transforms(\n                self.input, self.label, self.weight\n            )\n            yield input_, label_, weight_\n        else:\n            yield self.input, self.label, self.weight\n\n    def __len__(self):\n        return 1\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.IterableCSVDataset.num_samples","title":"<code>num_samples</code>  <code>property</code>","text":"<p>Number of samples within current dataset.</p>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.ContinuousNamedArrayDataset","title":"<code>ContinuousNamedArrayDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>ContinuousNamedArrayDataset for iterable sampling.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Callable</code> <p>Function generate input dict.</p> required <code>label</code> <code>Callable</code> <p>Function generate label dict.</p> required <code>weight</code> <code>Optional[Callable]</code> <p>Function generate weight dict. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; input = lambda : {\"x\": np.random.randn(100, 1)}\n&gt;&gt;&gt; label = lambda inp: {\"u\": np.random.randn(100, 1)}\n&gt;&gt;&gt; weight = lambda inp, label: {\"u\": 1 - (label[\"u\"] ** 2)}\n&gt;&gt;&gt; dataset = ppsci.data.dataset.ContinuousNamedArrayDataset(input, label, weight)\n&gt;&gt;&gt; input_batch, label_batch, weight_batch = next(iter(dataset))\n&gt;&gt;&gt; print(input_batch[\"x\"].shape)\n[100, 1]\n&gt;&gt;&gt; print(label_batch[\"u\"].shape)\n[100, 1]\n&gt;&gt;&gt; print(weight_batch[\"u\"].shape)\n[100, 1]\n</code></pre> Source code in <code>ppsci/data/dataset/array_dataset.py</code> <pre><code>class ContinuousNamedArrayDataset(io.IterableDataset):\n    \"\"\"ContinuousNamedArrayDataset for iterable sampling.\n\n    Args:\n        input (Callable): Function generate input dict.\n        label (Callable): Function generate label dict.\n        weight (Optional[Callable]): Function generate weight dict. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; input = lambda : {\"x\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; label = lambda inp: {\"u\": np.random.randn(100, 1)}\n        &gt;&gt;&gt; weight = lambda inp, label: {\"u\": 1 - (label[\"u\"] ** 2)}\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.ContinuousNamedArrayDataset(input, label, weight)\n        &gt;&gt;&gt; input_batch, label_batch, weight_batch = next(iter(dataset))\n        &gt;&gt;&gt; print(input_batch[\"x\"].shape)\n        [100, 1]\n        &gt;&gt;&gt; print(label_batch[\"u\"].shape)\n        [100, 1]\n        &gt;&gt;&gt; print(weight_batch[\"u\"].shape)\n        [100, 1]\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        input: Callable,\n        label: Callable,\n        weight: Optional[Callable] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input_fn = input\n        self.input_keys = tuple(self.input_fn().keys())\n\n        self.label_fn = label\n        input_ = self.input_fn()\n        self.label_keys = tuple(self.label_fn(input_).keys())\n\n        self.weight_fn = weight\n        self.transforms = transforms\n\n    @property\n    def num_samples(self):\n        \"\"\"Number of samples within current dataset.\"\"\"\n        raise NotImplementedError(\n            \"ContinuousNamedArrayDataset has no fixed number of samples.\"\n        )\n\n    def __iter__(self):\n        def to_tensor_dict(_dict):\n            if _dict is None:\n                return None\n            return {k: paddle.to_tensor(v) for k, v in _dict.items()}\n\n        while True:\n            input_batch = self.input_fn()\n            label_batch = self.label_fn(input_batch)\n            if callable(self.weight_fn):\n                weight_batch = self.weight_fn(input_batch, label_batch)\n            else:\n                weight_batch = None\n\n            if callable(self.transforms):\n                input_batch, label_batch, weight_batch = self.transforms(\n                    input_batch, label_batch, weight_batch\n                )\n            yield to_tensor_dict(input_batch), to_tensor_dict(\n                label_batch\n            ), to_tensor_dict(weight_batch)\n\n    def __len__(self):\n        return 1\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.ContinuousNamedArrayDataset.num_samples","title":"<code>num_samples</code>  <code>property</code>","text":"<p>Number of samples within current dataset.</p>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.ERA5Dataset","title":"<code>ERA5Dataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Class for ERA5 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Data set path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"input\",).</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"output\",).</p> required <code>precip_file_path</code> <code>Optional[str]</code> <p>Precipitation data set path. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, float]]</code> <p>Weight dictionary. Defaults to None.</p> <code>None</code> <code>vars_channel</code> <code>Optional[Tuple[int, ...]]</code> <p>The variable channel index in ERA5 dataset. Defaults to None.</p> <code>None</code> <code>num_label_timestamps</code> <code>int</code> <p>Number of timestamp of label. Defaults to 1.</p> <code>1</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <code>training</code> <code>bool</code> <p>Whether in train mode. Defaults to True.</p> <code>True</code> <code>stride</code> <code>int</code> <p>Stride of sampling data. Defaults to 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.ERA5Dataset(\n...     \"file_path\": \"/path/to/ERA5Dataset\",\n...     \"input_keys\": (\"input\",),\n...     \"label_keys\": (\"output\",),\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/era5_dataset.py</code> <pre><code>class ERA5Dataset(io.Dataset):\n    \"\"\"Class for ERA5 dataset.\n\n    Args:\n        file_path (str): Data set path.\n        input_keys (Tuple[str, ...]): Input keys, such as (\"input\",).\n        label_keys (Tuple[str, ...]): Output keys, such as (\"output\",).\n        precip_file_path (Optional[str]): Precipitation data set path. Defaults to None.\n        weight_dict (Optional[Dict[str, float]]): Weight dictionary. Defaults to None.\n        vars_channel (Optional[Tuple[int, ...]]): The variable channel index in ERA5 dataset. Defaults to None.\n        num_label_timestamps (int, optional): Number of timestamp of label. Defaults to 1.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n        training (bool, optional): Whether in train mode. Defaults to True.\n        stride (int, optional): Stride of sampling data. Defaults to 1.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.ERA5Dataset(\n        ...     \"file_path\": \"/path/to/ERA5Dataset\",\n        ...     \"input_keys\": (\"input\",),\n        ...     \"label_keys\": (\"output\",),\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        precip_file_path: Optional[str] = None,\n        weight_dict: Optional[Dict[str, float]] = None,\n        vars_channel: Optional[Tuple[int, ...]] = None,\n        num_label_timestamps: int = 1,\n        transforms: Optional[vision.Compose] = None,\n        training: bool = True,\n        stride: int = 1,\n    ):\n        super().__init__()\n        self.file_path = file_path\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n        self.precip_file_path = precip_file_path\n\n        self.weight_dict = {} if weight_dict is None else weight_dict\n        if weight_dict is not None:\n            self.weight_dict = {key: 1.0 for key in self.label_keys}\n            self.weight_dict.update(weight_dict)\n\n        self.vars_channel = list(range(20)) if vars_channel is None else vars_channel\n        self.num_label_timestamps = num_label_timestamps\n        self.transforms = transforms\n        self.training = training\n        self.stride = stride\n\n        self.files = self.read_data(file_path)\n        self.n_years = len(self.files)\n        self.num_samples_per_year = self.files[0].shape[0]\n        self.num_samples = self.n_years * self.num_samples_per_year\n        if self.precip_file_path is not None:\n            self.precip_files = self.read_data(precip_file_path, \"tp\")\n\n    def read_data(self, path: str, var=\"fields\"):\n        paths = [path] if path.endswith(\".h5\") else glob.glob(path + \"/*.h5\")\n        paths.sort()\n        files = []\n        for path_ in paths:\n            _file = h5py.File(path_, \"r\")\n            files.append(_file[var])\n        return files\n\n    def __len__(self):\n        return self.num_samples // self.stride\n\n    def __getitem__(self, global_idx):\n        global_idx *= self.stride\n        year_idx = global_idx // self.num_samples_per_year\n        local_idx = global_idx % self.num_samples_per_year\n        step = 0 if local_idx &gt;= self.num_samples_per_year - 1 else 1\n\n        if self.num_label_timestamps &gt; 1:\n            if local_idx &gt;= self.num_samples_per_year - self.num_label_timestamps:\n                local_idx = self.num_samples_per_year - self.num_label_timestamps - 1\n\n        input_file = self.files[year_idx]\n        label_file = (\n            self.precip_files[year_idx]\n            if self.precip_file_path is not None\n            else input_file\n        )\n        if self.precip_file_path is not None and year_idx == 0 and self.training:\n            # first year has 2 missing samples in precip (they are first two time points)\n            lim = self.num_samples_per_year - 2\n            local_idx = local_idx % lim\n            step = 0 if local_idx &gt;= lim - 1 else 1\n            input_idx = local_idx + 2\n            label_idx = local_idx + step\n        else:\n            input_idx, label_idx = local_idx, local_idx + step\n\n        input_item = {self.input_keys[0]: input_file[input_idx, self.vars_channel]}\n\n        label_item = {}\n        for i in range(self.num_label_timestamps):\n            if self.precip_file_path is not None:\n                label_item[self.label_keys[i]] = np.expand_dims(\n                    label_file[label_idx + i], 0\n                )\n            else:\n                label_item[self.label_keys[i]] = label_file[\n                    label_idx + i, self.vars_channel\n                ]\n\n        weight_shape = [1] * len(next(iter(label_item.values())).shape)\n        weight_item = {\n            key: np.full(weight_shape, value, paddle.get_default_dtype())\n            for key, value in self.weight_dict.items()\n        }\n\n        if self.transforms is not None:\n            input_item, label_item, weight_item = self.transforms(\n                input_item, label_item, weight_item\n            )\n\n        return input_item, label_item, weight_item\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.ERA5SampledDataset","title":"<code>ERA5SampledDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Class for ERA5 sampled dataset.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Data set path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"input\",).</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"output\",).</p> required <code>weight_dict</code> <code>Optional[Dict[str, float]]</code> <p>Weight dictionary. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.ERA5SampledDataset(\n...     \"file_path\": \"/path/to/ERA5SampledDataset\",\n...     \"input_keys\": (\"input\",),\n...     \"label_keys\": (\"output\",),\n... )\n&gt;&gt;&gt; # get the length of the dataset\n&gt;&gt;&gt; dataset_size = len(dataset)\n&gt;&gt;&gt; # get the first sample of the data\n&gt;&gt;&gt; first_sample = dataset[0]\n&gt;&gt;&gt; print(\"First sample:\", first_sample)\n</code></pre> Source code in <code>ppsci/data/dataset/era5_dataset.py</code> <pre><code>class ERA5SampledDataset(io.Dataset):\n    \"\"\"Class for ERA5 sampled dataset.\n\n    Args:\n        file_path (str): Data set path.\n        input_keys (Tuple[str, ...]): Input keys, such as (\"input\",).\n        label_keys (Tuple[str, ...]): Output keys, such as (\"output\",).\n        weight_dict (Optional[Dict[str, float]]): Weight dictionary. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.ERA5SampledDataset(\n        ...     \"file_path\": \"/path/to/ERA5SampledDataset\",\n        ...     \"input_keys\": (\"input\",),\n        ...     \"label_keys\": (\"output\",),\n        ... )  # doctest: +SKIP\n        &gt;&gt;&gt; # get the length of the dataset\n        &gt;&gt;&gt; dataset_size = len(dataset)  # doctest: +SKIP\n        &gt;&gt;&gt; # get the first sample of the data\n        &gt;&gt;&gt; first_sample = dataset[0]  # doctest: +SKIP\n        &gt;&gt;&gt; print(\"First sample:\", first_sample)  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        weight_dict: Optional[Dict[str, float]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.file_path = file_path\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n\n        self.weight_dict = {} if weight_dict is None else weight_dict\n        if weight_dict is not None:\n            self.weight_dict = {key: 1.0 for key in self.label_keys}\n            self.weight_dict.update(weight_dict)\n\n        self.transforms = transforms\n\n        self.files = self.read_data(file_path)\n        self.num_samples = len(self.files)\n\n    def read_data(self, path: str):\n        paths = glob.glob(path + \"/*.h5\")\n        paths.sort()\n        files = []\n        for _path in paths:\n            _file = h5py.File(_path, \"r\")\n            files.append(_file)\n        return files\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, global_idx):\n        _file = self.files[global_idx]\n\n        input_item = {}\n        for key in _file[\"input_dict\"]:\n            input_item[key] = np.asarray(\n                _file[\"input_dict\"][key], paddle.get_default_dtype()\n            )\n\n        label_item = {}\n        for key in _file[\"label_dict\"]:\n            label_item[key] = np.asarray(\n                _file[\"label_dict\"][key], paddle.get_default_dtype()\n            )\n\n        weight_shape = [1] * len(next(iter(label_item.values())).shape)\n        weight_item = {\n            key: np.full(weight_shape, value, paddle.get_default_dtype())\n            for key, value in self.weight_dict.items()\n        }\n\n        if self.transforms is not None:\n            input_item, label_item, weight_item = self.transforms(\n                input_item, label_item, weight_item\n            )\n\n        return input_item, label_item, weight_item\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.IterableMatDataset","title":"<code>IterableMatDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>IterableMatDataset for full-data loading.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Mat file path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>List of input keys.</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>List of label keys. Defaults to ().</p> <code>()</code> <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Dict of alias(es) for input and label keys. i.e. {inner_key: outer_key}. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, Union[Callable, float]]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>Optional[Tuple[float, ...]]</code> <p>The number of repetitions of the data in the time dimension. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.IterableMatDataset(\n...     \"/path/to/file.mat\"\n...     (\"x\",),\n...     (\"u\",),\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/mat_dataset.py</code> <pre><code>class IterableMatDataset(io.IterableDataset):\n    \"\"\"IterableMatDataset for full-data loading.\n\n    Args:\n        file_path (str): Mat file path.\n        input_keys (Tuple[str, ...]): List of input keys.\n        label_keys (Tuple[str, ...], optional): List of label keys. Defaults to ().\n        alias_dict (Optional[Dict[str, str]]): Dict of alias(es) for input and label keys.\n            i.e. {inner_key: outer_key}. Defaults to None.\n        weight_dict (Optional[Dict[str, Union[Callable, float]]]): Define the weight of\n            each constraint variable. Defaults to None.\n        timestamps (Optional[Tuple[float, ...]]): The number of repetitions of the data\n            in the time dimension. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.IterableMatDataset(\n        ...     \"/path/to/file.mat\"\n        ...     (\"x\",),\n        ...     (\"u\",),\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...] = (),\n        alias_dict: Optional[Dict[str, str]] = None,\n        weight_dict: Optional[Dict[str, Union[Callable, float]]] = None,\n        timestamps: Optional[Tuple[float, ...]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n\n        # read raw data from file\n        raw_data = reader.load_mat_file(\n            file_path,\n            input_keys + label_keys,\n            alias_dict,\n        )\n        # filter raw data by given timestamps if specified\n        if timestamps is not None:\n            if \"t\" in raw_data:\n                # filter data according to given timestamps\n                raw_time_array = raw_data[\"t\"]\n                mask = []\n                for ti in timestamps:\n                    mask.append(np.nonzero(np.isclose(raw_time_array, ti).flatten())[0])\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                mask = np.concatenate(mask, 0)\n                raw_data = raw_data[mask]\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n            else:\n                # repeat data according to given timestamps\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                raw_data = misc.combine_array_with_time(raw_data, timestamps)\n                self.input_keys = (\"t\",) + tuple(self.input_keys)\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n\n        # fetch input data\n        self.input = {\n            key: value for key, value in raw_data.items() if key in self.input_keys\n        }\n        # fetch label data\n        self.label = {\n            key: value for key, value in raw_data.items() if key in self.label_keys\n        }\n\n        # prepare weights\n        self.weight = (\n            {key: np.ones_like(next(iter(self.label.values()))) for key in self.label}\n            if weight_dict is not None\n            else {}\n        )\n        if weight_dict is not None:\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    self.weight[key] = np.full_like(\n                        next(iter(self.label.values())), value\n                    )\n                elif callable(value):\n                    func = value\n                    self.weight[key] = func(self.input)\n                    if isinstance(self.weight[key], (int, float)):\n                        self.weight[key] = np.full_like(\n                            next(iter(self.label.values())), self.weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        self.input = {key: paddle.to_tensor(value) for key, value in self.input.items()}\n        self.label = {key: paddle.to_tensor(value) for key, value in self.label.items()}\n        self.weight = {\n            key: paddle.to_tensor(value) for key, value in self.weight.items()\n        }\n\n        self.transforms = transforms\n        self._len = len(next(iter(self.input.values())))\n\n    @property\n    def num_samples(self):\n        \"\"\"Number of samples within current dataset.\"\"\"\n        return self._len\n\n    def __iter__(self):\n        if callable(self.transforms):\n            input_, label_, weight_ = self.transforms(\n                self.input, self.label, self.weight\n            )\n            yield input_, label_, weight_\n        else:\n            yield self.input, self.label, self.weight\n\n    def __len__(self):\n        return 1\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.IterableMatDataset.num_samples","title":"<code>num_samples</code>  <code>property</code>","text":"<p>Number of samples within current dataset.</p>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.MatDataset","title":"<code>MatDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for .mat file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Mat file path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>List of input keys.</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>List of label keys. Defaults to ().</p> <code>()</code> <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Dict of alias(es) for input and label keys. i.e. {inner_key: outer_key}. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, Union[Callable, float]]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>Optional[Tuple[float, ...]]</code> <p>The number of repetitions of the data in the time dimension. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.MatDataset(\n...     \"/path/to/file.mat\"\n...     (\"x\",),\n...     (\"u\",),\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/mat_dataset.py</code> <pre><code>class MatDataset(io.Dataset):\n    \"\"\"Dataset class for .mat file.\n\n    Args:\n        file_path (str): Mat file path.\n        input_keys (Tuple[str, ...]): List of input keys.\n        label_keys (Tuple[str, ...], optional): List of label keys. Defaults to ().\n        alias_dict (Optional[Dict[str, str]]): Dict of alias(es) for input and label keys.\n            i.e. {inner_key: outer_key}. Defaults to None.\n        weight_dict (Optional[Dict[str, Union[Callable, float]]]): Define the weight of\n            each constraint variable. Defaults to None.\n        timestamps (Optional[Tuple[float, ...]]): The number of repetitions of the data\n            in the time dimension. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.MatDataset(\n        ...     \"/path/to/file.mat\"\n        ...     (\"x\",),\n        ...     (\"u\",),\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = True\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...] = (),\n        alias_dict: Optional[Dict[str, str]] = None,\n        weight_dict: Optional[Dict[str, Union[Callable, float]]] = None,\n        timestamps: Optional[Tuple[float, ...]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n\n        # read raw data from file\n        raw_data = reader.load_mat_file(\n            file_path,\n            input_keys + label_keys,\n            alias_dict,\n        )\n        # filter raw data by given timestamps if specified\n        if timestamps is not None:\n            if \"t\" in raw_data:\n                # filter data according to given timestamps\n                raw_time_array = raw_data[\"t\"]\n                mask = []\n                for ti in timestamps:\n                    mask.append(np.nonzero(np.isclose(raw_time_array, ti).flatten())[0])\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                mask = np.concatenate(mask, 0)\n                raw_data = raw_data[mask]\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n            else:\n                # repeat data according to given timestamps\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                raw_data = misc.combine_array_with_time(raw_data, timestamps)\n                self.input_keys = (\"t\",) + tuple(self.input_keys)\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n\n        # fetch input data\n        self.input = {\n            key: value for key, value in raw_data.items() if key in self.input_keys\n        }\n        # fetch label data\n        self.label = {\n            key: value for key, value in raw_data.items() if key in self.label_keys\n        }\n\n        # prepare weights\n        self.weight = (\n            {key: np.ones_like(next(iter(self.label.values()))) for key in self.label}\n            if weight_dict is not None\n            else {}\n        )\n        if weight_dict is not None:\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    self.weight[key] = np.full_like(\n                        next(iter(self.label.values())), value\n                    )\n                elif callable(value):\n                    func = value\n                    self.weight[key] = func(self.input)\n                    if isinstance(self.weight[key], (int, float)):\n                        self.weight[key] = np.full_like(\n                            next(iter(self.label.values())), self.weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        self.transforms = transforms\n        self._len = len(next(iter(self.input.values())))\n\n    def __getitem__(self, idx):\n        input_item = {key: value[idx] for key, value in self.input.items()}\n        label_item = {key: value[idx] for key, value in self.label.items()}\n        weight_item = {key: value[idx] for key, value in self.weight.items()}\n\n        if self.transforms is not None:\n            input_item, label_item, weight_item = self.transforms(\n                input_item, label_item, weight_item\n            )\n\n        return (input_item, label_item, weight_item)\n\n    def __len__(self):\n        return self._len\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.IterableNPZDataset","title":"<code>IterableNPZDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>IterableNPZDataset for full-data loading.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Npz file path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>List of input keys.</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>List of label keys. Defaults to ().</p> <code>()</code> <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Dict of alias(es) for input and label keys. i.e. {inner_key: outer_key}. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, Union[Callable, float]]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>Optional[Tuple[float, ...]]</code> <p>The number of repetitions of the data in the time dimension. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.IterableNPZDataset(\n...     \"/path/to/file.npz\"\n...     (\"x\",),\n...     (\"u\",),\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/npz_dataset.py</code> <pre><code>class IterableNPZDataset(io.IterableDataset):\n    \"\"\"IterableNPZDataset for full-data loading.\n\n    Args:\n        file_path (str): Npz file path.\n        input_keys (Tuple[str, ...]): List of input keys.\n        label_keys (Tuple[str, ...], optional): List of label keys. Defaults to ().\n        alias_dict (Optional[Dict[str, str]]): Dict of alias(es) for input and label keys.\n            i.e. {inner_key: outer_key}. Defaults to None.\n        weight_dict (Optional[Dict[str, Union[Callable, float]]]): Define the weight of\n            each constraint variable. Defaults to None.\n        timestamps (Optional[Tuple[float, ...]]): The number of repetitions of the data\n            in the time dimension. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.IterableNPZDataset(\n        ...     \"/path/to/file.npz\"\n        ...     (\"x\",),\n        ...     (\"u\",),\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...] = (),\n        alias_dict: Optional[Dict[str, str]] = None,\n        weight_dict: Optional[Dict[str, Union[Callable, float]]] = None,\n        timestamps: Optional[Tuple[float, ...]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n\n        # read raw data from file\n        raw_data = reader.load_npz_file(\n            file_path,\n            input_keys + label_keys,\n            alias_dict,\n        )\n        # filter raw data by given timestamps if specified\n        if timestamps is not None:\n            if \"t\" in raw_data:\n                # filter data according to given timestamps\n                raw_time_array = raw_data[\"t\"]\n                mask = []\n                for ti in timestamps:\n                    mask.append(np.nonzero(np.isclose(raw_time_array, ti).flatten())[0])\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                mask = np.concatenate(mask, 0)\n                raw_data = raw_data[mask]\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n            else:\n                # repeat data according to given timestamps\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                raw_data = misc.combine_array_with_time(raw_data, timestamps)\n                self.input_keys = (\"t\",) + tuple(self.input_keys)\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n\n        # fetch input data\n        self.input = {\n            key: value for key, value in raw_data.items() if key in self.input_keys\n        }\n        # fetch label data\n        self.label = {\n            key: value for key, value in raw_data.items() if key in self.label_keys\n        }\n\n        # prepare weights\n        self.weight = {}\n        if weight_dict is not None:\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    self.weight[key] = np.full_like(\n                        next(iter(self.label.values())), value\n                    )\n                elif callable(value):\n                    func = value\n                    self.weight[key] = func(self.input)\n                    if isinstance(self.weight[key], (int, float)):\n                        self.weight[key] = np.full_like(\n                            next(iter(self.label.values())), self.weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        self.input = {key: paddle.to_tensor(value) for key, value in self.input.items()}\n        self.label = {key: paddle.to_tensor(value) for key, value in self.label.items()}\n        self.weight = {\n            key: paddle.to_tensor(value) for key, value in self.weight.items()\n        }\n\n        self.transforms = transforms\n        self._len = len(next(iter(self.input.values())))\n\n    @property\n    def num_samples(self):\n        \"\"\"Number of samples within current dataset.\"\"\"\n        return self._len\n\n    def __iter__(self):\n        if callable(self.transforms):\n            input_, label_, weight_ = self.transforms(\n                self.input, self.label, self.weight\n            )\n            yield input_, label_, weight_\n        else:\n            yield self.input, self.label, self.weight\n\n    def __len__(self):\n        return 1\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.IterableNPZDataset.num_samples","title":"<code>num_samples</code>  <code>property</code>","text":"<p>Number of samples within current dataset.</p>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.NPZDataset","title":"<code>NPZDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for .npz file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Npz file path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>List of input keys.</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>List of label keys. Defaults to ().</p> <code>()</code> <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Dict of alias(es) for input and label keys. i.e. {inner_key: outer_key}. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, Union[Callable, float]]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>Optional[Tuple[float, ...]]</code> <p>The number of repetitions of the data in the time dimension. Defaults to None.</p> <code>None</code> <code>transforms</code> <code>Optional[Compose]</code> <p>Compose object contains sample wise transform(s). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.NPZDataset(\n...     \"/path/to/file.npz\"\n...     (\"x\",),\n...     (\"u\",),\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/npz_dataset.py</code> <pre><code>class NPZDataset(io.Dataset):\n    \"\"\"Dataset class for .npz file.\n\n    Args:\n        file_path (str): Npz file path.\n        input_keys (Tuple[str, ...]): List of input keys.\n        label_keys (Tuple[str, ...], optional): List of label keys. Defaults to ().\n        alias_dict (Optional[Dict[str, str]]): Dict of alias(es) for input and label keys.\n            i.e. {inner_key: outer_key}. Defaults to None.\n        weight_dict (Optional[Dict[str, Union[Callable, float]]]): Define the weight of\n            each constraint variable. Defaults to None.\n        timestamps (Optional[Tuple[float, ...]]): The number of repetitions of the data\n            in the time dimension. Defaults to None.\n        transforms (Optional[vision.Compose]): Compose object contains sample wise\n            transform(s). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.NPZDataset(\n        ...     \"/path/to/file.npz\"\n        ...     (\"x\",),\n        ...     (\"u\",),\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = True\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...] = (),\n        alias_dict: Optional[Dict[str, str]] = None,\n        weight_dict: Optional[Dict[str, Union[Callable, float]]] = None,\n        timestamps: Optional[Tuple[float, ...]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n\n        # read raw data from file\n        raw_data = reader.load_npz_file(\n            file_path,\n            input_keys + label_keys,\n            alias_dict,\n        )\n        # filter raw data by given timestamps if specified\n        if timestamps is not None:\n            if \"t\" in raw_data:\n                # filter data according to given timestamps\n                raw_time_array = raw_data[\"t\"]\n                mask = []\n                for ti in timestamps:\n                    mask.append(np.nonzero(np.isclose(raw_time_array, ti).flatten())[0])\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                mask = np.concatenate(mask, 0)\n                raw_data = raw_data[mask]\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n            else:\n                # repeat data according to given timestamps\n                raw_data = misc.convert_to_array(\n                    raw_data, self.input_keys + self.label_keys\n                )\n                raw_data = misc.combine_array_with_time(raw_data, timestamps)\n                self.input_keys = (\"t\",) + tuple(self.input_keys)\n                raw_data = misc.convert_to_dict(\n                    raw_data, self.input_keys + self.label_keys\n                )\n\n        # fetch input data\n        self.input = {\n            key: value for key, value in raw_data.items() if key in self.input_keys\n        }\n        # fetch label data\n        self.label = {\n            key: value for key, value in raw_data.items() if key in self.label_keys\n        }\n\n        # prepare weights\n        self.weight = {}\n        if weight_dict is not None:\n            for key, value in weight_dict.items():\n                if isinstance(value, (int, float)):\n                    self.weight[key] = np.full_like(\n                        next(iter(self.label.values())), value\n                    )\n                elif callable(value):\n                    func = value\n                    self.weight[key] = func(self.input)\n                    if isinstance(self.weight[key], (int, float)):\n                        self.weight[key] = np.full_like(\n                            next(iter(self.label.values())), self.weight[key]\n                        )\n                else:\n                    raise NotImplementedError(f\"type of {type(value)} is invalid yet.\")\n\n        self.transforms = transforms\n        self._len = len(next(iter(self.input.values())))\n\n    def __getitem__(self, idx):\n        input_item = {key: value[idx] for key, value in self.input.items()}\n        label_item = {key: value[idx] for key, value in self.label.items()}\n        weight_item = {key: value[idx] for key, value in self.weight.items()}\n\n        if self.transforms is not None:\n            input_item, label_item, weight_item = self.transforms(\n                input_item, label_item, weight_item\n            )\n\n        return (input_item, label_item, weight_item)\n\n    def __len__(self):\n        return self._len\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.CylinderDataset","title":"<code>CylinderDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for training Cylinder model.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Data set path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"states\",\"visc\").</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"pred_states\", \"recover_states\").</p> required <code>block_size</code> <code>int</code> <p>Data block size.</p> required <code>stride</code> <code>int</code> <p>Data stride.</p> required <code>ndata</code> <code>Optional[int]</code> <p>Number of data series to use. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, float]]</code> <p>Weight dictionary. Defaults to None.</p> <code>None</code> <code>embedding_model</code> <code>Optional[Arch]</code> <p>Embedding model. Defaults to None.</p> <code>None</code> <code>embedding_batch_size</code> <code>int</code> <p>The batch size of embedding model. Defaults to 64.</p> <code>64</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.CylinderDataset(\n...     \"file_path\": \"/path/to/CylinderDataset\",\n...     \"input_keys\": (\"x\",),\n...     \"label_keys\": (\"v\",),\n...     \"block_size\": 32,\n...     \"stride\": 16,\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/trphysx_dataset.py</code> <pre><code>class CylinderDataset(io.Dataset):\n    \"\"\"Dataset for training Cylinder model.\n\n    Args:\n        file_path (str): Data set path.\n        input_keys (Tuple[str, ...]): Input keys, such as (\"states\",\"visc\").\n        label_keys (Tuple[str, ...]): Output keys, such as (\"pred_states\", \"recover_states\").\n        block_size (int): Data block size.\n        stride (int): Data stride.\n        ndata (Optional[int]): Number of data series to use. Defaults to None.\n        weight_dict (Optional[Dict[str, float]]): Weight dictionary. Defaults to None.\n        embedding_model (Optional[base.Arch]): Embedding model. Defaults to None.\n        embedding_batch_size (int, optional): The batch size of embedding model. Defaults to 64.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.CylinderDataset(\n        ...     \"file_path\": \"/path/to/CylinderDataset\",\n        ...     \"input_keys\": (\"x\",),\n        ...     \"label_keys\": (\"v\",),\n        ...     \"block_size\": 32,\n        ...     \"stride\": 16,\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        block_size: int,\n        stride: int,\n        ndata: Optional[int] = None,\n        weight_dict: Optional[Dict[str, float]] = None,\n        embedding_model: Optional[base.Arch] = None,\n        embedding_batch_size: int = 64,\n    ):\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(\n                f\"file_path({file_path}) not exists. Please download dataset first. \"\n                \"Training: https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_training.hdf5. \"\n                \"Valid: https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_valid.hdf5.\"\n            )\n        super().__init__()\n        self.file_path = file_path\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n\n        self.block_size = block_size\n        self.stride = stride\n        self.ndata = ndata\n        self.weight_dict = {key: 1.0 for key in self.label_keys}\n        if weight_dict is not None:\n            self.weight_dict.update(weight_dict)\n\n        self.data, self.visc = self.read_data(file_path, block_size, stride)\n        self.embedding_model = embedding_model\n        if embedding_model is None:\n            self.embedding_data = None\n        else:\n            embedding_model.eval()\n            with paddle.no_grad():\n                data_tensor = paddle.to_tensor(self.data)\n                visc_tensor = paddle.to_tensor(self.visc)\n                embedding_data = []\n                for i in range(0, len(data_tensor), embedding_batch_size):\n                    start, end = i, min(i + embedding_batch_size, len(data_tensor))\n                    embedding_data_batch = embedding_model.encoder(\n                        data_tensor[start:end], visc_tensor[start:end]\n                    )\n                    embedding_data.append(embedding_data_batch.numpy())\n                self.embedding_data = np.concatenate(embedding_data)\n\n    def read_data(self, file_path: str, block_size: int, stride: int):\n        data = []\n        visc = []\n        with h5py.File(file_path, \"r\") as f:\n            data_num = 0\n            for key in f.keys():\n                visc0 = 2.0 / float(key)\n                ux = np.asarray(f[key + \"/ux\"], dtype=paddle.get_default_dtype())\n                uy = np.asarray(f[key + \"/uy\"], dtype=paddle.get_default_dtype())\n                p = np.asarray(f[key + \"/p\"], dtype=paddle.get_default_dtype())\n                data_series = np.stack([ux, uy, p], axis=1)\n\n                for i in range(0, data_series.shape[0] - block_size + 1, stride):\n                    data.append(data_series[i : i + block_size])\n                    visc.append([visc0])\n\n                data_num += 1\n                if self.ndata is not None and data_num &gt;= self.ndata:\n                    break\n\n        data = np.asarray(data)\n        visc = np.asarray(visc, dtype=paddle.get_default_dtype())\n        return data, visc\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, i):\n        if self.embedding_data is None:\n            data_item = self.data[i]\n            input_item = {\n                self.input_keys[0]: data_item,\n                self.input_keys[1]: self.visc[i],\n            }\n            label_item = {\n                self.label_keys[0]: data_item[1:],\n                self.label_keys[1]: data_item,\n            }\n        else:\n            data_item = self.embedding_data[i]\n            input_item = {self.input_keys[0]: data_item[:-1, :]}\n            label_item = {self.label_keys[0]: data_item[1:, :]}\n            if len(self.label_keys) == 2:\n                label_item[self.label_keys[1]] = data_item[1:, :]\n        weight_shape = [1] * len(data_item.shape)\n        weight_item = {\n            key: np.full(weight_shape, value, paddle.get_default_dtype())\n            for key, value in self.weight_dict.items()\n        }\n        return (input_item, label_item, weight_item)\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.LorenzDataset","title":"<code>LorenzDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for training Lorenz model.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Data set path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"states\",).</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"pred_states\", \"recover_states\").</p> required <code>block_size</code> <code>int</code> <p>Data block size.</p> required <code>stride</code> <code>int</code> <p>Data stride.</p> required <code>ndata</code> <code>Optional[int]</code> <p>Number of data series to use. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, float]]</code> <p>Weight dictionary. Defaults to None.</p> <code>None</code> <code>embedding_model</code> <code>Optional[Arch]</code> <p>Embedding model. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.LorenzDataset(\n...     \"file_path\": \"/path/to/LorenzDataset\",\n...     \"input_keys\": (\"x\",),\n...     \"label_keys\": (\"v\",),\n...     \"block_size\": 32,\n...     \"stride\": 16,\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/trphysx_dataset.py</code> <pre><code>class LorenzDataset(io.Dataset):\n    \"\"\"Dataset for training Lorenz model.\n\n    Args:\n        file_path (str): Data set path.\n        input_keys (Tuple[str, ...]): Input keys, such as (\"states\",).\n        label_keys (Tuple[str, ...]): Output keys, such as (\"pred_states\", \"recover_states\").\n        block_size (int): Data block size.\n        stride (int): Data stride.\n        ndata (Optional[int]): Number of data series to use. Defaults to None.\n        weight_dict (Optional[Dict[str, float]]): Weight dictionary. Defaults to None.\n        embedding_model (Optional[base.Arch]): Embedding model. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.LorenzDataset(\n        ...     \"file_path\": \"/path/to/LorenzDataset\",\n        ...     \"input_keys\": (\"x\",),\n        ...     \"label_keys\": (\"v\",),\n        ...     \"block_size\": 32,\n        ...     \"stride\": 16,\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        block_size: int,\n        stride: int,\n        ndata: Optional[int] = None,\n        weight_dict: Optional[Dict[str, float]] = None,\n        embedding_model: Optional[base.Arch] = None,\n    ):\n        super().__init__()\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(\n                f\"file_path({file_path}) not exists. Please download dataset first. \"\n                \"Training: https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_training_rk.hdf5. \"\n                \"Valid: https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_valid_rk.hdf5.\"\n            )\n\n        self.file_path = file_path\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n\n        self.block_size = block_size\n        self.stride = stride\n        self.ndata = ndata\n        self.weight_dict = {key: 1.0 for key in self.label_keys}\n        if weight_dict is not None:\n            self.weight_dict.update(weight_dict)\n\n        self.data = self.read_data(file_path, block_size, stride)\n        self.embedding_model = embedding_model\n        if embedding_model is None:\n            self.embedding_data = None\n        else:\n            embedding_model.eval()\n            with paddle.no_grad():\n                data_tensor = paddle.to_tensor(self.data)\n                embedding_data_tensor = embedding_model.encoder(data_tensor)\n            self.embedding_data = embedding_data_tensor.numpy()\n\n    def read_data(self, file_path: str, block_size: int, stride: int):\n        data = []\n        with h5py.File(file_path, \"r\") as f:\n            data_num = 0\n            for key in f.keys():\n                data_series = np.asarray(f[key], dtype=paddle.get_default_dtype())\n                for i in range(0, data_series.shape[0] - block_size + 1, stride):\n                    data.append(data_series[i : i + block_size])\n                data_num += 1\n                if self.ndata is not None and data_num &gt;= self.ndata:\n                    break\n        return np.asarray(data)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # when embedding data is None\n        if self.embedding_data is None:\n            data_item = self.data[idx]\n            input_item = {self.input_keys[0]: data_item}\n            label_item = {\n                self.label_keys[0]: data_item[1:, :],\n                self.label_keys[1]: data_item,\n            }\n        else:\n            data_item = self.embedding_data[idx]\n            input_item = {self.input_keys[0]: data_item[:-1, :]}\n            label_item = {self.label_keys[0]: data_item[1:, :]}\n            if len(self.label_keys) == 2:\n                label_item[self.label_keys[1]] = self.data[idx][1:, :]\n\n        weight_shape = [1] * len(data_item.shape)\n        weight_item = {\n            key: np.full(weight_shape, value, paddle.get_default_dtype())\n            for key, value in self.weight_dict.items()\n        }\n        return (input_item, label_item, weight_item)\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.RosslerDataset","title":"<code>RosslerDataset</code>","text":"<p>               Bases: <code>LorenzDataset</code></p> <p>Dataset for training Rossler model.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Data set path.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"states\",).</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"pred_states\", \"recover_states\").</p> required <code>block_size</code> <code>int</code> <p>Data block size.</p> required <code>stride</code> <code>int</code> <p>Data stride.</p> required <code>ndata</code> <code>Optional[int]</code> <p>Number of data series to use. Defaults to None.</p> <code>None</code> <code>weight_dict</code> <code>Optional[Dict[str, float]]</code> <p>Weight dictionary. Defaults to None.</p> <code>None</code> <code>embedding_model</code> <code>Optional[Arch]</code> <p>Embedding model. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.RosslerDataset(\n...     \"file_path\": \"/path/to/RosslerDataset\",\n...     \"input_keys\": (\"x\",),\n...     \"label_keys\": (\"v\",),\n...     \"block_size\": 32,\n...     \"stride\": 16,\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/trphysx_dataset.py</code> <pre><code>class RosslerDataset(LorenzDataset):\n    \"\"\"Dataset for training Rossler model.\n\n    Args:\n        file_path (str): Data set path.\n        input_keys (Tuple[str, ...]): Input keys, such as (\"states\",).\n        label_keys (Tuple[str, ...]): Output keys, such as (\"pred_states\", \"recover_states\").\n        block_size (int): Data block size.\n        stride (int): Data stride.\n        ndata (Optional[int]): Number of data series to use. Defaults to None.\n        weight_dict (Optional[Dict[str, float]]): Weight dictionary. Defaults to None.\n        embedding_model (Optional[base.Arch]): Embedding model. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.RosslerDataset(\n        ...     \"file_path\": \"/path/to/RosslerDataset\",\n        ...     \"input_keys\": (\"x\",),\n        ...     \"label_keys\": (\"v\",),\n        ...     \"block_size\": 32,\n        ...     \"stride\": 16,\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        block_size: int,\n        stride: int,\n        ndata: Optional[int] = None,\n        weight_dict: Optional[Dict[str, float]] = None,\n        embedding_model: Optional[base.Arch] = None,\n    ):\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(\n                f\"file_path({file_path}) not exists. Please download dataset first. \"\n                \"Training: https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_training.hdf5. \"\n                \"Valid: https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_valid.hdf5.\"\n            )\n        super().__init__(\n            file_path,\n            input_keys,\n            label_keys,\n            block_size,\n            stride,\n            ndata,\n            weight_dict,\n            embedding_model,\n        )\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.VtuDataset","title":"<code>VtuDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for .vtu file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>*.vtu file path.</p> required <code>input_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Tuple of input keys. Defaults to None.</p> <code>None</code> <code>label_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Tuple of label keys. Defaults to None.</p> <code>None</code> <code>time_step</code> <code>Optional[int]</code> <p>Time step with unit second. Defaults to None.</p> <code>None</code> <code>time_index</code> <code>Optional[Tuple[int, ...]]</code> <p>Time index tuple in increasing order.</p> <code>None</code> <code>labels</code> <code>Optional[Dict[str, float]]</code> <p>Temporary variable for [load_vtk_with_time_file].</p> <code>None</code> <code>transforms</code> <code>Compose</code> <p>Compose object contains sample wise. transform(s).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ppsci.data.dataset import VtuDataset\n</code></pre> <pre><code>&gt;&gt;&gt; dataset = VtuDataset(file_path='example.vtu')\n</code></pre> <pre><code>&gt;&gt;&gt; # get the length of the dataset\n&gt;&gt;&gt; dataset_size = len(dataset)\n&gt;&gt;&gt; # get the first sample of the data\n&gt;&gt;&gt; first_sample = dataset[0]\n&gt;&gt;&gt; print(\"First sample:\", first_sample)\n</code></pre> Source code in <code>ppsci/data/dataset/vtu_dataset.py</code> <pre><code>class VtuDataset(io.Dataset):\n    \"\"\"Dataset class for .vtu file.\n\n    Args:\n        file_path (str): *.vtu file path.\n        input_keys (Optional[Tuple[str, ...]]): Tuple of input keys. Defaults to None.\n        label_keys (Optional[Tuple[str, ...]]): Tuple of label keys. Defaults to None.\n        time_step (Optional[int]): Time step with unit second. Defaults to None.\n        time_index (Optional[Tuple[int, ...]]): Time index tuple in increasing order.\n        labels (Optional[Dict[str, float]]): Temporary variable for [load_vtk_with_time_file].\n        transforms (vision.Compose, optional): Compose object contains sample wise.\n            transform(s).\n\n    Examples:\n        &gt;&gt;&gt; from ppsci.data.dataset import VtuDataset\n\n        &gt;&gt;&gt; dataset = VtuDataset(file_path='example.vtu') # doctest: +SKIP\n\n        &gt;&gt;&gt; # get the length of the dataset\n        &gt;&gt;&gt; dataset_size = len(dataset) # doctest: +SKIP\n        &gt;&gt;&gt; # get the first sample of the data\n        &gt;&gt;&gt; first_sample = dataset[0] # doctest: +SKIP\n        &gt;&gt;&gt; print(\"First sample:\", first_sample) # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = True\n\n    def __init__(\n        self,\n        file_path: str,\n        input_keys: Optional[Tuple[str, ...]] = None,\n        label_keys: Optional[Tuple[str, ...]] = None,\n        time_step: Optional[int] = None,\n        time_index: Optional[Tuple[int, ...]] = None,\n        labels: Optional[Dict[str, float]] = None,\n        transforms: Optional[vision.Compose] = None,\n    ):\n        super().__init__()\n\n        # load data from file\n        if time_step is not None and time_index is not None:\n            _input, _label = reader.load_vtk_file(\n                file_path, time_step, time_index, input_keys, label_keys\n            )\n            _label = {key: _label[key] for key in label_keys}\n        elif time_step is None and time_index is None:\n            _input = reader.load_vtk_with_time_file(file_path)\n            _label = {}\n            for key, value in labels.items():\n                if isinstance(value, (int, float)):\n                    _label[key] = np.full_like(\n                        next(iter(_input.values())), value, \"float32\"\n                    )\n                else:\n                    _label[key] = value\n        else:\n            raise ValueError(\n                \"Error, read vtu with time_step and time_index, or neither\"\n            )\n\n        # transform\n        _input = transforms(_input)\n        _label = transforms(_label)\n\n        self.input = _input\n        self.label = _label\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n        self.transforms = transforms\n        self.num_samples = len(next(iter(self.input.values())))\n\n    def __getitem__(self, idx):\n        input_item = {key: value[idx] for key, value in self.input.items()}\n        label_item = {key: value[idx] for key, value in self.label.items()}\n        return (input_item, label_item, {})\n\n    def __len__(self):\n        return self.num_samples\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.MeshAirfoilDataset","title":"<code>MeshAirfoilDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for <code>MeshAirfoil</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input data.</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Name of label data.</p> required <code>data_dir</code> <code>str</code> <p>Directory of MeshAirfoil data.</p> required <code>mesh_graph_path</code> <code>str</code> <p>Path of mesh graph.</p> required <code>transpose_edges</code> <code>bool</code> <p>Whether transpose the edges array from (2, num_edges) to (num_edges, 2) for convenient of slicing.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.MeshAirfoilDataset(\n...     \"input_keys\": (\"input\",),\n...     \"label_keys\": (\"output\",),\n...     \"data_dir\": \"/path/to/MeshAirfoilDataset\",\n...     \"mesh_graph_path\": \"/path/to/file.su2\",\n...     \"transpose_edges\": False,\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/airfoil_dataset.py</code> <pre><code>class MeshAirfoilDataset(io.Dataset):\n    \"\"\"Dataset for `MeshAirfoil`.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input data.\n        label_keys (Tuple[str, ...]): Name of label data.\n        data_dir (str): Directory of MeshAirfoil data.\n        mesh_graph_path (str): Path of mesh graph.\n        transpose_edges (bool, optional): Whether transpose the edges array from (2, num_edges) to (num_edges, 2) for convenient of slicing.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.MeshAirfoilDataset(\n        ...     \"input_keys\": (\"input\",),\n        ...     \"label_keys\": (\"output\",),\n        ...     \"data_dir\": \"/path/to/MeshAirfoilDataset\",\n        ...     \"mesh_graph_path\": \"/path/to/file.su2\",\n        ...     \"transpose_edges\": False,\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    use_pgl: bool = True\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        data_dir: str,\n        mesh_graph_path: str,\n        transpose_edges: bool = False,\n    ):\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n        self.data_dir = data_dir\n        self.file_list = os.listdir(self.data_dir)\n        self.len = len(self.file_list)\n        self.mesh_graph = _get_mesh_graph(mesh_graph_path)\n\n        with open(osp.join(osp.dirname(self.data_dir), \"train_max_min.pkl\"), \"rb\") as f:\n            self.normalization_factors = pickle.load(f)\n\n        self.nodes = self.mesh_graph[0]\n        self.edges = self.mesh_graph[1]\n        if transpose_edges:\n            self.edges = self.edges.transpose([1, 0])\n        self.elems_list = self.mesh_graph[2]\n        self.marker_dict = self.mesh_graph[3]\n        self.node_markers = np.full([self.nodes.shape[0], 1], fill_value=-1)\n        for i, (marker_tag, marker_elems) in enumerate(self.marker_dict.items()):\n            for elem in marker_elems:\n                self.node_markers[elem[0]] = i\n                self.node_markers[elem[1]] = i\n\n        self.raw_graphs = [self.get(i) for i in range(len(self))]\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        return (\n            {\n                self.input_keys[0]: self.raw_graphs[idx],\n            },\n            {\n                self.label_keys[0]: self.raw_graphs[idx],\n            },\n            None,\n        )\n\n    def get(self, idx):\n        with open(osp.join(self.data_dir, self.file_list[idx]), \"rb\") as f:\n            fields = pickle.load(f)\n        fields = self._preprocess(fields)\n        aoa, reynolds, mach = self._get_params_from_name(self.file_list[idx])\n        # aoa = aoa\n        mach_or_reynolds = mach if reynolds is None else reynolds\n        # mach_or_reynolds = mach_or_reynolds\n        norm_aoa = aoa / 10\n        norm_mach_or_reynolds = (\n            mach_or_reynolds if reynolds is None else (mach_or_reynolds - 1.5e6) / 1.5e6\n        )\n\n        nodes = np.concatenate(\n            [\n                self.nodes,\n                np.repeat(a=norm_aoa, repeats=self.nodes.shape[0])[:, np.newaxis],\n                np.repeat(a=norm_mach_or_reynolds, repeats=self.nodes.shape[0])[\n                    :, np.newaxis\n                ],\n                self.node_markers,\n            ],\n            axis=-1,\n        ).astype(paddle.get_default_dtype())\n\n        data = pgl.Graph(\n            num_nodes=nodes.shape[0],\n            edges=self.edges,\n        )\n        data.x = nodes\n        data.y = fields\n        data.pos = self.nodes\n        data.edge_index = self.edges\n\n        sender = data.x[data.edge_index[0]]\n        receiver = data.x[data.edge_index[1]]\n        relation_pos = sender[:, 0:2] - receiver[:, 0:2]\n        post = np.linalg.norm(relation_pos, ord=2, axis=1, keepdims=True).astype(\n            paddle.get_default_dtype()\n        )\n        data.edge_attr = post\n        std_epsilon = [1e-8]\n        a = np.mean(data.edge_attr, axis=0)\n        b = data.edge_attr.std(axis=0)\n        b = np.maximum(b, std_epsilon).astype(paddle.get_default_dtype())\n        data.edge_attr = (data.edge_attr - a) / b\n        data.aoa = aoa\n        data.norm_aoa = norm_aoa\n        data.mach_or_reynolds = mach_or_reynolds\n        data.norm_mach_or_reynolds = norm_mach_or_reynolds\n        return data\n\n    def _preprocess(self, tensor_list, stack_output=True):\n        data_max, data_min = self.normalization_factors\n        normalized_tensors = []\n        for i in range(len(tensor_list)):\n            normalized = (tensor_list[i] - data_min[i]) / (\n                data_max[i] - data_min[i]\n            ) * 2 - 1\n            normalized_tensors.append(normalized)\n        if stack_output:\n            normalized_tensors = np.stack(normalized_tensors, axis=1)\n        return normalized_tensors\n\n    def _get_params_from_name(self, filename):\n        s = filename.rsplit(\".\", 1)[0].split(\"_\")\n        aoa = np.array(s[s.index(\"aoa\") + 1])[np.newaxis].astype(\n            paddle.get_default_dtype()\n        )\n        reynolds = s[s.index(\"re\") + 1]\n        reynolds = (\n            np.array(reynolds)[np.newaxis].astype(paddle.get_default_dtype())\n            if reynolds != \"None\"\n            else None\n        )\n        mach = np.array(s[s.index(\"mach\") + 1])[np.newaxis].astype(\n            paddle.get_default_dtype()\n        )\n        return aoa, reynolds, mach\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.MeshCylinderDataset","title":"<code>MeshCylinderDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for <code>MeshCylinder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Name of input data.</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Name of label data.</p> required <code>data_dir</code> <code>str</code> <p>Directory of MeshCylinder data.</p> required <code>mesh_graph_path</code> <code>str</code> <p>Path of mesh graph.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.MeshAirfoilDataset(\n...     \"input_keys\": (\"input\",),\n...     \"label_keys\": (\"output\",),\n...     \"data_dir\": \"/path/to/MeshAirfoilDataset\",\n...     \"mesh_graph_path\": \"/path/to/file.su2\",\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/cylinder_dataset.py</code> <pre><code>class MeshCylinderDataset(io.Dataset):\n    \"\"\"Dataset for `MeshCylinder`.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input data.\n        label_keys (Tuple[str, ...]): Name of label data.\n        data_dir (str): Directory of MeshCylinder data.\n        mesh_graph_path (str): Path of mesh graph.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.MeshAirfoilDataset(\n        ...     \"input_keys\": (\"input\",),\n        ...     \"label_keys\": (\"output\",),\n        ...     \"data_dir\": \"/path/to/MeshAirfoilDataset\",\n        ...     \"mesh_graph_path\": \"/path/to/file.su2\",\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    use_pgl: bool = True\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        data_dir: str,\n        mesh_graph_path: str,\n    ):\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n        self.data_dir = data_dir\n        self.file_list = os.listdir(self.data_dir)\n        self.len = len(self.file_list)\n        self.mesh_graph = airfoil_dataset._get_mesh_graph(mesh_graph_path)\n\n        self.normalization_factors = np.array(\n            [[978.6001, 48.9258, 24.8404], [-692.3159, -6.9950, -24.8572]],\n            dtype=paddle.get_default_dtype(),\n        )\n\n        self.nodes = self.mesh_graph[0]\n        self.meshnodes = self.mesh_graph[0]\n        self.edges = self.mesh_graph[1]\n        self.elems_list = self.mesh_graph[2]\n        self.marker_dict = self.mesh_graph[3]\n        self.bounder = []\n        self.node_markers = np.full([self.nodes.shape[0], 1], fill_value=-1)\n        for i, (marker_tag, marker_elems) in enumerate(self.marker_dict.items()):\n            for elem in marker_elems:\n                self.node_markers[elem[0]] = i\n                self.node_markers[elem[1]] = i\n\n        self.raw_graphs = [self.get(i) for i in range(len(self))]\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        return (\n            {\n                self.input_keys[0]: self.raw_graphs[idx],\n            },\n            {\n                self.label_keys[0]: self.raw_graphs[idx],\n            },\n            None,\n        )\n\n    def get(self, idx):\n        with open(osp.join(self.data_dir, self.file_list[idx]), \"r\") as f:\n            field = []\n            pos = []\n            for line in f.read().splitlines()[1:]:\n                lines_pos = line.split(\",\")[1:3]\n                lines_field = line.split(\",\")[3:]\n                numbers_float = list(eval(i) for i in lines_pos)\n                array = np.array(numbers_float, paddle.get_default_dtype())\n                pos.append(array)\n                numbers_float = list(eval(i) for i in lines_field)\n                array = np.array(numbers_float, paddle.get_default_dtype())\n                field.append(array)\n\n        field = np.stack(field, axis=0)\n        pos = np.stack(pos, axis=0)\n        indexlist = []\n        for i in range(self.meshnodes.shape[0]):\n            b = self.meshnodes[i : (i + 1)]\n            b = np.squeeze(b)\n            index = np.nonzero(\n                np.sum((pos == b), axis=1, dtype=paddle.get_default_dtype())\n                == pos.shape[1]\n            )\n            indexlist.append(index)\n        indexlist = np.stack(indexlist, axis=0)\n        indexlist = np.squeeze(indexlist)\n        fields = field[indexlist]\n        velocity = self._get_params_from_name(self.file_list[idx])\n\n        norm_aoa = velocity / 40\n        # add physics parameters to graph\n        nodes = np.concatenate(\n            [\n                self.nodes,\n                np.repeat(a=norm_aoa, repeats=self.nodes.shape[0])[:, np.newaxis],\n                self.node_markers,\n            ],\n            axis=-1,\n        ).astype(paddle.get_default_dtype())\n\n        data = pgl.Graph(\n            num_nodes=nodes.shape[0],\n            edges=self.edges,\n        )\n        data.x = nodes\n        data.y = fields\n        data.pos = self.nodes\n        data.edge_index = self.edges\n        data.velocity = velocity\n\n        sender = data.x[data.edge_index[0]]\n        receiver = data.x[data.edge_index[1]]\n        relation_pos = sender[:, 0:2] - receiver[:, 0:2]\n        post = np.linalg.norm(relation_pos, ord=2, axis=1, keepdims=True).astype(\n            paddle.get_default_dtype()\n        )\n        data.edge_attr = post\n        std_epsilon = [1e-8]\n        a = np.mean(data.edge_attr, axis=0)\n        b = data.edge_attr.std(axis=0)\n        b = np.maximum(b, std_epsilon).astype(paddle.get_default_dtype())\n        data.edge_attr = (data.edge_attr - a) / b\n        a = np.mean(data.y, axis=0)\n        b = data.y.std(axis=0)\n        b = np.maximum(b, std_epsilon).astype(paddle.get_default_dtype())\n        data.y = (data.y - a) / b\n        data.norm_max = a\n        data.norm_min = b\n\n        # find the face of the boundary,our cylinder dataset come from fluent solver\n        with open(osp.join(osp.dirname(self.data_dir), \"bounder\"), \"r\") as f:\n            field = []\n            pos = []\n            for line in f.read().splitlines()[1:]:\n                lines_pos = line.split(\",\")[1:3]\n                lines_field = line.split(\",\")[3:]\n                numbers_float = list(eval(i) for i in lines_pos)\n                array = np.array(numbers_float, paddle.get_default_dtype())\n                pos.append(array)\n                numbers_float = list(eval(i) for i in lines_field)\n                array = np.array(numbers_float, paddle.get_default_dtype())\n                field.append(array)\n\n        field = np.stack(field, axis=0)\n        pos = np.stack(pos, axis=0)\n\n        indexlist = []\n        for i in range(pos.shape[0]):\n            b = pos[i : (i + 1)]\n            b = np.squeeze(b)\n            index = np.nonzero(\n                np.sum((self.nodes == b), axis=1, dtype=paddle.get_default_dtype())\n                == self.nodes.shape[1]\n            )\n            indexlist.append(index)\n\n        indexlist = np.stack(indexlist, axis=0)\n        indexlist = np.squeeze(indexlist)\n        self.bounder = indexlist\n        return data\n\n    def _get_params_from_name(self, filename):\n        s = filename.rsplit(\".\", 1)[0]\n        reynolds = np.array(s[13:])[np.newaxis].astype(paddle.get_default_dtype())\n        return reynolds\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.RadarDataset","title":"<code>RadarDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Class for Radar dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"input\",).</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"output\",).</p> required <code>image_width</code> <code>int</code> <p>Image width.</p> required <code>image_height</code> <code>int</code> <p>Image height.</p> required <code>total_length</code> <code>int</code> <p>Total length.</p> required <code>dataset_path</code> <code>str</code> <p>Dataset path.</p> required <code>data_type</code> <code>str</code> <p>Input and output data type. Defaults to paddle.get_default_dtype().</p> <code>get_default_dtype()</code> <code>weight_dict</code> <code>Optional[Dict[str, float]]</code> <p>Weight dictionary. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.RadarDataset(\n...     \"input_keys\": (\"input\",),\n...     \"label_keys\": (\"output\",),\n...     \"image_width\": 512,\n...     \"image_height\": 512,\n...     \"total_length\": 29,\n...     \"dataset_path\": \"datasets/mrms/figure\",\n...     \"data_type\": paddle.get_default_dtype(),\n... )\n</code></pre> Source code in <code>ppsci/data/dataset/radar_dataset.py</code> <pre><code>class RadarDataset(io.Dataset):\n    \"\"\"Class for Radar dataset.\n\n    Args:\n        input_keys (Tuple[str, ...]): Input keys, such as (\"input\",).\n        label_keys (Tuple[str, ...]): Output keys, such as (\"output\",).\n        image_width (int): Image width.\n        image_height (int): Image height.\n        total_length (int): Total length.\n        dataset_path (str): Dataset path.\n        data_type (str): Input and output data type. Defaults to paddle.get_default_dtype().\n        weight_dict (Optional[Dict[str, float]]): Weight dictionary. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.RadarDataset(\n        ...     \"input_keys\": (\"input\",),\n        ...     \"label_keys\": (\"output\",),\n        ...     \"image_width\": 512,\n        ...     \"image_height\": 512,\n        ...     \"total_length\": 29,\n        ...     \"dataset_path\": \"datasets/mrms/figure\",\n        ...     \"data_type\": paddle.get_default_dtype(),\n        ... )  # doctest: +SKIP\n    \"\"\"\n\n    # Whether support batch indexing for speeding up fetching process.\n    batch_index: bool = False\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        image_width: int,\n        image_height: int,\n        total_length: int,\n        dataset_path: str,\n        data_type: str = paddle.get_default_dtype(),\n        weight_dict: Optional[Dict[str, float]] = None,\n    ):\n        super().__init__()\n        if importlib.util.find_spec(\"cv2\") is None:\n            raise ModuleNotFoundError(\n                \"To use RadarDataset, please install 'opencv-python' with: `pip install \"\n                \"opencv-python` first.\"\n            )\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n        self.img_width = image_width\n        self.img_height = image_height\n        self.length = total_length\n        self.dataset_path = dataset_path\n        self.data_type = data_type\n\n        self.weight_dict = {} if weight_dict is None else weight_dict\n        if weight_dict is not None:\n            self.weight_dict = {key: 1.0 for key in self.label_keys}\n            self.weight_dict.update(weight_dict)\n\n        self.case_list = []\n        name_list = os.listdir(self.dataset_path)\n        name_list.sort()\n        for name in name_list:\n            case = []\n            for i in range(29):\n                case.append(\n                    self.dataset_path\n                    + \"/\"\n                    + name\n                    + \"/\"\n                    + name\n                    + \"-\"\n                    + str(i).zfill(2)\n                    + \".png\"\n                )\n            self.case_list.append(case)\n\n    def _load(self, index):\n        data = []\n        for img_path in self.case_list[index]:\n            img = cv2.imread(img_path, 2)\n            data.append(np.expand_dims(img, axis=0))\n        data = np.concatenate(data, axis=0).astype(self.data_type) / 10.0 - 3.0\n        assert data.shape[1] &lt;= 1024 and data.shape[2] &lt;= 1024\n        return data\n\n    def __getitem__(self, index):\n        data = self._load(index)[-self.length :].copy()\n        mask = np.ones_like(data)\n        mask[data &lt; 0] = 0\n        data[data &lt; 0] = 0\n        data = np.clip(data, 0, 128)\n        vid = np.zeros(\n            (self.length, self.img_height, self.img_width, 2), dtype=self.data_type\n        )\n        vid[..., 0] = data\n        vid[..., 1] = mask\n\n        input_item = {self.input_keys[0]: vid}\n        label_item = {}\n        weight_item = {}\n        for key in self.label_keys:\n            label_item[key] = np.asarray([], paddle.get_default_dtype())\n        if len(label_item) &gt; 0:\n            weight_shape = [1] * len(next(iter(label_item.values())).shape)\n            weight_item = {\n                key: np.full(weight_shape, value, paddle.get_default_dtype())\n                for key, value in self.weight_dict.items()\n            }\n        return input_item, label_item, weight_item\n\n    def __len__(self):\n        return len(self.case_list)\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.DGMRDataset","title":"<code>DGMRDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for DGMR (Deep Generative Model for Radar) model. This open-sourced UK dataset has been mirrored to HuggingFace Datasets https://huggingface.co/datasets/openclimatefix/nimrod-uk-1km. If the reader cannot load the dataset from Hugging Face, please manually download it and modify the dataset_path to the local path for loading.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"input\",).</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"output\",).</p> required <code>split</code> <code>str</code> <p>The split of the dataset, \"validation\" or \"train\". Defaults to \"validation\".</p> <code>'validation'</code> <code>num_input_frames</code> <code>int</code> <p>Number of input frames. Defaults to 4.</p> <code>4</code> <code>num_target_frames</code> <code>int</code> <p>Number of target frames. Defaults to 18.</p> <code>18</code> <code>dataset_path</code> <code>str</code> <p>Path to the dataset. Defaults to \"openclimatefix/nimrod-uk-1km\".</p> <code>'openclimatefix/nimrod-uk-1km'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dataset = ppsci.data.dataset.DGMRDataset((\"input\", ), (\"output\", ))\n</code></pre> Source code in <code>ppsci/data/dataset/dgmr_dataset.py</code> <pre><code>class DGMRDataset(io.Dataset):\n    \"\"\"\n    Dataset class for DGMR (Deep Generative Model for Radar) model.\n    This open-sourced UK dataset has been mirrored to HuggingFace Datasets https://huggingface.co/datasets/openclimatefix/nimrod-uk-1km.\n    If the reader cannot load the dataset from Hugging Face, please manually download it and modify the dataset_path to the local path for loading.\n\n    Args:\n        input_keys (Tuple[str, ...]): Input keys, such as (\"input\",).\n        label_keys (Tuple[str, ...]): Output keys, such as (\"output\",).\n        split (str, optional): The split of the dataset, \"validation\" or \"train\". Defaults to \"validation\".\n        num_input_frames (int, optional): Number of input frames. Defaults to 4.\n        num_target_frames (int, optional): Number of target frames. Defaults to 18.\n        dataset_path (str, optional): Path to the dataset. Defaults to \"openclimatefix/nimrod-uk-1km\".\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dataset = ppsci.data.dataset.DGMRDataset((\"input\", ), (\"output\", )) # doctest: +SKIP\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        split: str = \"validation\",\n        num_input_frames: int = 4,\n        num_target_frames: int = 18,\n        dataset_path: str = \"openclimatefix/nimrod-uk-1km\",\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n        self.num_input_frames = num_input_frames\n        self.num_target_frames = num_target_frames\n        if not importlib.util.find_spec(\"datasets\"):\n            raise ModuleNotFoundError(\n                \"Please install datasets with `pip install datasets`\"\n                \" before exporting onnx model.\"\n            )\n        import datasets\n\n        self.reader = datasets.load_dataset(\n            dataset_path, \"sample\", split=split, streaming=True, trust_remote_code=True\n        )\n        self.iter_reader = self.reader\n\n    def __len__(self):\n        return 1000\n\n    def __getitem__(self, idx):\n        try:\n            row = next(self.iter_reader)\n        except Exception:\n            rng = default_rng(42)\n            self.iter_reader = iter(\n                self.reader.shuffle(\n                    seed=rng.integers(low=0, high=100000), buffer_size=1000\n                )\n            )\n            row = next(self.iter_reader)\n        radar_frames = row[\"radar_frames\"]\n        input_frames = radar_frames[\n            -self.num_target_frames - self.num_input_frames : -self.num_target_frames\n        ]\n        target_frames = radar_frames[-self.num_target_frames :]\n        input_item = {\n            self.input_keys[0]: np.moveaxis(input_frames, [0, 1, 2, 3], [0, 2, 3, 1])\n        }\n        label_item = {\n            self.label_keys[0]: np.moveaxis(target_frames, [0, 1, 2, 3], [0, 2, 3, 1])\n        }\n        return input_item, label_item\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.DarcyFlowDataset","title":"<code>DarcyFlowDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Loads a small Darcy-Flow dataset</p> <p>Training contains 1000 samples in resolution 16x16. Testing contains 100 samples at resolution 16x16 and 50 samples at resolution 32x32.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"input\",).</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"output\",).</p> required <code>data_dir</code> <code>str</code> <p>The directory to load data from.</p> required <code>weight_dict</code> <code>Optional[Dict[str, float]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>test_resolutions</code> <code>List[int, ...]</code> <p>The resolutions to test dataset. Default is [16, 32].</p> <code>[32]</code> <code>grid_boundaries</code> <code>List[int, ...]</code> <p>The boundaries of the grid. Default is [[0,1],[0,1]].</p> <code>[[0, 1], [0, 1]]</code> <code>positional_encoding</code> <code>bool</code> <p>Whether to use positional encoding. Default is True</p> <code>True</code> <code>encode_input</code> <code>bool</code> <p>Whether to encode the input. Default is False</p> <code>False</code> <code>encode_output</code> <code>bool</code> <p>Whether to encode the output. Default is True</p> <code>True</code> <code>encoding</code> <code>str</code> <p>The type of encoding. Default is 'channel-wise'.</p> <code>'channel-wise'</code> <code>channel_dim</code> <code>int</code> <p>The location of unsqueeze. Default is 1. where to put the channel dimension. Defaults size is batch, channel, height, width</p> <code>1</code> <code>data_split</code> <code>str</code> <p>Wether to use training or test dataset. Default is 'train'.</p> <code>'train'</code> Source code in <code>ppsci/data/dataset/darcyflow_dataset.py</code> <pre><code>class DarcyFlowDataset(io.Dataset):\n    \"\"\"Loads a small Darcy-Flow dataset\n\n    Training contains 1000 samples in resolution 16x16.\n    Testing contains 100 samples at resolution 16x16 and\n    50 samples at resolution 32x32.\n\n    Args:\n        input_keys (Tuple[str, ...]): Input keys, such as (\"input\",).\n        label_keys (Tuple[str, ...]): Output keys, such as (\"output\",).\n        data_dir (str): The directory to load data from.\n        weight_dict (Optional[Dict[str, float]], optional): Define the weight of each constraint variable. Defaults to None.\n        test_resolutions (List[int,...]): The resolutions to test dataset. Default is [16, 32].\n        grid_boundaries (List[int,...]): The boundaries of the grid. Default is [[0,1],[0,1]].\n        positional_encoding (bool): Whether to use positional encoding. Default is True\n        encode_input (bool): Whether to encode the input. Default is False\n        encode_output (bool): Whether to encode the output. Default is True\n        encoding (str): The type of encoding. Default is 'channel-wise'.\n        channel_dim (int): The location of unsqueeze. Default is 1.\n            where to put the channel dimension. Defaults size is batch, channel, height, width\n        data_split (str): Wether to use training or test dataset. Default is 'train'.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        data_dir: str,\n        weight_dict: Optional[Dict[str, float]] = None,\n        test_resolutions: Tuple[int, ...] = [32],\n        train_resolution: int = 32,\n        grid_boundaries: Tuple[Tuple[int, ...], ...] = [[0, 1], [0, 1]],\n        positional_encoding: bool = True,\n        encode_input: bool = False,\n        encode_output: bool = True,\n        encoding: str = \"channel-wise\",\n        channel_dim: int = 1,\n        data_split: str = \"train\",\n    ):\n        super().__init__()\n        for res in test_resolutions:\n            if res not in [16, 32]:\n                raise ValueError(\n                    f\"Only 32 and 64 are supported for test resolution, but got {test_resolutions}\"\n                )\n\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n        self.data_dir = data_dir\n        self.weight_dict = {} if weight_dict is None else weight_dict\n        if weight_dict is not None:\n            self.weight_dict = {key: 1.0 for key in self.label_keys}\n            self.weight_dict.update(weight_dict)\n\n        self.test_resolutions = test_resolutions\n        self.train_resolution = train_resolution\n        self.grid_boundaries = grid_boundaries\n        self.positional_encoding = positional_encoding\n        self.encode_input = encode_input\n        self.encode_output = encode_output\n        self.encoding = encoding\n        self.channel_dim = channel_dim\n        self.data_split = data_split\n\n        # train path\n        path_train = (\n            Path(self.data_dir)\n            .joinpath(f\"darcy_train_{self.train_resolution}.npy\")\n            .as_posix()\n        )\n        self.x_train, self.y_train = self.read_data(path_train)\n        # test path\n        path_test_1 = (\n            Path(self.data_dir)\n            .joinpath(f\"darcy_test_{self.test_resolutions[0]}.npy\")\n            .as_posix()\n        )\n        self.x_test_1, self.y_test_1 = self.read_data(path_test_1)\n        path_test_2 = (\n            Path(self.data_dir)\n            .joinpath(f\"darcy_test_{self.test_resolutions[1]}.npy\")\n            .as_posix()\n        )\n        self.x_test_2, self.y_test_2 = self.read_data(path_test_2)\n\n        # input encoder\n        if self.encode_input:\n            self.input_encoder = self.encode_data(self.x_train)\n            self.x_train = self.input_encoder.encode(self.x_train)\n            self.x_test_1 = self.input_encoder.encode(self.x_test_1)\n            self.x_test_2 = self.input_encoder.encode(self.x_test_2)\n        else:\n            self.input_encoder = None\n        # output encoder\n        if self.encode_output:\n            self.output_encoder = self.encode_data(self.y_train)\n            self.y_train = self.output_encoder.encode(self.y_train)\n        else:\n            self.output_encoder = None\n\n        if positional_encoding:\n            self.transform_x = PositionalEmbedding2D(grid_boundaries)\n\n    def read_data(self, path):\n        # load with numpy\n        data = np.load(path, allow_pickle=True).item()\n        x = (\n            paddle.to_tensor(data[\"x\"])\n            .unsqueeze(self.channel_dim)\n            .astype(\"float32\")\n            .clone()\n        )\n        y = paddle.to_tensor(data[\"y\"]).unsqueeze(self.channel_dim).clone()\n        del data\n        return x, y\n\n    def encode_data(self, data):\n        if self.encoding == \"channel-wise\":\n            reduce_dims = list(range(data.ndim))\n        elif self.encoding == \"pixel-wise\":\n            reduce_dims = [0]\n        input_encoder = UnitGaussianNormalizer(data, reduce_dim=reduce_dims)\n        return input_encoder\n\n    def __len__(self):\n        if self.data_split == \"train\":\n            return self.x_train.shape[0]\n        elif self.data_split == \"test_16x16\":\n            return self.x_test_1.shape[0]\n        else:\n            return self.x_test_2.shape[0]\n\n    def __getitem__(self, index):\n        if self.data_split == \"train\":\n            x = self.x_train[index]\n            y = self.y_train[index]\n\n        elif self.data_split == \"test_16x16\":\n            x = self.x_test_1[index]\n            y = self.y_test_1[index]\n        else:\n            x = self.x_test_2[index]\n            y = self.y_test_2[index]\n\n        if self.transform_x is not None:\n            x = self.transform_x(x)\n\n        input_item = {self.input_keys[0]: x}\n        label_item = {self.label_keys[0]: y}\n        weight_item = self.weight_dict\n\n        return input_item, label_item, weight_item\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.SphericalSWEDataset","title":"<code>SphericalSWEDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Loads a Spherical Shallow Water equations dataset</p> <p>Training contains 200 samples in resolution 32x64. Testing contains 50 samples at resolution 32x64 and 50 samples at resolution 64x128.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input keys, such as (\"input\",).</p> required <code>label_keys</code> <code>Tuple[str, ...]</code> <p>Output keys, such as (\"output\",).</p> required <code>data_dir</code> <code>str</code> <p>The directory to load data from.</p> required <code>weight_dict</code> <code>Optional[Dict[str, float]]</code> <p>Define the weight of each constraint variable. Defaults to None.</p> <code>None</code> <code>test_resolutions</code> <code>Tuple[str, ...]</code> <p>The resolutions to test dataset. Defaults to [\"34x64\", \"64x128\"].</p> <code>['34x64', '64x128']</code> <code>train_resolution</code> <code>str</code> <p>The resolutions to train dataset. Defaults to \"34x64\".</p> <code>'34x64'</code> <code>data_split</code> <code>str</code> <p>Specify the dataset split, either 'train' , 'test_32x64',or 'test_64x128'. Defaults to \"train\".</p> <code>'train'</code> Source code in <code>ppsci/data/dataset/spherical_swe_dataset.py</code> <pre><code>class SphericalSWEDataset(io.Dataset):\n    \"\"\"Loads a Spherical Shallow Water equations dataset\n\n    Training contains 200 samples in resolution 32x64.\n    Testing contains 50 samples at resolution 32x64 and 50 samples at resolution 64x128.\n\n    Args:\n        input_keys (Tuple[str, ...]): Input keys, such as (\"input\",).\n        label_keys (Tuple[str, ...]): Output keys, such as (\"output\",).\n        data_dir (str): The directory to load data from.\n        weight_dict (Optional[Dict[str, float]], optional): Define the weight of each constraint variable.\n            Defaults to None.\n        test_resolutions (Tuple[str, ...], optional): The resolutions to test dataset. Defaults to [\"34x64\", \"64x128\"].\n        train_resolution (str, optional): The resolutions to train dataset. Defaults to \"34x64\".\n        data_split (str, optional): Specify the dataset split, either 'train' , 'test_32x64',or 'test_64x128'.\n            Defaults to \"train\".\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        label_keys: Tuple[str, ...],\n        data_dir: str,\n        weight_dict: Optional[Dict[str, float]] = None,\n        test_resolutions: Tuple[str, ...] = [\"34x64\", \"64x128\"],\n        train_resolution: str = \"34x64\",\n        data_split: str = \"train\",\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.label_keys = label_keys\n        self.data_dir = data_dir\n        self.weight_dict = {} if weight_dict is None else weight_dict\n        if weight_dict is not None:\n            self.weight_dict = {key: 1.0 for key in self.label_keys}\n            self.weight_dict.update(weight_dict)\n\n        self.test_resolutions = test_resolutions\n        self.train_resolution = train_resolution\n        self.data_split = data_split\n\n        # train path\n        path_train = (\n            Path(self.data_dir)\n            .joinpath(f\"train_SWE_{self.train_resolution}.npy\")\n            .as_posix()\n        )\n        self.x_train, self.y_train = self.read_data(path_train)\n        # test path\n        path_test_1 = (\n            Path(self.data_dir)\n            .joinpath(f\"test_SWE_{self.test_resolutions[0]}.npy\")\n            .as_posix()\n        )\n        self.x_test_1, self.y_test_1 = self.read_data(path_test_1)\n        path_test_2 = (\n            Path(self.data_dir)\n            .joinpath(f\"test_SWE_{self.test_resolutions[1]}.npy\")\n            .as_posix()\n        )\n        self.x_test_2, self.y_test_2 = self.read_data(path_test_2)\n\n    def read_data(self, path):\n        # load with numpy\n        data = np.load(path, allow_pickle=True).item()\n        x = data[\"x\"].astype(\"float32\")\n        y = data[\"y\"].astype(\"float32\")\n        del data\n        return x, y\n\n    def __len__(self):\n        if self.data_split == \"train\":\n            return self.x_train.shape[0]\n        elif self.data_split == \"test_32x64\":\n            return self.x_test_1.shape[0]\n        else:\n            return self.x_test_2.shape[0]\n\n    def __getitem__(self, index):\n        if self.data_split == \"train\":\n            x = self.x_train[index]\n            y = self.y_train[index]\n\n        elif self.data_split == \"test_32x64\":\n            x = self.x_test_1[index]\n            y = self.y_test_1[index]\n        else:\n            x = self.x_test_2[index]\n            y = self.y_test_2[index]\n\n        input_item = {self.input_keys[0]: x}\n        label_item = {self.label_keys[0]: y}\n        weight_item = self.weight_dict\n\n        return input_item, label_item, weight_item\n</code></pre>"},{"location":"zh/api/data/dataset/#ppsci.data.dataset.build_dataset","title":"<code>build_dataset(cfg)</code>","text":"<p>Build dataset</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>List[DictConfig]</code> <p>Dataset config list.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Dict[str, io.Dataset]: dataset.</p> Source code in <code>ppsci/data/dataset/__init__.py</code> <pre><code>def build_dataset(cfg) -&gt; \"io.Dataset\":\n    \"\"\"Build dataset\n\n    Args:\n        cfg (List[DictConfig]): Dataset config list.\n\n    Returns:\n        Dict[str, io.Dataset]: dataset.\n    \"\"\"\n    cfg = copy.deepcopy(cfg)\n\n    dataset_cls = cfg.pop(\"name\")\n    if \"transforms\" in cfg:\n        cfg[\"transforms\"] = transform.build_transforms(cfg.pop(\"transforms\"))\n\n    dataset = eval(dataset_cls)(**cfg)\n\n    logger.debug(str(dataset))\n\n    return dataset\n</code></pre>"},{"location":"zh/api/data/process/batch_transform/","title":"ppsci.data.batch_transform","text":""},{"location":"zh/api/data/process/batch_transform/#databatch_transform","title":"Data.batch_transform(\u6279\u9884\u5904\u7406) \u6a21\u5757","text":""},{"location":"zh/api/data/process/batch_transform/#ppsci.data.process.transform","title":"<code>ppsci.data.process.transform</code>","text":""},{"location":"zh/api/data/process/transform/","title":"ppsci.data.transform","text":""},{"location":"zh/api/data/process/transform/#datatransform","title":"Data.transform(\u9884\u5904\u7406) \u6a21\u5757","text":""},{"location":"zh/api/data/process/transform/#ppsci.data.process.transform","title":"<code>ppsci.data.process.transform</code>","text":""},{"location":"zh/api/data/process/transform/#ppsci.data.process.transform.CropData","title":"<code>CropData</code>","text":"<p>Crop data class.</p> <p>This class is used to crop data based on a specified bounding box.</p> <p>NOTE: This transform will modify the input data dict inplace.</p> <p>Parameters:</p> Name Type Description Default <code>xmin</code> <code>Tuple[int, ...]</code> <p>Bottom left corner point, [x0, y0].</p> required <code>xmax</code> <code>Tuple[int, ...]</code> <p>Top right corner point, [x1, y1].</p> required <code>apply_keys</code> <code>Tuple[str, ...]</code> <p>Which data is the crop method applied to. Defaults to (\"input\", \"label\").</p> <code>('input', 'label')</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; crop_data = ppsci.data.transform.CropData((0, 0), (256, 512))\n&gt;&gt;&gt; input_item = {\"input\": np.zeros((3, 720, 1440))}\n&gt;&gt;&gt; label_item = {\"label\": np.zeros((3, 720, 1440))}\n&gt;&gt;&gt; weight_item = {\"weight\": np.ones((3, 720, 1440))}\n&gt;&gt;&gt; input_item, label_item, weight_item = crop_data(input_item, label_item, weight_item)\n&gt;&gt;&gt; print(input_item[\"input\"].shape)\n(3, 256, 512)\n&gt;&gt;&gt; print(label_item[\"label\"].shape)\n(3, 256, 512)\n</code></pre> Source code in <code>ppsci/data/process/transform/preprocess.py</code> <pre><code>class CropData:\n    \"\"\"Crop data class.\n\n    This class is used to crop data based on a specified bounding box.\n\n    NOTE: This transform will modify the input data dict inplace.\n\n    Args:\n        xmin (Tuple[int, ...]): Bottom left corner point, [x0, y0].\n        xmax (Tuple[int, ...]): Top right corner point, [x1, y1].\n        apply_keys (Tuple[str, ...], optional): Which data is the crop method applied to. Defaults to (\"input\", \"label\").\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; crop_data = ppsci.data.transform.CropData((0, 0), (256, 512))\n        &gt;&gt;&gt; input_item = {\"input\": np.zeros((3, 720, 1440))}\n        &gt;&gt;&gt; label_item = {\"label\": np.zeros((3, 720, 1440))}\n        &gt;&gt;&gt; weight_item = {\"weight\": np.ones((3, 720, 1440))}\n        &gt;&gt;&gt; input_item, label_item, weight_item = crop_data(input_item, label_item, weight_item)\n        &gt;&gt;&gt; print(input_item[\"input\"].shape)\n        (3, 256, 512)\n        &gt;&gt;&gt; print(label_item[\"label\"].shape)\n        (3, 256, 512)\n    \"\"\"\n\n    def __init__(\n        self,\n        xmin: Tuple[int, ...],\n        xmax: Tuple[int, ...],\n        apply_keys: Tuple[str, ...] = (\"input\", \"label\"),\n    ):\n        if len(apply_keys) == 0 or len(set(apply_keys) | {\"input\", \"label\"}) &gt; 2:\n            raise ValueError(\n                f\"apply_keys should be a non empty subset of ('input', 'label'), but got {apply_keys}\"\n            )\n        self.xmin = xmin\n        self.xmax = xmax\n        self.apply_keys = apply_keys\n\n    def __call__(self, input_item, label_item, weight_item):\n        if \"input\" in self.apply_keys:\n            for key, value in input_item.items():\n                input_item[key] = value[\n                    :, self.xmin[0] : self.xmax[0], self.xmin[1] : self.xmax[1]\n                ]\n        if \"label\" in self.apply_keys:\n            for key, value in label_item.items():\n                label_item[key] = value[\n                    :, self.xmin[0] : self.xmax[0], self.xmin[1] : self.xmax[1]\n                ]\n        return input_item, label_item, weight_item\n</code></pre>"},{"location":"zh/api/data/process/transform/#ppsci.data.process.transform.FunctionalTransform","title":"<code>FunctionalTransform</code>","text":"<p>Functional data transform class, which allows to use custom data transform function from given transform_func for special cases.</p> <p>Parameters:</p> Name Type Description Default <code>transform_func</code> <code>Callable</code> <p>Function of data transform.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This is the transform_func function. It takes three dictionaries as input: data_dict, label_dict, and weight_dict.\n&gt;&gt;&gt; # The function will perform some transformations on the data in data_dict, convert all labels in label_dict to uppercase,\n&gt;&gt;&gt; # and modify the weights in weight_dict by dividing each weight by 10.\n&gt;&gt;&gt; # Finally, it returns the transformed data, labels, and weights as a tuple.\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; def transform_func(data_dict, label_dict, weight_dict):\n...     for key in data_dict:\n...         data_dict[key] = data_dict[key] * 2\n...     for key in label_dict:\n...         label_dict[key] = label_dict[key] + 1.0\n...     for key in weight_dict:\n...         weight_dict[key] = weight_dict[key] / 10\n...     return data_dict, label_dict, weight_dict\n&gt;&gt;&gt; transform = ppsci.data.transform.FunctionalTransform(transform_func)\n&gt;&gt;&gt; # Define some sample data, labels, and weights\n&gt;&gt;&gt; data = {'feature1': np.array([1, 2, 3]), 'feature2': np.array([4, 5, 6])}\n&gt;&gt;&gt; label = {'class': 0.0, 'instance': 0.1}\n&gt;&gt;&gt; weight = {'weight1': 0.5, 'weight2': 0.5}\n&gt;&gt;&gt; # Apply the transform function to the data, labels, and weights using the FunctionalTransform instance\n&gt;&gt;&gt; transformed_data = transform(data, label, weight)\n&gt;&gt;&gt; print(transformed_data)\n({'feature1': array([2, 4, 6]), 'feature2': array([ 8, 10, 12])}, {'class': 1.0, 'instance': 1.1}, {'weight1': 0.05, 'weight2': 0.05})\n</code></pre> Source code in <code>ppsci/data/process/transform/preprocess.py</code> <pre><code>class FunctionalTransform:\n    \"\"\"Functional data transform class, which allows to use custom data transform function from given transform_func for special cases.\n\n    Args:\n        transform_func (Callable): Function of data transform.\n\n    Examples:\n        &gt;&gt;&gt; # This is the transform_func function. It takes three dictionaries as input: data_dict, label_dict, and weight_dict.\n        &gt;&gt;&gt; # The function will perform some transformations on the data in data_dict, convert all labels in label_dict to uppercase,\n        &gt;&gt;&gt; # and modify the weights in weight_dict by dividing each weight by 10.\n        &gt;&gt;&gt; # Finally, it returns the transformed data, labels, and weights as a tuple.\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; def transform_func(data_dict, label_dict, weight_dict):\n        ...     for key in data_dict:\n        ...         data_dict[key] = data_dict[key] * 2\n        ...     for key in label_dict:\n        ...         label_dict[key] = label_dict[key] + 1.0\n        ...     for key in weight_dict:\n        ...         weight_dict[key] = weight_dict[key] / 10\n        ...     return data_dict, label_dict, weight_dict\n        &gt;&gt;&gt; transform = ppsci.data.transform.FunctionalTransform(transform_func)\n        &gt;&gt;&gt; # Define some sample data, labels, and weights\n        &gt;&gt;&gt; data = {'feature1': np.array([1, 2, 3]), 'feature2': np.array([4, 5, 6])}\n        &gt;&gt;&gt; label = {'class': 0.0, 'instance': 0.1}\n        &gt;&gt;&gt; weight = {'weight1': 0.5, 'weight2': 0.5}\n        &gt;&gt;&gt; # Apply the transform function to the data, labels, and weights using the FunctionalTransform instance\n        &gt;&gt;&gt; transformed_data = transform(data, label, weight)\n        &gt;&gt;&gt; print(transformed_data)\n        ({'feature1': array([2, 4, 6]), 'feature2': array([ 8, 10, 12])}, {'class': 1.0, 'instance': 1.1}, {'weight1': 0.05, 'weight2': 0.05})\n    \"\"\"\n\n    def __init__(\n        self,\n        transform_func: Callable,\n    ):\n        self.transform_func = transform_func\n\n    def __call__(\n        self, *data: Tuple[Dict[str, np.ndarray], ...]\n    ) -&gt; Tuple[Dict[str, np.ndarray], ...]:\n        data_dict, label_dict, weight_dict = data\n        data_dict_copy = {**data_dict}\n        label_dict_copy = {**label_dict}\n        weight_dict_copy = {**weight_dict} if weight_dict is not None else {}\n        return self.transform_func(data_dict_copy, label_dict_copy, weight_dict_copy)\n</code></pre>"},{"location":"zh/api/data/process/transform/#ppsci.data.process.transform.Log1p","title":"<code>Log1p</code>","text":"<p>Calculates the natural logarithm of one plus the data, element-wise.</p> <p>NOTE: This transform will modify the input data dict inplace.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>Scale data. Defaults to 1.0.</p> <code>1.0</code> <code>apply_keys</code> <code>Tuple[str, ...]</code> <p>Which data is the log1p method applied to. Defaults to (\"input\", \"label\").</p> <code>('input', 'label')</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; log1p = ppsci.data.transform.Log1p(1e-5)\n&gt;&gt;&gt; input_item = {\"data\": np.array([1.0, 2.0, 3.0])}\n&gt;&gt;&gt; label_item = {\"data\": np.array([4.0, 5.0, 6.0])}\n&gt;&gt;&gt; weight_item = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt; input_item_transformed, label_item_transformed, weight_item_transformed = log1p(input_item, label_item, weight_item)\n&gt;&gt;&gt; print(input_item_transformed)\n{'data': array([11.51293546, 12.20607765, 12.61154109])}\n&gt;&gt;&gt; print(label_item_transformed)\n{'data': array([12.89922233, 13.12236538, 13.3046866 ])}\n&gt;&gt;&gt; print(weight_item_transformed)\n[0.1 0.2 0.3]\n</code></pre> Source code in <code>ppsci/data/process/transform/preprocess.py</code> <pre><code>class Log1p:\n    \"\"\"Calculates the natural logarithm of one plus the data, element-wise.\n\n    NOTE: This transform will modify the input data dict inplace.\n\n    Args:\n        scale (float, optional): Scale data. Defaults to 1.0.\n        apply_keys (Tuple[str, ...], optional): Which data is the log1p method applied to. Defaults to (\"input\", \"label\").\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; log1p = ppsci.data.transform.Log1p(1e-5)\n        &gt;&gt;&gt; input_item = {\"data\": np.array([1.0, 2.0, 3.0])}\n        &gt;&gt;&gt; label_item = {\"data\": np.array([4.0, 5.0, 6.0])}\n        &gt;&gt;&gt; weight_item = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; input_item_transformed, label_item_transformed, weight_item_transformed = log1p(input_item, label_item, weight_item)\n        &gt;&gt;&gt; print(input_item_transformed)\n        {'data': array([11.51293546, 12.20607765, 12.61154109])}\n        &gt;&gt;&gt; print(label_item_transformed)\n        {'data': array([12.89922233, 13.12236538, 13.3046866 ])}\n        &gt;&gt;&gt; print(weight_item_transformed)\n        [0.1 0.2 0.3]\n    \"\"\"\n\n    def __init__(\n        self,\n        scale: float = 1.0,\n        apply_keys: Tuple[str, ...] = (\"input\", \"label\"),\n    ):\n        if len(apply_keys) == 0 or len(set(apply_keys) | {\"input\", \"label\"}) &gt; 2:\n            raise ValueError(\n                f\"apply_keys should be a non empty subset of ('input', 'label'), but got {apply_keys}\"\n            )\n        self.scale = scale\n        self.apply_keys = apply_keys\n\n    def __call__(self, input_item, label_item, weight_item):\n        if \"input\" in self.apply_keys:\n            for key, value in input_item.items():\n                input_item[key] = np.log1p(value / self.scale)\n        if \"label\" in self.apply_keys:\n            for key, value in label_item.items():\n                label_item[key] = np.log1p(value / self.scale)\n        return input_item, label_item, weight_item\n</code></pre>"},{"location":"zh/api/data/process/transform/#ppsci.data.process.transform.Normalize","title":"<code>Normalize</code>","text":"<p>Normalize data class.</p> <p>NOTE: This transform will modify the input data dict inplace.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Union[ndarray, Tuple[float, ...]]</code> <p>Mean of training dataset.</p> required <code>std</code> <code>Union[ndarray, Tuple[float, ...]]</code> <p>Standard Deviation of training dataset.</p> required <code>apply_keys</code> <code>Tuple[str, ...]</code> <p>Which data is the normalization method applied to. Defaults to (\"input\", \"label\").</p> <code>('input', 'label')</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; normalize = ppsci.data.transform.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))\n&gt;&gt;&gt; input_item = {\"data\": np.array([1.0, 2.0, 3.0])}\n&gt;&gt;&gt; label_item = {\"data\": np.array([4.0, 5.0, 6.0])}\n&gt;&gt;&gt; weight_item = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt; normalized_item = normalize(input_item, label_item, weight_item)\n&gt;&gt;&gt; print(normalized_item)\n({'data': array([1., 2., 3.])}, {'data': array([4., 5., 6.])}, array([0.1, 0.2, 0.3]))\n</code></pre> Source code in <code>ppsci/data/process/transform/preprocess.py</code> <pre><code>class Normalize:\n    \"\"\"Normalize data class.\n\n    NOTE: This transform will modify the input data dict inplace.\n\n    Args:\n        mean (Union[np.ndarray, Tuple[float, ...]]): Mean of training dataset.\n        std (Union[np.ndarray, Tuple[float, ...]]): Standard Deviation of training dataset.\n        apply_keys (Tuple[str, ...], optional): Which data is the normalization method applied to. Defaults to (\"input\", \"label\").\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; normalize = ppsci.data.transform.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))\n        &gt;&gt;&gt; input_item = {\"data\": np.array([1.0, 2.0, 3.0])}\n        &gt;&gt;&gt; label_item = {\"data\": np.array([4.0, 5.0, 6.0])}\n        &gt;&gt;&gt; weight_item = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; normalized_item = normalize(input_item, label_item, weight_item)\n        &gt;&gt;&gt; print(normalized_item)\n        ({'data': array([1., 2., 3.])}, {'data': array([4., 5., 6.])}, array([0.1, 0.2, 0.3]))\n    \"\"\"\n\n    def __init__(\n        self,\n        mean: Union[np.ndarray, Tuple[float, ...]],\n        std: Union[np.ndarray, Tuple[float, ...]],\n        apply_keys: Tuple[str, ...] = (\"input\", \"label\"),\n    ):\n        if len(apply_keys) == 0 or len(set(apply_keys) | {\"input\", \"label\"}) &gt; 2:\n            raise ValueError(\n                f\"apply_keys should be a non empty subset of ('input', 'label'), but got {apply_keys}\"\n            )\n        self.mean = mean\n        self.std = std\n        self.apply_keys = apply_keys\n\n    def __call__(self, input_item, label_item, weight_item):\n        if \"input\" in self.apply_keys:\n            for key, value in input_item.items():\n                input_item[key] = (value - self.mean) / self.std\n        if \"label\" in self.apply_keys:\n            for key, value in label_item.items():\n                label_item[key] = (value - self.mean) / self.std\n        return input_item, label_item, weight_item\n</code></pre>"},{"location":"zh/api/data/process/transform/#ppsci.data.process.transform.Scale","title":"<code>Scale</code>","text":"<p>Scale class for data transformation.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>Dict[str, float]</code> <p>Scale the input data according to the variable name and coefficient specified in scale.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; translate = ppsci.data.transform.Scale({\"x\": 1.5, \"y\": 2.0})\n&gt;&gt;&gt; input_dict = {\"x\": 10, \"y\": 20}\n&gt;&gt;&gt; label_dict = {\"x\": 100, \"y\": 200}\n&gt;&gt;&gt; weight_dict = {\"x\": 1000, \"y\": 2000}\n&gt;&gt;&gt; input_dict_scaled, label_dict_scaled, weight_dict_scaled = translate(input_dict, label_dict, weight_dict)\n&gt;&gt;&gt; print(input_dict_scaled)\n{'x': 15.0, 'y': 40.0}\n&gt;&gt;&gt; print(label_dict_scaled)\n{'x': 100, 'y': 200}\n&gt;&gt;&gt; print(weight_dict_scaled)\n{'x': 1000, 'y': 2000}\n</code></pre> Source code in <code>ppsci/data/process/transform/preprocess.py</code> <pre><code>class Scale:\n    \"\"\"Scale class for data transformation.\n\n    Args:\n        scale (Dict[str, float]): Scale the input data according to the variable name\n            and coefficient specified in scale.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; translate = ppsci.data.transform.Scale({\"x\": 1.5, \"y\": 2.0})\n        &gt;&gt;&gt; input_dict = {\"x\": 10, \"y\": 20}\n        &gt;&gt;&gt; label_dict = {\"x\": 100, \"y\": 200}\n        &gt;&gt;&gt; weight_dict = {\"x\": 1000, \"y\": 2000}\n        &gt;&gt;&gt; input_dict_scaled, label_dict_scaled, weight_dict_scaled = translate(input_dict, label_dict, weight_dict)\n        &gt;&gt;&gt; print(input_dict_scaled)\n        {'x': 15.0, 'y': 40.0}\n        &gt;&gt;&gt; print(label_dict_scaled)\n        {'x': 100, 'y': 200}\n        &gt;&gt;&gt; print(weight_dict_scaled)\n        {'x': 1000, 'y': 2000}\n    \"\"\"\n\n    def __init__(self, scale: Dict[str, float]):\n        self.scale = scale\n\n    def __call__(self, input_dict, label_dict, weight_dict):\n        input_dict_copy = {**input_dict}\n        for key in self.scale:\n            if key in input_dict:\n                input_dict_copy[key] *= self.scale[key]\n        return input_dict_copy, label_dict, weight_dict\n</code></pre>"},{"location":"zh/api/data/process/transform/#ppsci.data.process.transform.SqueezeData","title":"<code>SqueezeData</code>","text":"<p>Squeeze data class.</p> <p>NOTE: This transform will modify the input data dict inplace.</p> <p>Parameters:</p> Name Type Description Default <code>apply_keys</code> <code>Tuple[str, ...]</code> <p>Which data is the squeeze method applied to. Defaults to (\"input\", \"label\").</p> <code>('input', 'label')</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; squeeze_data = ppsci.data.transform.SqueezeData()\n&gt;&gt;&gt; input_data = {\"input\": np.random.rand(10, 224, 224)}\n&gt;&gt;&gt; label_data = {\"label\": np.random.rand(10, 224, 224)}\n&gt;&gt;&gt; weight_data = {\"weight\": np.random.rand(10, 224, 224)}\n&gt;&gt;&gt; input_data_squeezed, label_data_squeezed, weight_data_squeezed = squeeze_data(input_data, label_data, weight_data)\n</code></pre> Source code in <code>ppsci/data/process/transform/preprocess.py</code> <pre><code>class SqueezeData:\n    \"\"\"Squeeze data class.\n\n    NOTE: This transform will modify the input data dict inplace.\n\n    Args:\n        apply_keys (Tuple[str, ...], optional): Which data is the squeeze method applied to. Defaults to (\"input\", \"label\").\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; squeeze_data = ppsci.data.transform.SqueezeData()\n        &gt;&gt;&gt; input_data = {\"input\": np.random.rand(10, 224, 224)}\n        &gt;&gt;&gt; label_data = {\"label\": np.random.rand(10, 224, 224)}\n        &gt;&gt;&gt; weight_data = {\"weight\": np.random.rand(10, 224, 224)}\n        &gt;&gt;&gt; input_data_squeezed, label_data_squeezed, weight_data_squeezed = squeeze_data(input_data, label_data, weight_data)\n    \"\"\"\n\n    def __init__(self, apply_keys: Tuple[str, ...] = (\"input\", \"label\")):\n        if len(apply_keys) == 0 or len(set(apply_keys) | {\"input\", \"label\"}) &gt; 2:\n            raise ValueError(\n                f\"apply_keys should be a non empty subset of ('input', 'label'), but got {apply_keys}\"\n            )\n        self.apply_keys = apply_keys\n\n    def __call__(self, input_item, label_item, weight_item):\n        if \"input\" in self.apply_keys:\n            for key, value in input_item.items():\n                if value.ndim == 4:\n                    B, C, H, W = value.shape\n                    input_item[key] = value.reshape((B * C, H, W))\n                if value.ndim != 3:\n                    raise ValueError(\n                        f\"Only support squeeze data to ndim=3 now, but got ndim={value.ndim}\"\n                    )\n        if \"label\" in self.apply_keys:\n            for key, value in label_item.items():\n                if value.ndim == 4:\n                    B, C, H, W = value.shape\n                    label_item[key] = value.reshape((B * C, H, W))\n                if value.ndim != 3:\n                    raise ValueError(\n                        f\"Only support squeeze data to ndim=3 now, but got ndim={value.ndim}\"\n                    )\n        return input_item, label_item, weight_item\n</code></pre>"},{"location":"zh/api/data/process/transform/#ppsci.data.process.transform.Translate","title":"<code>Translate</code>","text":"<p>Translate class.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>Dict[str, float]</code> <p>Shift the input data according to the variable name and coefficient specified in offset.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import numpy as np\n</code></pre> <pre><code>&gt;&gt;&gt; input_dict = {\"x\": np.array([5.0, 10.0]), \"y\": np.array([20.0, 40.0])}\n&gt;&gt;&gt; label_dict = {\"x\": np.array([1.0, 2.0]), \"y\": np.array([3.0, 4.0])}\n&gt;&gt;&gt; weight_dict = {\"x\": np.array([10.0, 20.0]), \"y\": np.array([30.0, 40.0])}\n</code></pre> <pre><code>&gt;&gt;&gt; translate = ppsci.data.transform.Translate({\"x\": 1.0, \"y\": -1.0})\n&gt;&gt;&gt; translated_input_dict, translated_label_dict, translated_weight_dict = translate(input_dict, label_dict, weight_dict)\n</code></pre> <pre><code>&gt;&gt;&gt; print(translated_input_dict)\n{'x': array([ 6., 11.]), 'y': array([19., 39.])}\n&gt;&gt;&gt; print(translated_label_dict)\n{'x': array([1., 2.]), 'y': array([3., 4.])}\n&gt;&gt;&gt; print(translated_weight_dict)\n{'x': array([10., 20.]), 'y': array([30., 40.])}\n</code></pre> Source code in <code>ppsci/data/process/transform/preprocess.py</code> <pre><code>class Translate:\n    \"\"\"Translate class.\n\n    Args:\n        offset (Dict[str, float]): Shift the input data according to the variable name\n            and coefficient specified in offset.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import numpy as np\n\n        &gt;&gt;&gt; input_dict = {\"x\": np.array([5.0, 10.0]), \"y\": np.array([20.0, 40.0])}\n        &gt;&gt;&gt; label_dict = {\"x\": np.array([1.0, 2.0]), \"y\": np.array([3.0, 4.0])}\n        &gt;&gt;&gt; weight_dict = {\"x\": np.array([10.0, 20.0]), \"y\": np.array([30.0, 40.0])}\n\n        &gt;&gt;&gt; translate = ppsci.data.transform.Translate({\"x\": 1.0, \"y\": -1.0})\n        &gt;&gt;&gt; translated_input_dict, translated_label_dict, translated_weight_dict = translate(input_dict, label_dict, weight_dict)\n\n        &gt;&gt;&gt; print(translated_input_dict)\n        {'x': array([ 6., 11.]), 'y': array([19., 39.])}\n        &gt;&gt;&gt; print(translated_label_dict)\n        {'x': array([1., 2.]), 'y': array([3., 4.])}\n        &gt;&gt;&gt; print(translated_weight_dict)\n        {'x': array([10., 20.]), 'y': array([30., 40.])}\n    \"\"\"\n\n    def __init__(self, offset: Dict[str, float]):\n        self.offset = offset\n\n    def __call__(self, input_dict, label_dict, weight_dict):\n        input_dict_copy = {**input_dict}\n        for key in self.offset:\n            if key in input_dict:\n                input_dict_copy[key] += self.offset[key]\n        return input_dict_copy, label_dict, weight_dict\n</code></pre>"},{"location":"zh/api/data/process/transform/#ppsci.data.process.transform.build_transforms","title":"<code>build_transforms(cfg)</code>","text":"Source code in <code>ppsci/data/process/transform/__init__.py</code> <pre><code>def build_transforms(cfg):\n    if not cfg:\n        return Compose([])\n    cfg = copy.deepcopy(cfg)\n\n    transform_list = []\n    for _item in cfg:\n        transform_cls = next(iter(_item.keys()))\n        transform_cfg = _item[transform_cls]\n        transform = eval(transform_cls)(**transform_cfg)\n        transform_list.append(transform)\n\n    return Compose(transform_list)\n</code></pre>"},{"location":"zh/api/depoly/python_infer/","title":"deploy.python_infer","text":""},{"location":"zh/api/depoly/python_infer/#python_inferpython","title":"Python_infer(Python \u63a8\u7406) \u6a21\u5757","text":""},{"location":"zh/api/depoly/python_infer/#deploy.python_infer","title":"<code>deploy.python_infer</code>","text":""},{"location":"zh/api/depoly/python_infer/#deploy.python_infer.Predictor","title":"<code>Predictor</code>","text":"<p>Initializes the inference engine with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>pdmodel_path</code> <code>Optional[str]</code> <p>Path to the PaddlePaddle model file. Defaults to None.</p> <code>None</code> <code>pdiparams_path</code> <code>Optional[str]</code> <p>Path to the PaddlePaddle model parameters file. Defaults to None.</p> <code>None</code> <code>device</code> <code>Literal['gpu', 'cpu', 'npu', 'xpu']</code> <p>Device to use for inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>engine</code> <code>Literal['native', 'tensorrt', 'onnx', 'mkldnn']</code> <p>Inference engine to use. Defaults to \"native\".</p> <code>'native'</code> <code>precision</code> <code>Literal['fp32', 'fp16', 'int8']</code> <p>Precision to use for inference. Defaults to \"fp32\".</p> <code>'fp32'</code> <code>onnx_path</code> <code>Optional[str]</code> <p>Path to the ONNX model file. Defaults to None.</p> <code>None</code> <code>ir_optim</code> <code>bool</code> <p>Whether to use IR optimization. Defaults to True.</p> <code>True</code> <code>min_subgraph_size</code> <code>int</code> <p>Minimum subgraph size for IR optimization. Defaults to 15.</p> <code>15</code> <code>gpu_mem</code> <code>int</code> <p>Initial size of GPU memory pool(MB). Defaults to 500(MB).</p> <code>500</code> <code>gpu_id</code> <code>int</code> <p>GPU ID to use. Defaults to 0.</p> <code>0</code> <code>num_cpu_threads</code> <code>int</code> <p>Number of CPU threads to use. Defaults to 1.</p> <code>10</code> Source code in <code>deploy/python_infer/base.py</code> <pre><code>class Predictor:\n    \"\"\"\n    Initializes the inference engine with the given parameters.\n\n    Args:\n        pdmodel_path (Optional[str]): Path to the PaddlePaddle model file. Defaults to None.\n        pdiparams_path (Optional[str]): Path to the PaddlePaddle model parameters file. Defaults to None.\n        device (Literal[\"gpu\", \"cpu\", \"npu\", \"xpu\"], optional): Device to use for inference. Defaults to \"cpu\".\n        engine (Literal[\"native\", \"tensorrt\", \"onnx\", \"mkldnn\"], optional): Inference engine to use. Defaults to \"native\".\n        precision (Literal[\"fp32\", \"fp16\", \"int8\"], optional): Precision to use for inference. Defaults to \"fp32\".\n        onnx_path (Optional[str], optional): Path to the ONNX model file. Defaults to None.\n        ir_optim (bool, optional): Whether to use IR optimization. Defaults to True.\n        min_subgraph_size (int, optional): Minimum subgraph size for IR optimization. Defaults to 15.\n        gpu_mem (int, optional): Initial size of GPU memory pool(MB). Defaults to 500(MB).\n        gpu_id (int, optional): GPU ID to use. Defaults to 0.\n        num_cpu_threads (int, optional): Number of CPU threads to use. Defaults to 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        pdmodel_path: Optional[str] = None,\n        pdiparams_path: Optional[str] = None,\n        *,\n        device: Literal[\"gpu\", \"cpu\", \"npu\", \"xpu\"] = \"cpu\",\n        engine: Literal[\"native\", \"tensorrt\", \"onnx\", \"mkldnn\"] = \"native\",\n        precision: Literal[\"fp32\", \"fp16\", \"int8\"] = \"fp32\",\n        onnx_path: Optional[str] = None,\n        ir_optim: bool = True,\n        min_subgraph_size: int = 15,\n        gpu_mem: int = 500,\n        gpu_id: int = 0,\n        max_batch_size: int = 10,\n        num_cpu_threads: int = 10,\n    ):\n        self.pdmodel_path = pdmodel_path\n        self.pdiparams_path = pdiparams_path\n\n        self._check_device(device)\n        self.device = device\n        self._check_engine(engine)\n        self.engine = engine\n        self._check_precision(precision)\n        self.precision = precision\n        self._compatibility_check()\n\n        self.onnx_path = onnx_path\n        self.ir_optim = ir_optim\n        self.min_subgraph_size = min_subgraph_size\n        self.gpu_mem = gpu_mem\n        self.gpu_id = gpu_id\n        self.max_batch_size = max_batch_size\n        self.num_cpu_threads = num_cpu_threads\n\n        if self.engine == \"onnx\":\n            self.predictor, self.config = self._create_onnx_predictor()\n        else:\n            self.predictor, self.config = self._create_paddle_predictor()\n\n        logger.message(\n            f\"Inference with engine: {self.engine}, precision: {self.precision}, \"\n            f\"device: {self.device}.\"\n        )\n\n    def predict(self, input_dict):\n        raise NotImplementedError\n\n    def _create_paddle_predictor(\n        self,\n    ) -&gt; Tuple[paddle_inference.Predictor, paddle_inference.Config]:\n        if not osp.exists(self.pdmodel_path):\n            raise FileNotFoundError(\n                f\"Given 'pdmodel_path': {self.pdmodel_path} does not exist. \"\n                \"Please check if it is correct.\"\n            )\n        if not osp.exists(self.pdiparams_path):\n            raise FileNotFoundError(\n                f\"Given 'pdiparams_path': {self.pdiparams_path} does not exist. \"\n                \"Please check if it is correct.\"\n            )\n\n        config = paddle_inference.Config(self.pdmodel_path, self.pdiparams_path)\n        if self.device == \"gpu\":\n            config.enable_use_gpu(self.gpu_mem, self.gpu_id)\n            if self.engine == \"tensorrt\":\n                if self.precision == \"fp16\":\n                    precision = paddle_inference.Config.Precision.Half\n                elif self.precision == \"int8\":\n                    precision = paddle_inference.Config.Precision.Int8\n                else:\n                    precision = paddle_inference.Config.Precision.Float32\n                config.enable_tensorrt_engine(\n                    workspace_size=1 &lt;&lt; 30,\n                    precision_mode=precision,\n                    max_batch_size=self.max_batch_size,\n                    min_subgraph_size=self.min_subgraph_size,\n                    use_calib_mode=False,\n                )\n                # collect shape\n                pdmodel_dir = osp.dirname(self.pdmodel_path)\n                trt_shape_path = osp.join(pdmodel_dir, \"trt_dynamic_shape.txt\")\n\n                if not osp.exists(trt_shape_path):\n                    config.collect_shape_range_info(trt_shape_path)\n                    logger.message(\n                        f\"Save collected dynamic shape info to: {trt_shape_path}\"\n                    )\n                try:\n                    config.enable_tuned_tensorrt_dynamic_shape(trt_shape_path, True)\n                except Exception as e:\n                    logger.warning(e)\n                    logger.warning(\n                        \"TRT dynamic shape is disabled for your paddlepaddle &lt; 2.3.0\"\n                    )\n\n        elif self.device == \"npu\":\n            config.enable_custom_device(\"npu\")\n        elif self.device == \"xpu\":\n            config.enable_xpu(10 * 1024 * 1024)\n        else:\n            config.disable_gpu()\n            if self.engine == \"mkldnn\":\n                # 'set_mkldnn_cache_capatity' is not available on macOS\n                if platform.system() != \"Darwin\":\n                    ...\n                    # cache 10 different shapes for mkldnn to avoid memory leak\n                    # config.set_mkldnn_cache_capacity(10)\n                config.enable_mkldnn()\n\n                if self.precision == \"fp16\":\n                    config.enable_mkldnn_bfloat16()\n\n                config.set_cpu_math_library_num_threads(self.num_cpu_threads)\n\n        # enable memory optim\n        config.enable_memory_optim()\n        # config.disable_glog_info()\n        # enable zero copy\n        config.switch_use_feed_fetch_ops(False)\n        config.switch_ir_optim(self.ir_optim)\n\n        predictor = paddle_inference.create_predictor(config)\n        return predictor, config\n\n    def _create_onnx_predictor(\n        self,\n    ) -&gt; Tuple[\"onnxruntime.InferenceSession\", \"onnxruntime.SessionOptions\"]:\n        if not osp.exists(self.onnx_path):\n            raise FileNotFoundError(\n                f\"Given 'onnx_path' {self.onnx_path} does not exist. \"\n                \"Please check if it is correct.\"\n            )\n\n        try:\n            import onnxruntime as ort\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"Please install onnxruntime with `pip install onnxruntime`.\"\n            )\n\n        # set config for onnx predictor\n        config = ort.SessionOptions()\n        config.intra_op_num_threads = self.num_cpu_threads\n        if self.ir_optim:\n            config.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n\n        # instantiate onnx predictor\n        providers = (\n            [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n            if self.device != \"cpu\"\n            else [\"CPUExecutionProvider\"]\n        )\n        predictor = ort.InferenceSession(\n            self.onnx_path, sess_options=config, providers=providers\n        )\n        return predictor, config\n\n    def _check_device(self, device: str):\n        if device not in [\"gpu\", \"cpu\", \"npu\", \"xpu\"]:\n            raise ValueError(\n                \"Inference only supports 'gpu', 'cpu', 'npu' and 'xpu' devices, \"\n                f\"but got {device}.\"\n            )\n\n    def _check_engine(self, engine: str):\n        if engine not in [\"native\", \"tensorrt\", \"onnx\", \"mkldnn\"]:\n            raise ValueError(\n                \"Inference only supports 'native', 'tensorrt', 'onnx' and 'mkldnn' \"\n                f\"engines, but got {engine}.\"\n            )\n\n    def _check_precision(self, precision: str):\n        if precision not in [\"fp32\", \"fp16\", \"int8\"]:\n            raise ValueError(\n                \"Inference only supports 'fp32', 'fp16' and 'int8' \"\n                f\"precision, but got {precision}.\"\n            )\n\n    def _compatibility_check(self):\n        if self.engine == \"onnx\":\n            if not (\n                importlib.util.find_spec(\"onnxruntime\")\n                or importlib.util.find_spec(\"onnxruntime-gpu\")\n            ):\n                raise ModuleNotFoundError(\n                    \"\\nPlease install onnxruntime first when engine is 'onnx'\\n\"\n                    \"* For CPU inference, use `pip install onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple`\\n\"\n                    \"* For GPU inference, use `pip install onnxruntime-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple`\"\n                )\n            import onnxruntime as ort\n\n            if self.device == \"gpu\" and ort.get_device() != \"GPU\":\n                raise RuntimeError(\n                    \"Please install onnxruntime-gpu with `pip install onnxruntime-gpu`\"\n                    \" when device is set to 'gpu'\\n\"\n                )\n</code></pre>"},{"location":"zh/api/depoly/python_infer/#deploy.python_infer.GeneralPredictor","title":"<code>GeneralPredictor</code>","text":"<p>               Bases: <code>PINNPredictor</code></p> <p>Use PINNPredictor as GeneralPredictor.</p> Source code in <code>deploy/python_infer/__init__.py</code> <pre><code>class GeneralPredictor(PINNPredictor):\n    \"\"\"Use PINNPredictor as GeneralPredictor.\"\"\"\n\n    pass\n</code></pre>"},{"location":"zh/api/depoly/python_infer/#deploy.python_infer.PINNPredictor","title":"<code>PINNPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>General predictor for PINN-based models.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Running configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from omegaconf import DictConfig\n&gt;&gt;&gt; from paddle.static import InputSpec\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; from deploy.python_infer import pinn_predictor\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\", \"p\"), 3, 16)\n&gt;&gt;&gt; static_model = paddle.jit.to_static(\n...     model,\n...     input_spec=[\n...         {\n...             key: InputSpec([None, 1], \"float32\", name=key)\n...             for key in model.input_keys\n...         },\n...     ],\n... )\n&gt;&gt;&gt; paddle.jit.save(static_model, \"./inference\")\n&gt;&gt;&gt; cfg = DictConfig(\n...     {\n...         \"log_freq\": 10,\n...         \"INFER\": {\n...             \"pdmodel_path\": \"./inference.pdmodel\",\n...             \"pdiparams_path\": \"./inference.pdiparams\",\n...             \"device\": \"cpu\",\n...             \"engine\": \"native\",\n...             \"precision\": \"fp32\",\n...             \"onnx_path\": None,\n...             \"ir_optim\": True,\n...             \"min_subgraph_size\": 15,\n...             \"gpu_mem\": 500,\n...             \"gpu_id\": 0,\n...             \"max_batch_size\": 10,\n...             \"num_cpu_threads\": 10,\n...         }\n...     }\n... )\n&gt;&gt;&gt; predictor = pinn_predictor.PINNPredictor(cfg)\n&gt;&gt;&gt; pred = predictor.predict(\n...     {\n...         \"x\": np.random.randn(4, 1).astype(\"float32\"),\n...         \"y\": np.random.randn(4, 1).astype(\"float32\"),\n...     },\n...     batch_size=2,\n... )\n&gt;&gt;&gt; for k, v in pred.items():\n...     print(k, v.shape)\nsave_infer_model/scale_0.tmp_0 (4, 1)\nsave_infer_model/scale_1.tmp_0 (4, 1)\nsave_infer_model/scale_2.tmp_0 (4, 1)\n</code></pre> Source code in <code>deploy/python_infer/pinn_predictor.py</code> <pre><code>class PINNPredictor(base.Predictor):\n    \"\"\"General predictor for PINN-based models.\n\n    Args:\n        cfg (DictConfig): Running configuration.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from omegaconf import DictConfig\n        &gt;&gt;&gt; from paddle.static import InputSpec\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; from deploy.python_infer import pinn_predictor\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\", \"p\"), 3, 16)\n        &gt;&gt;&gt; static_model = paddle.jit.to_static(\n        ...     model,\n        ...     input_spec=[\n        ...         {\n        ...             key: InputSpec([None, 1], \"float32\", name=key)\n        ...             for key in model.input_keys\n        ...         },\n        ...     ],\n        ... )\n        &gt;&gt;&gt; paddle.jit.save(static_model, \"./inference\")\n        &gt;&gt;&gt; cfg = DictConfig(\n        ...     {\n        ...         \"log_freq\": 10,\n        ...         \"INFER\": {\n        ...             \"pdmodel_path\": \"./inference.pdmodel\",\n        ...             \"pdiparams_path\": \"./inference.pdiparams\",\n        ...             \"device\": \"cpu\",\n        ...             \"engine\": \"native\",\n        ...             \"precision\": \"fp32\",\n        ...             \"onnx_path\": None,\n        ...             \"ir_optim\": True,\n        ...             \"min_subgraph_size\": 15,\n        ...             \"gpu_mem\": 500,\n        ...             \"gpu_id\": 0,\n        ...             \"max_batch_size\": 10,\n        ...             \"num_cpu_threads\": 10,\n        ...         }\n        ...     }\n        ... )\n        &gt;&gt;&gt; predictor = pinn_predictor.PINNPredictor(cfg) # doctest: +SKIP\n        &gt;&gt;&gt; pred = predictor.predict(\n        ...     {\n        ...         \"x\": np.random.randn(4, 1).astype(\"float32\"),\n        ...         \"y\": np.random.randn(4, 1).astype(\"float32\"),\n        ...     },\n        ...     batch_size=2,\n        ... ) # doctest: +SKIP\n        &gt;&gt;&gt; for k, v in pred.items(): # doctest: +SKIP\n        ...     print(k, v.shape) # doctest: +SKIP\n        save_infer_model/scale_0.tmp_0 (4, 1)\n        save_infer_model/scale_1.tmp_0 (4, 1)\n        save_infer_model/scale_2.tmp_0 (4, 1)\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: DictConfig,\n    ):\n        super().__init__(\n            cfg.INFER.pdmodel_path,\n            cfg.INFER.pdiparams_path,\n            device=cfg.INFER.device,\n            engine=cfg.INFER.engine,\n            precision=cfg.INFER.precision,\n            onnx_path=cfg.INFER.onnx_path,\n            ir_optim=cfg.INFER.ir_optim,\n            min_subgraph_size=cfg.INFER.min_subgraph_size,\n            gpu_mem=cfg.INFER.gpu_mem,\n            gpu_id=cfg.INFER.gpu_id,\n            max_batch_size=cfg.INFER.max_batch_size,\n            num_cpu_threads=cfg.INFER.num_cpu_threads,\n        )\n        self.log_freq = cfg.log_freq\n\n    def predict(\n        self,\n        input_dict: Dict[str, Union[np.ndarray, paddle.Tensor]],\n        batch_size: int = 64,\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Predicts the output of the model for the given input.\n\n        Args:\n            input_dict (Dict[str, Union[np.ndarray, paddle.Tensor]]):\n                A dictionary containing the input data.\n            batch_size (int, optional): The batch size to use for prediction.\n                Defaults to 64.\n\n        Returns:\n            Dict[str, np.ndarray]: A dictionary containing the predicted output.\n        \"\"\"\n        if batch_size &gt; self.max_batch_size:\n            logger.warning(\n                f\"batch_size({batch_size}) is larger than \"\n                f\"max_batch_size({self.max_batch_size}), which may occur error.\"\n            )\n\n        if self.engine != \"onnx\":\n            # prepare input handle(s)\n            input_handles = {\n                name: self.predictor.get_input_handle(name) for name in input_dict\n            }\n            # prepare output handle(s)\n            output_handles = {\n                name: self.predictor.get_output_handle(name)\n                for name in self.predictor.get_output_names()\n            }\n        else:\n            # input_names = [node_arg.name for node_arg in self.predictor.get_inputs()]\n            output_names: List[str] = [\n                node_arg.name for node_arg in self.predictor.get_outputs()\n            ]\n\n        num_samples = len(next(iter(input_dict.values())))\n        batch_num = (num_samples + (batch_size - 1)) // batch_size\n        pred_dict = misc.Prettydefaultdict(list)\n\n        # inference by batch\n        for batch_id in range(1, batch_num + 1):\n            if batch_id == 1 or batch_id % self.log_freq == 0 or batch_id == batch_num:\n                logger.info(f\"Predicting batch {batch_id}/{batch_num}\")\n\n            # prepare batch input dict\n            st = (batch_id - 1) * batch_size\n            ed = min(num_samples, batch_id * batch_size)\n            batch_input_dict = {key: input_dict[key][st:ed] for key in input_dict}\n\n            # send batch input data to input handle(s)\n            if self.engine != \"onnx\":\n                for name, handle in input_handles.items():\n                    handle.copy_from_cpu(batch_input_dict[name])\n\n            # run predictor\n            if self.engine != \"onnx\":\n                self.predictor.run()\n                # receive batch output data from output handle(s)\n                batch_output_dict = {\n                    name: output_handles[name].copy_to_cpu() for name in output_handles\n                }\n            else:\n                batch_outputs = self.predictor.run(\n                    output_names=output_names,\n                    input_feed=batch_input_dict,\n                )\n                batch_output_dict = {\n                    name: output for (name, output) in zip(output_names, batch_outputs)\n                }\n\n            # collect batch output data\n            for key, batch_output in batch_output_dict.items():\n                pred_dict[key].append(batch_output)\n\n        # concatenate local predictions\n        pred_dict = {key: np.concatenate(value) for key, value in pred_dict.items()}\n\n        return pred_dict\n</code></pre>"},{"location":"zh/api/depoly/python_infer/#deploy.python_infer.PINNPredictor.predict","title":"<code>predict(input_dict, batch_size=64)</code>","text":"<p>Predicts the output of the model for the given input.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, Union[ndarray, Tensor]]</code> <p>A dictionary containing the input data.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use for prediction. Defaults to 64.</p> <code>64</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: A dictionary containing the predicted output.</p> Source code in <code>deploy/python_infer/pinn_predictor.py</code> <pre><code>def predict(\n    self,\n    input_dict: Dict[str, Union[np.ndarray, paddle.Tensor]],\n    batch_size: int = 64,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Predicts the output of the model for the given input.\n\n    Args:\n        input_dict (Dict[str, Union[np.ndarray, paddle.Tensor]]):\n            A dictionary containing the input data.\n        batch_size (int, optional): The batch size to use for prediction.\n            Defaults to 64.\n\n    Returns:\n        Dict[str, np.ndarray]: A dictionary containing the predicted output.\n    \"\"\"\n    if batch_size &gt; self.max_batch_size:\n        logger.warning(\n            f\"batch_size({batch_size}) is larger than \"\n            f\"max_batch_size({self.max_batch_size}), which may occur error.\"\n        )\n\n    if self.engine != \"onnx\":\n        # prepare input handle(s)\n        input_handles = {\n            name: self.predictor.get_input_handle(name) for name in input_dict\n        }\n        # prepare output handle(s)\n        output_handles = {\n            name: self.predictor.get_output_handle(name)\n            for name in self.predictor.get_output_names()\n        }\n    else:\n        # input_names = [node_arg.name for node_arg in self.predictor.get_inputs()]\n        output_names: List[str] = [\n            node_arg.name for node_arg in self.predictor.get_outputs()\n        ]\n\n    num_samples = len(next(iter(input_dict.values())))\n    batch_num = (num_samples + (batch_size - 1)) // batch_size\n    pred_dict = misc.Prettydefaultdict(list)\n\n    # inference by batch\n    for batch_id in range(1, batch_num + 1):\n        if batch_id == 1 or batch_id % self.log_freq == 0 or batch_id == batch_num:\n            logger.info(f\"Predicting batch {batch_id}/{batch_num}\")\n\n        # prepare batch input dict\n        st = (batch_id - 1) * batch_size\n        ed = min(num_samples, batch_id * batch_size)\n        batch_input_dict = {key: input_dict[key][st:ed] for key in input_dict}\n\n        # send batch input data to input handle(s)\n        if self.engine != \"onnx\":\n            for name, handle in input_handles.items():\n                handle.copy_from_cpu(batch_input_dict[name])\n\n        # run predictor\n        if self.engine != \"onnx\":\n            self.predictor.run()\n            # receive batch output data from output handle(s)\n            batch_output_dict = {\n                name: output_handles[name].copy_to_cpu() for name in output_handles\n            }\n        else:\n            batch_outputs = self.predictor.run(\n                output_names=output_names,\n                input_feed=batch_input_dict,\n            )\n            batch_output_dict = {\n                name: output for (name, output) in zip(output_names, batch_outputs)\n            }\n\n        # collect batch output data\n        for key, batch_output in batch_output_dict.items():\n            pred_dict[key].append(batch_output)\n\n    # concatenate local predictions\n    pred_dict = {key: np.concatenate(value) for key, value in pred_dict.items()}\n\n    return pred_dict\n</code></pre>"},{"location":"zh/api/loss/loss/","title":"ppsci.loss.loss","text":""},{"location":"zh/api/loss/loss/#lossloss","title":"Loss.loss(\u635f\u5931\u51fd\u6570) \u6a21\u5757","text":""},{"location":"zh/api/loss/loss/#ppsci.loss","title":"<code>ppsci.loss</code>","text":""},{"location":"zh/api/loss/loss/#ppsci.loss.Loss","title":"<code>Loss</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Base class for loss.</p> Source code in <code>ppsci/loss/base.py</code> <pre><code>class Loss(nn.Layer):\n    \"\"\"Base class for loss.\"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"],\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        super().__init__()\n        self.reduction = reduction\n        self.weight = weight\n\n    def __str__(self):\n        return f\"{self.__class__.__name__}(reduction={self.reduction}, weight={self.weight})\"\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.FunctionalLoss","title":"<code>FunctionalLoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Functional loss class, which allows to use custom loss computing function from given loss_expr for complex computation cases.</p> \\[ L = f(\\mathbf{x}, \\mathbf{y}) \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>Parameters:</p> Name Type Description Default <code>loss_expr</code> <code>Callable[..., Tensor]</code> <p>Function for custom loss computation.</p> required <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import FunctionalLoss\n&gt;&gt;&gt; import paddle.nn.functional as F\n&gt;&gt;&gt; def mse_sum_loss(output_dict, label_dict, weight_dict=None):\n...     losses = 0\n...     for key in output_dict.keys():\n...         loss = F.mse_loss(output_dict[key], label_dict[key], \"sum\")\n...         if weight_dict:\n...             loss *=  weight_dict[key]\n...         losses += loss\n...     return {\"mse_loss\": losses}\n&gt;&gt;&gt; loss = FunctionalLoss(mse_sum_loss)\n&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...             'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...             'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; weight_dict = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; result = loss(output_dict, label_dict, weight_dict)\n&gt;&gt;&gt; print(result)\n{'mse_loss': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       17.89600182)}\n</code></pre> Source code in <code>ppsci/loss/func.py</code> <pre><code>class FunctionalLoss(base.Loss):\n    r\"\"\"Functional loss class, which allows to use custom loss computing function from given loss_expr for complex computation cases.\n\n    $$\n    L = f(\\mathbf{x}, \\mathbf{y})\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    Args:\n        loss_expr (Callable[..., paddle.Tensor]): Function for custom loss computation.\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import FunctionalLoss\n        &gt;&gt;&gt; import paddle.nn.functional as F\n        &gt;&gt;&gt; def mse_sum_loss(output_dict, label_dict, weight_dict=None):\n        ...     losses = 0\n        ...     for key in output_dict.keys():\n        ...         loss = F.mse_loss(output_dict[key], label_dict[key], \"sum\")\n        ...         if weight_dict:\n        ...             loss *=  weight_dict[key]\n        ...         losses += loss\n        ...     return {\"mse_loss\": losses}\n        &gt;&gt;&gt; loss = FunctionalLoss(mse_sum_loss)\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...             'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...             'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; weight_dict = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; result = loss(output_dict, label_dict, weight_dict)\n        &gt;&gt;&gt; print(result)\n        {'mse_loss': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               17.89600182)}\n    \"\"\"\n\n    def __init__(\n        self,\n        loss_expr: Callable[..., paddle.Tensor],\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        super().__init__(None, weight)\n        self.loss_expr = loss_expr\n\n    def forward(\n        self, output_dict, label_dict=None, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = self.loss_expr(output_dict, label_dict, weight_dict)\n\n        assert isinstance(losses, dict), (\n            \"Loss computed by custom function should be type of 'dict', \"\n            f\"but got {type(losses)}.\"\n            \" Please check the return type of custom loss function.\"\n        )\n\n        for key in losses:\n            assert isinstance(\n                losses[key], (paddle.Tensor, paddle.static.Variable, paddle.pir.Value)\n            ), (\n                \"Loss computed by custom function should be type of 'paddle.Tensor', \"\n                f\"'paddle.static.Variable' or 'paddle.pir.Value', but got {type(losses[key])}.\"\n                \" Please check the return type of custom loss function.\"\n            )\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.L1Loss","title":"<code>L1Loss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for l1 loss.</p> \\[ L = \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>when <code>reduction</code> is set to \"mean\"</p> \\[ L = MEAN \\left( \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 \\right) \\] <p>when <code>reduction</code> is set to \"sum\"</p> \\[ L = SUM \\left( \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 \\right) \\] <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import L1Loss\n&gt;&gt;&gt; output_dict = {\"u\": paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                \"v\": paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {\"u\": paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               \"v\": paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; weight = {\"u\": 0.8, \"v\": 0.2}\n&gt;&gt;&gt; loss = L1Loss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       3.), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.35999998)}\n</code></pre> <pre><code>&gt;&gt;&gt; loss = L1Loss(reduction=\"sum\", weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       6.), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.71999997)}\n</code></pre> Source code in <code>ppsci/loss/l1.py</code> <pre><code>class L1Loss(base.Loss):\n    r\"\"\"Class for l1 loss.\n\n    $$\n    L = \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    when `reduction` is set to \"mean\"\n\n    $$\n    L = MEAN \\left( \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 \\right)\n    $$\n\n    when `reduction` is set to \"sum\"\n\n    $$\n    L = SUM \\left( \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 \\right)\n    $$\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Reduction method. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import L1Loss\n        &gt;&gt;&gt; output_dict = {\"u\": paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                \"v\": paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {\"u\": paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               \"v\": paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; weight = {\"u\": 0.8, \"v\": 0.2}\n        &gt;&gt;&gt; loss = L1Loss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               3.), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.35999998)}\n\n        &gt;&gt;&gt; loss = L1Loss(reduction=\"sum\", weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               6.), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.71999997)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            loss = F.l1_loss(output_dict[key], label_dict[key], \"none\")\n            if weight_dict and key in weight_dict:\n                loss *= weight_dict[key]\n\n            if \"area\" in output_dict:\n                loss *= output_dict[\"area\"]\n\n            loss = loss.sum(axis=1)\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.L2Loss","title":"<code>L2Loss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for l2 loss.</p> \\[ L =\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2 \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>when <code>reduction</code> is set to \"mean\"</p> \\[ L = MEAN \\left( \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2 \\right) \\] <p>when <code>reduction</code> is set to \"sum\"</p> \\[ L = SUM \\left( \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2 \\right) \\] <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import L2Loss\n&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; loss = L2Loss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       2.52735591), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.26148924)}\n&gt;&gt;&gt; loss = L2Loss(reduction=\"sum\", weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       5.05471182), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.52297848)}\n</code></pre> Source code in <code>ppsci/loss/l2.py</code> <pre><code>class L2Loss(base.Loss):\n    r\"\"\"Class for l2 loss.\n\n    $$\n    L =\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    when `reduction` is set to \"mean\"\n\n    $$\n    L = MEAN \\left( \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2 \\right)\n    $$\n\n    when `reduction` is set to \"sum\"\n\n    $$\n    L = SUM \\left( \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2 \\right)\n    $$\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Reduction method. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import L2Loss\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; loss = L2Loss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               2.52735591), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.26148924)}\n        &gt;&gt;&gt; loss = L2Loss(reduction=\"sum\", weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               5.05471182), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.52297848)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            loss = F.mse_loss(output_dict[key], label_dict[key], \"none\")\n            if weight_dict and key in weight_dict:\n                loss *= weight_dict[key]\n\n            if \"area\" in output_dict:\n                loss *= output_dict[\"area\"]\n\n            loss = loss.sum(axis=1).sqrt()\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.L2RelLoss","title":"<code>L2RelLoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for l2 relative loss.</p> \\[ L = \\dfrac{\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2}{\\Vert \\mathbf{y} \\Vert_2} \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>when <code>reduction</code> is set to \"mean\"</p> \\[ L = MEAN \\left( \\dfrac{\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2}{\\Vert \\mathbf{y} \\Vert_2} \\right) \\] <p>when <code>reduction</code> is set to \"sum\"</p> \\[ L = SUM \\left( \\dfrac{\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2}{\\Vert \\mathbf{y} \\Vert_2} \\right) \\] <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Specifies the reduction to apply to the output: 'mean' | 'sum'. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import L2RelLoss\n</code></pre> <pre><code>&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; loss = L2RelLoss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       1.08776188), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       1.84900820)}\n</code></pre> <pre><code>&gt;&gt;&gt; loss = L2RelLoss(reduction=\"sum\", weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       2.17552376), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       3.69801641)}\n</code></pre> Source code in <code>ppsci/loss/l2.py</code> <pre><code>class L2RelLoss(base.Loss):\n    r\"\"\"Class for l2 relative loss.\n\n    $$\n    L = \\dfrac{\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2}{\\Vert \\mathbf{y} \\Vert_2}\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    when `reduction` is set to \"mean\"\n\n    $$\n    L = MEAN \\left( \\dfrac{\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2}{\\Vert \\mathbf{y} \\Vert_2} \\right)\n    $$\n\n    when `reduction` is set to \"sum\"\n\n    $$\n    L = SUM \\left( \\dfrac{\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2}{\\Vert \\mathbf{y} \\Vert_2} \\right)\n    $$\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Specifies the reduction to apply to the output: 'mean' | 'sum'. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import L2RelLoss\n\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; loss = L2RelLoss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               1.08776188), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               1.84900820)}\n\n        &gt;&gt;&gt; loss = L2RelLoss(reduction=\"sum\", weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               2.17552376), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               3.69801641)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n\n    def rel_loss(self, x, y):\n        batch_size = x.shape[0]\n        x_ = x.reshape((batch_size, -1))\n        y_ = y.reshape((batch_size, -1))\n        diff_norms = paddle.norm(x_ - y_, p=2, axis=1)\n        y_norms = paddle.norm(y_, p=2, axis=1)\n        return diff_norms / y_norms\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            loss = self.rel_loss(output_dict[key], label_dict[key])\n            if weight_dict:\n                loss *= weight_dict[key]\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n\n            if isinstance(self.weight, float):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.MAELoss","title":"<code>MAELoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for mean absolute error loss.</p> \\[ L = \\begin{cases}     \\dfrac{1}{N} \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_1, &amp; \\text{if reduction='mean'} \\\\     \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_1, &amp; \\text{if reduction='sum'} \\end{cases} \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import MAELoss\n</code></pre> <pre><code>&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; loss = MAELoss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       1.50000000), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.17999999)}\n</code></pre> <pre><code>&gt;&gt;&gt; loss = MAELoss(reduction=\"sum\", weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       6.), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.71999997)}\n</code></pre> Source code in <code>ppsci/loss/mae.py</code> <pre><code>class MAELoss(base.Loss):\n    r\"\"\"Class for mean absolute error loss.\n\n    $$\n    L =\n    \\begin{cases}\n        \\dfrac{1}{N} \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_1, &amp; \\text{if reduction='mean'} \\\\\n        \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_1, &amp; \\text{if reduction='sum'}\n    \\end{cases}\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Reduction method. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import MAELoss\n\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; loss = MAELoss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               1.50000000), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.17999999)}\n\n        &gt;&gt;&gt; loss = MAELoss(reduction=\"sum\", weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               6.), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.71999997)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            loss = F.l1_loss(output_dict[key], label_dict[key], \"none\")\n            if weight_dict and key in weight_dict:\n                loss *= weight_dict[key]\n\n            if \"area\" in output_dict:\n                loss *= output_dict[\"area\"]\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.MSELoss","title":"<code>MSELoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for mean squared error loss.</p> \\[ L = \\begin{cases}     \\dfrac{1}{N} \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_2^2, &amp; \\text{if reduction='mean'} \\\\     \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_2^2, &amp; \\text{if reduction='sum'} \\end{cases} \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import MSELoss\n</code></pre> <pre><code>&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; loss = MSELoss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       4.28600025), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.18800001)}\n</code></pre> <pre><code>&gt;&gt;&gt; loss = MSELoss(reduction=\"sum\", weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       17.14400101), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.75200003)}\n</code></pre> Source code in <code>ppsci/loss/mse.py</code> <pre><code>class MSELoss(base.Loss):\n    r\"\"\"Class for mean squared error loss.\n\n    $$\n    L =\n    \\begin{cases}\n        \\dfrac{1}{N} \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_2^2, &amp; \\text{if reduction='mean'} \\\\\n        \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_2^2, &amp; \\text{if reduction='sum'}\n    \\end{cases}\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Reduction method. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import MSELoss\n\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; loss = MSELoss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               4.28600025), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.18800001)}\n\n        &gt;&gt;&gt; loss = MSELoss(reduction=\"sum\", weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               17.14400101), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.75200003)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            loss = F.mse_loss(output_dict[key], label_dict[key], \"none\")\n            if weight_dict and key in weight_dict:\n                loss *= weight_dict[key]\n\n            if \"area\" in output_dict:\n                loss *= output_dict[\"area\"]\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.ChamferLoss","title":"<code>ChamferLoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for Chamfe distance loss.</p> \\[ L = \\dfrac{1}{S_1} \\sum_{x \\in S_1} \\min_{y \\in S_2} \\Vert x - y \\Vert_2^2 + \\dfrac{1}{S_2} \\sum_{y \\in S_2} \\min_{x \\in S_1} \\Vert y - x \\Vert_2^2 \\] \\[ \\text{where } S_1 \\text{ and } S_2 \\text{ is the coordinate matrix of two point clouds}. \\] <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import ChamferLoss\n&gt;&gt;&gt; _ = paddle.seed(42)\n&gt;&gt;&gt; batch_point_cloud1 = paddle.rand([2, 100, 3])\n&gt;&gt;&gt; batch_point_cloud2 = paddle.rand([2, 50, 3])\n&gt;&gt;&gt; output_dict = {\"s1\": batch_point_cloud1}\n&gt;&gt;&gt; label_dict  = {\"s1\": batch_point_cloud2}\n&gt;&gt;&gt; weight = {\"s1\": 0.8}\n&gt;&gt;&gt; loss = ChamferLoss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'s1': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.04415882)}\n</code></pre> Source code in <code>ppsci/loss/chamfer.py</code> <pre><code>class ChamferLoss(base.Loss):\n    r\"\"\"Class for Chamfe distance loss.\n\n    $$\n    L = \\dfrac{1}{S_1} \\sum_{x \\in S_1} \\min_{y \\in S_2} \\Vert x - y \\Vert_2^2 + \\dfrac{1}{S_2} \\sum_{y \\in S_2} \\min_{x \\in S_1} \\Vert y - x \\Vert_2^2\n    $$\n\n    $$\n    \\text{where } S_1 \\text{ and } S_2 \\text{ is the coordinate matrix of two point clouds}.\n    $$\n\n    Args:\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import ChamferLoss\n        &gt;&gt;&gt; _ = paddle.seed(42)\n        &gt;&gt;&gt; batch_point_cloud1 = paddle.rand([2, 100, 3])\n        &gt;&gt;&gt; batch_point_cloud2 = paddle.rand([2, 50, 3])\n        &gt;&gt;&gt; output_dict = {\"s1\": batch_point_cloud1}\n        &gt;&gt;&gt; label_dict  = {\"s1\": batch_point_cloud2}\n        &gt;&gt;&gt; weight = {\"s1\": 0.8}\n        &gt;&gt;&gt; loss = ChamferLoss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'s1': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.04415882)}\n    \"\"\"\n\n    def __init__(\n        self,\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        super().__init__(\"mean\", weight)\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            s1 = output_dict[key]\n            s2 = label_dict[key]\n            N1, N2 = s1.shape[1], s2.shape[1]\n\n            # [B, N1, N2, 3]\n            s1_expand = paddle.expand(s1.reshape([-1, N1, 1, 3]), shape=[-1, N1, N2, 3])\n            # [B, N1, N2, 3]\n            s2_expand = paddle.expand(s2.reshape([-1, 1, N2, 3]), shape=[-1, N1, N2, 3])\n\n            dis = ((s1_expand - s2_expand) ** 2).sum(axis=3)  # [B, N1, N2]\n            loss_s12 = dis.min(axis=2)  # [B, N1]\n            loss_s21 = dis.min(axis=1)  # [B, N2]\n            loss = loss_s12.mean() + loss_s21.mean()\n\n            if weight_dict and key in weight_dict:\n                loss *= weight_dict[key]\n\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.CausalMSELoss","title":"<code>CausalMSELoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for mean squared error loss.</p> \\[ L = \\frac{1}{M} \\displaystyle\\sum_{i=1}^M{w_i} \\mathcal{L}_r^i, \\] <p>where \\(w_i=\\exp (-\\epsilon \\displaystyle\\sum_{k=1}^{i-1} \\mathcal{L}_r^k), i=2,3, \\ldots, M.\\)</p> <p>Parameters:</p> Name Type Description Default <code>n_chunks</code> <code>int</code> <p>\\(M\\), Number of split time windows.</p> required <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <code>tol</code> <code>float</code> <p>Causal tolerance, i.e. \\(\\epsilon\\) in paper. Defaults to 1.0.</p> <code>1.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import CausalMSELoss\n</code></pre> <pre><code>&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9, 1.0], [1.1, -1.3, 0.0]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0, -0.1], [-0.2, 2.5, 2.0]])}\n&gt;&gt;&gt; loss = CausalMSELoss(n_chunks=3)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.96841478)}\n</code></pre> Source code in <code>ppsci/loss/mse.py</code> <pre><code>class CausalMSELoss(base.Loss):\n    r\"\"\"Class for mean squared error loss.\n\n    $$\n    L = \\frac{1}{M} \\displaystyle\\sum_{i=1}^M{w_i} \\mathcal{L}_r^i,\n    $$\n\n    where $w_i=\\exp (-\\epsilon \\displaystyle\\sum_{k=1}^{i-1} \\mathcal{L}_r^k), i=2,3, \\ldots, M.$\n\n    Args:\n        n_chunks (int): $M$, Number of split time windows.\n        reduction (Literal[\"mean\", \"sum\"], optional): Reduction method. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n        tol (float, optional): Causal tolerance, i.e. $\\epsilon$ in paper. Defaults to 1.0.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import CausalMSELoss\n\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9, 1.0], [1.1, -1.3, 0.0]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0, -0.1], [-0.2, 2.5, 2.0]])}\n        &gt;&gt;&gt; loss = CausalMSELoss(n_chunks=3)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.96841478)}\n    \"\"\"\n\n    def __init__(\n        self,\n        n_chunks: int,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n        tol: float = 1.0,\n    ):\n        if n_chunks &lt;= 0:\n            raise ValueError(f\"n_chunks should be positive, but got {n_chunks}\")\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n        self.n_chunks = n_chunks\n        self.tol = tol\n        self.register_buffer(\n            \"acc_mat\", paddle.tril(paddle.ones([n_chunks, n_chunks]), -1)\n        )\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            loss = F.mse_loss(output_dict[key], label_dict[key], \"none\")\n            if weight_dict and key in weight_dict:\n                loss *= weight_dict[key]\n\n            if \"area\" in output_dict:\n                loss *= output_dict[\"area\"]\n\n            # causal weighting\n            loss_t = loss.reshape([self.n_chunks, -1])  # [nt, nx]\n            weight_t = paddle.exp(\n                -self.tol * (self.acc_mat @ loss_t.mean(-1, keepdim=True))\n            )  # [nt, nt] x [nt, 1] ==&gt; [nt, 1]\n            assert weight_t.shape[0] == self.n_chunks\n            loss = loss_t * weight_t.detach()\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.MSELossWithL2Decay","title":"<code>MSELossWithL2Decay</code>","text":"<p>               Bases: <code>MSELoss</code></p> <p>MSELoss with L2 decay.</p> \\[ L = \\begin{cases}     \\dfrac{1}{N} \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_2^2 + \\displaystyle\\sum_{i=1}^{M}{\\Vert \\mathbf{K_i} \\Vert_F^2}, &amp; \\text{if reduction='mean'} \\\\      \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_2^2 + \\displaystyle\\sum_{i=1}^{M}{\\Vert \\mathbf{K_i} \\Vert_F^2}, &amp; \\text{if reduction='sum'} \\end{cases} \\] \\[ \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}, \\mathbf{K_i} \\in \\mathcal{R}^{O_i \\times P_i} \\] <p>\\(M\\) is the number of  which apply regularization on.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Specifies the reduction to apply to the output: 'mean' | 'sum'. Defaults to \"mean\".</p> <code>'mean'</code> <code>regularization_dict</code> <code>Optional[Dict[str, float]]</code> <p>Regularization dictionary. Defaults to None.</p> <code>None</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>reduction should be 'mean' or 'sum'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import MSELossWithL2Decay\n</code></pre> <pre><code>&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; regularization_dict = {'u': 2.0}\n&gt;&gt;&gt; loss = MSELossWithL2Decay(regularization_dict=regularization_dict, weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       7.91999960), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.18800001)}\n</code></pre> <pre><code>&gt;&gt;&gt; regularization_dict = {'v': 1.0}\n&gt;&gt;&gt; loss = MSELossWithL2Decay(reduction=\"sum\", regularization_dict=regularization_dict, weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       17.14400101), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       3.95999980)}\n</code></pre> Source code in <code>ppsci/loss/mse.py</code> <pre><code>class MSELossWithL2Decay(MSELoss):\n    r\"\"\"MSELoss with L2 decay.\n\n    $$\n    L =\n    \\begin{cases}\n        \\dfrac{1}{N} \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_2^2 + \\displaystyle\\sum_{i=1}^{M}{\\Vert \\mathbf{K_i} \\Vert_F^2}, &amp; \\text{if reduction='mean'} \\\\\n         \\Vert {\\mathbf{x}-\\mathbf{y}} \\Vert_2^2 + \\displaystyle\\sum_{i=1}^{M}{\\Vert \\mathbf{K_i} \\Vert_F^2}, &amp; \\text{if reduction='sum'}\n    \\end{cases}\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{y} \\in \\mathcal{R}^{N}, \\mathbf{K_i} \\in \\mathcal{R}^{O_i \\times P_i}\n    $$\n\n    $M$ is the number of  which apply regularization on.\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Specifies the reduction to apply to the output: 'mean' | 'sum'. Defaults to \"mean\".\n        regularization_dict (Optional[Dict[str, float]]): Regularization dictionary. Defaults to None.\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Raises:\n        ValueError: reduction should be 'mean' or 'sum'.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import MSELossWithL2Decay\n\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; regularization_dict = {'u': 2.0}\n        &gt;&gt;&gt; loss = MSELossWithL2Decay(regularization_dict=regularization_dict, weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               7.91999960), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.18800001)}\n\n        &gt;&gt;&gt; regularization_dict = {'v': 1.0}\n        &gt;&gt;&gt; loss = MSELossWithL2Decay(reduction=\"sum\", regularization_dict=regularization_dict, weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               17.14400101), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               3.95999980)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        regularization_dict: Optional[Dict[str, float]] = None,\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n        self.regularization_dict = regularization_dict\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = super().forward(output_dict, label_dict, weight_dict)\n\n        if self.regularization_dict is not None:\n            for reg_key, reg_weight in self.regularization_dict.items():\n                loss = output_dict[reg_key].pow(2).sum()\n                losses[reg_key] = loss * reg_weight\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.IntegralLoss","title":"<code>IntegralLoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for integral loss with Monte-Carlo integration algorithm.</p> \\[ L = \\begin{cases}     \\dfrac{1}{N} \\Vert \\displaystyle\\sum_{i=1}^{M}{\\mathbf{s}_i \\cdot \\mathbf{x}_i} - \\mathbf{y} \\Vert_2^2, &amp; \\text{if reduction='mean'} \\\\      \\Vert \\displaystyle\\sum_{i=0}^{M}{\\mathbf{s}_i \\cdot \\mathbf{x}_i} - \\mathbf{y} \\Vert_2^2, &amp; \\text{if reduction='sum'} \\end{cases} \\] \\[ \\mathbf{x}, \\mathbf{s} \\in \\mathcal{R}^{M \\times N}, \\mathbf{y} \\in \\mathcal{R}^{N} \\] <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import IntegralLoss\n</code></pre> <pre><code>&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]]),\n...                'area': paddle.to_tensor([[0.01, 0.02, 0.03], [0.01, 0.02, 0.03]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([-1.8, 0.0]),\n...               'v': paddle.to_tensor([0.1, 0.1])}\n&gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; loss = IntegralLoss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       1.40780795), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.00131200)}\n</code></pre> <pre><code>&gt;&gt;&gt; loss = IntegralLoss(reduction=\"sum\", weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       2.81561589), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.00262400)}\n</code></pre> Source code in <code>ppsci/loss/integral.py</code> <pre><code>class IntegralLoss(base.Loss):\n    r\"\"\"Class for integral loss with Monte-Carlo integration algorithm.\n\n    $$\n    L =\n    \\begin{cases}\n        \\dfrac{1}{N} \\Vert \\displaystyle\\sum_{i=1}^{M}{\\mathbf{s}_i \\cdot \\mathbf{x}_i} - \\mathbf{y} \\Vert_2^2, &amp; \\text{if reduction='mean'} \\\\\n         \\Vert \\displaystyle\\sum_{i=0}^{M}{\\mathbf{s}_i \\cdot \\mathbf{x}_i} - \\mathbf{y} \\Vert_2^2, &amp; \\text{if reduction='sum'}\n    \\end{cases}\n    $$\n\n    $$\n    \\mathbf{x}, \\mathbf{s} \\in \\mathcal{R}^{M \\times N}, \\mathbf{y} \\in \\mathcal{R}^{N}\n    $$\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Reduction method. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import IntegralLoss\n\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]]),\n        ...                'area': paddle.to_tensor([[0.01, 0.02, 0.03], [0.01, 0.02, 0.03]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([-1.8, 0.0]),\n        ...               'v': paddle.to_tensor([0.1, 0.1])}\n        &gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; loss = IntegralLoss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               1.40780795), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.00131200)}\n\n        &gt;&gt;&gt; loss = IntegralLoss(reduction=\"sum\", weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               2.81561589), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.00262400)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            loss = F.mse_loss(\n                (output_dict[key] * output_dict[\"area\"]).sum(axis=1),\n                label_dict[key],\n                \"none\",\n            )\n            if weight_dict and key in weight_dict:\n                loss *= weight_dict[key]\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.PeriodicL1Loss","title":"<code>PeriodicL1Loss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for periodic l1 loss.</p> \\[ L = \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_1 \\] <p>\\(\\mathbf{x_l} \\in \\mathcal{R}^{N}\\) is the first half of batch output, \\(\\mathbf{x_r} \\in \\mathcal{R}^{N}\\) is the second half of batch output.</p> <p>when <code>reduction</code> is set to \"mean\"</p> \\[ L = MEAN \\left( \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_1 \\right) \\] <p>when <code>reduction</code> is set to \"sum\"</p> \\[ L = SUM \\left( \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_1 \\right) \\] <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import PeriodicL1Loss\n</code></pre> <pre><code>&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 0.0, 1.0], [-0.2, 0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]])}\n&gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; loss = PeriodicL1Loss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       3.35999990), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.83999997)}\n</code></pre> <pre><code>&gt;&gt;&gt; loss = PeriodicL1Loss(reduction=\"sum\", weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       3.35999990), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.83999997)}\n</code></pre> Source code in <code>ppsci/loss/l1.py</code> <pre><code>class PeriodicL1Loss(base.Loss):\n    r\"\"\"Class for periodic l1 loss.\n\n    $$\n    L = \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_1\n    $$\n\n    $\\mathbf{x_l} \\in \\mathcal{R}^{N}$ is the first half of batch output,\n    $\\mathbf{x_r} \\in \\mathcal{R}^{N}$ is the second half of batch output.\n\n    when `reduction` is set to \"mean\"\n\n    $$\n    L = MEAN \\left( \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_1 \\right)\n    $$\n\n    when `reduction` is set to \"sum\"\n\n    $$\n    L = SUM \\left( \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_1 \\right)\n    $$\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Reduction method. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import PeriodicL1Loss\n\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 0.0, 1.0], [-0.2, 0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]])}\n        &gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; loss = PeriodicL1Loss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               3.35999990), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.83999997)}\n\n        &gt;&gt;&gt; loss = PeriodicL1Loss(reduction=\"sum\", weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               3.35999990), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.83999997)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            n_output = len(output_dict[key])\n            if n_output % 2 &gt; 0:\n                raise ValueError(\n                    f\"Length of output({n_output}) of key({key}) should be even.\"\n                )\n\n            n_output //= 2\n            loss = F.l1_loss(\n                output_dict[key][:n_output], output_dict[key][n_output:], \"none\"\n            )\n            if weight_dict and key in weight_dict:\n                loss *= weight_dict[key]\n            if \"area\" in output_dict:\n                loss *= output_dict[\"area\"]\n\n            loss = loss.sum(axis=1)\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.PeriodicL2Loss","title":"<code>PeriodicL2Loss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for Periodic l2 loss.</p> \\[ L = \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2 \\] <p>\\(\\mathbf{x_l} \\in \\mathcal{R}^{N}\\) is the first half of batch output, \\(\\mathbf{x_r} \\in \\mathcal{R}^{N}\\) is the second half of batch output.</p> <p>when <code>reduction</code> is set to \"mean\"</p> \\[ L = MEAN \\left( \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2 \\right) \\] <p>when <code>reduction</code> is set to \"sum\"</p> \\[ L = SUM \\left( \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2 \\right) \\] <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import PeriodicL2Loss\n</code></pre> <pre><code>&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 0.0, 1.0], [-0.2, 0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]])}\n&gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; loss = PeriodicL2Loss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       2.14065409), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.53516352)}\n</code></pre> <pre><code>&gt;&gt;&gt; loss = PeriodicL2Loss(reduction=\"sum\", weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       2.14065409), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.53516352)}\n</code></pre> Source code in <code>ppsci/loss/l2.py</code> <pre><code>class PeriodicL2Loss(base.Loss):\n    r\"\"\"Class for Periodic l2 loss.\n\n    $$\n    L = \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2\n    $$\n\n    $\\mathbf{x_l} \\in \\mathcal{R}^{N}$ is the first half of batch output,\n    $\\mathbf{x_r} \\in \\mathcal{R}^{N}$ is the second half of batch output.\n\n    when `reduction` is set to \"mean\"\n\n    $$\n    L = MEAN \\left( \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2 \\right)\n    $$\n\n    when `reduction` is set to \"sum\"\n\n    $$\n    L = SUM \\left( \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2 \\right)\n    $$\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Reduction method. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import PeriodicL2Loss\n\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 2.2, 0.9], [1.1, 0.8, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 0.0, 1.0], [-0.2, 0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]])}\n        &gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; loss = PeriodicL2Loss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               2.14065409), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.53516352)}\n\n        &gt;&gt;&gt; loss = PeriodicL2Loss(reduction=\"sum\", weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               2.14065409), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.53516352)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            n_output = len(output_dict[key])\n            if n_output % 2 &gt; 0:\n                raise ValueError(\n                    f\"Length of output({n_output}) of key({key}) should be even.\"\n                )\n            n_output //= 2\n\n            loss = F.mse_loss(\n                output_dict[key][:n_output], output_dict[key][n_output:], \"none\"\n            )\n            if weight_dict and key in weight_dict:\n                loss *= weight_dict[key]\n\n            if \"area\" in output_dict:\n                loss *= output_dict[\"area\"]\n\n            loss = loss.sum(axis=1).sqrt()\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/loss/#ppsci.loss.PeriodicMSELoss","title":"<code>PeriodicMSELoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Class for periodic mean squared error loss.</p> \\[ L = \\begin{cases}     \\dfrac{1}{N} \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2^2, &amp; \\text{if reduction='mean'} \\\\     \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2^2, &amp; \\text{if reduction='sum'} \\end{cases} \\] <p>\\(\\mathbf{x_l} \\in \\mathcal{R}^{N}\\) is the first half of batch output, \\(\\mathbf{x_r} \\in \\mathcal{R}^{N}\\) is the second half of batch output.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean', 'sum']</code> <p>Reduction method. Defaults to \"mean\".</p> <code>'mean'</code> <code>weight</code> <code>Optional[Union[float, Dict[str, float]]]</code> <p>Weight for loss. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import PeriodicMSELoss\n</code></pre> <pre><code>&gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n&gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n&gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n&gt;&gt;&gt; loss = PeriodicMSELoss(weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       2.07999969), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       0.51999992)}\n</code></pre> <pre><code>&gt;&gt;&gt; loss = PeriodicMSELoss(reduction=\"sum\", weight=weight)\n&gt;&gt;&gt; result = loss(output_dict, label_dict)\n&gt;&gt;&gt; print(result)\n{'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       4.15999937), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       1.03999984)}\n</code></pre> Source code in <code>ppsci/loss/mse.py</code> <pre><code>class PeriodicMSELoss(base.Loss):\n    r\"\"\"Class for periodic mean squared error loss.\n\n    $$\n    L =\n    \\begin{cases}\n        \\dfrac{1}{N} \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2^2, &amp; \\text{if reduction='mean'} \\\\\n        \\Vert \\mathbf{x_l}-\\mathbf{x_r} \\Vert_2^2, &amp; \\text{if reduction='sum'}\n    \\end{cases}\n    $$\n\n    $\\mathbf{x_l} \\in \\mathcal{R}^{N}$ is the first half of batch output,\n    $\\mathbf{x_r} \\in \\mathcal{R}^{N}$ is the second half of batch output.\n\n    Args:\n        reduction (Literal[\"mean\", \"sum\"], optional): Reduction method. Defaults to \"mean\".\n        weight (Optional[Union[float, Dict[str, float]]]): Weight for loss. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import PeriodicMSELoss\n\n        &gt;&gt;&gt; output_dict = {'u': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]]),\n        ...                'v': paddle.to_tensor([[0.5, 0.9], [1.1, -1.3]])}\n        &gt;&gt;&gt; label_dict = {'u': paddle.to_tensor([[-1.8, 1.0], [-0.2, 2.5]]),\n        ...               'v': paddle.to_tensor([[0.1, 0.1], [0.1, 0.1]])}\n        &gt;&gt;&gt; weight = {'u': 0.8, 'v': 0.2}\n        &gt;&gt;&gt; loss = PeriodicMSELoss(weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               2.07999969), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               0.51999992)}\n\n        &gt;&gt;&gt; loss = PeriodicMSELoss(reduction=\"sum\", weight=weight)\n        &gt;&gt;&gt; result = loss(output_dict, label_dict)\n        &gt;&gt;&gt; print(result)\n        {'u': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               4.15999937), 'v': Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n               1.03999984)}\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\", \"sum\"] = \"mean\",\n        weight: Optional[Union[float, Dict[str, float]]] = None,\n    ):\n        if reduction not in [\"mean\", \"sum\"]:\n            raise ValueError(\n                f\"reduction should be 'mean' or 'sum', but got {reduction}\"\n            )\n        super().__init__(reduction, weight)\n\n    def forward(\n        self, output_dict, label_dict, weight_dict=None\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        losses = {}\n\n        for key in label_dict:\n            n_output = len(output_dict[key])\n            if n_output % 2 &gt; 0:\n                raise ValueError(\n                    f\"Length of output({n_output}) of key({key}) should be even.\"\n                )\n\n            n_output //= 2\n            loss = F.mse_loss(\n                output_dict[key][:n_output], output_dict[key][n_output:], \"none\"\n            )\n            if weight_dict:\n                loss *= weight_dict[key]\n            if \"area\" in output_dict:\n                loss *= output_dict[\"area\"]\n\n            if self.reduction == \"sum\":\n                loss = loss.sum()\n            elif self.reduction == \"mean\":\n                loss = loss.mean()\n\n            if isinstance(self.weight, (float, int)):\n                loss *= self.weight\n            elif isinstance(self.weight, dict) and key in self.weight:\n                loss *= self.weight[key]\n\n            losses[key] = loss\n\n        return losses\n</code></pre>"},{"location":"zh/api/loss/mtl/","title":"ppsci.loss.mtl","text":""},{"location":"zh/api/loss/mtl/#lossmtl","title":"Loss.mtl(\u591a\u4efb\u52a1\u5b66\u4e60) \u6a21\u5757","text":""},{"location":"zh/api/loss/mtl/#ppsci.loss.mtl","title":"<code>ppsci.loss.mtl</code>","text":""},{"location":"zh/api/loss/mtl/#ppsci.loss.mtl.AGDA","title":"<code>AGDA</code>","text":"<p>               Bases: <code>LossAggregator</code></p> <p>Adaptive Gradient Descent Algorithm</p> <p>Physics-informed neural network based on a new adaptive gradient descent algorithm for solving partial differential equations of flow problems</p> <p>NOTE: This loss aggregator is only suitable for two-task learning and the first task loss must be PDE loss.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>Training model.</p> required <code>M</code> <code>int</code> <p>Smoothing period. Defaults to 100.</p> <code>100</code> <code>gamma</code> <code>float</code> <p>Smooth factor. Defaults to 0.999.</p> <code>0.999</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import mtl\n&gt;&gt;&gt; model = paddle.nn.Linear(3, 4)\n&gt;&gt;&gt; loss_aggregator = mtl.AGDA(model)\n&gt;&gt;&gt; for i in range(5):\n...     x1 = paddle.randn([8, 3])\n...     x2 = paddle.randn([8, 3])\n...     y1 = model(x1)\n...     y2 = model(x2)\n...     pde_loss = paddle.sum(y1)\n...     bc_loss = paddle.sum((y2 - 2) ** 2)\n...     loss_aggregator({'pde_loss': pde_loss, 'bc_loss': bc_loss}).backward()\n</code></pre> Source code in <code>ppsci/loss/mtl/agda.py</code> <pre><code>class AGDA(base.LossAggregator):\n    r\"\"\"\n    **A**daptive **G**radient **D**escent **A**lgorithm\n\n    [Physics-informed neural network based on a new adaptive gradient descent algorithm for solving partial differential equations of flow problems](https://pubs.aip.org/aip/pof/article-abstract/35/6/063608/2899773/Physics-informed-neural-network-based-on-a-new)\n\n    NOTE: This loss aggregator is only suitable for two-task learning and the first task loss must be PDE loss.\n\n    Args:\n        model (nn.Layer): Training model.\n        M (int, optional): Smoothing period. Defaults to 100.\n        gamma (float, optional): Smooth factor. Defaults to 0.999.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import mtl\n        &gt;&gt;&gt; model = paddle.nn.Linear(3, 4)\n        &gt;&gt;&gt; loss_aggregator = mtl.AGDA(model)\n        &gt;&gt;&gt; for i in range(5):\n        ...     x1 = paddle.randn([8, 3])\n        ...     x2 = paddle.randn([8, 3])\n        ...     y1 = model(x1)\n        ...     y2 = model(x2)\n        ...     pde_loss = paddle.sum(y1)\n        ...     bc_loss = paddle.sum((y2 - 2) ** 2)\n        ...     loss_aggregator({'pde_loss': pde_loss, 'bc_loss': bc_loss}).backward()\n    \"\"\"\n\n    def __init__(self, model: nn.Layer, M: int = 100, gamma: float = 0.999) -&gt; None:\n        super().__init__(model)\n        self.M = M\n        self.gamma = gamma\n        self.Lf_smooth = 0\n        self.Lu_smooth = 0\n        self.Lf_tilde_acc = 0.0\n        self.Lu_tilde_acc = 0.0\n\n    def __call__(self, losses, step: int = 0) -&gt; \"AGDA\":\n        if len(losses) != 2:\n            raise ValueError(\n                f\"Number of losses(tasks) for AGDA shoule be 2, but got {len(losses)}\"\n            )\n        return super().__call__(losses, step)\n\n    def backward(self) -&gt; None:\n        grads_list = self._compute_grads()\n        with paddle.no_grad():\n            refined_grads = self._refine_grads(grads_list)\n            self._set_grads(refined_grads)\n\n    def _compute_grads(self) -&gt; List[paddle.Tensor]:\n        # compute all gradients derived by each loss\n        grads_list = []  # num_params x num_losses\n        for key in self.losses:\n            # backward with current loss\n            self.losses[key].backward()\n            grads_list.append(\n                paddle.concat(\n                    [\n                        param.grad.clone().reshape([-1])\n                        for param in self.model.parameters()\n                        if param.grad is not None\n                    ],\n                    axis=0,\n                )\n            )\n            # clear gradients for current loss for not affecting other loss\n            self.model.clear_gradients()\n\n        return grads_list\n\n    def _refine_grads(self, grads_list: List[paddle.Tensor]) -&gt; List[paddle.Tensor]:\n        # compute moving average of L^smooth_i(n) - eq.(16)\n        losses_seq = list(self.losses.values())\n        self.Lf_smooth = (\n            self.gamma * self.Lf_smooth + (1 - self.gamma) * losses_seq[0].item()\n        )\n        self.Lu_smooth = (\n            self.gamma * self.Lu_smooth + (1 - self.gamma) * losses_seq[1].item()\n        )\n\n        # compute L^smooth_i(kM) - eq.(17)\n        if self.step % self.M == 0:\n            Lf_smooth_kM = self.Lf_smooth\n            Lu_smooth_kM = self.Lu_smooth\n        Lf_tilde = self.Lf_smooth / Lf_smooth_kM\n        Lu_tilde = self.Lu_smooth / Lu_smooth_kM\n\n        # compute r_i(n) - eq.(18)\n        self.Lf_tilde_acc += Lf_tilde\n        self.Lu_tilde_acc += Lu_tilde\n        rf = Lf_tilde / self.Lf_tilde_acc\n        ru = Lu_tilde / self.Lu_tilde_acc\n\n        # compute E(g(n)) - step1(1)\n        gf_magn = (grads_list[0] * grads_list[0]).sum().sqrt()\n        gu_magn = (grads_list[1] * grads_list[1]).sum().sqrt()\n        Eg = (gf_magn + gu_magn) / 2\n\n        # compute \\omega_f(n) - step1(2)\n        omega_f = (rf * (Eg - gf_magn) + gf_magn) / gf_magn\n        omega_u = (ru * (Eg - gu_magn) + gu_magn) / gu_magn\n\n        # compute g_bar(n) - step1(3)\n        gf_bar = omega_f * grads_list[0]\n        gu_bar = omega_u * grads_list[1]\n\n        # compute gradient projection - step2(1)\n        dot_product = (gf_bar * gu_bar).sum()\n        if dot_product &lt; 0:\n            gu_bar = gu_bar - (dot_product / (gf_bar * gf_bar).sum()) * gf_bar\n        grads_list = [gf_bar, gu_bar]\n\n        proj_grads: List[paddle.Tensor] = []\n        for j in range(len(self.losses)):\n            start_idx = 0\n            for idx, var in enumerate(self.model.parameters()):\n                grad_shape = var.shape\n                flatten_dim = var.numel()\n                refined_grad = grads_list[j][start_idx : start_idx + flatten_dim]\n                refined_grad = paddle.reshape(refined_grad, grad_shape)\n                if len(proj_grads) &lt; self.param_num:\n                    proj_grads.append(refined_grad)\n                else:\n                    proj_grads[idx] += refined_grad\n                start_idx += flatten_dim\n        return proj_grads\n\n    def _set_grads(self, grads_list: List[paddle.Tensor]) -&gt; None:\n        for i, param in enumerate(self.model.parameters()):\n            param.grad = grads_list[i]\n</code></pre>"},{"location":"zh/api/loss/mtl/#ppsci.loss.mtl.GradNorm","title":"<code>GradNorm</code>","text":"<p>               Bases: <code>LossAggregator</code></p> <p>GradNorm loss weighting algorithm.</p> <p>reference: https://github.com/PredictiveIntelligenceLab/jaxpi/blob/main/jaxpi/models.py#L132-L146</p> \\[ \\begin{align*} L^t &amp;= \\sum_{i=1}^{N}{\\tilde{w}_i^t\\cdot L_i^t}, \\\\     \\text{where } \\\\     \\tilde{w}_i^0&amp;=1, \\\\     \\tilde{w}_i^t&amp;=\\tilde{w}_i^{t-1}\\cdot m+w_i^t\\cdot (1-m), t\\ge1\\\\     w_i^t&amp;=\\dfrac{\\overline{\\Vert \\nabla_{\\theta}{L_i^t} \\Vert_2}}{\\Vert \\nabla_{\\theta}{L_i^t} \\Vert_2}, \\\\     \\overline{\\Vert \\nabla_{\\theta}{L_i^t} \\Vert_2}&amp;=\\dfrac{1}{N}\\sum_{i=1}^N{\\Vert \\nabla_{\\theta}{L_i^t} \\Vert_2}, \\\\     &amp;t \\text{ is the training step started from 0}. \\end{align*} \\] <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>Training model.</p> required <code>num_losses</code> <code>int</code> <p>Number of losses. Defaults to 1.</p> <code>1</code> <code>update_freq</code> <code>int</code> <p>Weight updating frequency. Defaults to 1000.</p> <code>1000</code> <code>momentum</code> <code>float</code> <p>Momentum \\(m\\) for moving weight. Defaults to 0.9.</p> <code>0.9</code> <code>init_weights</code> <code>List[float]</code> <p>Initial weights list. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import mtl\n&gt;&gt;&gt; model = paddle.nn.Linear(3, 4)\n&gt;&gt;&gt; loss_aggregator = mtl.GradNorm(model, num_losses=2)\n&gt;&gt;&gt; for i in range(5):\n...     x1 = paddle.randn([8, 3])\n...     x2 = paddle.randn([8, 3])\n...     y1 = model(x1)\n...     y2 = model(x2)\n...     loss1 = paddle.sum(y1)\n...     loss2 = paddle.sum((y2 - 2) ** 2)\n...     loss_aggregator({'loss1': loss1, 'loss2': loss2}).backward()\n</code></pre> Source code in <code>ppsci/loss/mtl/grad_norm.py</code> <pre><code>class GradNorm(base.LossAggregator):\n    r\"\"\"GradNorm loss weighting algorithm.\n\n    reference: [https://github.com/PredictiveIntelligenceLab/jaxpi/blob/main/jaxpi/models.py#L132-L146](https://github.com/PredictiveIntelligenceLab/jaxpi/blob/main/jaxpi/models.py#L132-L146)\n\n    $$\n    \\begin{align*}\n    L^t &amp;= \\sum_{i=1}^{N}{\\tilde{w}_i^t\\cdot L_i^t}, \\\\\n        \\text{where } \\\\\n        \\tilde{w}_i^0&amp;=1, \\\\\n        \\tilde{w}_i^t&amp;=\\tilde{w}_i^{t-1}\\cdot m+w_i^t\\cdot (1-m), t\\ge1\\\\\n        w_i^t&amp;=\\dfrac{\\overline{\\Vert \\nabla_{\\theta}{L_i^t} \\Vert_2}}{\\Vert \\nabla_{\\theta}{L_i^t} \\Vert_2}, \\\\\n        \\overline{\\Vert \\nabla_{\\theta}{L_i^t} \\Vert_2}&amp;=\\dfrac{1}{N}\\sum_{i=1}^N{\\Vert \\nabla_{\\theta}{L_i^t} \\Vert_2}, \\\\\n        &amp;t \\text{ is the training step started from 0}.\n    \\end{align*}\n    $$\n\n    Args:\n        model (nn.Layer): Training model.\n        num_losses (int, optional): Number of losses. Defaults to 1.\n        update_freq (int, optional): Weight updating frequency. Defaults to 1000.\n        momentum (float, optional): Momentum $m$ for moving weight. Defaults to 0.9.\n        init_weights (List[float]): Initial weights list. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import mtl\n        &gt;&gt;&gt; model = paddle.nn.Linear(3, 4)\n        &gt;&gt;&gt; loss_aggregator = mtl.GradNorm(model, num_losses=2)\n        &gt;&gt;&gt; for i in range(5):\n        ...     x1 = paddle.randn([8, 3])\n        ...     x2 = paddle.randn([8, 3])\n        ...     y1 = model(x1)\n        ...     y2 = model(x2)\n        ...     loss1 = paddle.sum(y1)\n        ...     loss2 = paddle.sum((y2 - 2) ** 2)\n        ...     loss_aggregator({'loss1': loss1, 'loss2': loss2}).backward()\n    \"\"\"\n    weight: paddle.Tensor\n\n    def __init__(\n        self,\n        model: nn.Layer,\n        num_losses: int = 1,\n        update_freq: int = 1000,\n        momentum: float = 0.9,\n        init_weights: List[float] = None,\n    ) -&gt; None:\n        super().__init__(model)\n        self.step = 0\n        self.num_losses = num_losses\n        self.update_freq = update_freq\n        self.momentum = momentum\n        if init_weights is not None and num_losses != len(init_weights):\n            raise ValueError(\n                f\"Length of init_weights({len(init_weights)}) should be equal to \"\n                f\"num_losses({num_losses}).\"\n            )\n        self.register_buffer(\n            \"weight\",\n            paddle.to_tensor(init_weights, dtype=\"float32\")\n            if init_weights is not None\n            else paddle.ones([num_losses]),\n        )\n\n    def _compute_weight(self, losses: List[\"paddle.Tensor\"]) -&gt; List[\"paddle.Tensor\"]:\n        grad_norms = []\n        for loss in losses:\n            loss.backward(retain_graph=True)  # NOTE: Keep graph for loss backward\n            with paddle.no_grad():\n                grad_vector = paddle.concat(\n                    [\n                        p.grad.reshape([-1])\n                        for p in self.model.parameters()\n                        if p.grad is not None\n                    ]\n                )\n                grad_norms.append(paddle.linalg.norm(grad_vector, p=2))\n                self.model.clear_gradients()\n\n        mean_grad_norm = paddle.mean(paddle.stack(grad_norms))\n        weight = [(mean_grad_norm / x) for x in grad_norms]\n\n        return weight\n\n    def __call__(\n        self, losses: Dict[str, \"paddle.Tensor\"], step: int = 0\n    ) -&gt; \"paddle.Tensor\":\n        assert len(losses) == self.num_losses, (\n            f\"Length of given losses({len(losses)}) should be equal to \"\n            f\"num_losses({self.num_losses}).\"\n        )\n        self.step = step\n\n        # compute current loss with moving weights\n        loss = 0.0\n        for i, key in enumerate(losses):\n            if i == 0:\n                loss = self.weight[i] * losses[key]\n            else:\n                loss += self.weight[i] * losses[key]\n\n        # update moving weights every 'update_freq' steps\n        if self.step % self.update_freq == 0:\n            weight = self._compute_weight(list(losses.values()))\n            for i in range(self.num_losses):\n                self.weight[i].set_value(\n                    self.momentum * self.weight[i] + (1 - self.momentum) * weight[i]\n                )\n            # logger.message(f\"weight at step {self.step}: {self.weight.numpy()}\")\n\n        return loss\n</code></pre>"},{"location":"zh/api/loss/mtl/#ppsci.loss.mtl.LossAggregator","title":"<code>LossAggregator</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Base class of loss aggregator mainly for multitask learning.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>Training model.</p> required Source code in <code>ppsci/loss/mtl/base.py</code> <pre><code>class LossAggregator(nn.Layer):\n    \"\"\"Base class of loss aggregator mainly for multitask learning.\n\n    Args:\n        model (nn.Layer): Training model.\n    \"\"\"\n\n    def __init__(self, model: nn.Layer) -&gt; None:\n        super().__init__()\n        self.model = model\n        self.step = 0\n        self.param_num = 0\n        for param in self.model.parameters():\n            if not param.stop_gradient:\n                self.param_num += 1\n\n    def forward(\n        self, losses: Dict[str, \"paddle.Tensor\"], step: int = 0\n    ) -&gt; Union[\"paddle.Tensor\", \"LossAggregator\"]:\n        self.losses = losses\n        self.loss_num = len(losses)\n        self.step = step\n        return self\n\n    def backward(self) -&gt; None:\n        raise NotImplementedError(\n            f\"'backward' should be implemented in subclass {self.__class__.__name__}\"\n        )\n</code></pre>"},{"location":"zh/api/loss/mtl/#ppsci.loss.mtl.PCGrad","title":"<code>PCGrad</code>","text":"<p>               Bases: <code>LossAggregator</code></p> <p>Projecting Conflicting Gradients</p> <p>Gradient Surgery for Multi-Task Learning</p> <p>Code reference: https://github.com/tianheyu927/PCGrad/blob/master/PCGrad_tf.py</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>Training model.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import mtl\n&gt;&gt;&gt; model = paddle.nn.Linear(3, 4)\n&gt;&gt;&gt; loss_aggregator = mtl.PCGrad(model)\n&gt;&gt;&gt; for i in range(5):\n...     x1 = paddle.randn([8, 3])\n...     x2 = paddle.randn([8, 3])\n...     y1 = model(x1)\n...     y2 = model(x2)\n...     loss1 = paddle.sum(y1)\n...     loss2 = paddle.sum((y2 - 2) ** 2)\n...     loss_aggregator({'loss1': loss1, 'loss2': loss2}).backward()\n</code></pre> Source code in <code>ppsci/loss/mtl/pcgrad.py</code> <pre><code>class PCGrad(base.LossAggregator):\n    r\"\"\"\n    **P**rojecting **C**onflicting Gradients\n\n    [Gradient Surgery for Multi-Task Learning](https://papers.nips.cc/paper/2020/hash/3fe78a8acf5fda99de95303940a2420c-Abstract.html)\n\n    Code reference: [https://github.com/tianheyu927/PCGrad/blob/master/PCGrad_tf.py](https://github.com/tianheyu927/PCGrad/blob/master/PCGrad_tf.py)\n\n    Args:\n        model (nn.Layer): Training model.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import mtl\n        &gt;&gt;&gt; model = paddle.nn.Linear(3, 4)\n        &gt;&gt;&gt; loss_aggregator = mtl.PCGrad(model)\n        &gt;&gt;&gt; for i in range(5):\n        ...     x1 = paddle.randn([8, 3])\n        ...     x2 = paddle.randn([8, 3])\n        ...     y1 = model(x1)\n        ...     y2 = model(x2)\n        ...     loss1 = paddle.sum(y1)\n        ...     loss2 = paddle.sum((y2 - 2) ** 2)\n        ...     loss_aggregator({'loss1': loss1, 'loss2': loss2}).backward()\n    \"\"\"\n\n    def __init__(self, model: nn.Layer) -&gt; None:\n        super().__init__(model)\n        self._zero = paddle.zeros([])\n\n    def backward(self) -&gt; None:\n        # shuffle order of losses\n        keys = list(self.losses.keys())\n        np.random.shuffle(keys)\n        self.losses = {key: self.losses[key] for key in keys}\n\n        grads_list = self._compute_grads()\n        with paddle.no_grad():\n            refined_grads = self._refine_grads(grads_list)\n            self._set_grads(refined_grads)\n\n    def _compute_grads(self) -&gt; List[paddle.Tensor]:\n        # compute all gradients derived by each loss\n        grads_list = []  # num_params x num_losses\n        for key in self.losses:\n            # backward with current loss\n            self.losses[key].backward()\n            grads_list.append(\n                paddle.concat(\n                    [\n                        param.grad.clone().reshape([-1])\n                        for param in self.model.parameters()\n                        if param.grad is not None\n                    ],\n                    axis=0,\n                )\n            )\n            # clear gradients for current loss for not affecting other loss\n            self.model.clear_gradients()\n\n        return grads_list\n\n    def _refine_grads(self, grads_list: List[paddle.Tensor]) -&gt; List[paddle.Tensor]:\n        def proj_grad(grad: paddle.Tensor):\n            for k in range(self.loss_num):\n                inner_product = paddle.sum(grad * grads_list[k])\n                proj_direction = inner_product / paddle.sum(\n                    grads_list[k] * grads_list[k]\n                )\n                grad = grad - paddle.minimum(proj_direction, self._zero) * grads_list[k]\n            return grad\n\n        grads_list = [proj_grad(grad) for grad in grads_list]\n\n        # Unpack flattened projected gradients back to their original shapes.\n        proj_grads: List[paddle.Tensor] = []\n        for j in range(self.loss_num):\n            start_idx = 0\n            for idx, var in enumerate(self.model.parameters()):\n                grad_shape = var.shape\n                flatten_dim = var.numel()\n                refined_grad = grads_list[j][start_idx : start_idx + flatten_dim]\n                refined_grad = paddle.reshape(refined_grad, grad_shape)\n                if len(proj_grads) &lt; self.param_num:\n                    proj_grads.append(refined_grad)\n                else:\n                    proj_grads[idx] += refined_grad\n                start_idx += flatten_dim\n        return proj_grads\n\n    def _set_grads(self, grads_list: List[paddle.Tensor]) -&gt; None:\n        for i, param in enumerate(self.model.parameters()):\n            param.grad = grads_list[i]\n</code></pre>"},{"location":"zh/api/loss/mtl/#ppsci.loss.mtl.Relobralo","title":"<code>Relobralo</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Relative Loss Balancing with Random Lookback</p> <p>Multi-Objective Loss Balancing for Physics-Informed Deep Learning</p> <p>Parameters:</p> Name Type Description Default <code>num_losses</code> <code>int</code> <p>Number of losses.</p> required <code>alpha</code> <code>float</code> <p>Ability for remembering past in paper. Defaults to 0.95.</p> <code>0.95</code> <code>beta</code> <code>float</code> <p>Parameter for generating \\(\\rho\\) from bernoulli distribution, and \\(E[\\rho](=\\beta)\\) should be close to 1. Defaults to 0.99.</p> <code>0.99</code> <code>tau</code> <code>float</code> <p>Temperature factor. Equivalent to softmax when \\(\\tau\\)=1.0, equivalent to argmax when \\(\\tau\\)=0. Defaults to 1.0.</p> <code>1.0</code> <code>eps</code> <code>float</code> <p>\\(\\epsilon\\) to avoid divided by 0 in losses. Defaults to 1e-8.</p> <code>1e-08</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.loss import mtl\n&gt;&gt;&gt; model = paddle.nn.Linear(3, 4)\n&gt;&gt;&gt; loss_aggregator = mtl.Relobralo(num_losses=2)\n&gt;&gt;&gt; for i in range(5):\n...     x1 = paddle.randn([8, 3])\n...     x2 = paddle.randn([8, 3])\n...     y1 = model(x1)\n...     y2 = model(x2)\n...     loss1 = paddle.sum(y1)\n...     loss2 = paddle.sum((y2 - 2) ** 2)\n...     loss_aggregator({'loss1': loss1, 'loss2': loss2}).backward()\n</code></pre> Source code in <code>ppsci/loss/mtl/relobralo.py</code> <pre><code>class Relobralo(nn.Layer):\n    r\"\"\"\n    **Re**lative **Lo**ss **B**alancing with **Ra**ndom **Lo**okback\n\n    [Multi-Objective Loss Balancing for Physics-Informed Deep Learning](https://arxiv.org/abs/2110.09813)\n\n    Args:\n        num_losses (int): Number of losses.\n        alpha (float, optional): Ability for remembering past in paper. Defaults to 0.95.\n        beta (float, optional): Parameter for generating $\\rho$ from bernoulli distribution,\n            and $E[\\rho](=\\beta)$ should be close to 1. Defaults to 0.99.\n        tau (float, optional): Temperature factor. Equivalent to softmax when $\\tau$=1.0,\n            equivalent to argmax when $\\tau$=0. Defaults to 1.0.\n        eps (float, optional): $\\epsilon$ to avoid divided by 0 in losses. Defaults to 1e-8.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.loss import mtl\n        &gt;&gt;&gt; model = paddle.nn.Linear(3, 4)\n        &gt;&gt;&gt; loss_aggregator = mtl.Relobralo(num_losses=2)\n        &gt;&gt;&gt; for i in range(5):\n        ...     x1 = paddle.randn([8, 3])\n        ...     x2 = paddle.randn([8, 3])\n        ...     y1 = model(x1)\n        ...     y2 = model(x2)\n        ...     loss1 = paddle.sum(y1)\n        ...     loss2 = paddle.sum((y2 - 2) ** 2)\n        ...     loss_aggregator({'loss1': loss1, 'loss2': loss2}).backward()\n    \"\"\"\n\n    def __init__(\n        self,\n        num_losses: int,\n        alpha: float = 0.95,\n        beta: float = 0.99,\n        tau: float = 1.0,\n        eps: float = 1e-8,\n    ) -&gt; None:\n        super().__init__()\n        self.step = 0\n        self.num_losses: int = num_losses\n        self.alpha: float = alpha\n        self.beta: float = beta\n        self.tau: float = tau\n        self.eps: float = eps\n        self.register_buffer(\"losses_init\", paddle.zeros([self.num_losses]))\n        self.register_buffer(\"losses_prev\", paddle.zeros([self.num_losses]))\n        self.register_buffer(\"lmbda\", paddle.ones([self.num_losses]))\n\n    def _softmax(self, vec: \"paddle.Tensor\") -&gt; \"paddle.Tensor\":\n        max_item = vec.max()\n        result = paddle.exp(vec - max_item) / paddle.exp(vec - max_item).sum()\n        return result\n\n    def _compute_bal(\n        self, losses_vec1: \"paddle.Tensor\", losses_vec2: \"paddle.Tensor\"\n    ) -&gt; \"paddle.Tensor\":\n        return self.num_losses * (\n            self._softmax(losses_vec1 / (self.tau * losses_vec2 + self.eps))\n        )\n\n    def __call__(\n        self, losses: Dict[str, \"paddle.Tensor\"], step: int = 0\n    ) -&gt; \"paddle.Tensor\":\n        assert len(losses) == self.num_losses, (\n            f\"Length of given losses({len(losses)}) should be equal to \"\n            f\"num_losses({self.num_losses}).\"\n        )\n        self.step = step\n        losses_stacked = paddle.stack(list(losses.values()))  # [num_losses, ]\n\n        if self.step == 0:\n            loss = losses_stacked.sum()\n            with paddle.no_grad():\n                paddle.assign(losses_stacked.detach(), self.losses_init)\n        else:\n            with paddle.no_grad():\n                # 1. update lambda_hist\n                rho = paddle.bernoulli(paddle.to_tensor(self.beta))\n                lmbda_hist = rho * self.lmbda + (1 - rho) * self._compute_bal(\n                    losses_stacked, self.losses_init\n                )\n\n                # 2. update lambda\n                paddle.assign(\n                    self.alpha * lmbda_hist\n                    + (1 - self.alpha)\n                    * self._compute_bal(losses_stacked, self.losses_prev),\n                    self.lmbda,\n                )\n\n            # 3. compute reweighted total loss with lambda\n            loss = (losses_stacked * self.lmbda).sum()\n\n        # update losses_prev at the end of each step\n        with paddle.no_grad():\n            paddle.assign(losses_stacked.detach(), self.losses_prev)\n\n        return loss\n</code></pre>"},{"location":"zh/api/loss/mtl/#ppsci.loss.mtl.Sum","title":"<code>Sum</code>","text":"<p>               Bases: <code>LossAggregator</code></p> <p>Default loss aggregator which do simple summation for given losses as below.</p> \\[ loss = \\sum_i^N losses_i \\] Source code in <code>ppsci/loss/mtl/sum.py</code> <pre><code>class Sum(LossAggregator):\n    r\"\"\"\n    **Default loss aggregator** which do simple summation for given losses as below.\n\n    $$\n    loss = \\sum_i^N losses_i\n    $$\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self.step = 0\n\n    def __call__(\n        self, losses: Dict[str, \"paddle.Tensor\"], step: int = 0\n    ) -&gt; \"paddle.Tensor\":\n        assert (\n            len(losses) &gt; 0\n        ), f\"Number of given losses({len(losses)}) can not be empty.\"\n        self.step = step\n\n        total_loss = 0.0\n        for i, key in enumerate(losses):\n            if i == 0:\n                total_loss = losses[key]\n            else:\n                total_loss += losses[key]\n\n        return total_loss\n</code></pre>"},{"location":"zh/api/utils/checker/","title":"ppsci.utils.checker","text":""},{"location":"zh/api/utils/checker/#utilschecker","title":"Utils.checker(\u68c0\u67e5) \u6a21\u5757","text":""},{"location":"zh/api/utils/checker/#ppsci.utils.checker","title":"<code>ppsci.utils.checker</code>","text":""},{"location":"zh/api/utils/checker/#ppsci.utils.checker.run_check","title":"<code>run_check()</code>","text":"<p>Check whether PaddleScience is installed correctly and running successfully on your system.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; ppsci.utils.run_check()\n</code></pre> Source code in <code>ppsci/utils/checker.py</code> <pre><code>def run_check() -&gt; None:\n    \"\"\"Check whether PaddleScience is installed correctly and running successfully on\n    your system.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; ppsci.utils.run_check()  # doctest: +SKIP\n    \"\"\"\n    # test demo code below.\n    import ppsci\n\n    try:\n        ppsci.utils.set_random_seed(42)\n        ppsci.utils.logger.init_logger()\n        model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\", \"p\"), 3, 16, \"tanh\")\n\n        equation = {\"NavierStokes\": ppsci.equation.NavierStokes(0.01, 1.0, 2, False)}\n\n        geom = {\"rect\": ppsci.geometry.Rectangle((-0.05, -0.05), (0.05, 0.05))}\n\n        ITERS_PER_EPOCH = 5\n        train_dataloader_cfg = {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"iters_per_epoch\": ITERS_PER_EPOCH,\n        }\n\n        NPOINT_PDE = 8**2\n        pde_constraint = ppsci.constraint.InteriorConstraint(\n            equation[\"NavierStokes\"].equations,\n            {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n            geom[\"rect\"],\n            {**train_dataloader_cfg, \"batch_size\": NPOINT_PDE},\n            ppsci.loss.MSELoss(\"sum\"),\n            evenly=True,\n            weight_dict={\n                \"continuity\": 0.0001,\n                \"momentum_x\": 0.0001,\n                \"momentum_y\": 0.0001,\n            },\n            name=\"EQ\",\n        )\n        constraint = {pde_constraint.name: pde_constraint}\n\n        residual_validator = ppsci.validate.GeometryValidator(\n            equation[\"NavierStokes\"].equations,\n            {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n            geom[\"rect\"],\n            {\n                \"dataset\": \"NamedArrayDataset\",\n                \"total_size\": 8**2,\n                \"batch_size\": 32,\n                \"sampler\": {\"name\": \"BatchSampler\"},\n            },\n            ppsci.loss.MSELoss(\"sum\"),\n            evenly=True,\n            metric={\"MSE\": ppsci.metric.MSE(False)},\n            name=\"Residual\",\n        )\n        validator = {residual_validator.name: residual_validator}\n\n        EPOCHS = 2\n        optimizer = ppsci.optimizer.Adam(0.001)(model)\n        solver = ppsci.solver.Solver(\n            model,\n            constraint,\n            None,\n            optimizer,\n            None,\n            EPOCHS,\n            ITERS_PER_EPOCH,\n            device=paddle.device.get_device(),\n            equation=equation,\n            validator=validator,\n        )\n        solver.train()\n        solver.eval(EPOCHS)\n    except Exception as e:\n        traceback.print_exc()\n        logger.error(\n            f\"PaddleScience meets some problem with \\n {repr(e)} \\nplease check whether \"\n            \"Paddle's version and PaddleScience's version are both correct.\"\n        )\n    else:\n        logger.message(\"PaddleScience is installed successfully.\u2728 \ud83c\udf70 \u2728\")\n</code></pre>"},{"location":"zh/api/utils/checker/#ppsci.utils.checker.run_check_mesh","title":"<code>run_check_mesh()</code>","text":"<p>Check whether geometry packages is installed correctly and <code>ppsci.geometry.Mesh</code> can running successfully on your system.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; ppsci.utils.run_check_mesh()\n</code></pre> Source code in <code>ppsci/utils/checker.py</code> <pre><code>def run_check_mesh() -&gt; None:\n    \"\"\"Check whether geometry packages is installed correctly and `ppsci.geometry.Mesh`\n    can running successfully on your system.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; ppsci.utils.run_check_mesh()  # doctest: +SKIP\n    \"\"\"\n    # test demo code below.\n    if importlib.util.find_spec(\"open3d\") is None:\n        raise ModuleNotFoundError(\n            \"Please install open3d first with: \" \"`pip install open3d`\"\n        )\n    if importlib.util.find_spec(\"pysdf\") is None:\n        raise ModuleNotFoundError(\n            \"Please install pysdf first with: `pip install pysdf`\"\n        )\n    if importlib.util.find_spec(\"pymesh\") is None:\n        raise ModuleNotFoundError(\n            \"Please install pymesh first as \"\n            \"https://paddlescience-docs.readthedocs.io/zh/latest/zh/install_setup/#__tabbed_4_4\"\n        )\n\n    import numpy as np\n    import pymesh\n\n    import ppsci\n\n    try:\n        ppsci.utils.set_random_seed(42)\n        ppsci.utils.logger.init_logger()\n        model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\", \"p\"), 3, 16, \"tanh\")\n\n        equation = {\"NavierStokes\": ppsci.equation.NavierStokes(0.01, 1.0, 2, False)}\n\n        # create a 1x1x1 simple cube geometry\n        vertices = np.array(\n            [\n                [0.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0],\n                [0.0, 0.0, 1.0],\n                [1.0, 0.0, 1.0],\n                [0.0, 1.0, 0.0],\n                [1.0, 1.0, 0.0],\n                [0.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0],\n            ]\n        )  # 8 vertices for mesh\n        faces = np.array(\n            [\n                [4, 7, 5],\n                [4, 6, 7],\n                [0, 2, 4],\n                [2, 6, 4],\n                [0, 1, 2],\n                [1, 3, 2],\n                [1, 5, 7],\n                [1, 7, 3],\n                [2, 3, 7],\n                [2, 7, 6],\n                [0, 4, 1],\n                [1, 4, 5],\n            ]\n        )  # 12 triangle faces for mesh\n        box_mesh = pymesh.form_mesh(vertices, faces)\n        geom = {\"rect\": ppsci.geometry.Mesh(box_mesh)}\n\n        ITERS_PER_EPOCH = 5\n        train_dataloader_cfg = {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"iters_per_epoch\": ITERS_PER_EPOCH,\n        }\n\n        NPOINT_PDE = 8**2\n        pde_constraint = ppsci.constraint.InteriorConstraint(\n            equation[\"NavierStokes\"].equations,\n            {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n            geom[\"rect\"],\n            {**train_dataloader_cfg, \"batch_size\": NPOINT_PDE},\n            ppsci.loss.MSELoss(\"sum\"),\n            weight_dict={\n                \"continuity\": \"sdf\",\n                \"momentum_x\": \"sdf\",\n                \"momentum_y\": \"sdf\",\n            },\n            name=\"EQ\",\n        )\n        constraint = {pde_constraint.name: pde_constraint}\n\n        residual_validator = ppsci.validate.GeometryValidator(\n            equation[\"NavierStokes\"].equations,\n            {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n            geom[\"rect\"],\n            {\n                \"dataset\": \"NamedArrayDataset\",\n                \"total_size\": 8**2,\n                \"batch_size\": 32,\n                \"sampler\": {\"name\": \"BatchSampler\"},\n            },\n            ppsci.loss.MSELoss(\"sum\"),\n            metric={\"MSE\": ppsci.metric.MSE(False)},\n            name=\"Residual\",\n        )\n        validator = {residual_validator.name: residual_validator}\n\n        EPOCHS = 2\n        optimizer = ppsci.optimizer.Adam(0.001)(model)\n        solver = ppsci.solver.Solver(\n            model,\n            constraint,\n            None,\n            optimizer,\n            None,\n            EPOCHS,\n            ITERS_PER_EPOCH,\n            device=paddle.device.get_device(),\n            equation=equation,\n            validator=validator,\n        )\n        solver.train()\n        solver.eval(EPOCHS)\n    except Exception as e:\n        traceback.print_exc()\n        logger.error(\n            f\"PaddleScience meets some problem with \\n {repr(e)} \\nplease check whether \"\n            \"open3d, pysdf, pybind11, PyMesh are all installed correctly.\"\n        )\n    else:\n        logger.message(\"ppsci.geometry.Mesh module running successfully.\u2728 \ud83c\udf70 \u2728\")\n</code></pre>"},{"location":"zh/api/utils/checker/#ppsci.utils.checker.dynamic_import_to_globals","title":"<code>dynamic_import_to_globals(names, alias=None)</code>","text":"<p>Import module and add it to globals() by given names dynamically.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>Union[str, Sequence[str]]</code> <p>Module name or sequence of module names.</p> required <code>alias</code> <code>Dict[str, str]</code> <p>Alias name of module when imported into globals().</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether given names all exist.</p> Source code in <code>ppsci/utils/checker.py</code> <pre><code>def dynamic_import_to_globals(\n    names: Union[str, Sequence[str]], alias: Dict[str, str] = None\n) -&gt; bool:\n    \"\"\"Import module and add it to globals() by given names dynamically.\n\n    Args:\n        names (Union[str, Sequence[str]]): Module name or sequence of module names.\n        alias (Dict[str, str]): Alias name of module when imported into globals().\n\n    Returns:\n        bool: Whether given names all exist.\n    \"\"\"\n    if isinstance(names, str):\n        names = (names,)\n\n    if alias is None:\n        alias = {}\n\n    for name in names:\n        # find module in environment by it's name and alias(if given)\n        module_spec = importlib.util.find_spec(name)\n        if module_spec is None and name in alias:\n            module_spec = importlib.util.find_spec(alias[name])\n\n        # log error and return False if module do not exist\n        if not module_spec:\n            logger.error(f\"Module {name} should be installed first.\")\n            return False\n\n        # module exist, add to globals() if not in globals()\n        add_name = name\n        if add_name in alias:\n            add_name = alias[add_name]\n        if add_name not in globals():\n            globals()[add_name] = importlib.import_module(name)\n\n    return True\n</code></pre>"},{"location":"zh/api/utils/ema/","title":"ppsci.utils.ema","text":""},{"location":"zh/api/utils/ema/#utilsema","title":"Utils.ema(\u6a21\u578b\u5e73\u5747) \u6a21\u5757","text":""},{"location":"zh/api/utils/ema/#ppsci.utils.ema","title":"<code>ppsci.utils.ema</code>","text":""},{"location":"zh/api/utils/ema/#ppsci.utils.ema.AveragedModel","title":"<code>AveragedModel</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Base class for Averaged Model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>The model to be averaged.</p> required <code>decay</code> <code>float</code> <p>The decay rate for averaging.</p> <code>None</code> Source code in <code>ppsci/utils/ema.py</code> <pre><code>class AveragedModel(nn.Layer):\n    \"\"\"Base class for Averaged Model.\n\n    Args:\n        model (nn.Layer): The model to be averaged.\n        decay (float): The decay rate for averaging.\n    \"\"\"\n\n    def __init__(self, model: nn.Layer, decay: Optional[float] = None):\n        super().__init__()\n        self.model = model  # As a quick reference to online model\n        self.decay = decay\n\n        self.params_shadow: Dict[str, paddle.Tensor] = {}  # ema param or buffer\n        self.params_backup: Dict[str, paddle.Tensor] = {}  # used for apply and restore\n        for name, param_or_buffer in itertools.chain(\n            self.model.named_parameters(), self.model.named_buffers()\n        ):\n            self.params_shadow[name] = param_or_buffer.clone().detach()\n\n        self.register_buffer(\"n_avg\", paddle.to_tensor(0, \"int64\"), True)\n\n    def _update_fn_(\n        self,\n        shadow_param: paddle.Tensor,\n        model_param: paddle.Tensor,\n        step: paddle.Tensor,\n    ):\n        raise NotImplementedError(\"AveragedModel._update_fn_ should be implemented.\")\n\n    def update(self):\n        for name, param_or_buffer in itertools.chain(\n            self.model.named_parameters(), self.model.named_buffers()\n        ):\n            if not param_or_buffer.stop_gradient:\n                assert (\n                    name in self.params_shadow\n                ), f\"Parameter: {name} should be in params_shadow dict, but not found.\"\n\n                # only update floating and complex data\n                if paddle.is_floating_point(param_or_buffer) or paddle.is_complex(\n                    param_or_buffer\n                ):\n                    with paddle.no_grad():\n                        self._update_fn_(\n                            self.params_shadow[name],\n                            param_or_buffer,\n                            self.n_avg,\n                        )\n        self.n_avg += 1\n\n    def apply_shadow(self):\n        \"\"\"Set averaged model parameters to online model.\"\"\"\n        for name, param_or_buffer in itertools.chain(\n            self.model.named_parameters(), self.model.named_buffers()\n        ):\n            if name in self.params_shadow:\n                stop_gradient = param_or_buffer.stop_gradient\n                with paddle.no_grad():\n                    self.params_backup[name] = paddle.assign(param_or_buffer)\n                    paddle.assign(self.params_shadow[name], param_or_buffer)\n                param_or_buffer.stop_gradient = stop_gradient\n\n    def restore(self):\n        \"\"\"Restore online model parameters from backup parameter dict.\"\"\"\n        assert self.params_backup, (\n            \"params_backup should not be empty, may be caused by calling 'restore' \"\n            \"before 'apply_shadow'.\"\n        )\n        for name, param_or_buffer in itertools.chain(\n            self.model.named_parameters(), self.model.named_buffers()\n        ):\n            if name in self.params_backup:\n                assert name in self.params_shadow\n                stop_gradient = param_or_buffer.stop_gradient\n                with paddle.no_grad():\n                    paddle.assign(self.params_backup[name], param_or_buffer)\n                param_or_buffer.stop_gradient = stop_gradient\n\n        self.params_backup = {}\n\n    def set_state_dict(self, state_dict: Dict[str, paddle.Tensor]):\n        assert (\n            \"n_avg\" in state_dict\n        ), \"state_dict should contain 'n_avg' key, but not found.\"\n        self.n_avg.set_value(state_dict.pop(\"n_avg\"))\n        self.params_shadow.update(state_dict)\n\n    def state_dict(self) -&gt; Dict[str, paddle.Tensor]:\n        return {\n            **self.params_shadow,\n            \"n_avg\": self.n_avg,\n        }\n</code></pre>"},{"location":"zh/api/utils/ema/#ppsci.utils.ema.AveragedModel.apply_shadow","title":"<code>apply_shadow()</code>","text":"<p>Set averaged model parameters to online model.</p> Source code in <code>ppsci/utils/ema.py</code> <pre><code>def apply_shadow(self):\n    \"\"\"Set averaged model parameters to online model.\"\"\"\n    for name, param_or_buffer in itertools.chain(\n        self.model.named_parameters(), self.model.named_buffers()\n    ):\n        if name in self.params_shadow:\n            stop_gradient = param_or_buffer.stop_gradient\n            with paddle.no_grad():\n                self.params_backup[name] = paddle.assign(param_or_buffer)\n                paddle.assign(self.params_shadow[name], param_or_buffer)\n            param_or_buffer.stop_gradient = stop_gradient\n</code></pre>"},{"location":"zh/api/utils/ema/#ppsci.utils.ema.AveragedModel.restore","title":"<code>restore()</code>","text":"<p>Restore online model parameters from backup parameter dict.</p> Source code in <code>ppsci/utils/ema.py</code> <pre><code>def restore(self):\n    \"\"\"Restore online model parameters from backup parameter dict.\"\"\"\n    assert self.params_backup, (\n        \"params_backup should not be empty, may be caused by calling 'restore' \"\n        \"before 'apply_shadow'.\"\n    )\n    for name, param_or_buffer in itertools.chain(\n        self.model.named_parameters(), self.model.named_buffers()\n    ):\n        if name in self.params_backup:\n            assert name in self.params_shadow\n            stop_gradient = param_or_buffer.stop_gradient\n            with paddle.no_grad():\n                paddle.assign(self.params_backup[name], param_or_buffer)\n            param_or_buffer.stop_gradient = stop_gradient\n\n    self.params_backup = {}\n</code></pre>"},{"location":"zh/api/utils/ema/#ppsci.utils.ema.ExponentialMovingAverage","title":"<code>ExponentialMovingAverage</code>","text":"<p>               Bases: <code>AveragedModel</code></p> <p>Implements the exponential moving average (EMA) of the model.</p> <p>All parameters are updated by the formula as below:</p> \\[ \\mathbf{\\theta}_{EMA}^{t+1} = \\alpha \\mathbf{\\theta}_{EMA}^{t} + (1 - \\alpha) \\mathbf{\\theta}^{t} \\] <p>Where \\(\\alpha\\) is the decay rate, \\(\\theta_{EMA}^{t}\\) is the moving average parameters and \\(\\theta^{t}\\) is the online parameters at step \\(t\\).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>The model to be averaged.</p> required <code>decay</code> <code>float</code> <p>The decay rate for averaging.</p> <code>0.9</code> Source code in <code>ppsci/utils/ema.py</code> <pre><code>class ExponentialMovingAverage(AveragedModel):\n    r\"\"\"Implements the exponential moving average (EMA) of the model.\n\n    All parameters are updated by the formula as below:\n\n    $$\n    \\mathbf{\\theta}_{EMA}^{t+1} = \\alpha \\mathbf{\\theta}_{EMA}^{t} + (1 - \\alpha) \\mathbf{\\theta}^{t}\n    $$\n\n    Where $\\alpha$ is the decay rate, $\\theta_{EMA}^{t}$ is the moving average parameters and $\\theta^{t}$ is the online parameters at step $t$.\n\n    Args:\n        model (nn.Layer): The model to be averaged.\n        decay (float): The decay rate for averaging.\n    \"\"\"\n\n    def __init__(self, model: nn.Layer, decay: float = 0.9):\n        super().__init__(model, decay)\n\n    def _update_fn_(self, shadow_param, model_param, step):\n        shadow_param.lerp_(model_param, 1.0 - self.decay)\n</code></pre>"},{"location":"zh/api/utils/ema/#ppsci.utils.ema.StochasticWeightAverage","title":"<code>StochasticWeightAverage</code>","text":"<p>               Bases: <code>AveragedModel</code></p> <p>Implements the stochastic weight averaging (SWA) of the model.</p> <p>Stochastic Weight Averaging was proposed in Averaging Weights Leads to Wider Optima and Better Generalization,</p> <p>All parameters are updated by the formula as below:</p> \\[ \\mathbf{\\theta}_{SWA}^{t} = \\frac{1}{t-t_0+1}\\sum_{i=t_0}^t{\\mathbf{\\theta}^{i}} \\] <p>Where \\(\\theta_{SWA}^{t}\\) is the average parameters between step \\(t_0\\) and \\(t\\), \\(\\theta^{i}\\) is the online parameters at step \\(i\\).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>The model to be averaged.</p> required Source code in <code>ppsci/utils/ema.py</code> <pre><code>class StochasticWeightAverage(AveragedModel):\n    r\"\"\"Implements the stochastic weight averaging (SWA) of the model.\n\n    Stochastic Weight Averaging was proposed in [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407),\n\n    All parameters are updated by the formula as below:\n\n    $$\n    \\mathbf{\\theta}_{SWA}^{t} = \\frac{1}{t-t_0+1}\\sum_{i=t_0}^t{\\mathbf{\\theta}^{i}}\n    $$\n\n    Where $\\theta_{SWA}^{t}$ is the average parameters between step $t_0$ and $t$, $\\theta^{i}$ is the online parameters at step $i$.\n\n    Args:\n        model (nn.Layer): The model to be averaged.\n    \"\"\"\n\n    def __init__(self, model: nn.Layer):\n        super().__init__(model, None)\n        self.n_avg += 1  # Set to 1 for model already initialized\n\n    def _update_fn_(self, shadow_param, model_param, step):\n        dynamic_decay = step / (step + 1)\n        shadow_param.lerp_(model_param, 1.0 - dynamic_decay)\n</code></pre>"},{"location":"zh/api/utils/expression/","title":"ppsci.utils.expression","text":""},{"location":"zh/api/utils/expression/#utilsexpression","title":"Utils.expression(\u65b9\u7a0b\u8ba1\u7b97) \u6a21\u5757","text":""},{"location":"zh/api/utils/expression/#ppsci.utils.expression","title":"<code>ppsci.utils.expression</code>","text":""},{"location":"zh/api/utils/expression/#ppsci.utils.expression.ExpressionSolver","title":"<code>ExpressionSolver</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Expression computing helper, which compute named result according to corresponding function and related inputs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\"), 5, 128)\n&gt;&gt;&gt; expr_solver = ExpressionSolver()\n</code></pre> Source code in <code>ppsci/utils/expression.py</code> <pre><code>class ExpressionSolver(nn.Layer):\n    \"\"\"Expression computing helper, which compute named result according to corresponding\n    function and related inputs.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\"), 5, 128)\n        &gt;&gt;&gt; expr_solver = ExpressionSolver()\n    \"\"\"\n\n    nvtx_flag: bool  # only for nsight analysis\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, *args, **kwargs):\n        raise NotImplementedError(\n            \"Use train_forward/eval_forward/visu_forward instead of forward.\"\n        )\n\n    @jit.to_static\n    def train_forward(\n        self,\n        expr_dicts: Tuple[Dict[str, Callable], ...],\n        input_dicts: Tuple[Dict[str, \"paddle.Tensor\"], ...],\n        model: arch.Arch,\n        constraint: Dict[str, \"constraint.Constraint\"],\n        label_dicts: Tuple[Dict[str, \"paddle.Tensor\"], ...],\n        weight_dicts: Tuple[Dict[str, \"paddle.Tensor\"], ...],\n    ) -&gt; Tuple[Dict[str, \"paddle.Tensor\"], Dict[str, float]]:\n        \"\"\"Forward computation for training, including model forward and equation\n        forward.\n\n        Args:\n            expr_dicts (Tuple[Dict[str, Callable], ...]): Tuple of expression dicts.\n            input_dicts (Tuple[Dict[str, paddle.Tensor], ...]): Tuple of input dicts.\n            model (arch.Arch): NN model.\n            constraint (Dict[str, \"constraint.Constraint\"]): Constraint dict.\n            label_dicts (Tuple[Dict[str, paddle.Tensor], ...]): Tuple of label dicts.\n            weight_dicts (Tuple[Dict[str, paddle.Tensor], ...]): Tuple of weight dicts.\n\n        Returns:\n            Tuple[Dict[str, \"paddle.Tensor\"], Dict[str, float]]:\n                all_losses: A loss dictionary containing the output terms of all constraints,\n                constraint_losses: The loss values of all constraints.\n        \"\"\"\n        losses_all: Dict[str, \"paddle.Tensor\"] = {}\n        losses_constraint: Dict[str, float] = {}\n\n        for i, cst_name in enumerate(constraint):\n            cst_obj = constraint[cst_name]\n\n            # model forward\n            if self.nvtx_flag:  # only for nsight analysis\n                core.nvprof_nvtx_push(f\"Constraint {cst_name}\")\n\n            output_dict = model(input_dicts[i])\n\n            # equation forward\n            data_dict = {k: v for k, v in input_dicts[i].items()}\n            data_dict.update(output_dict)\n            for name, expr in expr_dicts[i].items():\n                output_dict[name] = expr(data_dict)\n\n            # put field 'area' into output_dict\n            if \"area\" in input_dicts[i]:\n                output_dict[\"area\"] = input_dicts[i][\"area\"]\n\n            # clear differentiation cache\n            clear()\n\n            # compute loss for each constraint according to its' own output, label and weight\n            losses: Dict[str, \"paddle.Tensor\"] = cst_obj.loss(\n                output_dict,\n                label_dicts[i],\n                weight_dicts[i],\n            )\n            # update losses into 'losses_all' and 'losses_constraint'\n            # 'losses_all': Will be send to loss aggregator for further computing final loss(scalar)\n            # 'losses_constraint': Will be used in logging\n            losses_constraint[cst_name] = 0.0\n            for key in losses:\n                losses_constraint[cst_name] += losses[key].item()\n                if key in losses_all:\n                    losses_all[key] += losses[key]\n                else:\n                    losses_all[key] = losses[key]\n\n            if self.nvtx_flag:  # only for nsight analysis\n                core.nvprof_nvtx_pop()\n\n        return losses_all, losses_constraint\n\n    @jit.to_static\n    def eval_forward(\n        self,\n        expr_dict: Dict[str, Callable],\n        input_dict: Dict[str, \"paddle.Tensor\"],\n        model: arch.Arch,\n        validator: \"validate.Validator\",\n        label_dict: Dict[str, \"paddle.Tensor\"],\n        weight_dict: Dict[str, \"paddle.Tensor\"],\n    ) -&gt; Tuple[Dict[str, \"paddle.Tensor\"], Dict[str, \"paddle.Tensor\"]]:\n        \"\"\"Forward computation for evaluation, including model forward and equation\n        forward.\n\n        Args:\n            expr_dict (Dict[str, Callable]): Expression dict.\n            input_dict (Dict[str, paddle.Tensor]): Input dict.\n            model (arch.Arch): NN model.\n            validator (validate.Validator): Validator.\n            label_dict (Dict[str, paddle.Tensor]): Label dict.\n            weight_dict (Dict[str, paddle.Tensor]): Weight dict.\n\n        Returns:\n            Tuple[Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]]: Result dict and loss for\n                given validator.\n        \"\"\"\n        # model forward\n        output_dict = model(input_dict)\n\n        # equation forward\n        data_dict = {k: v for k, v in input_dict.items()}\n        data_dict.update(output_dict)\n        for name, expr in expr_dict.items():\n            output_dict[name] = expr(data_dict)\n\n        # put field 'area' into output_dict\n        if \"area\" in input_dict:\n            output_dict[\"area\"] = input_dict[\"area\"]\n\n        # clear differentiation cache\n        clear()\n\n        # compute loss for each validator according to its' own output, label and weight\n        validator_losses = validator.loss(\n            output_dict,\n            label_dict,\n            weight_dict,\n        )\n        return output_dict, validator_losses\n\n    def visu_forward(\n        self,\n        expr_dict: Optional[Dict[str, Callable]],\n        input_dict: Dict[str, \"paddle.Tensor\"],\n        model: arch.Arch,\n    ) -&gt; Dict[str, \"paddle.Tensor\"]:\n        \"\"\"Forward computation for visualization, including model forward and equation\n        forward.\n\n        Args:\n            expr_dict (Optional[Dict[str, Callable]]): Expression dict.\n            input_dict (Dict[str, paddle.Tensor]): Input dict.\n            model (arch.Arch): NN model.\n\n        Returns:\n            Dict[str, paddle.Tensor]: Result dict for given expression dict.\n        \"\"\"\n        # model forward\n        output_dict = model(input_dict)\n\n        if isinstance(expr_dict, dict):\n            # equation forward\n            data_dict = {k: v for k, v in input_dict.items()}\n            data_dict.update(output_dict)\n            for name, expr in expr_dict.items():\n                output_dict[name] = expr(data_dict)\n\n        # clear differentiation cache\n        clear()\n\n        return output_dict\n</code></pre>"},{"location":"zh/api/utils/expression/#ppsci.utils.expression.ExpressionSolver.eval_forward","title":"<code>eval_forward(expr_dict, input_dict, model, validator, label_dict, weight_dict)</code>","text":"<p>Forward computation for evaluation, including model forward and equation forward.</p> <p>Parameters:</p> Name Type Description Default <code>expr_dict</code> <code>Dict[str, Callable]</code> <p>Expression dict.</p> required <code>input_dict</code> <code>Dict[str, Tensor]</code> <p>Input dict.</p> required <code>model</code> <code>Arch</code> <p>NN model.</p> required <code>validator</code> <code>Validator</code> <p>Validator.</p> required <code>label_dict</code> <code>Dict[str, Tensor]</code> <p>Label dict.</p> required <code>weight_dict</code> <code>Dict[str, Tensor]</code> <p>Weight dict.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, 'paddle.Tensor'], Dict[str, 'paddle.Tensor']]</code> <p>Tuple[Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]]: Result dict and loss for given validator.</p> Source code in <code>ppsci/utils/expression.py</code> <pre><code>@jit.to_static\ndef eval_forward(\n    self,\n    expr_dict: Dict[str, Callable],\n    input_dict: Dict[str, \"paddle.Tensor\"],\n    model: arch.Arch,\n    validator: \"validate.Validator\",\n    label_dict: Dict[str, \"paddle.Tensor\"],\n    weight_dict: Dict[str, \"paddle.Tensor\"],\n) -&gt; Tuple[Dict[str, \"paddle.Tensor\"], Dict[str, \"paddle.Tensor\"]]:\n    \"\"\"Forward computation for evaluation, including model forward and equation\n    forward.\n\n    Args:\n        expr_dict (Dict[str, Callable]): Expression dict.\n        input_dict (Dict[str, paddle.Tensor]): Input dict.\n        model (arch.Arch): NN model.\n        validator (validate.Validator): Validator.\n        label_dict (Dict[str, paddle.Tensor]): Label dict.\n        weight_dict (Dict[str, paddle.Tensor]): Weight dict.\n\n    Returns:\n        Tuple[Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]]: Result dict and loss for\n            given validator.\n    \"\"\"\n    # model forward\n    output_dict = model(input_dict)\n\n    # equation forward\n    data_dict = {k: v for k, v in input_dict.items()}\n    data_dict.update(output_dict)\n    for name, expr in expr_dict.items():\n        output_dict[name] = expr(data_dict)\n\n    # put field 'area' into output_dict\n    if \"area\" in input_dict:\n        output_dict[\"area\"] = input_dict[\"area\"]\n\n    # clear differentiation cache\n    clear()\n\n    # compute loss for each validator according to its' own output, label and weight\n    validator_losses = validator.loss(\n        output_dict,\n        label_dict,\n        weight_dict,\n    )\n    return output_dict, validator_losses\n</code></pre>"},{"location":"zh/api/utils/expression/#ppsci.utils.expression.ExpressionSolver.train_forward","title":"<code>train_forward(expr_dicts, input_dicts, model, constraint, label_dicts, weight_dicts)</code>","text":"<p>Forward computation for training, including model forward and equation forward.</p> <p>Parameters:</p> Name Type Description Default <code>expr_dicts</code> <code>Tuple[Dict[str, Callable], ...]</code> <p>Tuple of expression dicts.</p> required <code>input_dicts</code> <code>Tuple[Dict[str, Tensor], ...]</code> <p>Tuple of input dicts.</p> required <code>model</code> <code>Arch</code> <p>NN model.</p> required <code>constraint</code> <code>Dict[str, 'constraint.Constraint']</code> <p>Constraint dict.</p> required <code>label_dicts</code> <code>Tuple[Dict[str, Tensor], ...]</code> <p>Tuple of label dicts.</p> required <code>weight_dicts</code> <code>Tuple[Dict[str, Tensor], ...]</code> <p>Tuple of weight dicts.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, 'paddle.Tensor'], Dict[str, float]]</code> <p>Tuple[Dict[str, \"paddle.Tensor\"], Dict[str, float]]: all_losses: A loss dictionary containing the output terms of all constraints, constraint_losses: The loss values of all constraints.</p> Source code in <code>ppsci/utils/expression.py</code> <pre><code>@jit.to_static\ndef train_forward(\n    self,\n    expr_dicts: Tuple[Dict[str, Callable], ...],\n    input_dicts: Tuple[Dict[str, \"paddle.Tensor\"], ...],\n    model: arch.Arch,\n    constraint: Dict[str, \"constraint.Constraint\"],\n    label_dicts: Tuple[Dict[str, \"paddle.Tensor\"], ...],\n    weight_dicts: Tuple[Dict[str, \"paddle.Tensor\"], ...],\n) -&gt; Tuple[Dict[str, \"paddle.Tensor\"], Dict[str, float]]:\n    \"\"\"Forward computation for training, including model forward and equation\n    forward.\n\n    Args:\n        expr_dicts (Tuple[Dict[str, Callable], ...]): Tuple of expression dicts.\n        input_dicts (Tuple[Dict[str, paddle.Tensor], ...]): Tuple of input dicts.\n        model (arch.Arch): NN model.\n        constraint (Dict[str, \"constraint.Constraint\"]): Constraint dict.\n        label_dicts (Tuple[Dict[str, paddle.Tensor], ...]): Tuple of label dicts.\n        weight_dicts (Tuple[Dict[str, paddle.Tensor], ...]): Tuple of weight dicts.\n\n    Returns:\n        Tuple[Dict[str, \"paddle.Tensor\"], Dict[str, float]]:\n            all_losses: A loss dictionary containing the output terms of all constraints,\n            constraint_losses: The loss values of all constraints.\n    \"\"\"\n    losses_all: Dict[str, \"paddle.Tensor\"] = {}\n    losses_constraint: Dict[str, float] = {}\n\n    for i, cst_name in enumerate(constraint):\n        cst_obj = constraint[cst_name]\n\n        # model forward\n        if self.nvtx_flag:  # only for nsight analysis\n            core.nvprof_nvtx_push(f\"Constraint {cst_name}\")\n\n        output_dict = model(input_dicts[i])\n\n        # equation forward\n        data_dict = {k: v for k, v in input_dicts[i].items()}\n        data_dict.update(output_dict)\n        for name, expr in expr_dicts[i].items():\n            output_dict[name] = expr(data_dict)\n\n        # put field 'area' into output_dict\n        if \"area\" in input_dicts[i]:\n            output_dict[\"area\"] = input_dicts[i][\"area\"]\n\n        # clear differentiation cache\n        clear()\n\n        # compute loss for each constraint according to its' own output, label and weight\n        losses: Dict[str, \"paddle.Tensor\"] = cst_obj.loss(\n            output_dict,\n            label_dicts[i],\n            weight_dicts[i],\n        )\n        # update losses into 'losses_all' and 'losses_constraint'\n        # 'losses_all': Will be send to loss aggregator for further computing final loss(scalar)\n        # 'losses_constraint': Will be used in logging\n        losses_constraint[cst_name] = 0.0\n        for key in losses:\n            losses_constraint[cst_name] += losses[key].item()\n            if key in losses_all:\n                losses_all[key] += losses[key]\n            else:\n                losses_all[key] = losses[key]\n\n        if self.nvtx_flag:  # only for nsight analysis\n            core.nvprof_nvtx_pop()\n\n    return losses_all, losses_constraint\n</code></pre>"},{"location":"zh/api/utils/expression/#ppsci.utils.expression.ExpressionSolver.visu_forward","title":"<code>visu_forward(expr_dict, input_dict, model)</code>","text":"<p>Forward computation for visualization, including model forward and equation forward.</p> <p>Parameters:</p> Name Type Description Default <code>expr_dict</code> <code>Optional[Dict[str, Callable]]</code> <p>Expression dict.</p> required <code>input_dict</code> <code>Dict[str, Tensor]</code> <p>Input dict.</p> required <code>model</code> <code>Arch</code> <p>NN model.</p> required <p>Returns:</p> Type Description <code>Dict[str, 'paddle.Tensor']</code> <p>Dict[str, paddle.Tensor]: Result dict for given expression dict.</p> Source code in <code>ppsci/utils/expression.py</code> <pre><code>def visu_forward(\n    self,\n    expr_dict: Optional[Dict[str, Callable]],\n    input_dict: Dict[str, \"paddle.Tensor\"],\n    model: arch.Arch,\n) -&gt; Dict[str, \"paddle.Tensor\"]:\n    \"\"\"Forward computation for visualization, including model forward and equation\n    forward.\n\n    Args:\n        expr_dict (Optional[Dict[str, Callable]]): Expression dict.\n        input_dict (Dict[str, paddle.Tensor]): Input dict.\n        model (arch.Arch): NN model.\n\n    Returns:\n        Dict[str, paddle.Tensor]: Result dict for given expression dict.\n    \"\"\"\n    # model forward\n    output_dict = model(input_dict)\n\n    if isinstance(expr_dict, dict):\n        # equation forward\n        data_dict = {k: v for k, v in input_dict.items()}\n        data_dict.update(output_dict)\n        for name, expr in expr_dict.items():\n            output_dict[name] = expr(data_dict)\n\n    # clear differentiation cache\n    clear()\n\n    return output_dict\n</code></pre>"},{"location":"zh/api/utils/initializer/","title":"ppsci.utils.initializer","text":""},{"location":"zh/api/utils/initializer/#utilsinitializer","title":"Utils.initializer(\u521d\u59cb\u5316) \u6a21\u5757","text":""},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer","title":"<code>ppsci.utils.initializer</code>","text":"<p>The initialization method under this module is aligned with pytorch initialization. If you need to use the initialization method of PaddlePaddle, please refer to paddle.nn.initializer</p> <p>This code is based on torch.nn.init Ths copyright of pytorch/pytorch is a BSD-style license, as found in the LICENSE file.</p>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.uniform_","title":"<code>uniform_(tensor, a, b)</code>","text":"<p>Modify tensor inplace using uniform_.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <code>a</code> <code>float</code> <p>Min value.</p> required <code>b</code> <code>float</code> <p>Max value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.uniform_(param, -1, 1)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def uniform_(tensor: paddle.Tensor, a: float, b: float) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using uniform_.\n\n    Args:\n        tensor (paddle.Tensor): Paddle Tensor.\n        a (float): Min value.\n        b (float): Max value.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.uniform_(param, -1, 1)\n    \"\"\"\n    return _no_grad_uniform_(tensor, a, b)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.normal_","title":"<code>normal_(tensor, mean=0.0, std=1.0)</code>","text":"<p>Modify tensor inplace using normal_.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <code>mean</code> <code>float</code> <p>Mean value. Defaults to 0.0.</p> <code>0.0</code> <code>std</code> <code>float</code> <p>Std value. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.normal_(param, 0, 1)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def normal_(\n    tensor: paddle.Tensor, mean: float = 0.0, std: float = 1.0\n) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using normal_.\n\n    Args:\n        tensor (paddle.Tensor): Paddle Tensor.\n        mean (float, optional): Mean value. Defaults to 0.0.\n        std (float, optional): Std value. Defaults to 1.0.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.normal_(param, 0, 1)\n    \"\"\"\n    return _no_grad_normal_(tensor, mean, std)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.trunc_normal_","title":"<code>trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0)</code>","text":"<p>Modify tensor inplace using trunc_normal_.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <code>mean</code> <code>float</code> <p>The mean of the normal distribution. Defaults to 0.0.</p> <code>0.0</code> <code>std</code> <code>float</code> <p>The standard deviation of the normal distribution. Defaults to 1.0.</p> <code>1.0</code> <code>a</code> <code>float</code> <p>The minimum cutoff value. Defaults to -2.0.</p> <code>-2.0</code> <code>b</code> <code>float</code> <p>The maximum cutoff value. Defaults to 2.0.</p> <code>2.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.trunc_normal_(param, 0.0, 1.0)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def trunc_normal_(\n    tensor: paddle.Tensor,\n    mean: float = 0.0,\n    std: float = 1.0,\n    a: float = -2.0,\n    b: float = 2.0,\n) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using trunc_normal_.\n\n    Args:\n        tensor (paddle.Tensor): Paddle Tensor.\n        mean (float, optional): The mean of the normal distribution. Defaults to 0.0.\n        std (float, optional): The standard deviation of the normal distribution. Defaults to 1.0.\n        a (float, optional): The minimum cutoff value. Defaults to -2.0.\n        b (float, optional): The maximum cutoff value. Defaults to 2.0.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.trunc_normal_(param, 0.0, 1.0)\n    \"\"\"\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.constant_","title":"<code>constant_(tensor, value=0.0)</code>","text":"<p>Modify tensor inplace using constant_.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <code>value</code> <code>float</code> <p>Value to fill tensor. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.constant_(param, 2)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def constant_(tensor: paddle.Tensor, value: float = 0.0) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using constant_.\n\n    Args:\n        tensor (paddle.Tensor): Paddle Tensor.\n        value (float, optional): Value to fill tensor. Defaults to 0.0.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.constant_(param, 2)\n    \"\"\"\n    return _no_grad_fill_(tensor, value)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.ones_","title":"<code>ones_(tensor)</code>","text":"<p>Modify tensor inplace using ones_.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.ones_(param)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def ones_(tensor: paddle.Tensor) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using ones_.\n\n    Args:\n        tensor (paddle.Tensor): Paddle Tensor.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.ones_(param)\n    \"\"\"\n    return _no_grad_fill_(tensor, 1)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.zeros_","title":"<code>zeros_(tensor)</code>","text":"<p>Modify tensor inplace using zeros_.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.zeros_(param)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def zeros_(tensor: paddle.Tensor) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using zeros_.\n\n    Args:\n        tensor (paddle.Tensor): Paddle Tensor.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.zeros_(param)\n    \"\"\"\n    return _no_grad_fill_(tensor, 0)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.xavier_uniform_","title":"<code>xavier_uniform_(tensor, gain=1.0, reverse=False)</code>","text":"<p>Modify tensor inplace using xavier_uniform_.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <code>gain</code> <code>float</code> <p>Hyperparameter. Defaults to 1.0.</p> <code>1.0</code> <code>reverse</code> <code>bool</code> <p>Tensor data format order, False by default as [fout, fin, ...].. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.xavier_uniform_(param)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def xavier_uniform_(\n    tensor: paddle.Tensor, gain: float = 1.0, reverse: bool = False\n) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using xavier_uniform_.\n\n    Args:\n        tensor (paddle.Tensor): Paddle Tensor.\n        gain (float, optional): Hyperparameter. Defaults to 1.0.\n        reverse (bool, optional): Tensor data format order, False by default as\n            [fout, fin, ...].. Defaults to False.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.xavier_uniform_(param)\n    \"\"\"\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor, reverse=reverse)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    k = math.sqrt(3.0) * std\n    return _no_grad_uniform_(tensor, -k, k)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.xavier_normal_","title":"<code>xavier_normal_(tensor, gain=1.0, reverse=False)</code>","text":"<p>Modify tensor inplace using xavier_normal_.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <code>gain</code> <code>float</code> <p>Hyperparameter. Defaults to 1.0.</p> <code>1.0</code> <code>reverse</code> <code>bool</code> <p>Tensor data format order, False by default as [fout, fin, ...]. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.xavier_normal_(param)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def xavier_normal_(\n    tensor: paddle.Tensor, gain: float = 1.0, reverse: bool = False\n) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using xavier_normal_.\n\n    Args:\n        tensor (paddle.Tensor): Paddle Tensor.\n        gain (float, optional): Hyperparameter. Defaults to 1.0.\n        reverse (bool, optional): Tensor data format order, False by\n            default as [fout, fin, ...]. Defaults to False.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.xavier_normal_(param)\n    \"\"\"\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor, reverse=reverse)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    return _no_grad_normal_(tensor, 0, std)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.kaiming_uniform_","title":"<code>kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu', reverse=False)</code>","text":"<p>Modify tensor inplace using kaiming_uniform method.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <code>a</code> <code>float</code> <p>The negative slope of the rectifier used after this layer. Defaults to 0.</p> <code>0</code> <code>mode</code> <code>Literal[&amp;quot;fan_in&amp;quot;, &amp;quot;fan_out&amp;quot;]</code> <p>[\"fan_in\", \"fan_out\"]. Defaults to \"fan_in\".</p> <code>'fan_in'</code> <code>nonlinearity</code> <code>str</code> <p>Nonlinearity method name. Defaults to \"leaky_relu\".</p> <code>'leaky_relu'</code> <code>reverse</code> <code>bool</code> <p>Tensor data format order, False by default as [fout, fin, ...].. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.kaiming_uniform_(param)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def kaiming_uniform_(\n    tensor: paddle.Tensor,\n    a: float = 0,\n    mode: Literal[\"fan_in\", \"fan_out\"] = \"fan_in\",\n    nonlinearity: str = \"leaky_relu\",\n    reverse: bool = False,\n) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using kaiming_uniform method.\n\n    Args:\n        tensor (paddle.Tensor):  Paddle Tensor.\n        a (float, optional): The negative slope of the rectifier used after this layer.\n            Defaults to 0.\n        mode (Literal[&amp;quot;fan_in&amp;quot;, &amp;quot;fan_out&amp;quot;], optional):\n            [\"fan_in\", \"fan_out\"]. Defaults to \"fan_in\".\n        nonlinearity (str, optional): Nonlinearity method name. Defaults to \"leaky_relu\".\n        reverse (bool, optional): Tensor data format order, False by default as\n            [fout, fin, ...].. Defaults to False.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.kaiming_uniform_(param)\n    \"\"\"\n    fan = _calculate_correct_fan(tensor, mode, reverse)\n    gain = _calculate_gain(nonlinearity, a)\n    std = gain / math.sqrt(fan)\n    k = math.sqrt(3.0) * std\n    return _no_grad_uniform_(tensor, -k, k)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.kaiming_normal_","title":"<code>kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu', reverse=False)</code>","text":"<p>Modify tensor inplace using kaiming_normal_.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Paddle Tensor.</p> required <code>a</code> <code>float</code> <p>The negative slope of the rectifier used after this layer. Defaults to 0.</p> <code>0</code> <code>mode</code> <code>Literal[&amp;quot;fan_in&amp;quot;, &amp;quot;fan_out&amp;quot;]</code> <p>Either 'fan_in' (default) or 'fan_out'. Defaults to \"fan_in\".</p> <code>'fan_in'</code> <code>nonlinearity</code> <code>str</code> <p>Nonlinearity method name. Defaults to \"leaky_relu\".</p> <code>'leaky_relu'</code> <code>reverse</code> <code>bool</code> <p>Tensor data format order. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>paddle.Tensor: Initialized tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n&gt;&gt;&gt; param = ppsci.utils.initializer.kaiming_normal_(param)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def kaiming_normal_(\n    tensor: paddle.Tensor,\n    a: float = 0,\n    mode: Literal[\"fan_in\", \"fan_out\"] = \"fan_in\",\n    nonlinearity: str = \"leaky_relu\",\n    reverse: bool = False,\n) -&gt; paddle.Tensor:\n    \"\"\"Modify tensor inplace using kaiming_normal_.\n\n    Args:\n        tensor (paddle.Tensor): Paddle Tensor.\n        a (float, optional): The negative slope of the rectifier used after this layer.\n            Defaults to 0.\n        mode (Literal[&amp;quot;fan_in&amp;quot;, &amp;quot;fan_out&amp;quot;], optional): Either\n            'fan_in' (default) or 'fan_out'. Defaults to \"fan_in\".\n        nonlinearity (str, optional): Nonlinearity method name. Defaults to \"leaky_relu\".\n        reverse (bool, optional): Tensor data format order. Defaults to False.\n\n    Returns:\n        paddle.Tensor: Initialized tensor.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; param = paddle.empty((128, 256), \"float32\")\n        &gt;&gt;&gt; param = ppsci.utils.initializer.kaiming_normal_(param)\n    \"\"\"\n    fan = _calculate_correct_fan(tensor, mode, reverse)\n    gain = _calculate_gain(nonlinearity, a)\n    std = gain / math.sqrt(fan)\n    return _no_grad_normal_(tensor, 0, std)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.linear_init_","title":"<code>linear_init_(module)</code>","text":"<p>Initialize module's weight and bias as it is a linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Layer</code> <p>Linear Layer to be initialized.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; layer = paddle.nn.Linear(128, 256)\n&gt;&gt;&gt; ppsci.utils.initializer.linear_init_(layer)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def linear_init_(module: nn.Layer) -&gt; None:\n    \"\"\"Initialize module's weight and bias as it is a linear layer.\n\n    Args:\n        module (nn.Layer): Linear Layer to be initialized.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; layer = paddle.nn.Linear(128, 256)\n        &gt;&gt;&gt; ppsci.utils.initializer.linear_init_(layer)\n    \"\"\"\n    kaiming_uniform_(module.weight, a=math.sqrt(5))\n    if module.bias is not None:\n        fan_in, _ = _calculate_fan_in_and_fan_out(module.weight, reverse=True)\n        bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n        uniform_(module.bias, -bound, bound)\n</code></pre>"},{"location":"zh/api/utils/initializer/#ppsci.utils.initializer.conv_init_","title":"<code>conv_init_(module)</code>","text":"<p>Initialize module's weight and bias as it is a conv layer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Layer</code> <p>Convolution Layer to be initialized.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; layer = paddle.nn.Conv2D(4, 16, 2)\n&gt;&gt;&gt; ppsci.utils.initializer.conv_init_(layer)\n</code></pre> Source code in <code>ppsci/utils/initializer.py</code> <pre><code>def conv_init_(module: nn.Layer) -&gt; None:\n    \"\"\"Initialize module's weight and bias as it is a conv layer.\n\n    Args:\n        module (nn.Layer): Convolution Layer to be initialized.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; layer = paddle.nn.Conv2D(4, 16, 2)\n        &gt;&gt;&gt; ppsci.utils.initializer.conv_init_(layer)\n    \"\"\"\n    kaiming_uniform_(module.weight, a=math.sqrt(5))\n    if module.bias is not None:\n        fan_in, _ = _calculate_fan_in_and_fan_out(module.weight, reverse=False)\n        if fan_in != 0:\n            bound = 1 / math.sqrt(fan_in)\n            uniform_(module.bias, -bound, bound)\n</code></pre>"},{"location":"zh/api/utils/logger/","title":"ppsci.utils.logger","text":""},{"location":"zh/api/utils/logger/#utilslogger","title":"Utils.logger(\u65e5\u5fd7\u8bb0\u5f55) \u6a21\u5757","text":""},{"location":"zh/api/utils/logger/#ppsci.utils.logger","title":"<code>ppsci.utils.logger</code>","text":""},{"location":"zh/api/utils/logger/#ppsci.utils.logger.init_logger","title":"<code>init_logger(name='ppsci', log_file=None, log_level=logging.INFO)</code>","text":"<p>Initialize and get a logger by name.</p> <p>If the logger has not been initialized, this method will initialize the logger by adding one or two handlers, otherwise the initialized logger will be directly returned. During initialization, a StreamHandler will always be added. If <code>log_file</code> is specified a FileHandler will also be added.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name. Defaults to \"ppsci\".</p> <code>'ppsci'</code> <code>log_file</code> <code>Optional[str]</code> <p>The log filename. If specified, a FileHandler will be added to the logger. Defaults to None.</p> <code>None</code> <code>log_level</code> <code>int</code> <p>The logger level. Note that only the process of rank 0 is affected, and other processes will set the level to \"Error\" thus be silent most of the time. Defaults to logging.INFO.</p> <code>INFO</code> Source code in <code>ppsci/utils/logger.py</code> <pre><code>def init_logger(\n    name: str = \"ppsci\",\n    log_file: Optional[str] = None,\n    log_level: int = logging.INFO,\n) -&gt; None:\n    \"\"\"Initialize and get a logger by name.\n\n    If the logger has not been initialized, this method will initialize the logger by\n    adding one or two handlers, otherwise the initialized logger will be directly\n    returned. During initialization, a StreamHandler will always be added. If `log_file`\n    is specified a FileHandler will also be added.\n\n    Args:\n        name (str, optional): Logger name. Defaults to \"ppsci\".\n        log_file (Optional[str]): The log filename. If specified, a FileHandler\n            will be added to the logger. Defaults to None.\n        log_level (int, optional): The logger level. Note that only the process of\n            rank 0 is affected, and other processes will set the level to\n            \"Error\" thus be silent most of the time. Defaults to logging.INFO.\n    \"\"\"\n    # Add custom log level MESSAGE(25), between WARNING(30) and INFO(20)\n    logging.addLevelName(_MESSAGE_LEVEL, \"MESSAGE\")\n\n    if isinstance(log_level, str):\n        log_level = getattr(logging, log_level.upper())\n\n    global _logger\n\n    # get a clean logger\n    _logger = logging.getLogger(name)\n    _logger.handlers.clear()\n\n    # add stream_handler, output to stdout such as terminal\n    stream_formatter = colorlog.ColoredFormatter(\n        \"%(log_color)s[%(asctime)s] %(name)s %(levelname)s: %(message)s\",\n        datefmt=\"%Y/%m/%d %H:%M:%S\",\n        log_colors=_COLORLOG_CONFIG,\n    )\n    stream_handler = logging.StreamHandler(stream=sys.stdout)\n    stream_handler.setFormatter(stream_formatter)\n    stream_handler._name = \"stream_handler\"\n    _logger.addHandler(stream_handler)\n\n    # add file_handler, output to log_file(if specified), only for rank 0 device\n    if log_file is not None and dist.get_rank() == 0:\n        log_file_folder = os.path.dirname(log_file)\n        if len(log_file_folder):\n            os.makedirs(log_file_folder, exist_ok=True)\n        file_formatter = logging.Formatter(\n            \"[%(asctime)s] %(name)s %(levelname)s: %(message)s\",\n            datefmt=\"%Y/%m/%d %H:%M:%S\",\n        )\n        file_handler = logging.FileHandler(log_file, \"a\")  # append mode\n        file_handler.setFormatter(file_formatter)\n        file_handler._name = \"file_handler\"\n        _logger.addHandler(file_handler)\n\n    if dist.get_rank() == 0:\n        _logger.setLevel(log_level)\n    else:\n        _logger.setLevel(logging.ERROR)\n\n    _logger.propagate = False\n</code></pre>"},{"location":"zh/api/utils/logger/#ppsci.utils.logger.set_log_level","title":"<code>set_log_level(log_level)</code>","text":"<p>Set logger level, only message of level &gt;= <code>log_level</code> will be printed.</p> <p>Built-in log level are below:</p> <p>CRITICAL = 50, FATAL = 50, ERROR = 40, WARNING = 30, WARN = 30, INFO = 20, DEBUG = 10, NOTSET = 0.</p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <code>int</code> <p>Log level.</p> required Source code in <code>ppsci/utils/logger.py</code> <pre><code>def set_log_level(log_level: int):\n    \"\"\"Set logger level, only message of level &gt;= `log_level` will be printed.\n\n    Built-in log level are below:\n\n    CRITICAL = 50,\n    FATAL = 50,\n    ERROR = 40,\n    WARNING = 30,\n    WARN = 30,\n    INFO = 20,\n    DEBUG = 10,\n    NOTSET = 0.\n\n    Args:\n        log_level (int): Log level.\n    \"\"\"\n    if dist.get_rank() == 0:\n        _logger.setLevel(log_level)\n    else:\n        _logger.setLevel(logging.ERROR)\n</code></pre>"},{"location":"zh/api/utils/logger/#ppsci.utils.logger.info","title":"<code>info(msg, *args)</code>","text":"Source code in <code>ppsci/utils/logger.py</code> <pre><code>@ensure_logger\n@misc.run_at_rank0\ndef info(msg, *args):\n    _logger.info(msg, *args)\n</code></pre>"},{"location":"zh/api/utils/logger/#ppsci.utils.logger.message","title":"<code>message(msg, *args)</code>","text":"Source code in <code>ppsci/utils/logger.py</code> <pre><code>@ensure_logger\n@misc.run_at_rank0\ndef message(msg, *args):\n    _logger.log(_MESSAGE_LEVEL, msg, *args)\n</code></pre>"},{"location":"zh/api/utils/logger/#ppsci.utils.logger.debug","title":"<code>debug(msg, *args)</code>","text":"Source code in <code>ppsci/utils/logger.py</code> <pre><code>@ensure_logger\n@misc.run_at_rank0\ndef debug(msg, *args):\n    _logger.debug(msg, *args)\n</code></pre>"},{"location":"zh/api/utils/logger/#ppsci.utils.logger.warning","title":"<code>warning(msg, *args)</code>","text":"Source code in <code>ppsci/utils/logger.py</code> <pre><code>@ensure_logger\n@misc.run_at_rank0\ndef warning(msg, *args):\n    _logger.warning(msg, *args)\n</code></pre>"},{"location":"zh/api/utils/logger/#ppsci.utils.logger.error","title":"<code>error(msg, *args)</code>","text":"Source code in <code>ppsci/utils/logger.py</code> <pre><code>@ensure_logger\n@misc.run_at_rank0\ndef error(msg, *args):\n    _logger.error(msg, *args)\n</code></pre>"},{"location":"zh/api/utils/logger/#ppsci.utils.logger.scalar","title":"<code>scalar(metric_dict, step, vdl_writer=None, wandb_writer=None, tbd_writer=None)</code>","text":"<p>This function will add scalar data to VisualDL or WandB for plotting curve(s).</p> <p>Parameters:</p> Name Type Description Default <code>metric_dict</code> <code>Dict[str, float]</code> <p>Metrics dict with metric name and value.</p> required <code>step</code> <code>int</code> <p>The step of the metric.</p> required <code>vdl_writer</code> <code>Optional[LogWriter]</code> <p>VisualDL writer to record metrics. Defaults to None.</p> <code>None</code> <code>wandb_writer</code> <code>Optional[run]</code> <p>Run object of WandB to record metrics. Defaults to None.</p> <code>None</code> <code>tbd_writer</code> <code>Optional[SummaryWriter]</code> <p>Run object of WandB to record metrics. Defaults to None.</p> <code>None</code> Source code in <code>ppsci/utils/logger.py</code> <pre><code>def scalar(\n    metric_dict: Dict[str, float],\n    step: int,\n    vdl_writer: Optional[\"visualdl.LogWriter\"] = None,\n    wandb_writer: Optional[\"wandb.run\"] = None,\n    tbd_writer: Optional[\"tbd.SummaryWriter\"] = None,\n):\n    \"\"\"This function will add scalar data to VisualDL or WandB for plotting curve(s).\n\n    Args:\n        metric_dict (Dict[str, float]): Metrics dict with metric name and value.\n        step (int): The step of the metric.\n        vdl_writer (Optional[visualdl.LogWriter]): VisualDL writer to record metrics. Defaults to None.\n        wandb_writer (Optional[wandb.run]): Run object of WandB to record metrics. Defaults to None.\n        tbd_writer (Optional[tbd.SummaryWriter]): Run object of WandB to record metrics. Defaults to None.\n    \"\"\"\n    if vdl_writer is not None:\n        with misc.RankZeroOnly() as is_master:\n            if is_master:\n                for name, value in metric_dict.items():\n                    vdl_writer.add_scalar(name, value, step)\n\n    if wandb_writer is not None:\n        with misc.RankZeroOnly() as is_master:\n            if is_master:\n                wandb_writer.log({\"step\": step, **metric_dict})\n\n    if tbd_writer is not None:\n        with misc.RankZeroOnly() as is_master:\n            if is_master:\n                for name, value in metric_dict.items():\n                    tbd_writer.add_scalar(name, value, global_step=step)\n</code></pre>"},{"location":"zh/api/utils/misc/","title":"ppsci.utils.misc","text":""},{"location":"zh/api/utils/misc/#utilsmisc","title":"Utils.misc(\u901a\u7528\u51fd\u6570) \u6a21\u5757","text":""},{"location":"zh/api/utils/misc/#ppsci.utils.misc","title":"<code>ppsci.utils.misc</code>","text":""},{"location":"zh/api/utils/misc/#ppsci.utils.misc.AverageMeter","title":"<code>AverageMeter</code>","text":"<p>Computes and stores the average and current value Code was based on https://github.com/pytorch/examples/blob/master/imagenet/main.py</p> Source code in <code>ppsci/utils/misc.py</code> <pre><code>class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    Code was based on https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    \"\"\"\n\n    def __init__(self, name=\"\", fmt=\"f\", postfix=\"\", need_avg=True):\n        self.name = name\n        self.fmt = fmt\n        self.postfix = postfix\n        self.need_avg = need_avg\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset.\"\"\"\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.history = []\n\n    def update(self, val, n=1):\n        \"\"\"Update.\"\"\"\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        self.history.append(val)\n\n    @property\n    def avg_info(self):\n        if isinstance(self.avg, paddle.Tensor):\n            self.avg = float(self.avg)\n        return f\"{self.name}: {self.avg:.5f}\"\n\n    @property\n    def total(self):\n        return f\"{self.name}_sum: {self.sum:{self.fmt}}{self.postfix}\"\n\n    @property\n    def total_minute(self):\n        return f\"{self.name} {self.sum / 60:{self.fmt}}{self.postfix} min\"\n\n    @property\n    def mean(self):\n        return (\n            f\"{self.name}: {self.avg:{self.fmt}}{self.postfix}\" if self.need_avg else \"\"\n        )\n\n    @property\n    def value(self):\n        return f\"{self.name}: {self.val:{self.fmt}}{self.postfix}\"\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.AverageMeter.reset","title":"<code>reset()</code>","text":"<p>Reset.</p> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def reset(self):\n    \"\"\"Reset.\"\"\"\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n    self.history = []\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.AverageMeter.update","title":"<code>update(val, n=1)</code>","text":"<p>Update.</p> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def update(self, val, n=1):\n    \"\"\"Update.\"\"\"\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n    self.history.append(val)\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.PrettyOrderedDict","title":"<code>PrettyOrderedDict</code>","text":"<p>               Bases: <code>OrderedDict</code></p> <p>The ordered dict which can be prettily printed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dic = ppsci.utils.misc.PrettyOrderedDict()\n&gt;&gt;&gt; dic.update({'a':1, 'b':2, 'c':3})\n&gt;&gt;&gt; print(dic)\n('a', 1)('b', 2)('c', 3)\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>class PrettyOrderedDict(collections.OrderedDict):\n    \"\"\"\n    The ordered dict which can be prettily printed.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dic = ppsci.utils.misc.PrettyOrderedDict()\n        &gt;&gt;&gt; dic.update({'a':1, 'b':2, 'c':3})\n        &gt;&gt;&gt; print(dic)\n        ('a', 1)('b', 2)('c', 3)\n    \"\"\"\n\n    def __str__(self):\n        return \"\".join([str((k, v)) for k, v in self.items()])\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.Prettydefaultdict","title":"<code>Prettydefaultdict</code>","text":"<p>               Bases: <code>defaultdict</code></p> <p>The default dict which can be prettily printed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dic = ppsci.utils.misc.Prettydefaultdict()\n&gt;&gt;&gt; dic.update({'a':1, 'b':2, 'c':3})\n&gt;&gt;&gt; print(dic)\n('a', 1)('b', 2)('c', 3)\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>class Prettydefaultdict(collections.defaultdict):\n    \"\"\"\n    The default dict which can be prettily printed.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dic = ppsci.utils.misc.Prettydefaultdict()\n        &gt;&gt;&gt; dic.update({'a':1, 'b':2, 'c':3})\n        &gt;&gt;&gt; print(dic)\n        ('a', 1)('b', 2)('c', 3)\n    \"\"\"\n\n    def __str__(self):\n        return \"\".join([str((k, v)) for k, v in self.items()])\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.RankZeroOnly","title":"<code>RankZeroOnly</code>","text":"<p>A context manager that ensures the code inside it is only executed by the process with rank zero. All rank will be synchronized by <code>dist.barrier</code> in distributed environment.</p> <p>NOTE: Always used for time consuming code blocks, such as initialization of log writer, saving result to disk, etc.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>Optional[int]</code> <p>The rank of the current process. If not provided, it will be obtained from <code>dist.get_rank()</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle.distributed as dist\n&gt;&gt;&gt; with RankZeroOnly(dist.get_rank()) as is_master:\n...     if is_master:\n...         # code here which should only be executed in the master process\n...         pass\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>class RankZeroOnly:\n    \"\"\"\n    A context manager that ensures the code inside it is only executed by the process\n    with rank zero. All rank will be synchronized by `dist.barrier` in\n    distributed environment.\n\n    NOTE: Always used for time consuming code blocks, such as initialization of log\n    writer, saving result to disk, etc.\n\n    Args:\n        rank (Optional[int]): The rank of the current process. If not provided,\n            it will be obtained from `dist.get_rank()`.\n\n    Examples:\n        &gt;&gt;&gt; import paddle.distributed as dist\n        &gt;&gt;&gt; with RankZeroOnly(dist.get_rank()) as is_master:\n        ...     if is_master:\n        ...         # code here which should only be executed in the master process\n        ...         pass\n    \"\"\"\n\n    def __init__(self, rank: Optional[int] = None):\n        \"\"\"\n        Enter the context and check if the current process is the master.\n\n        Args:\n            rank (Optional[int]): The rank of the current process. If not provided,\n                it will be obtained from `dist.get_rank()`.\n        \"\"\"\n        super().__init__()\n        self.rank = rank if (rank is not None) else dist.get_rank()\n        self.is_master = self.rank == 0\n\n    def __enter__(self) -&gt; bool:\n        \"\"\"\n        Enter the context and check if the current process is the master.\n\n        Returns:\n            bool: True if the current process is the master (rank zero), False otherwise.\n        \"\"\"\n        return self.is_master\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if dist.get_world_size() &gt; 1:\n            dist.barrier()\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.RankZeroOnly.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter the context and check if the current process is the master.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the current process is the master (rank zero), False otherwise.</p> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def __enter__(self) -&gt; bool:\n    \"\"\"\n    Enter the context and check if the current process is the master.\n\n    Returns:\n        bool: True if the current process is the master (rank zero), False otherwise.\n    \"\"\"\n    return self.is_master\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.RankZeroOnly.__init__","title":"<code>__init__(rank=None)</code>","text":"<p>Enter the context and check if the current process is the master.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>Optional[int]</code> <p>The rank of the current process. If not provided, it will be obtained from <code>dist.get_rank()</code>.</p> <code>None</code> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def __init__(self, rank: Optional[int] = None):\n    \"\"\"\n    Enter the context and check if the current process is the master.\n\n    Args:\n        rank (Optional[int]): The rank of the current process. If not provided,\n            it will be obtained from `dist.get_rank()`.\n    \"\"\"\n    super().__init__()\n    self.rank = rank if (rank is not None) else dist.get_rank()\n    self.is_master = self.rank == 0\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.Timer","title":"<code>Timer</code>","text":"<p>               Bases: <code>ContextDecorator</code></p> <p>Count time cost for code block within context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of timer discriminate different code block. Defaults to \"Timer\".</p> <code>'Timer'</code> <code>auto_print</code> <code>bool</code> <p>Whether print time cost when exit context. Defaults to True.</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.utils import misc\n&gt;&gt;&gt; with misc.Timer(\"test1\", auto_print=False) as timer:\n...     w = sum(range(0, 10))\n&gt;&gt;&gt; print(f\"time cost of 'sum(range(0, 10))' is {timer.interval:.2f}\")\ntime cost of 'sum(range(0, 10))' is 0.00\n</code></pre> <pre><code>&gt;&gt;&gt; @misc.Timer(\"test2\", auto_print=True)\n... def func():\n...     w = sum(range(0, 10))\n&gt;&gt;&gt; func()\n</code></pre> <pre><code>&gt;&gt;&gt; timer = misc.Timer(\"cost_of_func\", auto_print=False)\n&gt;&gt;&gt; timer.start()\n&gt;&gt;&gt; def func():\n...     w = sum(range(0, 10))\n&gt;&gt;&gt; func()\n&gt;&gt;&gt; timer.end()\n&gt;&gt;&gt; print(f\"time cost of 'cost_of_func' is {timer.interval:.2f}\")\ntime cost of 'cost_of_func' is 0.00\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>class Timer(ContextDecorator):\n    \"\"\"Count time cost for code block within context.\n\n    Args:\n        name (str, optional): Name of timer discriminate different code block.\n            Defaults to \"Timer\".\n        auto_print (bool, optional): Whether print time cost when exit context.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.utils import misc\n        &gt;&gt;&gt; with misc.Timer(\"test1\", auto_print=False) as timer:\n        ...     w = sum(range(0, 10))\n        &gt;&gt;&gt; print(f\"time cost of 'sum(range(0, 10))' is {timer.interval:.2f}\")  # doctest: +SKIP\n        time cost of 'sum(range(0, 10))' is 0.00\n\n        &gt;&gt;&gt; @misc.Timer(\"test2\", auto_print=True)\n        ... def func():\n        ...     w = sum(range(0, 10))\n        &gt;&gt;&gt; func()  # doctest: +SKIP\n\n        &gt;&gt;&gt; timer = misc.Timer(\"cost_of_func\", auto_print=False)\n        &gt;&gt;&gt; timer.start()\n        &gt;&gt;&gt; def func():\n        ...     w = sum(range(0, 10))\n        &gt;&gt;&gt; func()\n        &gt;&gt;&gt; timer.end()\n        &gt;&gt;&gt; print(f\"time cost of 'cost_of_func' is {timer.interval:.2f}\")  # doctest: +SKIP\n        time cost of 'cost_of_func' is 0.00\n    \"\"\"\n\n    interval: float  # Time cost for code within Timer context\n\n    def __init__(self, name: str = \"Timer\", auto_print: bool = True):\n        super().__init__()\n        self.name = name\n        self.auto_print = auto_print\n\n    def __enter__(self):\n        paddle.device.synchronize()\n        self.start_time = time.perf_counter()\n        return self\n\n    def __exit__(self, type, value, traceback):\n        paddle.device.synchronize()\n        self.end_time = time.perf_counter()\n        self.interval = self.end_time - self.start_time\n        if self.auto_print:\n            logger.message(f\"{self.name}.time_cost = {self.interval:.2f} s\")\n\n    def start(self, name: str = \"Timer\"):\n        \"\"\"Push a new timer context.\n\n        Args:\n            name (str, optional): Name of code block to be clocked. Defaults to \"Timer\".\n        \"\"\"\n        paddle.device.synchronize()\n        self.start_time = time.perf_counter()\n\n    def end(self):\n        \"\"\"End current timer context and print time cost.\"\"\"\n        paddle.device.synchronize()\n        self.end_time = time.perf_counter()\n        self.interval = self.end_time - self.start_time\n        if self.auto_print:\n            logger.message(f\"{self.name}.time_cost = {self.interval:.2f} s\")\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.Timer.end","title":"<code>end()</code>","text":"<p>End current timer context and print time cost.</p> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def end(self):\n    \"\"\"End current timer context and print time cost.\"\"\"\n    paddle.device.synchronize()\n    self.end_time = time.perf_counter()\n    self.interval = self.end_time - self.start_time\n    if self.auto_print:\n        logger.message(f\"{self.name}.time_cost = {self.interval:.2f} s\")\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.Timer.start","title":"<code>start(name='Timer')</code>","text":"<p>Push a new timer context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of code block to be clocked. Defaults to \"Timer\".</p> <code>'Timer'</code> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def start(self, name: str = \"Timer\"):\n    \"\"\"Push a new timer context.\n\n    Args:\n        name (str, optional): Name of code block to be clocked. Defaults to \"Timer\".\n    \"\"\"\n    paddle.device.synchronize()\n    self.start_time = time.perf_counter()\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.all_gather","title":"<code>all_gather(tensor, concat=True, axis=0)</code>","text":"<p>Gather tensor from all devices, concatenate them along given axis if specified.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Tensor to be gathered from all GPUs.</p> required <code>concat</code> <code>bool</code> <p>Whether to concatenate gathered Tensors. Defaults to True.</p> <code>True</code> <code>axis</code> <code>int</code> <p>Axis which concatenated along. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Union[Tensor, List[Tensor]]</code> <p>Union[paddle.Tensor, List[paddle.Tensor]]: Gathered Tensors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import paddle.distributed as dist\n&gt;&gt;&gt; dist.init_parallel_env()\n&gt;&gt;&gt; if dist.get_rank() == 0:\n...     data = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])\n... else:\n...     data = paddle.to_tensor([[7, 8, 9], [10, 11, 12]])\n&gt;&gt;&gt; result = ppsci.utils.misc.all_gather(data)\n&gt;&gt;&gt; print(result.numpy())\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def all_gather(\n    tensor: paddle.Tensor, concat: bool = True, axis: int = 0\n) -&gt; Union[paddle.Tensor, List[paddle.Tensor]]:\n    \"\"\"Gather tensor from all devices, concatenate them along given axis if specified.\n\n    Args:\n        tensor (paddle.Tensor): Tensor to be gathered from all GPUs.\n        concat (bool, optional): Whether to concatenate gathered Tensors. Defaults to True.\n        axis (int, optional): Axis which concatenated along. Defaults to 0.\n\n    Returns:\n        Union[paddle.Tensor, List[paddle.Tensor]]: Gathered Tensors.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import paddle.distributed as dist\n        &gt;&gt;&gt; dist.init_parallel_env()      # doctest: +SKIP\n        &gt;&gt;&gt; if dist.get_rank() == 0:      # doctest: +SKIP\n        ...     data = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])\n        ... else:\n        ...     data = paddle.to_tensor([[7, 8, 9], [10, 11, 12]])\n        &gt;&gt;&gt; result = ppsci.utils.misc.all_gather(data)    # doctest: +SKIP\n        &gt;&gt;&gt; print(result.numpy())     # doctest: +SKIP\n        [[ 1  2  3]\n         [ 4  5  6]\n         [ 7  8  9]\n         [10 11 12]]\n    \"\"\"\n    result: List[paddle.Tensor] = []\n\n    # NOTE: Put tensor to CUDAPlace from CUDAPinnedPlace to use communication.\n    if tensor.place.is_cuda_pinned_place():\n        tensor = tensor.cuda()\n\n    # TODO(HydrogenSulfate): As non-contiguous(strided) tensor is not supported in\n    # dist.all_gather, manually convert given Tensor to contiguous below. Strided tensor\n    # will be supported in future.\n    dist.all_gather(result, tensor.contiguous())\n\n    if concat:\n        return paddle.concat(result, axis)\n    return result\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.concat_dict_list","title":"<code>concat_dict_list(dict_list)</code>","text":"<p>Concatenate arrays in tuple of dicts at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>dict_list</code> <code>Sequence[Dict[str, ndarray]]</code> <p>Sequence of dicts.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: A dict with concatenated arrays for each key.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dic1 = {\"x\": np.array([[1., 2.], [3., 4.]]), \"y\": np.array([[5., 6.], [7., 8.]])}\n&gt;&gt;&gt; dic2 = {\"x\": np.array([[1., 2.], [3., 4.]]), \"y\": np.array([[5., 6.], [7., 8.]])}\n&gt;&gt;&gt; result = ppsci.utils.misc.concat_dict_list((dic1, dic2))\n&gt;&gt;&gt; print(result)\n{'x': array([[1., 2.],\n       [3., 4.],\n       [1., 2.],\n       [3., 4.]]), 'y': array([[5., 6.],\n       [7., 8.],\n       [5., 6.],\n       [7., 8.]])}\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def concat_dict_list(\n    dict_list: Sequence[Dict[str, np.ndarray]]\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Concatenate arrays in tuple of dicts at axis 0.\n\n    Args:\n        dict_list (Sequence[Dict[str, np.ndarray]]): Sequence of dicts.\n\n    Returns:\n        Dict[str, np.ndarray]: A dict with concatenated arrays for each key.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dic1 = {\"x\": np.array([[1., 2.], [3., 4.]]), \"y\": np.array([[5., 6.], [7., 8.]])}\n        &gt;&gt;&gt; dic2 = {\"x\": np.array([[1., 2.], [3., 4.]]), \"y\": np.array([[5., 6.], [7., 8.]])}\n        &gt;&gt;&gt; result = ppsci.utils.misc.concat_dict_list((dic1, dic2))\n        &gt;&gt;&gt; print(result)\n        {'x': array([[1., 2.],\n               [3., 4.],\n               [1., 2.],\n               [3., 4.]]), 'y': array([[5., 6.],\n               [7., 8.],\n               [5., 6.],\n               [7., 8.]])}\n    \"\"\"\n    ret = {}\n    for key in dict_list[0].keys():\n        ret[key] = np.concatenate([_dict[key] for _dict in dict_list], axis=0)\n    return ret\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.convert_to_array","title":"<code>convert_to_array(dict_, keys)</code>","text":"<p>Concatenate arrays in axis -1 in order of given keys.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>Dict[str, ndarray]</code> <p>Dict contains arrays.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Concatenate keys used in concatenation.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Concatenated array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dic = {\"x\": np.array([[1., 2.], [3., 4.]]),\n...        \"y\": np.array([[5., 6.], [7., 8.]]),\n...        \"z\": np.array([[9., 10.], [11., 12.]])}\n&gt;&gt;&gt; result = ppsci.utils.misc.convert_to_array(dic, (\"x\", \"z\"))\n&gt;&gt;&gt; print(result)\n[[ 1.  2.  9. 10.]\n [ 3.  4. 11. 12.]]\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def convert_to_array(dict_: Dict[str, np.ndarray], keys: Tuple[str, ...]) -&gt; np.ndarray:\n    \"\"\"Concatenate arrays in axis -1 in order of given keys.\n\n    Args:\n        dict_ (Dict[str, np.ndarray]): Dict contains arrays.\n        keys (Tuple[str, ...]): Concatenate keys used in concatenation.\n\n    Returns:\n        np.ndarray: Concatenated array.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dic = {\"x\": np.array([[1., 2.], [3., 4.]]),\n        ...        \"y\": np.array([[5., 6.], [7., 8.]]),\n        ...        \"z\": np.array([[9., 10.], [11., 12.]])}\n        &gt;&gt;&gt; result = ppsci.utils.misc.convert_to_array(dic, (\"x\", \"z\"))\n        &gt;&gt;&gt; print(result)\n        [[ 1.  2.  9. 10.]\n         [ 3.  4. 11. 12.]]\n    \"\"\"\n    return np.concatenate([dict_[key] for key in keys], axis=-1)\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.convert_to_dict","title":"<code>convert_to_dict(array, keys)</code>","text":"<p>Split given array into single channel array at axis -1 in order of given keys.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Array to be split.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Keys used in split.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: Split dict.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; arr = np.array([[1., 2., 3.], [4., 5., 6.]])\n&gt;&gt;&gt; result = ppsci.utils.misc.convert_to_dict(arr, (\"x\", \"y\", \"z\"))\n&gt;&gt;&gt; print(arr.shape)\n(2, 3)\n&gt;&gt;&gt; for k, v in result.items():\n...    print(k, v.shape)\nx (2, 1)\ny (2, 1)\nz (2, 1)\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def convert_to_dict(array: np.ndarray, keys: Tuple[str, ...]) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Split given array into single channel array at axis -1 in order of given keys.\n\n    Args:\n        array (np.ndarray): Array to be split.\n        keys (Tuple[str, ...]): Keys used in split.\n\n    Returns:\n        Dict[str, np.ndarray]: Split dict.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; arr = np.array([[1., 2., 3.], [4., 5., 6.]])\n        &gt;&gt;&gt; result = ppsci.utils.misc.convert_to_dict(arr, (\"x\", \"y\", \"z\"))\n        &gt;&gt;&gt; print(arr.shape)\n        (2, 3)\n        &gt;&gt;&gt; for k, v in result.items():\n        ...    print(k, v.shape)\n        x (2, 1)\n        y (2, 1)\n        z (2, 1)\n    \"\"\"\n    if array.shape[-1] != len(keys):\n        raise ValueError(\n            f\"dim of array({array.shape[-1]}) must equal to \" f\"len(keys)({len(keys)})\"\n        )\n\n    split_array = np.split(array, len(keys), axis=-1)\n    return {key: split_array[i] for i, key in enumerate(keys)}\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.stack_dict_list","title":"<code>stack_dict_list(dict_list)</code>","text":"<p>Stack arrays in tuple of dicts at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>dict_list</code> <code>Sequence[Dict[str, ndarray]]</code> <p>Sequence of dicts.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: A dict with stacked arrays for each key.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; dic1 = {\"x\": np.array([[1., 2.], [3., 4.]]), \"y\": np.array([[5., 6.], [7., 8.]])}\n&gt;&gt;&gt; dic2 = {\"x\": np.array([[1., 2.], [3., 4.]]), \"y\": np.array([[5., 6.], [7., 8.]])}\n&gt;&gt;&gt; result = ppsci.utils.misc.stack_dict_list((dic1, dic2))\n&gt;&gt;&gt; for k, v in result.items():\n...     print(k, v.shape)\nx (2, 2, 2)\ny (2, 2, 2)\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def stack_dict_list(\n    dict_list: Sequence[Dict[str, np.ndarray]]\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Stack arrays in tuple of dicts at axis 0.\n\n    Args:\n        dict_list (Sequence[Dict[str, np.ndarray]]): Sequence of dicts.\n\n    Returns:\n        Dict[str, np.ndarray]: A dict with stacked arrays for each key.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; dic1 = {\"x\": np.array([[1., 2.], [3., 4.]]), \"y\": np.array([[5., 6.], [7., 8.]])}\n        &gt;&gt;&gt; dic2 = {\"x\": np.array([[1., 2.], [3., 4.]]), \"y\": np.array([[5., 6.], [7., 8.]])}\n        &gt;&gt;&gt; result = ppsci.utils.misc.stack_dict_list((dic1, dic2))\n        &gt;&gt;&gt; for k, v in result.items():\n        ...     print(k, v.shape)\n        x (2, 2, 2)\n        y (2, 2, 2)\n    \"\"\"\n    ret = {}\n    for key in dict_list[0].keys():\n        ret[key] = np.stack([_dict[key] for _dict in dict_list], axis=0)\n    return ret\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.cartesian_product","title":"<code>cartesian_product(*arrays)</code>","text":"<p>Cartesian product for input sequence of array(s).</p> <p>Reference: https://stackoverflow.com/questions/11144513/cartesian-product-of-x-and-y-array-points-into-single-array-of-2d-points</p> <p>Assume shapes of input arrays are: \\((N_1,), (N_2,), (N_3,), ..., (N_M,)\\), then the cartesian product result will be shape of \\((N_1xN_2xN_3x...xN_M, M)\\).</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>ndarray</code> <p>Input arrays.</p> <code>()</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Cartesian product result of shape \\((N_1xN_2xN_3x...xN_M, M)\\).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = np.array([1, 2])\n&gt;&gt;&gt; x = np.array([10, 20])\n&gt;&gt;&gt; y = np.array([100, 200])\n&gt;&gt;&gt; txy = cartesian_product(t, x, y)\n&gt;&gt;&gt; print(txy)\n[[  1  10 100]\n [  1  10 200]\n [  1  20 100]\n [  1  20 200]\n [  2  10 100]\n [  2  10 200]\n [  2  20 100]\n [  2  20 200]]\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def cartesian_product(*arrays: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Cartesian product for input sequence of array(s).\n\n    Reference: https://stackoverflow.com/questions/11144513/cartesian-product-of-x-and-y-array-points-into-single-array-of-2d-points\n\n    Assume shapes of input arrays are: $(N_1,), (N_2,), (N_3,), ..., (N_M,)$,\n    then the cartesian product result will be shape of $(N_1xN_2xN_3x...xN_M, M)$.\n\n    Args:\n        arrays (np.ndarray): Input arrays.\n\n    Returns:\n        np.ndarray: Cartesian product result of shape $(N_1xN_2xN_3x...xN_M, M)$.\n\n    Examples:\n        &gt;&gt;&gt; t = np.array([1, 2])\n        &gt;&gt;&gt; x = np.array([10, 20])\n        &gt;&gt;&gt; y = np.array([100, 200])\n        &gt;&gt;&gt; txy = cartesian_product(t, x, y)\n        &gt;&gt;&gt; print(txy)\n        [[  1  10 100]\n         [  1  10 200]\n         [  1  20 100]\n         [  1  20 200]\n         [  2  10 100]\n         [  2  10 200]\n         [  2  20 100]\n         [  2  20 200]]\n    \"\"\"\n    la = len(arrays)\n    dtype = np.result_type(*arrays)\n    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n    for i, a in enumerate(np.ix_(*arrays)):\n        arr[..., i] = a\n    return arr.reshape(-1, la)\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.combine_array_with_time","title":"<code>combine_array_with_time(x, t)</code>","text":"<p>Combine given data x with time sequence t. Given x with shape (N, D) and t with shape (T, ), this function will repeat t_i for N times and will concat it with data x for each t_i in t, finally return the stacked result, which is of shape (N\u00d7T, D+1).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Points data with shape (N, D).</p> required <code>t</code> <code>Tuple[int, ...]</code> <p>Time sequence with shape (T, ).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Combined data with shape of (N\u00d7T, D+1).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; data_point = np.arange(10).reshape((2, 5))\n&gt;&gt;&gt; time = (1, 2, 3)\n&gt;&gt;&gt; result = ppsci.utils.misc.combine_array_with_time(data_point, time)\n&gt;&gt;&gt; print(result)\n[[1. 0. 1. 2. 3. 4.]\n [1. 5. 6. 7. 8. 9.]\n [2. 0. 1. 2. 3. 4.]\n [2. 5. 6. 7. 8. 9.]\n [3. 0. 1. 2. 3. 4.]\n [3. 5. 6. 7. 8. 9.]]\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def combine_array_with_time(x: np.ndarray, t: Tuple[int, ...]) -&gt; np.ndarray:\n    \"\"\"Combine given data x with time sequence t.\n    Given x with shape (N, D) and t with shape (T, ),\n    this function will repeat t_i for N times and will concat it with data x for each t_i in t,\n    finally return the stacked result, which is of shape (N\u00d7T, D+1).\n\n    Args:\n        x (np.ndarray): Points data with shape (N, D).\n        t (Tuple[int, ...]): Time sequence with shape (T, ).\n\n    Returns:\n        np.ndarray: Combined data with shape of (N\u00d7T, D+1).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; data_point = np.arange(10).reshape((2, 5))\n        &gt;&gt;&gt; time = (1, 2, 3)\n        &gt;&gt;&gt; result = ppsci.utils.misc.combine_array_with_time(data_point, time)\n        &gt;&gt;&gt; print(result)\n        [[1. 0. 1. 2. 3. 4.]\n         [1. 5. 6. 7. 8. 9.]\n         [2. 0. 1. 2. 3. 4.]\n         [2. 5. 6. 7. 8. 9.]\n         [3. 0. 1. 2. 3. 4.]\n         [3. 5. 6. 7. 8. 9.]]\n    \"\"\"\n    nx = len(x)\n    tx = []\n    for ti in t:\n        tx.append(\n            np.hstack(\n                (np.full([nx, 1], float(ti), dtype=paddle.get_default_dtype()), x)\n            )\n        )\n    tx = np.vstack(tx)\n    return tx\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.set_random_seed","title":"<code>set_random_seed(seed)</code>","text":"<p>Set numpy, random, paddle random_seed to given seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed.</p> required Source code in <code>ppsci/utils/misc.py</code> <pre><code>def set_random_seed(seed: int):\n    \"\"\"Set numpy, random, paddle random_seed to given seed.\n\n    Args:\n        seed (int): Random seed.\n    \"\"\"\n    paddle.seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.run_on_eval_mode","title":"<code>run_on_eval_mode(func)</code>","text":"<p>A decorator automatically running given class method in eval mode and keep training state unchanged after function finished.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Class method which is expected running in eval mode.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Decorated class method.</p> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def run_on_eval_mode(func: Callable) -&gt; Callable:\n    \"\"\"A decorator automatically running given class method in eval mode and keep\n    training state unchanged after function finished.\n\n    Args:\n        func (Callable): Class method which is expected running in eval mode.\n\n    Returns:\n        Callable: Decorated class method.\n    \"\"\"\n\n    @functools.wraps(func)\n    def function_with_eval_state(self, *args, **kwargs):\n        # log original state\n        train_state = self.model.training\n\n        # switch to eval mode\n        if train_state:\n            self.model.eval()\n\n        # run func in eval mode\n        result = func(self, *args, **kwargs)\n\n        # restore state\n        if train_state:\n            self.model.train()\n\n        return result\n\n    return function_with_eval_state\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.run_at_rank0","title":"<code>run_at_rank0(func)</code>","text":"<p>A decorator that allow given function run only at rank 0 to avoid multiple logs or other events. Usually effected in distributed environment.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Given function.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Wrapped function which will only run at at rank 0, skipped at other rank.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.utils import misc\n&gt;&gt;&gt; @misc.run_at_rank0\n... def func():\n...     print(f\"now_rank is {paddle.distributed.get_rank()}\")\n&gt;&gt;&gt; func()\nnow_rank is 0\n</code></pre> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def run_at_rank0(func: Callable) -&gt; Callable:\n    \"\"\"A decorator that allow given function run only at rank 0 to avoid\n    multiple logs or other events. Usually effected in distributed environment.\n\n    Args:\n        func (Callable): Given function.\n\n    Returns:\n        Callable: Wrapped function which will only run at at rank 0,\n            skipped at other rank.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.utils import misc\n        &gt;&gt;&gt; @misc.run_at_rank0\n        ... def func():\n        ...     print(f\"now_rank is {paddle.distributed.get_rank()}\")\n        &gt;&gt;&gt; func()\n        now_rank is 0\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapped_func(*args, **kwargs):\n        if dist.get_rank() == 0:\n            return func(*args, **kwargs)\n\n    return wrapped_func\n</code></pre>"},{"location":"zh/api/utils/misc/#ppsci.utils.misc.plot_curve","title":"<code>plot_curve(data, xlabel='X', ylabel='Y', output_dir='./output/', smooth_step=1, use_semilogy=False)</code>","text":"<p>Plotting curve.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, List]</code> <p>Dict of all data, keys are curves' name.</p> required <code>xlabel</code> <code>str</code> <p>Label of x-axis. Defaults to \"X\".</p> <code>'X'</code> <code>ylabel</code> <code>str</code> <p>Label of y-axis. Defaults to \"Y\".</p> <code>'Y'</code> <code>output_dir</code> <code>str</code> <p>Output directory of figure. Defaults to \"./output/\".</p> <code>'./output/'</code> <code>smooth_step</code> <code>int</code> <p>How many points are squeezed to one point to smooth the curve. Defaults to 1.</p> <code>1</code> <code>use_semilogy</code> <code>bool</code> <p>Whether to set non-uniform coordinates for the y-axis. Defaults to False.</p> <code>False</code> Source code in <code>ppsci/utils/misc.py</code> <pre><code>def plot_curve(\n    data: Dict[str, List],\n    xlabel: str = \"X\",\n    ylabel: str = \"Y\",\n    output_dir: str = \"./output/\",\n    smooth_step: int = 1,\n    use_semilogy: bool = False,\n) -&gt; None:\n    \"\"\"Plotting curve.\n\n    Args:\n        data (Dict[str, List]): Dict of all data, keys are curves' name.\n        xlabel (str, optional): Label of x-axis. Defaults to \"X\".\n        ylabel (str, optional): Label of y-axis. Defaults to \"Y\".\n        output_dir (str, optional): Output directory of figure. Defaults to \"./output/\".\n        smooth_step (int, optional): How many points are squeezed to one point to smooth the curve. Defaults to 1.\n        use_semilogy (bool, optional): Whether to set non-uniform coordinates for the y-axis. Defaults to False.\n    \"\"\"\n    data_arr = np.concatenate(\n        [np.asarray(arr).reshape(-1, 1) for arr in data.values()], axis=1\n    )\n\n    # smooth\n    if data_arr.shape[0] % smooth_step != 0:\n        data_arr = np.reshape(\n            data_arr[: -(data_arr.shape[0] % smooth_step), :],\n            (-1, smooth_step, data_arr.shape[1]),\n        )\n    else:\n        data_arr = np.reshape(data_arr, (-1, smooth_step, data_arr.shape[1]))\n    data_arr = np.mean(data_arr, axis=1)\n\n    # plot\n    plt.figure()\n    if use_semilogy:\n        plt.yscale(\"log\")\n        plt.xscale(\"log\")\n    plt.plot(np.arange(data_arr.shape[0]) * smooth_step, data_arr)\n    plt.legend(\n        list(data.keys()),\n        loc=\"upper left\",\n        bbox_to_anchor=(1, 1),\n    )\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid()\n    plt.yticks(size=10)\n    plt.xticks(size=10)\n    plt.tight_layout()\n\n    plt.savefig(os.path.join(output_dir, f\"{xlabel}-{ylabel}_curve.jpg\"), dpi=200)\n    plt.clf()\n    plt.close()\n</code></pre>"},{"location":"zh/api/utils/reader/","title":"ppsci.utils.reader","text":""},{"location":"zh/api/utils/reader/#utilsreader","title":"Utils.reader(\u8bfb\u53d6\u51fd\u6570) \u6a21\u5757","text":""},{"location":"zh/api/utils/reader/#ppsci.utils.reader","title":"<code>ppsci.utils.reader</code>","text":""},{"location":"zh/api/utils/reader/#ppsci.utils.reader.load_csv_file","title":"<code>load_csv_file(file_path, keys, alias_dict=None, delimiter=',', encoding='utf-8')</code>","text":"<p>Load *.csv file and fetch data as given keys.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>CSV file path.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Required fetching keys.</p> required <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Alias for keys, i.e. {inner_key: outer_key}. Defaults to None.</p> <code>None</code> <code>encoding</code> <code>str</code> <p>Encoding code when open file. Defaults to \"utf-8\".</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: Loaded data in dict.</p> Source code in <code>ppsci/utils/reader.py</code> <pre><code>def load_csv_file(\n    file_path: str,\n    keys: Tuple[str, ...],\n    alias_dict: Optional[Dict[str, str]] = None,\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Load *.csv file and fetch data as given keys.\n\n    Args:\n        file_path (str): CSV file path.\n        keys (Tuple[str, ...]): Required fetching keys.\n        alias_dict (Optional[Dict[str, str]]): Alias for keys,\n            i.e. {inner_key: outer_key}. Defaults to None.\n        encoding (str, optional): Encoding code when open file. Defaults to \"utf-8\".\n\n    Returns:\n        Dict[str, np.ndarray]: Loaded data in dict.\n    \"\"\"\n    if alias_dict is None:\n        alias_dict = {}\n\n    try:\n        # read all data from csv file\n        with open(file_path, \"r\", encoding=encoding) as csv_file:\n            reader = csv.DictReader(csv_file, delimiter=delimiter)\n            raw_data = collections.defaultdict(list)\n            for _, line_dict in enumerate(reader):\n                for key, value in line_dict.items():\n                    raw_data[key].append(value)\n    except FileNotFoundError as e:\n        raise e\n\n    # convert to numpy array\n    data_dict = {}\n    for key in keys:\n        fetch_key = alias_dict[key] if key in alias_dict else key\n        if fetch_key not in raw_data:\n            raise KeyError(f\"fetch_key({fetch_key}) do not exist in raw_data.\")\n        data_dict[key] = np.asarray(raw_data[fetch_key])\n        if not np.issubdtype(data_dict[key].dtype, np.integer):\n            data_dict[key] = data_dict[key].astype(paddle.get_default_dtype())\n        data_dict[key] = data_dict[key].reshape([-1, 1])\n\n    return data_dict\n</code></pre>"},{"location":"zh/api/utils/reader/#ppsci.utils.reader.load_mat_file","title":"<code>load_mat_file(file_path, keys, alias_dict=None)</code>","text":"<p>Load *.mat file and fetch data as given keys.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Mat file path.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Required fetching keys.</p> required <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Alias for keys, i.e. {original_key: original_key}. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: Loaded data in dict.</p> Source code in <code>ppsci/utils/reader.py</code> <pre><code>def load_mat_file(\n    file_path: str, keys: Tuple[str, ...], alias_dict: Optional[Dict[str, str]] = None\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Load *.mat file and fetch data as given keys.\n\n    Args:\n        file_path (str): Mat file path.\n        keys (Tuple[str, ...]): Required fetching keys.\n        alias_dict (Optional[Dict[str, str]]): Alias for keys,\n            i.e. {original_key: original_key}. Defaults to None.\n\n    Returns:\n        Dict[str, np.ndarray]: Loaded data in dict.\n    \"\"\"\n\n    if alias_dict is None:\n        alias_dict = {}\n\n    try:\n        # read all data from mat file\n        raw_data = sio.loadmat(file_path)\n    except FileNotFoundError as e:\n        raise e\n\n    # convert to numpy array\n    data_dict = {}\n    for key in keys:\n        fetch_key = alias_dict[key] if key in alias_dict else key\n        if fetch_key not in raw_data:\n            raise KeyError(f\"fetch_key({fetch_key}) do not exist in raw_data.\")\n        data_dict[key] = np.asarray(raw_data[fetch_key])\n        if not np.issubdtype(data_dict[key].dtype, np.integer):\n            data_dict[key] = data_dict[key].astype(paddle.get_default_dtype())\n        data_dict[key] = data_dict[key].reshape([-1, 1])\n\n    return data_dict\n</code></pre>"},{"location":"zh/api/utils/reader/#ppsci.utils.reader.load_npz_file","title":"<code>load_npz_file(file_path, keys, alias_dict=None)</code>","text":"<p>Load *.npz file and fetch data as given keys.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Npz file path.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Required fetching keys.</p> required <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Alias for keys, i.e. {original_key: original_key}. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: Loaded data in dict.</p> Source code in <code>ppsci/utils/reader.py</code> <pre><code>def load_npz_file(\n    file_path: str, keys: Tuple[str, ...], alias_dict: Optional[Dict[str, str]] = None\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Load *.npz file and fetch data as given keys.\n\n    Args:\n        file_path (str): Npz file path.\n        keys (Tuple[str, ...]): Required fetching keys.\n        alias_dict (Optional[Dict[str, str]]): Alias for keys,\n            i.e. {original_key: original_key}. Defaults to None.\n\n    Returns:\n        Dict[str, np.ndarray]: Loaded data in dict.\n    \"\"\"\n\n    if alias_dict is None:\n        alias_dict = {}\n\n    try:\n        # read all data from npz file\n        raw_data = np.load(file_path, allow_pickle=True)\n    except FileNotFoundError as e:\n        raise e\n\n    # convert to numpy array\n    data_dict = {}\n    for key in keys:\n        fetch_key = alias_dict[key] if key in alias_dict else key\n        if fetch_key not in raw_data:\n            raise KeyError(f\"fetch_key({fetch_key}) do not exist in raw_data.\")\n        data_dict[key] = np.asarray(raw_data[fetch_key])\n        if data_dict[key].dtype in (np.float16, np.float32, np.float64):\n            data_dict[key] = data_dict[key].astype(paddle.get_default_dtype())\n\n    return data_dict\n</code></pre>"},{"location":"zh/api/utils/reader/#ppsci.utils.reader.load_vtk_file","title":"<code>load_vtk_file(filename_without_timeid, time_step, time_index, input_keys, label_keys)</code>","text":"<p>Load coordinates and attached label from the *.vtu file.</p> <p>Parameters:</p> Name Type Description Default <code>filename_without_timeid</code> <code>str</code> <p>File name without time id.</p> required <code>time_step</code> <code>float</code> <p>Physical time step.</p> required <code>time_index</code> <code>Tuple[int, ...]</code> <p>Physical time indexes.</p> required <code>input_keys</code> <code>Tuple[str, ...]</code> <p>Input coordinates name keys.</p> required <code>label_keys</code> <code>Optional[Tuple[str, ...]]</code> <p>Input label name keys.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: Input coordinates dict, label coordinates dict</p> Source code in <code>ppsci/utils/reader.py</code> <pre><code>def load_vtk_file(\n    filename_without_timeid: str,\n    time_step: float,\n    time_index: Tuple[int, ...],\n    input_keys: Tuple[str, ...],\n    label_keys: Optional[Tuple[str, ...]],\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Load coordinates and attached label from the *.vtu file.\n\n    Args:\n        filename_without_timeid (str): File name without time id.\n        time_step (float): Physical time step.\n        time_index (Tuple[int, ...]): Physical time indexes.\n        input_keys (Tuple[str, ...]): Input coordinates name keys.\n        label_keys (Optional[Tuple[str, ...]]): Input label name keys.\n\n    Returns:\n        Dict[str, np.ndarray]: Input coordinates dict, label coordinates dict\n    \"\"\"\n    input_dict = {var: [] for var in input_keys}\n    label_dict = {var: [] for var in label_keys}\n    for index in time_index:\n        file = filename_without_timeid + f\"{index}.vtu\"\n        mesh = meshio.read(file)\n        n = mesh.points.shape[0]\n        i = 0\n        for key in input_dict:\n            if key == \"t\":\n                input_dict[key].append(\n                    np.full((n, 1), index * time_step, paddle.get_default_dtype())\n                )\n            else:\n                input_dict[key].append(\n                    mesh.points[:, i].reshape(n, 1).astype(paddle.get_default_dtype())\n                )\n                i += 1\n        for i, key in enumerate(label_dict):\n            label_dict[key].append(\n                np.array(mesh.point_data[key], paddle.get_default_dtype())\n            )\n    for key in input_dict:\n        input_dict[key] = np.concatenate(input_dict[key])\n    for key in label_dict:\n        label_dict[key] = np.concatenate(label_dict[key])\n\n    return input_dict, label_dict\n</code></pre>"},{"location":"zh/api/utils/reader/#ppsci.utils.reader.load_vtk_with_time_file","title":"<code>load_vtk_with_time_file(file)</code>","text":"<p>Temporary interface for points cloud, will be banished sooner.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Input file name.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict[str, np.ndarray]: Input coordinates dict.</p> Source code in <code>ppsci/utils/reader.py</code> <pre><code>def load_vtk_with_time_file(file: str) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Temporary interface for points cloud, will be banished sooner.\n\n    Args:\n        file (str): Input file name.\n\n    Returns:\n        Dict[str, np.ndarray]: Input coordinates dict.\n    \"\"\"\n    mesh = meshio.read(file)\n    n = mesh.points.shape[0]\n    t = np.array(mesh.point_data[\"time\"])\n    x = mesh.points[:, 0].reshape(n, 1)\n    y = mesh.points[:, 1].reshape(n, 1)\n    z = mesh.points[:, 2].reshape(n, 1)\n    input_dict = {\"t\": t, \"x\": x, \"y\": y, \"z\": z}\n    return input_dict\n</code></pre>"},{"location":"zh/api/utils/save_load/","title":"ppsci.utils.save_load","text":""},{"location":"zh/api/utils/save_load/#utilssave_load","title":"Utils.save_load(\u53c2\u6570\u52a0\u8f7d\u4e0e\u4fdd\u5b58) \u6a21\u5757","text":""},{"location":"zh/api/utils/save_load/#ppsci.utils.save_load","title":"<code>ppsci.utils.save_load</code>","text":""},{"location":"zh/api/utils/save_load/#ppsci.utils.save_load.load_checkpoint","title":"<code>load_checkpoint(path, model, optimizer, grad_scaler=None, equation=None, ema_model=None)</code>","text":"<p>Load from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path for checkpoint.</p> required <code>model</code> <code>Layer</code> <p>Model with parameters.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimizer for model.</p> required <code>grad_scaler</code> <code>Optional[GradScaler]</code> <p>GradScaler for AMP. Defaults to None.</p> <code>None</code> <code>equation</code> <code>Optional[Dict[str, PDE]]</code> <p>Equations. Defaults to None.</p> <code>None</code> <code>ema_model</code> <code>Optional[AveragedModel]</code> <p>Optional[ema.AveragedModel]: Average model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Loaded metric information.</p> Source code in <code>ppsci/utils/save_load.py</code> <pre><code>def load_checkpoint(\n    path: str,\n    model: nn.Layer,\n    optimizer: optimizer.Optimizer,\n    grad_scaler: Optional[amp.GradScaler] = None,\n    equation: Optional[Dict[str, equation.PDE]] = None,\n    ema_model: Optional[ema.AveragedModel] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Load from checkpoint.\n\n    Args:\n        path (str): Path for checkpoint.\n        model (nn.Layer): Model with parameters.\n        optimizer (optimizer.Optimizer): Optimizer for model.\n        grad_scaler (Optional[amp.GradScaler]): GradScaler for AMP. Defaults to None.\n        equation (Optional[Dict[str, equation.PDE]]): Equations. Defaults to None.\n        ema_model: Optional[ema.AveragedModel]: Average model. Defaults to None.\n\n    Returns:\n        Dict[str, Any]: Loaded metric information.\n    \"\"\"\n    if not os.path.exists(f\"{path}.pdparams\"):\n        raise FileNotFoundError(f\"{path}.pdparams not exist.\")\n    if not os.path.exists(f\"{path}.pdopt\"):\n        raise FileNotFoundError(f\"{path}.pdopt not exist.\")\n    if grad_scaler is not None and not os.path.exists(f\"{path}.pdscaler\"):\n        raise FileNotFoundError(f\"{path}.scaler not exist.\")\n\n    # load state dict\n    param_dict = paddle.load(f\"{path}.pdparams\")\n    optim_dict = paddle.load(f\"{path}.pdopt\")\n    metric_dict = paddle.load(f\"{path}.pdstates\")\n    if grad_scaler is not None:\n        scaler_dict = paddle.load(f\"{path}.pdscaler\")\n    if equation is not None:\n        if not os.path.exists(f\"{path}.pdeqn\"):\n            logger.warning(f\"{path}.pdeqn not found.\")\n            equation_dict = None\n        else:\n            equation_dict = paddle.load(f\"{path}.pdeqn\")\n\n    # set state dict\n    missing_keys, unexpected_keys = model.set_state_dict(param_dict)\n    if missing_keys:\n        logger.warning(\n            f\"There are missing keys when loading checkpoint: {missing_keys}, \"\n            \"and corresponding parameters will be initialized by default.\"\n        )\n    if unexpected_keys:\n        logger.warning(\n            f\"There are redundant keys: {unexpected_keys}, \"\n            \"and corresponding weights will be ignored.\"\n        )\n\n    optimizer.set_state_dict(optim_dict)\n    if grad_scaler is not None:\n        grad_scaler.load_state_dict(scaler_dict)\n    if equation is not None and equation_dict is not None:\n        for name, _equation in equation.items():\n            _equation.set_state_dict(equation_dict[name])\n\n    if ema_model:\n        avg_param_dict = paddle.load(f\"{path}_ema.pdparams\")\n        ema_model.set_state_dict(avg_param_dict)\n\n    logger.message(f\"Finish loading checkpoint from {path}\")\n    return metric_dict\n</code></pre>"},{"location":"zh/api/utils/save_load/#ppsci.utils.save_load.save_checkpoint","title":"<code>save_checkpoint(model, optimizer, metric, grad_scaler=None, output_dir=None, prefix='model', equation=None, print_log=True, ema_model=None)</code>","text":"<p>Save checkpoint, including model params, optimizer params, metric information.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>Model with parameters.</p> required <code>optimizer</code> <code>Optional[Optimizer]</code> <p>Optimizer for model.</p> required <code>metric</code> <code>Dict[str, float]</code> <p>Metric information, such as {\"RMSE\": 0.1, \"MAE\": 0.2}.</p> required <code>grad_scaler</code> <code>Optional[GradScaler]</code> <p>GradScaler for AMP. Defaults to None.</p> <code>None</code> <code>output_dir</code> <code>Optional[str]</code> <p>Directory for checkpoint storage.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for storage. Defaults to \"model\".</p> <code>'model'</code> <code>equation</code> <code>Optional[Dict[str, PDE]]</code> <p>Equations. Defaults to None.</p> <code>None</code> <code>print_log</code> <code>bool</code> <p>Whether print saving log information, mainly for keeping log tidy without duplicate 'Finish saving checkpoint ...' log strings. Defaults to True.</p> <code>True</code> <code>ema_model</code> <code>Optional[AveragedModel]</code> <p>Optional[ema.AveragedModel]: Average model. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppsci.utils import save_load\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\", \"z\"), (\"u\", \"v\", \"w\"), 5, 64, \"tanh\")\n&gt;&gt;&gt; optimizer = ppsci.optimizer.Adam(0.001)(model)\n&gt;&gt;&gt; save_load.save_checkpoint(model, optimizer, {\"RMSE\": 0.1}, output_dir=\"path/to/output/dir\")\n</code></pre> Source code in <code>ppsci/utils/save_load.py</code> <pre><code>def save_checkpoint(\n    model: nn.Layer,\n    optimizer: Optional[optimizer.Optimizer],\n    metric: Dict[str, float],\n    grad_scaler: Optional[amp.GradScaler] = None,\n    output_dir: Optional[str] = None,\n    prefix: str = \"model\",\n    equation: Optional[Dict[str, equation.PDE]] = None,\n    print_log: bool = True,\n    ema_model: Optional[ema.AveragedModel] = None,\n):\n    \"\"\"\n    Save checkpoint, including model params, optimizer params, metric information.\n\n    Args:\n        model (nn.Layer): Model with parameters.\n        optimizer (Optional[optimizer.Optimizer]): Optimizer for model.\n        metric (Dict[str, float]): Metric information, such as {\"RMSE\": 0.1, \"MAE\": 0.2}.\n        grad_scaler (Optional[amp.GradScaler]): GradScaler for AMP. Defaults to None.\n        output_dir (Optional[str]): Directory for checkpoint storage.\n        prefix (str, optional): Prefix for storage. Defaults to \"model\".\n        equation (Optional[Dict[str, equation.PDE]]): Equations. Defaults to None.\n        print_log (bool, optional): Whether print saving log information, mainly for\n            keeping log tidy without duplicate 'Finish saving checkpoint ...' log strings.\n            Defaults to True.\n        ema_model: Optional[ema.AveragedModel]: Average model. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppsci.utils import save_load\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\", \"z\"), (\"u\", \"v\", \"w\"), 5, 64, \"tanh\")\n        &gt;&gt;&gt; optimizer = ppsci.optimizer.Adam(0.001)(model)\n        &gt;&gt;&gt; save_load.save_checkpoint(model, optimizer, {\"RMSE\": 0.1}, output_dir=\"path/to/output/dir\") # doctest: +SKIP\n    \"\"\"\n    if paddle.distributed.get_rank() != 0:\n        return\n\n    if output_dir is None:\n        logger.warning(\"output_dir is None, skip save_checkpoint\")\n        return\n\n    ckpt_dir = os.path.join(output_dir, \"checkpoints\")\n    ckpt_path = os.path.join(ckpt_dir, prefix)\n    os.makedirs(ckpt_dir, exist_ok=True)\n\n    paddle.save(model.state_dict(), f\"{ckpt_path}.pdparams\")\n    if optimizer:\n        paddle.save(optimizer.state_dict(), f\"{ckpt_path}.pdopt\")\n    paddle.save(metric, f\"{ckpt_path}.pdstates\")\n    if grad_scaler is not None:\n        paddle.save(grad_scaler.state_dict(), f\"{ckpt_path}.pdscaler\")\n    if equation is not None:\n        num_learnable_params = sum(\n            [len(eq.learnable_parameters) for eq in equation.values()]\n        )\n        if num_learnable_params &gt; 0:\n            paddle.save(\n                {key: eq.state_dict() for key, eq in equation.items()},\n                f\"{ckpt_path}.pdeqn\",\n            )\n\n    if ema_model:\n        paddle.save(ema_model.state_dict(), f\"{ckpt_path}_ema.pdparams\")\n\n    if print_log:\n        log_str = f\"Finish saving checkpoint to: {ckpt_path}\"\n        if prefix == \"latest\":\n            log_str += (\n                \"(latest checkpoint will be saved every epoch as expected, \"\n                \"but this log will be printed only once for tidy logging)\"\n            )\n        logger.message(log_str)\n</code></pre>"},{"location":"zh/api/utils/save_load/#ppsci.utils.save_load.load_pretrain","title":"<code>load_pretrain(model, path, equation=None)</code>","text":"<p>Load pretrained model from given path or url.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Layer</code> <p>Model with parameters.</p> required <code>path</code> <code>str</code> <p>File path or url of pretrained model, i.e. <code>/path/to/model.pdparams</code> or <code>http://xxx.com/model.pdparams</code>.</p> required <code>equation</code> <code>Optional[Dict[str, PDE]]</code> <p>Equations. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; from ppsci.utils import save_load\n&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\", \"p\"), 9, 50, \"tanh\")\n&gt;&gt;&gt; save_load.load_pretrain(\n...     model=model,\n...     path=\"path/to/pretrain_model\")\n</code></pre> Source code in <code>ppsci/utils/save_load.py</code> <pre><code>def load_pretrain(\n    model: nn.Layer, path: str, equation: Optional[Dict[str, equation.PDE]] = None\n):\n    \"\"\"\n    Load pretrained model from given path or url.\n\n    Args:\n        model (nn.Layer): Model with parameters.\n        path (str): File path or url of pretrained model, i.e. `/path/to/model.pdparams`\n            or `http://xxx.com/model.pdparams`.\n        equation (Optional[Dict[str, equation.PDE]]): Equations. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; from ppsci.utils import save_load\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\", \"p\"), 9, 50, \"tanh\")\n        &gt;&gt;&gt; save_load.load_pretrain(\n        ...     model=model,\n        ...     path=\"path/to/pretrain_model\") # doctest: +SKIP\n    \"\"\"\n    if path.startswith(\"http\"):\n        # download from path(url) and get its' physical path\n        eqn_path = path.replace(\".pdparams\", \".pdeqn\", 1)\n        path = download.get_weights_path_from_url(path)\n\n        # automatically download additional equation weights if avaiable\n        def is_url_accessible(url: str):\n            try:\n                import requests\n\n                response = requests.head(url, timeout=5)\n                return response.status_code == requests.codes.ok\n            except requests.RequestException:\n                return False\n            except Exception:\n                return False\n\n        if is_url_accessible(eqn_path):\n            download.get_weights_path_from_url(eqn_path)\n\n    # remove \".pdparams\" in suffix of path for convenient\n    if path.endswith(\".pdparams\"):\n        path = path[:-9]\n    _load_pretrain_from_path(path, model, equation)\n</code></pre>"},{"location":"zh/api/utils/symbolic/","title":"ppsci.utils.symbolic","text":""},{"location":"zh/api/utils/symbolic/#utilssymbolic","title":"Utils.symbolic(\u7b26\u53f7\u8ba1\u7b97) \u6a21\u5757","text":""},{"location":"zh/api/utils/symbolic/#ppsci.utils.symbolic","title":"<code>ppsci.utils.symbolic</code>","text":"<p>Sympy to python function conversion module</p>"},{"location":"zh/api/utils/symbolic/#ppsci.utils.symbolic.lambdify","title":"<code>lambdify(expr, models=None, extra_parameters=None, graph_filename=None, create_graph=True, retain_graph=None, fuse_derivative=False)</code>","text":"<p>Convert sympy expression to callable function.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>Union[Basic, List[Basic]]</code> <p>Sympy expression(s) to be converted. Will return callable functions in list if multiple expressions are given, else return one single callable function.</p> required <code>models</code> <code>Optional[Union[Arch, Tuple[Arch, ...]]]</code> <p>Model(s) for computing forward result in <code>LayerNode</code>.</p> <code>None</code> <code>extra_parameters</code> <code>Optional[ParameterList]</code> <p>Extra learnable parameters. Defaults to None.</p> <code>None</code> <code>graph_filename</code> <code>Optional[str]</code> <p>Save computational graph to <code>graph_filename.png</code> for given <code>expr</code>, if <code>graph_filename</code> is not None and a valid string, such as 'momentum_x'. Defaults to None.</p> <code>None</code> <code>create_graph</code> <code>bool</code> <p>Whether to create the gradient graphs of the computing process. When it is True, higher order derivatives are supported to compute. When it is False, the gradient graphs of the computing process would be discarded. Defaults to True.</p> <code>True</code> <code>retain_graph</code> <code>Optional[bool]</code> <p>Whether to retain the forward graph which is used to calculate the gradient. When it is True, the graph would be retained, in which way users can calculate backward twice for the same graph. When it is False, the graph would be freed. Defaults to None, which means it is equal to <code>create_graph</code>.</p> <code>None</code> <code>fuse_derivative</code> <code>bool</code> <p>Whether to fuse the derivative nodes. For example, if <code>expr</code> is 'Derivative(u, x) + Derivative(u, y)' It will compute grad(u, x) + grad(u, y) if fuse_derivative=False, else will compute sum(grad(u, [x, y])) if fuse_derivative=True as is more efficient in backward-graph. Defaults to False, as it is experimental so not enabled by default if used independently.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ComposedNode, List[ComposedNode]]</code> <p>Union[ComposedNode, List[ComposedNode]]: Callable object(s) for computing expr with necessary input(s) data in dict given.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; import ppsci\n&gt;&gt;&gt; import sympy as sp\n</code></pre> <pre><code>&gt;&gt;&gt; a, b, c, x, y = sp.symbols(\"a b c x y\")\n&gt;&gt;&gt; u = sp.Function(\"u\")(x, y)\n&gt;&gt;&gt; v = sp.Function(\"v\")(x, y)\n&gt;&gt;&gt; z = -a + b * (c ** 2) + u * v + 2.3\n</code></pre> <pre><code>&gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\"), 4, 16)\n</code></pre> <pre><code>&gt;&gt;&gt; batch_size = 13\n&gt;&gt;&gt; a_tensor = paddle.randn([batch_size, 1])\n&gt;&gt;&gt; b_tensor = paddle.randn([batch_size, 1])\n&gt;&gt;&gt; c_tensor = paddle.randn([batch_size, 1])\n&gt;&gt;&gt; x_tensor = paddle.randn([batch_size, 1])\n&gt;&gt;&gt; y_tensor = paddle.randn([batch_size, 1])\n</code></pre> <pre><code>&gt;&gt;&gt; model_output_dict = model({\"x\": x_tensor, \"y\": y_tensor})\n&gt;&gt;&gt; u_tensor, v_tensor = model_output_dict[\"u\"], model_output_dict[\"v\"]\n</code></pre> <pre><code>&gt;&gt;&gt; z_tensor_manually = (\n...     -a_tensor + b_tensor * (c_tensor ** 2)\n...     + u_tensor * v_tensor + 2.3\n... )\n&gt;&gt;&gt; z_tensor_sympy = ppsci.lambdify(z, model)(\n...     {\n...         \"a\": a_tensor,\n...         \"b\": b_tensor,\n...         \"c\": c_tensor,\n...         \"x\": x_tensor,\n...         \"y\": y_tensor,\n...     }\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; paddle.allclose(z_tensor_manually, z_tensor_sympy).item()\nTrue\n</code></pre> Source code in <code>ppsci/utils/symbolic.py</code> <pre><code>def lambdify(\n    expr: Union[sp.Basic, List[sp.Basic]],\n    models: Optional[Union[arch.Arch, Tuple[arch.Arch, ...]]] = None,\n    extra_parameters: Optional[Sequence[paddle.Tensor]] = None,\n    graph_filename: Optional[str] = None,\n    create_graph: bool = True,\n    retain_graph: Optional[bool] = None,\n    fuse_derivative: bool = False,\n) -&gt; Union[ComposedNode, List[ComposedNode]]:\n    \"\"\"Convert sympy expression to callable function.\n\n    Args:\n        expr (Union[sp.Basic, List[sp.Basic]]): Sympy expression(s) to be converted.\n            Will return callable functions in list if multiple expressions are given,\n            else return one single callable function.\n        models (Optional[Union[arch.Arch, Tuple[arch.Arch, ...]]]): Model(s) for\n            computing forward result in `LayerNode`.\n        extra_parameters (Optional[nn.ParameterList]): Extra learnable parameters.\n            Defaults to None.\n        graph_filename (Optional[str]): Save computational graph to `graph_filename.png`\n            for given `expr`, if `graph_filename` is not None and a valid string,\n            such as 'momentum_x'. Defaults to None.\n        create_graph (bool, optional): Whether to create the gradient graphs of\n            the computing process. When it is True, higher order derivatives are\n            supported to compute. When it is False, the gradient graphs of the\n            computing process would be discarded. Defaults to True.\n        retain_graph (Optional[bool]): Whether to retain the forward graph which\n            is used to calculate the gradient. When it is True, the graph would\n            be retained, in which way users can calculate backward twice for the\n            same graph. When it is False, the graph would be freed. Defaults to None,\n            which means it is equal to `create_graph`.\n        fuse_derivative (bool, optional): Whether to fuse the derivative nodes.\n            For example, if `expr` is 'Derivative(u, x) + Derivative(u, y)'\n            It will compute grad(u, x) + grad(u, y) if fuse_derivative=False,\n            else will compute sum(grad(u, [x, y])) if fuse_derivative=True as is more\n            efficient in backward-graph. Defaults to False, as it is experimental so not\n            enabled by default if used independently.\n\n    Returns:\n        Union[ComposedNode, List[ComposedNode]]: Callable object(s) for computing expr\n            with necessary input(s) data in dict given.\n\n    Examples:\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; import sympy as sp\n\n        &gt;&gt;&gt; a, b, c, x, y = sp.symbols(\"a b c x y\")\n        &gt;&gt;&gt; u = sp.Function(\"u\")(x, y)\n        &gt;&gt;&gt; v = sp.Function(\"v\")(x, y)\n        &gt;&gt;&gt; z = -a + b * (c ** 2) + u * v + 2.3\n\n        &gt;&gt;&gt; model = ppsci.arch.MLP((\"x\", \"y\"), (\"u\", \"v\"), 4, 16)\n\n        &gt;&gt;&gt; batch_size = 13\n        &gt;&gt;&gt; a_tensor = paddle.randn([batch_size, 1])\n        &gt;&gt;&gt; b_tensor = paddle.randn([batch_size, 1])\n        &gt;&gt;&gt; c_tensor = paddle.randn([batch_size, 1])\n        &gt;&gt;&gt; x_tensor = paddle.randn([batch_size, 1])\n        &gt;&gt;&gt; y_tensor = paddle.randn([batch_size, 1])\n\n        &gt;&gt;&gt; model_output_dict = model({\"x\": x_tensor, \"y\": y_tensor})\n        &gt;&gt;&gt; u_tensor, v_tensor = model_output_dict[\"u\"], model_output_dict[\"v\"]\n\n        &gt;&gt;&gt; z_tensor_manually = (\n        ...     -a_tensor + b_tensor * (c_tensor ** 2)\n        ...     + u_tensor * v_tensor + 2.3\n        ... )\n        &gt;&gt;&gt; z_tensor_sympy = ppsci.lambdify(z, model)(\n        ...     {\n        ...         \"a\": a_tensor,\n        ...         \"b\": b_tensor,\n        ...         \"c\": c_tensor,\n        ...         \"x\": x_tensor,\n        ...         \"y\": y_tensor,\n        ...     }\n        ... )\n\n        &gt;&gt;&gt; paddle.allclose(z_tensor_manually, z_tensor_sympy).item()\n        True\n    \"\"\"\n    if not extra_parameters:\n        extra_parameters = ()\n\n    if isinstance(models, arch.ModelList):\n        models = tuple(models.model_list[i] for i in range(len(models.model_list)))\n    if not isinstance(models, (tuple, list)):\n        models = (models,)\n\n    def _expr_to_callable_nodes(\n        single_expr: sp.Basic, graph_filename_: Optional[str] = None\n    ) -&gt; List[Node]:\n        \"\"\"Convert sympy expression to a sequence of nodes in topologic order.\n\n        Args:\n            single_expr (sp.Basic): Single sympy expression, such as \"a+b*c\".\n            graph_filename_ (Optional[str]): Save computational graph to\n            `/path/to/graph_filename.png` for given `expr`, if `graph_filename` is not\n            None and a valid string, such as 'momentum_x'. Defaults to None.\n\n        Returns:\n            List[Node]: Sequence of callable nodes.\n        \"\"\"\n        # NOTE: Those simplify methods may complicate given expr instead, so not use here\n        # simplify expression to reduce nodes in tree\n        # expr = sp.nsimplify(expr)\n        # expr = sp.expand(expr)\n        # expr = sp.simplify(expr)\n\n        # remove 1.0 from sympy expression tree\n        single_expr = single_expr.subs(1.0, 1)\n\n        # convert sympy expression tree to list of nodes in post-order\n        sympy_nodes: List[sp.Basic] = []\n        sympy_nodes = _post_traverse(single_expr, sympy_nodes)\n\n        # remove unnecessary symbol nodes already in input dict(except for parameter symbol)\n        _parameter_names = tuple(param.name for param in extra_parameters)\n        sympy_nodes = [\n            node\n            for node in sympy_nodes\n            if (not node.is_Symbol) or (_cvt_to_key(node) in _parameter_names)\n        ]\n\n        # remove duplicated node(s) with topological order kept\n        sympy_nodes = list(dict.fromkeys(sympy_nodes))\n\n        # convert sympy node to callable node\n        callable_nodes = []\n        for i, node in enumerate(sympy_nodes):\n            if isinstance(\n                node, tuple(SYMPY_TO_PADDLE.keys()) + (sp.Add, sp.Mul, sp.Derivative)\n            ):\n                if isinstance(node, sp.Derivative):\n                    callable_nodes.append(\n                        DerivativeNode(node, create_graph, retain_graph)\n                    )\n                else:\n                    callable_nodes.append(OperatorNode(node))\n            elif isinstance(node, sp.Function):\n                if str(node.func) == equation.DETACH_FUNC_NAME:\n                    callable_nodes.append(DetachNode(node))\n                    logger.debug(f\"Detected detach node {node}\")\n                else:\n                    match_index = None\n                    for j, model in enumerate(models):\n                        if str(node.func) in model.output_keys:\n                            callable_nodes.append(\n                                LayerNode(\n                                    node,\n                                    model,\n                                )\n                            )\n                            if match_index is not None:\n                                raise ValueError(\n                                    f\"Name of function: '{node}' should be unique along given\"\n                                    f\" models, but got same output_key: '{str(node.func)}' \"\n                                    f\"in given models[{match_index}] and models[{j}].\"\n                                )\n                            match_index = j\n                    # NOTE: Skip 'sdf' function, which should be already generated in\n                    # given data_dict\n                    if match_index is None and str(node.func) != \"sdf\":\n                        raise ValueError(\n                            f\"Node {node} can not match any model in given model(s).\"\n                        )\n            elif node.is_Number or node.is_NumberSymbol:\n                callable_nodes.append(ConstantNode(node))\n            elif isinstance(node, sp.Symbol):\n                callable_nodes.append(\n                    ParameterNode(\n                        node,\n                        *[\n                            param\n                            for param in extra_parameters\n                            if param.name == node.name\n                        ],\n                    )\n                )\n            else:\n                raise NotImplementedError(\n                    f\"The node {node} is not supported in lambdify.\"\n                )\n\n        # NOTE: visualize computational graph using 'pygraphviz'\n        if isinstance(graph_filename, str):\n            _visualize_graph(sympy_nodes, os.path.join(graph_filename, graph_filename_))\n\n        return callable_nodes\n\n    if isinstance(expr, sp.Basic):\n        callable_nodes_group = [_expr_to_callable_nodes(expr, \"expr\")]\n    else:\n        callable_nodes_group = [\n            _expr_to_callable_nodes(expr_i, f\"expr_{i}\")\n            for i, expr_i in enumerate(expr)\n        ]\n\n    # [Optional] Fused derivatives nodes that with same function to be differentiated\n    while fuse_derivative:\n        candidate_pos: List[Tuple[int, int]] = []  # [(group_id, node_id), ...]\n\n        # use 4-nested for-loop to find all potential mergable derivative nodes\n        for i in range(len(callable_nodes_group)):\n            for j in range(len(callable_nodes_group[i])):\n                # skip non-derivative node\n                if not isinstance(callable_nodes_group[i][j], DerivativeNode):\n                    continue\n                # skip sdf function since it is always already given in data_dict\n                if callable_nodes_group[i][j].expr.args[0].name == \"sdf\":\n                    continue\n                # skip merged node\n                if callable_nodes_group[i][j].merged:\n                    continue\n\n                candidate_pos = [[i, j]]\n                for ii in range(len(callable_nodes_group)):\n                    for jj in range(len(callable_nodes_group[ii])):\n                        # skip non-derivative node\n                        if not isinstance(callable_nodes_group[ii][jj], DerivativeNode):\n                            continue\n\n                        # skip same node\n                        if i == ii and j == jj:\n                            continue\n                        # skip merged node\n                        if callable_nodes_group[ii][jj].merged:\n                            continue\n\n                        # has same function item\n                        if (\n                            callable_nodes_group[i][j].expr.args[0]\n                            == callable_nodes_group[ii][jj].expr.args[0]\n                        ):\n                            candidate_pos.append([ii, jj])\n\n                if len(candidate_pos) &gt; 1:\n                    break\n            if len(candidate_pos) &gt; 1:\n                break\n\n        # merge all candidate nodes into one or more FusedDerivativeNode node\n        if len(candidate_pos) &gt; 1:\n            fused_node_seq = _fuse_derivative_nodes(\n                [callable_nodes_group[gid][nid].expr for gid, nid in candidate_pos]\n            )\n            assert isinstance(\n                fused_node_seq, list\n            ), \"'fused_node_seq' should be list of 'FusedDerivativeNode'\"\n            gid0, nid0 = candidate_pos[0]\n            logger.debug(\n                f\"Fused {len(candidate_pos)} derivatives nodes: \"\n                f\"{[callable_nodes_group[i][j].expr for i, j in candidate_pos]} into\"\n                f\" {len(fused_node_seq)} fuse node sequence: {fused_node_seq} at position: ([{gid0}][{nid0}])\"\n            )\n\n            # mark merged node\n            for i, (gid, nid) in enumerate(candidate_pos):\n                assert isinstance(callable_nodes_group[gid][nid], DerivativeNode)\n                callable_nodes_group[gid][nid].merged = True\n\n            # replace first mergable node with fused node sequence(packed in list)\n            # then mask the rest merged node to None(except [gid0, nid0])\n            for i, (gid, nid) in enumerate(candidate_pos[1:]):\n                # keep the end node of each group to avoid generating empty callable\n                # node sequence, this will not effect performance since cache strategy\n                # in Node.forward\n                if nid != len(callable_nodes_group[gid]) - 1:\n                    callable_nodes_group[gid][nid] = None\n\n            if nid0 == len(callable_nodes_group[gid0]) - 1:\n                callable_nodes_group[gid0].insert(nid0, fused_node_seq)\n            else:\n                callable_nodes_group[gid0][nid0] = fused_node_seq\n\n            # re-organize callable_nodes_group, remove None element and unpack list\n            for i in range(len(callable_nodes_group)):\n                tmp = []\n                for j in range(len(callable_nodes_group[i])):\n                    if isinstance(\n                        callable_nodes_group[i][j], (Node, FusedDerivativeNode)\n                    ):\n                        tmp.append(callable_nodes_group[i][j])\n                    elif isinstance(callable_nodes_group[i][j], list) and isinstance(\n                        callable_nodes_group[i][j][0], FusedDerivativeNode\n                    ):\n                        tmp.extend(callable_nodes_group[i][j])\n                    else:\n                        assert (\n                            callable_nodes_group[i][j] is None\n                        ), f\"Unexpected element: {callable_nodes_group[i][j]}\"\n                callable_nodes_group[i] = tmp\n        else:\n            # exit while loop if no more fused\n            break\n\n    # Compose callable nodes into one callable object\n    if isinstance(expr, sp.Basic):\n        return ComposedNode(callable_nodes_group[0])\n    else:\n        return [ComposedNode(callable_nodes) for callable_nodes in callable_nodes_group]\n</code></pre>"},{"location":"zh/api/utils/writer/","title":"ppsci.utils.writer","text":""},{"location":"zh/api/utils/writer/#utilswriter","title":"Utils.writer(\u6587\u4ef6\u4fdd\u5b58\u51fd\u6570) \u6a21\u5757","text":""},{"location":"zh/api/utils/writer/#ppsci.utils.writer","title":"<code>ppsci.utils.writer</code>","text":""},{"location":"zh/api/utils/writer/#ppsci.utils.writer.save_csv_file","title":"<code>save_csv_file(filename, data_dict, keys, alias_dict=None, use_header=True, delimiter=',', encoding='utf-8')</code>","text":"<p>Write numpy or tensor data into csv file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Dump file path.</p> required <code>data_dict</code> <code>Dict[str, Union[ndarray, Tensor]]</code> <p>Numpy or tensor data in dict.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Keys for data_dict to be fetched.</p> required <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Alias dict for keys, i.e. {dump_key: dict_key}. Defaults to None.</p> <code>None</code> <code>use_header</code> <code>bool</code> <p>Whether save csv with header. Defaults to True.</p> <code>True</code> <code>delimiter</code> <code>str</code> <p>Delemiter for splitting different data field. Defaults to \",\".</p> <code>','</code> <code>encoding</code> <code>str</code> <p>Encoding. Defaults to \"utf-8\".</p> <code>'utf-8'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from ppsci.utils import save_csv_file\n&gt;&gt;&gt; data_dict = {\n...     \"a\": np.array([[1], [2], [3]]).astype(\"int64\"), # [3, 1]\n...     \"b\": np.array([[4.12], [5.25], [6.3370]]).astype(\"float32\"), # [3, 1]\n... }\n&gt;&gt;&gt; save_csv_file(\n...     \"test.csv\",\n...     data_dict,\n...     (\"A\", \"B\"),\n...     alias_dict={\"A\": \"a\", \"B\": \"b\"},\n...     use_header=True,\n...     delimiter=\",\",\n...     encoding=\"utf-8\",\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # == test.csv ==\n&gt;&gt;&gt; # A,B\n&gt;&gt;&gt; # 1,4.12\n&gt;&gt;&gt; # 2,5.25\n&gt;&gt;&gt; # 3,6.337\n</code></pre> Source code in <code>ppsci/utils/writer.py</code> <pre><code>def save_csv_file(\n    filename: str,\n    data_dict: Dict[str, Union[np.ndarray, \"paddle.Tensor\"]],\n    keys: Tuple[str, ...],\n    alias_dict: Optional[Dict[str, str]] = None,\n    use_header: bool = True,\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n):\n    \"\"\"Write numpy or tensor data into csv file.\n\n    Args:\n        filename (str): Dump file path.\n        data_dict (Dict[str, Union[np.ndarray, paddle.Tensor]]): Numpy or tensor data in dict.\n        keys (Tuple[str, ...]): Keys for data_dict to be fetched.\n        alias_dict (Optional[Dict[str, str]], optional): Alias dict for keys,\n            i.e. {dump_key: dict_key}. Defaults to None.\n        use_header (bool, optional): Whether save csv with header. Defaults to True.\n        delimiter (str, optional): Delemiter for splitting different data field. Defaults to \",\".\n        encoding (str, optional): Encoding. Defaults to \"utf-8\".\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from ppsci.utils import save_csv_file\n        &gt;&gt;&gt; data_dict = {\n        ...     \"a\": np.array([[1], [2], [3]]).astype(\"int64\"), # [3, 1]\n        ...     \"b\": np.array([[4.12], [5.25], [6.3370]]).astype(\"float32\"), # [3, 1]\n        ... }\n        &gt;&gt;&gt; save_csv_file(\n        ...     \"test.csv\",\n        ...     data_dict,\n        ...     (\"A\", \"B\"),\n        ...     alias_dict={\"A\": \"a\", \"B\": \"b\"},\n        ...     use_header=True,\n        ...     delimiter=\",\",\n        ...     encoding=\"utf-8\",\n        ... )  # doctest: +SKIP\n\n        &gt;&gt;&gt; # == test.csv ==\n        &gt;&gt;&gt; # A,B\n        &gt;&gt;&gt; # 1,4.12\n        &gt;&gt;&gt; # 2,5.25\n        &gt;&gt;&gt; # 3,6.337\n    \"\"\"\n    if alias_dict is None:\n        alias_dict = {}\n\n    # convert to numpy array\n    data_fields = []\n    header = []\n    for key in keys:\n        fetch_key = alias_dict.get(key, key)\n        data = data_dict[fetch_key]\n        if isinstance(data, paddle.Tensor):\n            data = data.numpy()  # [num_of_samples, ]\n\n        if isinstance(data, np.ndarray):\n            data = data.flatten()\n        data_fields.append(data)\n\n        header.append(key)\n\n    assert len(header) == len(data_fields)\n\n    data_fields = zip(*data_fields)  # transpose col data to row data\n    with open(filename, \"w\", newline=\"\", encoding=encoding) as file:\n        writer = csv.writer(file, delimiter=delimiter)\n\n        if use_header:\n            writer.writerow(header)\n\n        writer.writerows(data_fields)\n\n    logger.message(f\"csv file has been dumped to: {filename}\")\n</code></pre>"},{"location":"zh/api/utils/writer/#ppsci.utils.writer.save_tecplot_file","title":"<code>save_tecplot_file(filename, data_dict, keys, num_x, num_y, alias_dict=None, delimiter=' ', encoding='utf-8', num_timestamps=1)</code>","text":"<p>Write numpy or tensor data into tecplot file(s).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Tecplot file path.</p> required <code>data_dict</code> <code>Dict[str, Union[ndarray, Tensor]]</code> <p>Numpy or Tensor data in dict.</p> required <code>keys</code> <code>Tuple[str, ...]</code> <p>Target keys to be dumped.</p> required <code>num_x</code> <code>int</code> <p>The number of discrete points of the grid in the X-axis. Assuming the discrete grid size is 20 x 30, then num_x=20.</p> required <code>num_y</code> <code>int</code> <p>The number of discrete points of the grid in the Y-axis. Assuming the discrete grid size is 20 x 30, then num_y=30.</p> required <code>alias_dict</code> <code>Optional[Dict[str, str]]</code> <p>Alias dict for keys, i.e. {dump_key: dict_key}. Defaults to None.</p> <code>None</code> <code>delimiter</code> <code>str</code> <p>Delemiter for splitting different data field. Defaults to \" \".</p> <code>' '</code> <code>encoding</code> <code>str</code> <p>Encoding. Defaults to \"utf-8\".</p> <code>'utf-8'</code> <code>num_timestamps</code> <code>int</code> <p>Number of timestamp over coord and value. Defaults to 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from ppsci.utils import save_tecplot_file\n&gt;&gt;&gt; data_dict = {\n...     \"x\": np.array([[-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]), # [6, 1]\n...     \"y\": np.array([[1.0], [2.0], [3.0], [1.0], [2.0], [3.0]]), # [6, 1]\n...     \"value\": np.array([[3], [33], [333], [3333], [33333], [333333]]), # [6, 1]\n... }\n&gt;&gt;&gt; save_tecplot_file(\n...    \"./test.dat\",\n...    data_dict,\n...    (\"X\", \"Y\", \"value\"),\n...    num_x=1,\n...    num_y=3,\n...    alias_dict={\"X\": \"x\", \"Y\": \"y\"},\n...    num_timestamps=2,\n... )\n&gt;&gt;&gt; # == test_t-0.dat ==\n&gt;&gt;&gt; # title = \"./test_t-0.dat\"\n&gt;&gt;&gt; # variables = \"X\", \"Y\"\n&gt;&gt;&gt; # Zone I = 3, J = 1, F = POINT\n&gt;&gt;&gt; # -1.0 1.0 3.0\n&gt;&gt;&gt; # -1.0 2.0 33.0\n&gt;&gt;&gt; # -1.0 3.0 333.0\n</code></pre> <pre><code>&gt;&gt;&gt; # == test_t-1.dat ==\n&gt;&gt;&gt; # title = \"./test_t-1.dat\"\n&gt;&gt;&gt; # variables = \"X\", \"Y\"\n&gt;&gt;&gt; # Zone I = 3, J = 1, F = POINT\n&gt;&gt;&gt; # -1.0 1.0 3333.0\n&gt;&gt;&gt; # -1.0 2.0 33333.0\n&gt;&gt;&gt; # -1.0 3.0 333333.0\n</code></pre> Source code in <code>ppsci/utils/writer.py</code> <pre><code>def save_tecplot_file(\n    filename: str,\n    data_dict: Dict[str, Union[np.ndarray, \"paddle.Tensor\"]],\n    keys: Tuple[str, ...],\n    num_x: int,\n    num_y: int,\n    alias_dict: Optional[Dict[str, str]] = None,\n    delimiter: str = \" \",\n    encoding: str = \"utf-8\",\n    num_timestamps: int = 1,\n):\n    \"\"\"Write numpy or tensor data into tecplot file(s).\n\n    Args:\n        filename (str): Tecplot file path.\n        data_dict (Dict[str, Union[np.ndarray, paddle.Tensor]]): Numpy or Tensor data in dict.\n        keys (Tuple[str, ...]): Target keys to be dumped.\n        num_x (int): The number of discrete points of the grid in the X-axis. Assuming\n            the discrete grid size is 20 x 30, then num_x=20.\n        num_y (int): The number of discrete points of the grid in the Y-axis. Assuming\n            the discrete grid size is 20 x 30, then num_y=30.\n        alias_dict (Optional[Dict[str, str]], optional): Alias dict for keys,\n            i.e. {dump_key: dict_key}. Defaults to None.\n        delimiter (str, optional): Delemiter for splitting different data field. Defaults to \" \".\n        encoding (str, optional): Encoding. Defaults to \"utf-8\".\n        num_timestamps (int, optional): Number of timestamp over coord and value. Defaults to 1.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from ppsci.utils import save_tecplot_file\n        &gt;&gt;&gt; data_dict = {\n        ...     \"x\": np.array([[-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]), # [6, 1]\n        ...     \"y\": np.array([[1.0], [2.0], [3.0], [1.0], [2.0], [3.0]]), # [6, 1]\n        ...     \"value\": np.array([[3], [33], [333], [3333], [33333], [333333]]), # [6, 1]\n        ... }\n        &gt;&gt;&gt; save_tecplot_file(\n        ...    \"./test.dat\",\n        ...    data_dict,\n        ...    (\"X\", \"Y\", \"value\"),\n        ...    num_x=1,\n        ...    num_y=3,\n        ...    alias_dict={\"X\": \"x\", \"Y\": \"y\"},\n        ...    num_timestamps=2,\n        ... )  # doctest: +SKIP\n        &gt;&gt;&gt; # == test_t-0.dat ==\n        &gt;&gt;&gt; # title = \"./test_t-0.dat\"\n        &gt;&gt;&gt; # variables = \"X\", \"Y\"\n        &gt;&gt;&gt; # Zone I = 3, J = 1, F = POINT\n        &gt;&gt;&gt; # -1.0 1.0 3.0\n        &gt;&gt;&gt; # -1.0 2.0 33.0\n        &gt;&gt;&gt; # -1.0 3.0 333.0\n\n\n        &gt;&gt;&gt; # == test_t-1.dat ==\n        &gt;&gt;&gt; # title = \"./test_t-1.dat\"\n        &gt;&gt;&gt; # variables = \"X\", \"Y\"\n        &gt;&gt;&gt; # Zone I = 3, J = 1, F = POINT\n        &gt;&gt;&gt; # -1.0 1.0 3333.0\n        &gt;&gt;&gt; # -1.0 2.0 33333.0\n        &gt;&gt;&gt; # -1.0 3.0 333333.0\n    \"\"\"\n    if alias_dict is None:\n        alias_dict = {}\n\n    ntxy = len(next(iter(data_dict.values())))\n    if ntxy % num_timestamps != 0:\n        raise ValueError(\n            f\"num_points({ntxy}) must be a multiple of \"\n            f\"num_timestamps({num_timestamps}).\"\n        )\n    nxy = ntxy // num_timestamps\n\n    nx, ny = num_x, num_y\n    assert nx * ny == nxy, f\"nx({nx}) * ny({ny}) != nxy({nxy})\"\n\n    if len(os.path.dirname(filename)):\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\n\n    if filename.endswith(\".dat\"):\n        filename = filename[:-4]\n\n    for t in range(num_timestamps):\n        # write 1 tecplot file for each timestep\n        if num_timestamps &gt; 1:\n            dump_filename = f\"{filename}_t-{t}.dat\"\n        else:\n            dump_filename = f\"{filename}.dat\"\n\n        fetch_keys = [alias_dict.get(key, key) for key in keys]\n        with open(dump_filename, \"w\", encoding=encoding) as f:\n            # write meta information of tec\n            f.write(f'title = \"{dump_filename}\"\\n')\n            header = \", \".join([f'\"{key}\"' for key in keys])\n            f.write(f\"variables = {header}\\n\")\n\n            # NOTE: Tecplot is column-major, so we need to specify I = ny, J = nx,\n            # which is in contrast to our habits.\n            f.write(f\"Zone I = {ny}, J = {nx}, F = POINT\\n\")\n\n            # write points data into file\n            data_cur_time_step = [\n                data_dict[key][t * nxy : (t + 1) * nxy] for key in fetch_keys\n            ]\n\n            for items in zip(*data_cur_time_step):\n                f.write(delimiter.join([str(float(x)) for x in items]) + \"\\n\")\n\n    if num_timestamps &gt; 1:\n        logger.message(\n            f\"tecplot files are saved to: {filename}_t-0.dat ~ {filename}_t-{num_timestamps - 1}.dat\"\n        )\n    else:\n        logger.message(f\"tecplot file is saved to: {filename}.dat\")\n</code></pre>"},{"location":"zh/examples/RegAE/","title":"Learning to regularize with a variational autoencoder for hydrologic inverse analysis","text":""},{"location":"zh/examples/RegAE/#learning-to-regularize-with-a-variational-autoencoder-for-hydrologic-inverse-analysis","title":"Learning to regularize with a variational autoencoder for hydrologic inverse analysis","text":""},{"location":"zh/examples/RegAE/#1","title":"1.\u7b80\u4ecb","text":"<p>\u672c\u9879\u76ee\u57fa\u4e8epaddle\u6846\u67b6\u590d\u73b0\u3002</p> <p>\u8bba\u6587\u4e3b\u8981\u70b9\u5982\u4e0b\uff1a</p> <ul> <li>\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668 (VAE)\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff1b</li> <li>\u8fd9\u79cd\u65b9\u6cd5\u7684\u4f18\u70b91: \u5bf9\u6765\u81eaVAE\u7684\u6f5c\u5728\u53d8\u91cf\uff08\u6b64\u5904\u6307encoder\u7684\u8f93\u51fa\uff09\u6267\u884c\u6b63\u5219\u5316\uff0c\u53ef\u4ee5\u7b80\u5355\u5730\u5bf9\u5176\u8fdb\u884c\u6b63\u5219\u5316\uff1b</li> <li>\u8fd9\u79cd\u65b9\u6cd5\u7684\u4f18\u70b92: VAE\u51cf\u5c11\u4e86\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u53d8\u91cf\u6570\u91cf\uff0c\u4ece\u800c\u5728\u4f34\u968f\u65b9\u6cd5\u4e0d\u53ef\u7528\u65f6\u4f7f\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5728\u8ba1\u7b97\u4e0a\u66f4\u52a0\u9ad8\u6548\u3002</li> </ul> <p>\u672c\u9879\u76ee\u5173\u952e\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li>\u5b9e\u73b0paddle\u548cjulia\u6df7\u5199\u4ee3\u7801\u68af\u5ea6\u4f20\u9012\uff0c\u907f\u514d\u5927\u9762\u79ef\u91cd\u5199julia\u4ee3\u7801\u5e76\u80fd\u591f\u8c03\u7528\u4f18\u79c0\u7684julia\u4ee3\u7801\uff1b</li> <li>\u53d1\u73b0paddle minimize_lbfgs\u5b58\u5728\u95ee\u9898, \u5f85\u63d0\u4ea4issue\u786e\u8ba4\u3002</li> </ul> <p>\u5b9e\u9a8c\u7ed3\u679c\u8981\u70b9\uff1a</p> <ul> <li>\u6210\u529f\u590d\u73b0\u8bba\u6587\u4ee3\u7801\u6846\u67b6\u53ca\u5168\u6d41\u7a0b\u8fd0\u884c\u6d4b\u8bd5\uff1b</li> <li>\u672c\u6b21\u590d\u73b0\u7cbe\u5ea6\u56e0\u65e0\u6cd5\u4f7f\u7528\u76f8\u540c\u6837\u672c\uff0c\u65e0\u6cd5\u4e0e\u8bba\u6587\u4e2d\u6570\u636e\u8fdb\u884c\u4e00\u4e00\u6bd4\u8f83\u3002\u672c\u9879\u76ee\u7ed9\u51fa\u4e86\u91c7\u7528paddle\u7f16\u5199\u7684\u6846\u67b6\u7ed3\u679c\u3002</li> </ul> <p>\u8bba\u6587\u4fe1\u606f\uff1a O'Malley D, Golden J K, Vesselinov V V. Learning to regularize with a variational autoencoder for hydrologic inverse analysis[J]. arXiv preprint arXiv:1906.02401, 2019.</p> <p>\u53c2\u8003GitHub\u5730\u5740\uff1a https://github.com/madsjulia/RegAE.jl</p> <p>\u9879\u76eeaistudio\u5730\u5740\uff1a https://aistudio.baidu.com/aistudio/projectdetail/5541961</p> <p>\u6a21\u578b\u7ed3\u6784 </p>"},{"location":"zh/examples/RegAE/#2","title":"2.\u6570\u636e\u96c6","text":"<p>\u672c\u9879\u76ee\u6570\u636e\u96c6\u901a\u8fc7\u4f5c\u8005\u63d0\u4f9b\u7684julia\u4ee3\u7801\u751f\u6210\uff0c\u751f\u6210\u540e\u4fdd\u5b58\u4e3anpz\u6587\u4ef6\uff0c\u5df2\u4e0a\u4f20aistudio\u6570\u636e\u96c6\u5e76\u5173\u8054\u672c\u9879\u76ee\u3002</p> <p>\u4ee5\u4e0b\u4e3a\u6570\u636e\u751f\u6210\u548c\u6570\u636e\u4fdd\u5b58\u4ee3\u7801\u7684\u8bf4\u660e</p> <p>\uff081\uff09\u4f5c\u8005\u901a\u8fc7julia\u4e2d\u7684DPFEHM\u548cGaussianRandomFields\u8fdb\u884c\u6570\u636e\u751f\u6210\uff0c\u4ee3\u7801\u53ef\u53c2\u8003\u672c\u9879\u76ee/home/aistudio/RegAE.jl/examples/hydrology/ex_gaussian.jl\uff0c\u53ef\u6839\u636e\u5176\u4e2d\u53c2\u6570\u8fdb\u884c\u4fee\u6539\uff1b</p> <p>\uff082\uff09\u6570\u636e\u4fdd\u5b58\u4ee3\u7801\u3002\u5728/home/aistudio/RegAE.jl/examples/hydrology/ex.jl\u4ee3\u7801\u4e2d\u589e\u52a0\u4ee5\u4e0b\u4ee3\u7801\uff0c\u53ef\u5c06\u6570\u636e\u901a\u8fc7\u8f6c\u6362\u4e3anumpy\u6570\u636e\u5e76\u4fdd\u5b58\u4e3anpz\u3002</p> <pre><code>using Distributed\nusing PyCall # \u589e\u52a0\u6b64\u5904\u5f15\u7528\n\n@everywhere variablename = \"allloghycos\"\n@everywhere datafilename = \"$(results_dir)/trainingdata.jld2\"\nif !isfile(datafilename)\n    if nworkers() == 1\n        error(\"Please run in parallel: julia -p 32\")\n    end\n    numsamples = 10^5\n    @time allloghycos = SharedArrays.SharedArray{Float32}(numsamples, ns[2], ns[1]; init=A -&gt; samplehyco!(A; setseed=true))\n    # @time JLD2.@save datafilename allloghycos\n\n    ########### \u6b64\u5904\u4e3a\u589e\u52a0\u90e8\u5206 ###########\n    p_trues = SharedArrays.SharedArray{Float32}(3, ns[2], ns[1]; init=samplehyco!) # \u8ba1\u7b97p_true\n\n    np = pyimport(\"numpy\")\n    training_data = np.asarray(allloghycos)\n    test_data = np.asarray(p_trues)\n\n    np_coords = np.asarray(coords)\n    np_neighbors = np.asarray(neighbors)\n    np_areasoverlengths = np.asarray(areasoverlengths)\n    np_dirichletnodes = np.asarray(dirichletnodes)\n    np_dirichletheads = np.asarray(dirichletheads)\n\n    np.savez(\"$(results_dir)/gaussian_train.npz\",\n        data=training_data,\n        test_data=test_data,\n        coords=np_coords,\n        neighbors=np_neighbors,\n        areasoverlengths=np_areasoverlengths,\n        dirichletnodes=np_dirichletnodes,\n        dirichletheads=np_dirichletheads)\nend\n</code></pre>"},{"location":"zh/examples/RegAE/#_1","title":"\u6570\u636e\u6807\u51c6\u5316","text":"<ul> <li>\u6570\u636e\u6807\u51c6\u5316\u65b9\u5f0f\uff1a \\(z = (x - \\mu)/ \\sigma\\)</li> </ul> <pre><code>  class ScalerStd(object):\n      \"\"\"\n      Desc: Normalization utilities with std mean\n      \"\"\"\n\n      def __init__(self):\n          self.mean = 0.\n          self.std = 1.\n\n      def fit(self, data):\n          self.mean = np.mean(data)\n          self.std = np.std(data)\n\n      def transform(self, data):\n          mean = paddle.to_tensor(self.mean).type_as(data).to(\n              data.device) if paddle.is_tensor(data) else self.mean\n          std = paddle.to_tensor(self.std).type_as(data).to(\n              data.device) if paddle.is_tensor(data) else self.std\n          return (data - mean) / std\n\n      def inverse_transform(self, data):\n          mean = paddle.to_tensor(self.mean) if paddle.is_tensor(data) else self.mean\n          std = paddle.to_tensor(self.std) if paddle.is_tensor(data) else self.std\n          return (data * std) + mean\n</code></pre>"},{"location":"zh/examples/RegAE/#dataset","title":"\u5b9a\u4e49Dataset","text":"<ol> <li>\u901a\u8fc7\u8bfb\u53d6\u9884\u4fdd\u5b58npz\u52a0\u8f7d\u6570\u636e\uff0c\u5f53\u524d\u6570\u636e\u7c7b\u578b\u4e3a [data_nums, 100, 100], \u6b64\u5904100\u4e3a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u4e2d\u6307\u5b9a</li> <li>\u6570\u636ereshape\u4e3a [data_nums, 10000]</li> <li>\u6570\u636e\u5212\u5206\u4e3a8:2\u7528\u4e0etrain\u548ctest</li> <li>\u901a\u8fc7\u5bf9train\u6570\u636e\u5f97\u5230\u6807\u51c6\u5316\u53c2\u6570mean\u548cstd\uff0c\u5e76\u7528\u6b64\u53c2\u6570\u6807\u51c6\u5316train\u548ctest\u6570\u636e\u96c6</li> <li>\u901a\u8fc7dataloader\u5f97\u5230\u7684\u6570\u636e\u5f62\u5f0f\u4e3a [batch_size, 10000]</li> </ol> <pre><code>   class CustomDataset(Dataset):\n       def __init__(self, file_path, data_type=\"train\"):\n           \"\"\"\n\n           :param file_path:\n           :param data_type: train or test\n           \"\"\"\n           super().__init__()\n           all_data = np.load(file_path)\n           data = all_data[\"data\"]\n           num, _, _ = data.shape\n           data = data.reshape(num, -1)\n\n           self.neighbors = all_data['neighbors']\n           self.areasoverlengths = all_data['areasoverlengths']\n           self.dirichletnodes = all_data['dirichletnodes']\n           self.dirichleths = all_data['dirichletheads']\n           self.Qs = np.zeros([all_data['coords'].shape[-1]])\n           self.val_data = all_data[\"test_data\"]\n\n           self.data_type = data_type\n\n           self.train_len = int(num * 0.8)\n           self.test_len = num - self.train_len\n\n           self.train_data = data[:self.train_len]\n           self.test_data = data[self.train_len:]\n\n           self.scaler = ScalerStd()\n           self.scaler.fit(self.train_data)\n\n           self.train_data = self.scaler.transform(self.train_data)\n           self.test_data = self.scaler.transform(self.test_data)\n\n       def __getitem__(self, idx):\n           if self.data_type == \"train\":\n               return self.train_data[idx]\n           else:\n               return self.test_data[idx]\n\n       def __len__(self):\n           if self.data_type == \"train\":\n               return self.train_len\n           else:\n               return self.test_len\n</code></pre>"},{"location":"zh/examples/RegAE/#iterablenpzdataset","title":"\u5c06\u6570\u636e\u8f6c\u6362\u4e3aIterableNPZDataset\u7684\u5f62\u5f0f","text":"<pre><code>np.savez(\"data.npz\", p_train=train_data.train_data, p_test=train_data.test_data)\n</code></pre>"},{"location":"zh/examples/RegAE/#3","title":"3.\u73af\u5883\u4f9d\u8d56","text":"<p>\u672c\u9879\u76ee\u4e3ajulia\u548cpython\u6df7\u5408\u9879\u76ee\u3002</p>"},{"location":"zh/examples/RegAE/#julia","title":"julia\u4f9d\u8d56","text":"<ul> <li>DPFEHM</li> <li>Zygote</li> </ul>"},{"location":"zh/examples/RegAE/#python","title":"python\u4f9d\u8d56","text":"<ul> <li>paddle</li> <li>julia (pip\u5b89\u88c5)</li> <li>matplotlib</li> </ul> <p>\u672c\u9879\u76ee\u5df2\u7ecf\u63d0\u4f9b\u5b89\u88c5\u540e\u538b\u7f29\u6587\u6863\uff0c\u53effork\u672c\u9879\u76ee\u540e\u6267\u884c\u4ee5\u4e0b\u4ee3\u7801\u8fdb\u884c\u89e3\u538b\u5b89\u88c5\u3002</p> <pre><code># \u89e3\u538b\u9884\u4e0b\u8f7d\u6587\u4ef6\u548c\u9884\u7f16\u8bd1\u6587\u4ef6\n!tar zxf /home/aistudio/opt/curl-7.88.1.tar.gz -C /home/aistudio/opt # curl \u9884\u4e0b\u8f7d\u6587\u4ef6\n!tar zxf /home/aistudio/opt/curl-7.88.1-build.tgz -C /home/aistudio/opt # curl \u9884\u7f16\u8bd1\u6587\u4ef6\n!tar zxf /home/aistudio/opt/julia-1.8.5-linux-x86_64.tar.gz -C /home/aistudio/opt # julia \u9884\u4e0b\u8f7d\u6587\u4ef6\n!tar zxf /home/aistudio/opt/julia_package.tgz -C /home/aistudio/opt # julia\u4f9d\u8d56\u5e93\u6587\u4ef6\n!tar zxf /home/aistudio/opt/external-libraries.tgz -C /home/aistudio/opt # pip\u4f9d\u8d56\u5e93\u6587\u4ef6\n</code></pre> <pre><code>####### \u4ee5\u4e0b\u6307\u4ee4\u9700\u8981\u65f6\u53ef\u53c2\u8003\u6267\u884c\uff0c\u4e0a\u8ff0\u538b\u7f29\u5305\u5df2\u7ecf\u5b8c\u6210\u4ee5\u4e0b\u5185\u5bb9 #######\n\n# curl \u7f16\u8bd1\u6307\u4ee4\uff0c\u5f53\u89e3\u538b\u540e\u65e0\u6548\u4f7f\u7528\n!mkdir -p /home/aistudio/opt/curl-7.88.1-build\n!/home/aistudio/opt/curl-7.88.1/configure --prefix=/home/aistudio/opt/curl-7.88.1-build --with-ssl --enable-tls-srp\n!make install -j4\n\n# \u6307\u5b9acurl\u9884\u7f16\u8bd1\u6587\u4ef6\n!export LD_LIBRARY_PATH=/home/aistudio/opt/curl-7.88.1-build/lib:$LD_LIBRARY_PATH\n!export PATH=/home/aistudio/opt/curl-7.88.1-build/bin:$PATH\n!export CPATH=/home/aistudio/opt/curl-7.88.1-build/include:$CPATH\n!export LIBRARY_PATH=/home/aistudio/opt/curl-7.88.1-build/lib:$LIBRARY_PATH\n\n# \u6307\u5b9a\u5df2\u7ecf\u5b89\u88c5\u7684julia\u5305\n!export JULIA_DEPOT_PATH=/home/aistudio/opt/julia_package\n# \u6307\u5b9ajulia\u4f7f\u7528\u6e05\u534e\u6e90\n!export JULIA_PKG_SERVER=https://mirrors.tuna.tsinghua.edu.cn/julia\n# julia \u5b89\u88c5\u4f9d\u8d56\u5e93\n# \u9700\u8981\u5148export JULIA_DEPOT_PATH \u73af\u5883\u53d8\u91cf\uff0c\u5426\u5219\u5b89\u88c5\u4f4d\u7f6e\u4e3a~/.julia, aistudio\u65e0\u6cd5\u4fdd\u5b58\n!/home/aistudio/opt/julia-1.8.5/bin/julia -e \"using Pkg; Pkg.add(\\\"DPFEHM\\\")\"\n!/home/aistudio/opt/julia-1.8.5/bin/julia -e \"using Pkg; Pkg.add(\\\"Zygote\\\")\"\n!/home/aistudio/opt/julia-1.8.5/bin/julia -e \"using Pkg; Pkg.add(\\\"PyCall\\\")\"\n</code></pre> <p>\u4f7f\u7528\u65b9\u6cd5\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u4ee3\u7801\u548cjulia\u5bfc\u6570\u4f20\u9012\u6d4b\u8bd5.ipynb\u6587\u4ef6\u3002</p> <pre><code>import paddle\nimport os\nimport sys\n\n# julia \u4f9d\u8d56\nos.environ['JULIA_DEPOT_PATH'] = '/home/aistudio/opt/julia_package'\n# pip \u4f9d\u8d56\nsys.path.append('/home/aistudio/opt/external-libraries')\n\n# julieries\nfrom julia.api import Julia\n\njl = Julia(compiled_modules=False,runtime=\"/home/aistudio/opt/julia-1.8.5/bin/julia\")\n# import julia\nfrom julia import Main\n</code></pre>"},{"location":"zh/examples/RegAE/#4","title":"4.\u5feb\u901f\u5f00\u59cb","text":"<p>\u672c\u9879\u76ee\u8fd0\u884c\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a</p> <ul> <li>\uff081\uff09\u8bad\u7ec3\u6b65\u9aa4\u3002\u901a\u8fc7\u8fd0\u884ctrain.ipynb\u6587\u4ef6\uff0c\u53ef\u4ee5\u5f97\u5230\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u53c2\u6570\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u53c2\u8003train.ipynb\u6587\u4ef6\u53ca\u5176\u4e2d\u6ce8\u91ca\u8bf4\u660e\uff1b</li> <li>\uff082\uff09\u6d4b\u8bd5\u6b65\u9aa4\u3002\u901a\u8fc7\u8fd0\u884ctest.ipynb\u6587\u4ef6\uff0c\u5e94\u7528\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u53c2\u6570\uff0c\u5bf9latent domain\u8fdb\u884c\u4f18\u5316\u3002</li> </ul>"},{"location":"zh/examples/RegAE/#5","title":"5.\u4ee3\u7801\u7ed3\u6784\u4e0e\u8be6\u7ec6\u8bf4\u660e","text":"<pre><code>\u251c\u2500\u2500 data                        #\u9884\u751f\u6210\u6570\u636e\u6587\u4ef6\n\u2502   \u2514\u2500\u2500 data193595\n\u251c\u2500\u2500 main.ipynb                  #\u672c\u8bf4\u660e\u6587\u4ef6\n\u251c\u2500\u2500 opt                         #\u73af\u5883\u914d\u7f6e\u6587\u4ef6\uff0c\u5df2\u538b\u7f29\uff0c\u89e3\u538b\u5373\u53ef\u4f7f\u7528\n\u2502   \u251c\u2500\u2500 curl-7.88.1\n\u2502   \u251c\u2500\u2500 curl-7.88.1-build\n\u2502   \u251c\u2500\u2500 curl-7.88.1-build.tgz\n\u2502   \u251c\u2500\u2500 curl-7.88.1.tar.gz\n\u2502   \u251c\u2500\u2500 external-libraries\n\u2502   \u251c\u2500\u2500 external-libraries.tgz\n\u2502   \u251c\u2500\u2500 julia-1.8.5\n\u2502   \u251c\u2500\u2500 julia-1.8.5-linux-x86_64.tar.gz\n\u2502   \u251c\u2500\u2500 julia_package\n\u2502   \u2514\u2500\u2500 julia_package.tgz\n\u251c\u2500\u2500 params_vae_nz100            #\u6a21\u578b\u53c2\u6570\u6587\u4ef6\n\u2502   \u2514\u2500\u2500 model.pdparams\n\u251c\u2500\u2500 params_vae_nz200\n\u2502   \u2514\u2500\u2500 model.pdparams\n\u251c\u2500\u2500 params_vae_nz400\n\u2502   \u2514\u2500\u2500 model.pdparams\n\u251c\u2500\u2500 test.ipynb                  #\u6d4b\u8bd5\u6587\u4ef6\n\u251c\u2500\u2500 train.ipynb                 #\u8bad\u7ec3\u6587\u4ef6\n\u251c\u2500\u2500 julia\u5bfc\u6570\u4f20\u9012\u6d4b\u8bd5.ipynb       #julia\u548cpython\u6df7\u5408\u6d4b\u8bd5\u6587\u4ef6\n</code></pre>"},{"location":"zh/examples/RegAE/#traintest","title":"train\u6587\u4ef6\u548ctest\u6587\u4ef6\u5173\u8054\u6027\u8bf4\u660e","text":"<p>\u6211\u4eec\u4f9d\u7167\u8bba\u6587\u4f5c\u8005\u7684\u7b26\u53f7\u8fdb\u884c\u8bf4\u660e\uff0c\\(p\\)\u4e3a\u6570\u636e\u8f93\u5165\uff0c\\(\\hat{p}\\)\u4e3a\u6570\u636e\u8f93\u51fa\uff0c\\(loss=mse(p,\\hat{p}) + loss_{kl}(\\hat{p},N(0,1))\\)\u3002</p> <ul> <li>\uff081\uff09\u901a\u8fc7train\u80fd\u591f\u5f97\u5230\u8bad\u7ec3\u540e\u7684Autoencoder(\u5305\u542bencoder\u548cdecoder)\uff1b</li> <li>\uff082\uff09\u901a\u8fc7test\u8c03\u7528\u8bad\u7ec3\u540e\u7684encoder\u9488\u5bf9testdata\u751f\u6210latent_test\uff0c\u5e76\u5f97\u5230latent_mean\uff1b</li> <li>\uff083\uff09\u9488\u5bf9\u65b0\u751f\u6210\u7684\u6837\u672c\\(p_{new}\\)\uff0c\u901a\u8fc7LBFGS\u65b9\u6cd5\u4e0d\u65ad\u4f18\u5316latent_mean\uff0c\u76f4\u5230obj_fun\u6700\u5c0f\uff0c\u5176\u4e2dobj_fun = mse(\\(p_{new}\\),\\(\\hat{p}_{new}\\))+mse(sci_fun(\\(p_{new}\\)),sci_fun(\\(\\hat{p}_{new}\\)))\uff0csci_fun\u4e3a\u4efb\u4f55\u5176\u4ed6\u79d1\u5b66\u8ba1\u7b97\u6a21\u62df\u65b9\u6cd5\u3002</li> </ul>"},{"location":"zh/examples/RegAE/#paddleincubateoptimizerfunctionalminimize_lbfgs","title":"paddle.incubate.optimizer.functional.minimize_lbfgs \u95ee\u9898","text":"<p>\u4ee5\u4e0b\u4e3apaddle\u5b98\u65b9minimize_lbfgs API:</p> <pre><code>paddle.incubate.optimizer.functional.minimize_lbfgs(objective_func, initial_position, history_size=100, max_iters=50, tolerance_grad=1e-08, tolerance_change=1e-08, initial_inverse_hessian_estimate=None, line_search_fn='strong_wolfe', max_line_search_iters=50, initial_step_length=1.0, dtype='float32', name=None)\n</code></pre> <ul> <li>\uff081\uff09\u53c2\u6570max_line_search_iters\u65e0\u6548\u3002\u867d\u7136\u8bbe\u7f6e\u4e86\u6b64\u53c2\u6570\uff0c\u4f46\u662f\u5185\u90e8\u6ca1\u6709\u4f20\u9012\u5bf9\u5e94\u53c2\u6570\uff1b</li> <li>\uff082\uff09\u4e2dwolfe\u6761\u4ef61\u9519\u8bef\u3002line256\u5904\u5e94\u4e3a<code>phi_2 &gt;= phi_1</code>\uff0c\u4ee5\u4e0b\u4e3apaddle\u90e8\u5206\u6e90\u7801\u3002</li> </ul> <pre><code>        # 1. If phi(a2) &gt; phi(0) + c_1 * a2 * phi'(0) or [phi(a2) &gt;= phi(a1) and i &gt; 1],\n        #         a_star= zoom(a1, a2) and stop;\n        pred1 = ~done &amp; ((phi_2 &gt; phi_0 + c1 * a2 * derphi_0) |\n                         ((phi_2 &gt;= phi_0) &amp; (i &gt; 1)))\n</code></pre>"},{"location":"zh/examples/RegAE/#6","title":"6.\u590d\u73b0\u7ed3\u679c","text":""},{"location":"zh/examples/RegAE/#latent","title":"\u4e0d\u540clatent\u7ef4\u5ea6\u5bf9\u6bd4","text":"<p>\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\u53ef\u4ee5\u53d1\u73b0\uff1a</p> <ul> <li>\uff081\uff09\u4e0d\u540c\u6837\u672c\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u5e76\u4e0d\u662f\u6240\u6709\u6837\u672c\u90fd\u80fd\u4f18\u5316\u5f97\u5230\u826f\u597d\u7684latent\u53d8\u91cf\uff1b</li> <li>\uff082\uff09\u968f\u7740\u6a21\u578blatent\u7ef4\u5ea6\u7684\u4e0a\u5347\uff0c\u6a21\u578b\u6548\u679c\u9010\u6e10\u63d0\u5347\u3002</li> </ul>"},{"location":"zh/examples/RegAE/#latent_randomlatent_mean","title":"latent_random\u548clatent_mean\u5bf9\u6bd4","text":"<p>\u672c\u9879\u76ee\u8fd8\u589e\u52a0\u4e86latent_random\u548clatent_mean\u5bf9\u751f\u6210\u7ed3\u679c\u7684\u5bf9\u6bd4\u3002\u6b64\u5904\u5bf9latent_random\u548clatent_mean\u518d\u6b21\u8bf4\u660e\uff1a</p> <ul> <li>latent_random\uff1a\u901a\u8fc7paddle.randn\u751f\u6210\u7684\u9ad8\u65af\u566a\u58f0\u5f97\u5230\uff1b</li> <li>latent_mean\uff1a\u901a\u8fc7\u5bf9\u6240\u6709testdata\u8fdb\u884cencoder\u7ed3\u679c\u5e73\u5747\u5f97\u5230\u3002</li> </ul> <p>\u4ee5\u4e0b\u4e3a\u901a\u8fc7latent_random\u5f97\u5230\u7684\u5b9e\u9a8c\u7ed3\u679c </p> <p>\u901a\u8fc7\u5bf9\u6bd4\uff0c\u53ef\u4ee5\u53d1\u73b0latent_mean\u5bf9\u4f18\u5316\u7ed3\u679c\u91cd\u8981\u5f71\u54cd\u3002\u8fd1\u4f3c\u6b63\u786e\u7684latent\u53d8\u91cf\u80fd\u591f\u5f97\u5230\u66f4\u4f18\u7684\u751f\u6210\u7ed3\u679c\u3002</p>"},{"location":"zh/examples/RegAE/#lbfgs","title":"LBFGS\u4f18\u5316\u6536\u655b\u60c5\u51b5","text":"<p>\u53ef\u4ee5\u4ece\u5982\u4e0b\u56fe\u4e2d\u770b\u51fa\uff0c\u4f7f\u7528paddle minimize_lbfgs\u80fd\u591f\u6709\u6548\u4f18\u5316\u6536\u655b\u3002 </p>"},{"location":"zh/examples/RegAE/#7","title":"7.\u5ef6\u4f38\u601d\u8003","text":"<p>\u5982\u679c\u6df1\u5165\u601d\u8003\u672c\u9879\u76ee\uff0c\u4f1a\u53d1\u73b0\u6a21\u578b\u5728test\u8fc7\u7a0b\u4e2d\u662f\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u4f5c\u4e3a\u76ee\u6807\u8fdb\u884clbfgs\u4f18\u5316\uff0c\u8fd9\u79cd\u8ba1\u7b97\u65b9\u5f0f\u8fd8\u6709\u610f\u4e49\u5417\uff1f</p> <p>\u56de\u7b54\u662f\u80af\u5b9a\u7684\uff01\u6709\u610f\u4e49\uff01</p> <p>\u4ee5\u4e0b\u4e3a\u672c\u4eba\u4e2a\u4eba\u89c2\u70b9\uff1a</p> <ul> <li>\uff081\uff09\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4latent_random\u548clatent_mean\u7684\u6700\u7ec8\u751f\u6210\u7ed3\u679c\u5dee\u8ddd\uff0c\u53ef\u4ee5\u53d1\u73b0\u4e00\u4e2a\u826f\u597d\u7684\u521d\u503c\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u662f\u5de8\u5927\u7684\u3002\u5f53\u524ddiffusion\u6a21\u578b\u5728sample\u751f\u6210\u8fc7\u7a0b\u5bf9\u751f\u6210\u7684\u9ad8\u65af\u566a\u58f0\u4e0d\u65ad\u505adenoise\u64cd\u4f5c\uff0c\u8fd9\u5176\u4e2d\u751f\u6210\u7684\u566a\u58f0\u6570\u636e\u5982\u679c\u7ecf\u8fc7\u9884\u5148\u4f18\u5316\uff0c\u4e0d\u4ec5\u80fd\u591f\u52a0\u901fdiffusion\u7684\u751f\u6210\u901f\u5ea6\uff0c\u800c\u4e14\u80fd\u591f\u63d0\u5347\u6570\u636e\u7684\u751f\u6210\u8d28\u91cf\u3002</li> <li>\uff082\uff09\u5728\u57df\u8fc1\u79fb\u7b49\u7814\u7a76\u9886\u57df\uff0c\u53ef\u4ee5\u4f7f\u7528\u8fd9\u79cdlatent\u9010\u6e10\u751f\u6210\u4e2d\u95f4\u8fc7\u6e21\u53d8\u91cf\uff0c\u8fbe\u5230\u4e0d\u540c\u57df\u6570\u636e\u7684\u8fc1\u79fb\u751f\u6210\u3002</li> </ul>"},{"location":"zh/examples/RegAE/#7_1","title":"7.\u6a21\u578b\u4fe1\u606f","text":"\u4fe1\u606f \u8bf4\u660e \u53d1\u5e03\u8005 \u6731\u536b\u56fd (DrownFish19) \u53d1\u5e03\u65f6\u95f4 2023.03 \u6846\u67b6\u7248\u672c paddle 2.4.1 \u652f\u6301\u786c\u4ef6 GPU\u3001CPU aistudio notebook <p>\u8bf7\u70b9\u51fb\u6b64\u5904\u67e5\u770b\u672c\u73af\u5883\u57fa\u672c\u7528\u6cd5. Please click here for more detailed instructions.</p>"},{"location":"zh/examples/allen_cahn/","title":"AllenCahn","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#allen-cahn","title":"Allen-Cahn","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/AllenCahn/allen_cahn.mat -P ./dataset/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/AllenCahn/allen_cahn.mat --create-dirs -o ./dataset/allen_cahn.mat\npython allen_cahn_piratenet.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/AllenCahn/allen_cahn.mat -P ./dataset/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/AllenCahn/allen_cahn.mat --create-dirs -o ./dataset/allen_cahn.mat\npython allen_cahn_piratenet.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/AllenCahn/allen_cahn_piratenet_pretrained.pdparams\n</code></pre> <pre><code>python allen_cahn_piratenet.py mode=export\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/AllenCahn/allen_cahn.mat -P ./dataset/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/AllenCahn/allen_cahn.mat --create-dirs -o ./dataset/allen_cahn.mat\npython allen_cahn_piratenet.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 allen_cahn_piratenet_pretrained.pdparams L2Rel.u: 1.2e-05","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>Allen-Cahn \u65b9\u7a0b\uff08\u6709\u65f6\u4e5f\u53eb\u4f5c\u6a21\u578b\u65b9\u7a0b\u6216\u76f8\u573a\u65b9\u7a0b\uff09\u662f\u4e00\u79cd\u6570\u5b66\u6a21\u578b\uff0c\u901a\u5e38\u7528\u4e8e\u63cf\u8ff0\u4e24\u79cd\u4e0d\u540c\u76f8\u4e4b\u95f4\u7684\u754c\u9762\u6f14\u5316\u3002\u8fd9\u4e2a\u65b9\u7a0b\u6700\u65e9\u7531Samuel Allen\u548cJohn Cahn\u57281970\u5e74\u4ee3\u63d0\u51fa\uff0c\u7528\u4ee5\u63cf\u8ff0\u5408\u91d1\u4e2d\u76f8\u5206\u79bb\u7684\u8fc7\u7a0b\u3002Allen-Cahn \u65b9\u7a0b\u662f\u4e00\u79cd\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u5176\u4e00\u822c\u5f62\u5f0f\u53ef\u4ee5\u5199\u4e3a\uff1a</p> \\[ \\frac{\\partial u}{\\partial t} = \\varepsilon^2 \\Delta u - F'(u) \\] <p>\u8fd9\u91cc\uff1a</p> <ul> <li>\\(u(\\mathbf{x},t)\\) \u662f\u4e00\u4e2a\u573a\u53d8\u91cf\uff0c\u4ee3\u8868\u67d0\u4e2a\u7269\u7406\u91cf\uff0c\u4f8b\u5982\u5408\u91d1\u7684\u7ec4\u5206\u6d53\u5ea6\u6216\u8005\u6676\u4f53\u4e2d\u7684\u6709\u5e8f\u53c2\u6570\u3002</li> <li>\\(t\\) \u8868\u793a\u65f6\u95f4\u3002</li> <li>\\(\\mathbf{x}\\) \u8868\u793a\u7a7a\u95f4\u4f4d\u7f6e\u3002</li> <li>\\(\\Delta\\) \u662fLaplace\u7b97\u5b50\uff0c\u5bf9\u5e94\u4e8e\u7a7a\u95f4\u53d8\u91cf\u7684\u4e8c\u9636\u504f\u5bfc\u6570\uff08\u5373 \\(\\Delta u = \\nabla^2 u\\) \uff09\uff0c\u7528\u6765\u63cf\u8ff0\u7a7a\u95f4\u6269\u6563\u8fc7\u7a0b\u3002</li> <li>\\(\\varepsilon\\) \u662f\u4e00\u4e2a\u6b63\u7684\u5c0f\u53c2\u6570\uff0c\u5b83\u4e0e\u76f8\u754c\u9762\u7684\u5bbd\u5ea6\u76f8\u5173\u3002</li> <li>\\(F(u)\\) \u662f\u4e00\u4e2a\u53cc\u7a33\u6001\u52bf\u80fd\u51fd\u6570\uff0c\u901a\u5e38\u53d6\u4e3a\\(F(u) = \\frac{1}{4}(u^2-1)^2\\)\uff0c\u8fd9\u4f7f\u5f97 \\(F'(u) = u^3 - u\\) \u662f\u5176\u5bfc\u6570\uff0c\u8fd9\u4ee3\u8868\u4e86\u975e\u7ebf\u6027\u7684\u53cd\u5e94\u9879\uff0c\u8d1f\u8d23\u9a71\u52a8\u7cfb\u7edf\u5411\u7a33\u5b9a\u72b6\u6001\u6f14\u5316\u3002</li> </ul> <p>\u8fd9\u4e2a\u65b9\u7a0b\u4e2d\u7684 \\(F'(u)\\) \u9879\u4f7f\u5f97\u5728 \\(u=1\\) \u548c \\(u=-1\\) \u9644\u8fd1\u6709\u4e24\u4e2a\u7a33\u5b9a\u7684\u5e73\u8861\u6001\uff0c\u8fd9\u5bf9\u5e94\u4e8e\u4e0d\u540c\u7684\u7269\u7406\u76f8\u3002\u800c \\(\\varepsilon^2 \\Delta u\\) \u9879\u5219\u63cf\u8ff0\u4e86\u76f8\u754c\u9762\u7684\u66f2\u7387\u5f15\u8d77\u7684\u6269\u6563\u6548\u5e94\uff0c\u8fd9\u5bfc\u81f4\u754c\u9762\u8d8b\u5411\u4e8e\u51cf\u5c0f\u66f2\u7387\u3002\u56e0\u6b64\uff0cAllen-Cahn \u65b9\u7a0b\u63cf\u8ff0\u4e86\u7531\u4e8e\u76f8\u754c\u9762\u66f2\u7387\u548c\u52bf\u80fd\u5f71\u54cd\u800c\u53d1\u751f\u7684\u76f8\u53d8\u3002</p> <p>\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u7a0b\u8fd8\u53ef\u80fd\u5305\u542b\u8fb9\u754c\u6761\u4ef6\u548c\u521d\u59cb\u6761\u4ef6\uff0c\u4ee5\u4fbf\u5bf9\u7279\u5b9a\u95ee\u9898\u8fdb\u884c\u6570\u503c\u6a21\u62df\u548c\u5206\u6790\u3002\u4f8b\u5982\uff0c\u5728\u7279\u5b9a\u7684\u7269\u7406\u95ee\u9898\u4e2d\uff0c\u53ef\u80fd\u4f1a\u6709 Neumann \u8fb9\u754c\u6761\u4ef6\uff08\u5bfc\u6570\u4e3a\u96f6\uff0c\u8868\u793a\u65e0\u901a\u91cf\u7a7f\u8fc7\u8fb9\u754c\uff09\u6216 Dirichlet \u8fb9\u754c\u6761\u4ef6\uff08\u56fa\u5b9a\u7684\u8fb9\u754c\u503c\uff09\u3002</p> <p>\u672c\u6848\u4f8b\u89e3\u51b3\u4ee5\u4e0b Allen-Cahn \u65b9\u7a0b\uff1a</p> \\[ \\begin{aligned}     &amp; u_t - 0.0001 u_{xx} + 5 u^3 - 5 u  = 0,\\quad t \\in [0, 1],\\ x\\in[-1, 1],\\\\     &amp;u(x,0) = x^2 \\cos(\\pi x),\\\\     &amp;u(t, -1) = u(t, 1),\\\\     &amp;u_x(t, -1) = u_x(t, 1). \\end{aligned} \\]","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u6839\u636e\u4e0a\u8ff0\u65b9\u7a0b\uff0c\u53ef\u77e5\u8ba1\u7b97\u57df\u4e3a\\([0, 1]\\times [-1, 1]\\)\uff0c\u542b\u6709\u4e00\u4e2a\u521d\u59cb\u6761\u4ef6\uff1a \\(u(x,0) = x^2 \\cos(\\pi x)\\)\uff0c\u4e24\u4e2a\u5468\u671f\u8fb9\u754c\u6761\u4ef6\uff1a\\(u(t, -1) = u(t, 1)\\)\u3001\\(u_x(t, -1) = u_x(t, 1)\\)\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 Allen-Cahn \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((t, x)\\) \u90fd\u6709\u5bf9\u5e94\u7684\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf \\((u)\\)\uff0c \uff0c\u5728\u8fd9\u91cc\u4f7f\u7528 PirateNet \u6765\u8868\u793a \\((t, x)\\) \u5230 \\((u)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^1\\) \uff0c\u5373\uff1a</p> \\[ u = f(t, x) \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a PirateNet \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.PirateNet(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"t\", \"x\")</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>(\"u\")</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a PirateNet \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\uff0c\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 3 \u4e2a PiraBlock\uff0c\u6bcf\u4e2a PiraBlock \u7684\u9690\u5c42\u795e\u7ecf\u5143\u4e2a\u6570\u4e3a 256 \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\uff0c \u5e76\u4e14\u4f7f\u7528 <code>tanh</code> \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u3002</p> <pre><code># model settings\nMODEL:\n  input_keys: [t, x]\n  output_keys: [u]\n  num_blocks: 3\n  hidden_size: 256\n  activation: tanh\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>Allen-Cahn \u5fae\u5206\u65b9\u7a0b\u53ef\u4ee5\u7528\u5982\u4e0b\u4ee3\u7801\u8868\u793a\uff1a</p> <pre><code># set equation\nequation = {\"AllenCahn\": ppsci.equation.AllenCahn(eps=0.01)}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u7684\u8ba1\u7b97\u57df\u4e3a \\([0, 1]\\times [-1, 1]\\)\uff0c\u5176\u4e2d\u7528\u4e8e\u8bad\u7ec3\u7684\u6570\u636e\u5df2\u63d0\u524d\u751f\u6210\uff0c\u4fdd\u5b58\u5728 <code>./dataset/allen_cahn.mat</code> \u4e2d\uff0c\u8bfb\u53d6\u5e76\u751f\u6210\u8ba1\u7b97\u57df\u5185\u7684\u79bb\u6563\u70b9\u3002</p> <pre><code># set constraint\ndata = sio.loadmat(cfg.DATA_PATH)\nu_ref = data[\"usol\"].astype(dtype)  # (nt, nx)\nt_star = data[\"t\"].flatten().astype(dtype)  # [nt, ]\nx_star = data[\"x\"].flatten().astype(dtype)  # [nx, ]\n\nu0 = u_ref[0, :]  # [nx, ]\n\nt0 = t_star[0]  # float\nt1 = t_star[-1]  # float\n\nx0 = x_star[0]  # float\nx1 = x_star[-1]  # float\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u5185\u90e8\u70b9\u4e0a\u7684 <code>SupervisedConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>def gen_label_batch(input_batch):\n    return {\"allen_cahn\": np.zeros([cfg.TRAIN.batch_size, 1], dtype)}\n\npde_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"ContinuousNamedArrayDataset\",\n            \"input\": gen_input_batch,\n            \"label\": gen_label_batch,\n        },\n    },\n    output_expr=equation[\"AllenCahn\"].equations,\n    loss=ppsci.loss.CausalMSELoss(\n        cfg.TRAIN.causal.n_chunks, \"mean\", tol=cfg.TRAIN.causal.tol\n    ),\n    name=\"PDE\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u7528\u4e8e\u8bad\u7ec3\u7684\u6570\u636e\u914d\u7f6e\uff0c\u7531\u4e8e\u6211\u4eec\u4f7f\u7528\u5b9e\u65f6\u968f\u673a\u751f\u6210\u7684\u6570\u636e\uff0c\u800c\u4e0d\u662f\u56fa\u5b9a\u6570\u636e\u70b9\uff0c\u56e0\u6b64\u586b\u5165\u81ea\u5b9a\u4e49\u7684\u8f93\u5165\u6570\u636e/\u6807\u7b7e\u751f\u6210\u51fd\u6570\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u56e0\u6b64\u4f20\u5165 Allen-Cahn \u7684\u65b9\u7a0b\u5bf9\u8c61\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u9009\u7528 <code>CausalMSELoss</code> \u51fd\u6570\uff0c\u5176\u4f1a\u6839\u636e <code>causal</code> \u548c <code>tol</code> \u53c2\u6570\uff0c\u5bf9\u4e0d\u540c\u7684\u65f6\u95f4\u7a97\u53e3\u8fdb\u884c\u91cd\u65b0\u52a0\u6743\uff0c \u80fd\u66f4\u597d\u5730\u4f18\u5316\u77ac\u6001\u95ee\u9898\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"PDE\" \u5373\u53ef\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#342","title":"3.4.2 \u5468\u671f\u8fb9\u754c\u7ea6\u675f","text":"<p>\u6b64\u5904\u6211\u4eec\u91c7\u7528 hard-constraint \u7684\u65b9\u5f0f\uff0c\u5728\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\uff0c\u5bf9\u8f93\u5165\u6570\u636e\u4f7f\u7528cos\u3001sin\u7b49\u5468\u671f\u51fd\u6570\u8fdb\u884c\u5468\u671f\u5316\uff0c\u4ece\u800c\u8ba9\\(u_{\\theta}\\)\u5728\u6570\u5b66\u4e0a\u76f4\u63a5\u6ee1\u8db3\u65b9\u7a0b\u7684\u5468\u671f\u6027\u8d28\u3002 \u6839\u636e\u65b9\u7a0b\u53ef\u5f97\u51fd\u6570\\(u(t, x)\\)\u5728\\(x\\)\u8f74\u4e0a\u7684\u5468\u671f\u4e3a 2\uff0c\u56e0\u6b64\u5c06\u8be5\u5468\u671f\u8bbe\u7f6e\u5230\u6a21\u578b\u914d\u7f6e\u91cc\u5373\u53ef\u3002</p> <pre><code>periods:\n  x: [2.0, false]\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#343","title":"3.4.3 \u521d\u503c\u7ea6\u675f","text":"<p>\u7b2c\u4e09\u4e2a\u7ea6\u675f\u6761\u4ef6\u662f\u521d\u503c\u7ea6\u675f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>ic_input = {\"t\": np.full([len(x_star), 1], t0), \"x\": x_star.reshape([-1, 1])}\nic_label = {\"u\": u0.reshape([-1, 1])}\nic = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableNamedArrayDataset\",\n            \"input\": ic_input,\n            \"label\": ic_label,\n        },\n    },\n    output_expr={\"u\": lambda out: out[\"u\"]},\n    loss=ppsci.loss.MSELoss(\"mean\"),\n    name=\"IC\",\n)\n</code></pre> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u3001\u521d\u503c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    pde_constraint.name: pde_constraint,\n    ic.name: ic,\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 300 \u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c0.001 \u7684\u521d\u59cb\u5b66\u4e60\u7387\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 300\n  iters_per_epoch: 1000\n  save_freq: 10\n  eval_during_train: true\n  eval_freq: 10\n  lr_scheduler:\n    epochs: ${TRAIN.epochs}\n    iters_per_epoch: ${TRAIN.iters_per_epoch}\n    learning_rate: 1.0e-3\n    gamma: 0.9\n    decay_steps: 5000\n    by_epoch: false\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u914d\u5408\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u5e38\u7528\u7684 ExponentialDecay \u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u3002</p> <pre><code># set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n    **cfg.TRAIN.lr_scheduler\n)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\ntx_star = misc.cartesian_product(t_star, x_star).astype(dtype)\neval_data = {\"t\": tx_star[:, 0:1], \"x\": tx_star[:, 1:2]}\neval_label = {\"u\": u_ref.reshape([-1, 1])}\nu_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": eval_data,\n            \"label\": eval_label,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    {\"u\": lambda out: out[\"u\"]},\n    metric={\"L2Rel\": ppsci.metric.L2Rel()},\n    name=\"u_validator\",\n)\nvalidator = {u_validator.name: u_validator}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    optimizer=optimizer,\n    equation=equation,\n    validator=validator,\n    loss_aggregator=mtl.GradNorm(\n        model,\n        len(constraint),\n        cfg.TRAIN.grad_norm.update_freq,\n        cfg.TRAIN.grad_norm.momentum,\n    ),\n    cfg=cfg,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nu_pred = solver.predict(\n    eval_data, batch_size=cfg.EVAL.batch_size, return_numpy=True\n)[\"u\"]\nu_pred = u_pred.reshape([len(t_star), len(x_star)])\n\n# plot\nplot(t_star, x_star, u_ref, u_pred, cfg.output_dir)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"allen_cahn_piratenet.py<pre><code>\"\"\"\nReference: https://github.com/PredictiveIntelligenceLab/jaxpi/tree/main/examples/allen_cahn\n\"\"\"\n\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle\nimport scipy.io as sio\nfrom matplotlib import pyplot as plt\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.loss import mtl\nfrom ppsci.utils import misc\n\ndtype = paddle.get_default_dtype()\n\n\ndef plot(\n    t_star: np.ndarray,\n    x_star: np.ndarray,\n    u_ref: np.ndarray,\n    u_pred: np.ndarray,\n    output_dir: str,\n):\n    fig = plt.figure(figsize=(18, 5))\n    TT, XX = np.meshgrid(t_star, x_star, indexing=\"ij\")\n    u_ref = u_ref.reshape([len(t_star), len(x_star)])\n\n    plt.subplot(1, 3, 1)\n    plt.pcolor(TT, XX, u_ref, cmap=\"jet\")\n    plt.colorbar()\n    plt.xlabel(\"t\")\n    plt.ylabel(\"x\")\n    plt.title(\"Exact\")\n    plt.tight_layout()\n\n    plt.subplot(1, 3, 2)\n    plt.pcolor(TT, XX, u_pred, cmap=\"jet\")\n    plt.colorbar()\n    plt.xlabel(\"t\")\n    plt.ylabel(\"x\")\n    plt.title(\"Predicted\")\n    plt.tight_layout()\n\n    plt.subplot(1, 3, 3)\n    plt.pcolor(TT, XX, np.abs(u_ref - u_pred), cmap=\"jet\")\n    plt.colorbar()\n    plt.xlabel(\"t\")\n    plt.ylabel(\"x\")\n    plt.title(\"Absolute error\")\n    plt.tight_layout()\n\n    fig_path = osp.join(output_dir, \"ac.png\")\n    print(f\"Saving figure to {fig_path}\")\n    fig.savefig(fig_path, bbox_inches=\"tight\", dpi=400)\n    plt.close()\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.PirateNet(**cfg.MODEL)\n\n    # set equation\n    equation = {\"AllenCahn\": ppsci.equation.AllenCahn(eps=0.01)}\n\n    # set constraint\n    data = sio.loadmat(cfg.DATA_PATH)\n    u_ref = data[\"usol\"].astype(dtype)  # (nt, nx)\n    t_star = data[\"t\"].flatten().astype(dtype)  # [nt, ]\n    x_star = data[\"x\"].flatten().astype(dtype)  # [nx, ]\n\n    u0 = u_ref[0, :]  # [nx, ]\n\n    t0 = t_star[0]  # float\n    t1 = t_star[-1]  # float\n\n    x0 = x_star[0]  # float\n    x1 = x_star[-1]  # float\n\n    def gen_input_batch():\n        tx = np.random.uniform(\n            [t0, x0],\n            [t1, x1],\n            (cfg.TRAIN.batch_size, 2),\n        ).astype(dtype)\n        return {\n            \"t\": np.sort(tx[:, 0:1], axis=0),\n            \"x\": tx[:, 1:2],\n        }\n\n    def gen_label_batch(input_batch):\n        return {\"allen_cahn\": np.zeros([cfg.TRAIN.batch_size, 1], dtype)}\n\n    pde_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"ContinuousNamedArrayDataset\",\n                \"input\": gen_input_batch,\n                \"label\": gen_label_batch,\n            },\n        },\n        output_expr=equation[\"AllenCahn\"].equations,\n        loss=ppsci.loss.CausalMSELoss(\n            cfg.TRAIN.causal.n_chunks, \"mean\", tol=cfg.TRAIN.causal.tol\n        ),\n        name=\"PDE\",\n    )\n\n    ic_input = {\"t\": np.full([len(x_star), 1], t0), \"x\": x_star.reshape([-1, 1])}\n    ic_label = {\"u\": u0.reshape([-1, 1])}\n    ic = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableNamedArrayDataset\",\n                \"input\": ic_input,\n                \"label\": ic_label,\n            },\n        },\n        output_expr={\"u\": lambda out: out[\"u\"]},\n        loss=ppsci.loss.MSELoss(\"mean\"),\n        name=\"IC\",\n    )\n    # wrap constraints together\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        ic.name: ic,\n    }\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n        **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    # set validator\n    tx_star = misc.cartesian_product(t_star, x_star).astype(dtype)\n    eval_data = {\"t\": tx_star[:, 0:1], \"x\": tx_star[:, 1:2]}\n    eval_label = {\"u\": u_ref.reshape([-1, 1])}\n    u_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": eval_data,\n                \"label\": eval_label,\n            },\n            \"batch_size\": cfg.EVAL.batch_size,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"u\": lambda out: out[\"u\"]},\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"u_validator\",\n    )\n    validator = {u_validator.name: u_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        optimizer=optimizer,\n        equation=equation,\n        validator=validator,\n        loss_aggregator=mtl.GradNorm(\n            model,\n            len(constraint),\n            cfg.TRAIN.grad_norm.update_freq,\n            cfg.TRAIN.grad_norm.momentum,\n        ),\n        cfg=cfg,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    u_pred = solver.predict(\n        eval_data, batch_size=cfg.EVAL.batch_size, return_numpy=True\n    )[\"u\"]\n    u_pred = u_pred.reshape([len(t_star), len(x_star)])\n\n    # plot\n    plot(t_star, x_star, u_ref, u_pred, cfg.output_dir)\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.PirateNet(**cfg.MODEL)\n\n    data = sio.loadmat(cfg.DATA_PATH)\n    u_ref = data[\"usol\"].astype(dtype)  # (nt, nx)\n    t_star = data[\"t\"].flatten().astype(dtype)  # [nt, ]\n    x_star = data[\"x\"].flatten().astype(dtype)  # [nx, ]\n\n    # set validator\n    tx_star = misc.cartesian_product(t_star, x_star).astype(dtype)\n    eval_data = {\"t\": tx_star[:, 0:1], \"x\": tx_star[:, 1:2]}\n    eval_label = {\"u\": u_ref.reshape([-1, 1])}\n    u_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": eval_data,\n                \"label\": eval_label,\n            },\n            \"batch_size\": cfg.EVAL.batch_size,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"u\": lambda out: out[\"u\"]},\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"u_validator\",\n    )\n    validator = {u_validator.name: u_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        validator=validator,\n        cfg=cfg,\n    )\n\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    u_pred = solver.predict(\n        eval_data, batch_size=cfg.EVAL.batch_size, return_numpy=True\n    )[\"u\"]\n    u_pred = u_pred.reshape([len(t_star), len(x_star)])\n\n    # plot\n    plot(t_star, x_star, u_ref, u_pred, cfg.output_dir)\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.PirateNet(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(model, cfg=cfg)\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path, with_onnx=False)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n    data = sio.loadmat(cfg.DATA_PATH)\n    u_ref = data[\"usol\"].astype(dtype)  # (nt, nx)\n    t_star = data[\"t\"].flatten().astype(dtype)  # [nt, ]\n    x_star = data[\"x\"].flatten().astype(dtype)  # [nx, ]\n    tx_star = misc.cartesian_product(t_star, x_star).astype(dtype)\n\n    input_dict = {\"t\": tx_star[:, 0:1], \"x\": tx_star[:, 1:2]}\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n    u_pred = output_dict[\"u\"].reshape([len(t_star), len(x_star)])\n\n    plot(t_star, x_star, u_ref, u_pred, cfg.output_dir)\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"allen_cahn_piratenet.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u5728\u8ba1\u7b97\u57df\u4e0a\u5747\u5300\u91c7\u6837\u51fa \\(201\\times501\\) \u4e2a\u70b9\uff0c\u5176\u9884\u6d4b\u7ed3\u679c\u548c\u89e3\u6790\u89e3\u5982\u4e0b\u56fe\u6240\u793a\u3002</p> <p> </p>  \u5de6\u4fa7\u4e3a PaddleScience \u9884\u6d4b\u7ed3\u679c\uff0c\u4e2d\u95f4\u4e3a\u89e3\u6790\u89e3\u7ed3\u679c\uff0c\u53f3\u4fa7\u4e3a\u4e24\u8005\u7684\u5dee\u503c <p>\u53ef\u4ee5\u770b\u5230\u5bf9\u4e8e\u51fd\u6570\\(u(t, x)\\)\uff0c\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u548c\u89e3\u6790\u89e3\u7684\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/allen_cahn/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>PIRATENETS: PHYSICS-INFORMED DEEP LEARNING WITHRESIDUAL ADAPTIVE NETWORKS</li> <li>Allen-Cahn equation</li> </ul>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Allen-Cahn\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/amgnet/","title":"AMGNet","text":"","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#amgnet","title":"AMGNet","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 amgnet_airfoilamgnet_cylinder <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/AMGNet/data.zip\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/AMGNet/data.zip -o data.zip\n# unzip it\nunzip data.zip\npython amgnet_airfoil.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/AMGNet/data.zip\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/AMGNet/data.zip -o data.zip\n# unzip it\nunzip data.zip\npython amgnet_cylinder.py\n</code></pre> amgnet_airfoilamgnet_cylinder <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/AMGNet/data.zip\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/AMGNet/data.zip -o data.zip\n# unzip it\nunzip data.zip\npython amgnet_airfoil.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/amgnet/amgnet_airfoil_pretrained.pdparams\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/AMGNet/data.zip\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/AMGNet/data.zip -o data.zip\n# unzip it\nunzip data.zip\npython amgnet_cylinder.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/amgnet/amgnet_cylinder_pretrained.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 amgnet_airfoil_pretrained.pdparams loss(RMSE_validator): 0.0001  RMSE.RMSE(RMSE_validator): 0.01315 amgnet_cylinder_pretrained.pdparams loss(RMSE_validator): 0.00048  RMSE.RMSE(RMSE_validator): 0.02197","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u7684\u6210\u529f\u5e94\u7528\uff0c\u4fc3\u4f7f\u4eba\u4eec\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66(CFD)\u9886\u57df\u7684\u5e94\u7528\u3002</p> <p>\u6d41\u4f53\u662f\u975e\u5e38\u590d\u6742\u7684\u7269\u7406\u7cfb\u7edf\uff0c\u6d41\u4f53\u7684\u884c\u4e3a\u7531 Navier-Stokes \u65b9\u7a0b\u63a7\u5236\u3002\u57fa\u4e8e\u7f51\u683c\u7684\u6709\u9650\u4f53\u79ef\u6216\u6709\u9650\u5143\u6a21\u62df\u65b9\u6cd5\u662f CFD \u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u503c\u65b9\u6cd5\u3002\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u7814\u7a76\u7684\u7269\u7406\u95ee\u9898\u5f80\u5f80\u975e\u5e38\u590d\u6742\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u624d\u80fd\u6c42\u51fa\u95ee\u9898\u7684\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u5728\u6c42\u89e3\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u4e3a\u4e86\u8fdb\u884c\u6570\u503c\u6a21\u62df\uff0c\u8ba1\u7b97\u57df\u901a\u5e38\u88ab\u7f51\u683c\u79bb\u6563\u5316\uff0c\u7531\u4e8e\u7f51\u683c\u5177\u6709\u826f\u597d\u7684\u51e0\u4f55\u548c\u7269\u7406\u95ee\u9898\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u548c\u56fe\u7ed3\u6784\u76f8\u5951\u5408\uff0c\u6240\u4ee5\u8fd9\u7bc7\u6587\u7ae0\u7684\u4f5c\u8005\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u8bad\u7ec3 CFD \u4eff\u771f\u6570\u636e\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u6a21\u578b\u6765\u8fdb\u884c\u6d41\u573a\u9884\u6d4b\u3002</p>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684 CFD \u8ba1\u7b97\u6a21\u578b\uff0c\u79f0\u4e3a AMGNET(A Multi-scale Graph neural Network)\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u9884\u6d4b\u5728\u4e0d\u540c\u7269\u7406\u53c2\u6570\u4e0b\u7684\u6d41\u573a\u3002\u8be5\u65b9\u6cd5\u6709\u4ee5\u4e0b\u51e0\u4e2a\u7279\u70b9\uff1a</p> <ul> <li> <p>AMGNET \u628a CFD \u4e2d\u7684\u7f51\u683c\u8f6c\u5316\u4e3a\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4fe1\u606f\u7684\u5904\u7406\u548c\u805a\u5408\uff0c\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684 GCN \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7684\u9884\u6d4b\u8bef\u5dee\u660e\u663e\u66f4\u4f4e\u3002</p> </li> <li> <p>AMGNET \u53ef\u4ee5\u540c\u65f6\u8ba1\u7b97\u6d41\u4f53\u5728 x \u548c y \u65b9\u5411\u7684\u901f\u5ea6\uff0c\u540c\u65f6\u8fd8\u80fd\u8ba1\u7b97\u6d41\u4f53\u538b\u5f3a\u3002</p> </li> <li> <p>AMGNET \u901a\u8fc7 RS \u7b97\u6cd5(Olson and Schroder, 2018)\u8fdb\u884c\u4e86\u56fe\u7684\u7c97\u5316\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u8282\u70b9\u5373\u53ef\u8fdb\u884c\u9884\u6d4b\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u9884\u6d4b\u901f\u5ea6\u3002</p> </li> </ul> <p>\u4e0b\u56fe\u4e3a\u8be5\u65b9\u6cd5\u7684\u7f51\u7edc\u7ed3\u6784\u56fe\u3002\u8be5\u6a21\u578b\u7684\u57fa\u672c\u539f\u7406\u5c31\u662f\u5c06\u7f51\u683c\u7ed3\u6784\u8f6c\u5316\u4e3a\u56fe\u7ed3\u6784\uff0c\u7136\u540e\u901a\u8fc7\u7f51\u683c\u4e2d\u8282\u70b9\u7684\u7269\u7406\u4fe1\u606f\u3001\u4f4d\u7f6e\u4fe1\u606f\u4ee5\u53ca\u8282\u70b9\u7c7b\u578b\u5bf9\u56fe\u4e2d\u7684\u8282\u70b9\u548c\u8fb9\u8fdb\u884c\u7f16\u7801\u3002\u63a5\u7740\u5bf9\u5f97\u5230\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u4f7f\u7528\u57fa\u4e8e\u4ee3\u6570\u591a\u91cd\u7f51\u683c\u7b97\u6cd5(RS)\u7684\u7c97\u5316\u5c42\u8fdb\u884c\u7c97\u5316\uff0c\u5c06\u6240\u6709\u8282\u70b9\u5206\u7c7b\u4e3a\u7c97\u8282\u70b9\u96c6\u548c\u7ec6\u8282\u70b9\u96c6\uff0c\u5176\u4e2d\u7c97\u8282\u70b9\u96c6\u662f\u7ec6\u8282\u70b9\u96c6\u7684\u5b50\u96c6\u3002\u7c97\u56fe\u7684\u8282\u70b9\u96c6\u5408\u5c31\u662f\u7c97\u8282\u70b9\u96c6\uff0c\u4e8e\u662f\u5b8c\u6210\u4e86\u56fe\u7684\u7c97\u5316\uff0c\u7f29\u5c0f\u4e86\u56fe\u7684\u89c4\u6a21\u3002\u7c97\u5316\u5b8c\u6210\u540e\u901a\u8fc7\u8bbe\u8ba1\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u4fe1\u606f\u4f20\u9012\u5757(GN)\u6765\u603b\u7ed3\u548c\u63d0\u53d6\u56fe\u7684\u7279\u5f81\u3002\u4e4b\u540e\u56fe\u6062\u590d\u5c42\u91c7\u7528\u53cd\u5411\u64cd\u4f5c\uff0c\u4f7f\u7528\u7a7a\u95f4\u63d2\u503c\u6cd5(Qi et al.,2017)\u5bf9\u56fe\u8fdb\u884c\u4e0a\u91c7\u6837\u3002\u4f8b\u5982\u8981\u5bf9\u8282\u70b9 \\(i\\) \u63d2\u503c\uff0c\u5219\u5728\u7c97\u56fe\u4e2d\u627e\u5230\u8ddd\u79bb\u8282\u70b9 \\(i\\) \u6700\u8fd1\u7684 \\(k\\) \u4e2a\u8282\u70b9\uff0c\u7136\u540e\u901a\u8fc7\u516c\u5f0f\u8ba1\u7b97\u5f97\u5230\u8282\u70b9 \\(i\\) \u7684\u7279\u5f81\u3002\u6700\u540e\uff0c\u901a\u8fc7\u89e3\u7801\u5668\u5f97\u5230\u6bcf\u4e2a\u8282\u70b9\u7684\u901f\u5ea6\u4e0e\u538b\u529b\u4fe1\u606f\u3002</p> <p></p>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p> <p>\u6ce8\u610f\u4e8b\u9879</p> <p>\u672c\u6848\u4f8b\u8fd0\u884c\u524d\u9700\u901a\u8fc7 <code>pip install pgl pyamg</code> \u547d\u4ee4\uff0c\u5b89\u88c5 Paddle Graph Learning \u56fe\u5b66\u4e60\u5de5\u5177\u548c PyAMG \u4ee3\u6570\u591a\u91cd\u7f51\u683c\u5de5\u5177\u3002</p>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#31","title":"3.1 \u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>\u8be5\u6848\u4f8b\u4f7f\u7528\u7684\u673a\u7ffc\u6570\u636e\u96c6 Airfoil \u6765\u81ea de Avila Belbute-Peres \u7b49\u4eba\uff0c\u5176\u4e2d\u7ffc\u578b\u6570\u636e\u96c6\u91c7\u7528 NACA0012 \u7ffc\u578b\uff0c\u5305\u62ec train, test \u4ee5\u53ca\u5bf9\u5e94\u7684\u7f51\u683c\u6570\u636e mesh_fine\uff1b\u5706\u67f1\u6570\u636e\u96c6\u662f\u539f\u4f5c\u8005\u5229\u7528\u8f6f\u4ef6\u8ba1\u7b97\u7684 CFD \u7b97\u4f8b\u3002</p> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7d\u5e76\u89e3\u538b\u6570\u636e\u96c6\u3002</p> <pre><code>wget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/AMGNet/data.zip\nunzip data.zip\n</code></pre>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u672c\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc <code>AMGNet</code> \u4f5c\u4e3a\u6a21\u578b\uff0c\u5176\u63a5\u6536\u56fe\u7ed3\u6784\u6570\u636e\uff0c\u8f93\u51fa\u9884\u6d4b\u7ed3\u679c\u3002</p> airfoilcylinder <pre><code># set airfoil model\nmodel = ppsci.arch.AMGNet(**cfg.MODEL)\n</code></pre> <pre><code># set cylinder model\nmodel = ppsci.arch.AMGNet(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"input\", )</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>(\"pred\", )</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#33","title":"3.3 \u7ea6\u675f\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u76d1\u7763\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u6307\u5b9a\u6570\u636e\u96c6\u7684\u8def\u5f84\u7b49\u76f8\u5173\u914d\u7f6e\uff0c\u5c06\u8fd9\u4e9b\u4fe1\u606f\u5b58\u653e\u5230\u5bf9\u5e94\u7684 YAML \u6587\u4ef6\u4e2d\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> airfoilcylinder <pre><code># general settings\nmode: train # running mode: train/eval\nseed: 42\noutput_dir: ${hydra:run.dir}\nlog_freq: 20\n</code></pre> <pre><code># general settings\nmode: train # running mode: train/eval\nseed: 42\noutput_dir: ${hydra:run.dir}\nlog_freq: 20\n</code></pre> <p>\u63a5\u7740\u5b9a\u4e49\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> airfoilcylinder <pre><code>def train_mse_func(\n    output_dict: Dict[str, \"paddle.Tensor\"],\n    label_dict: Dict[str, \"pgl.Graph\"],\n    *args,\n) -&gt; paddle.Tensor:\n    return {\"pred\": F.mse_loss(output_dict[\"pred\"], label_dict[\"label\"].y)}\n</code></pre> <pre><code>def train_mse_func(\n    output_dict: Dict[str, \"paddle.Tensor\"],\n    label_dict: Dict[str, \"pgl.Graph\"],\n    *args,\n) -&gt; paddle.Tensor:\n    return {\"pred\": F.mse_loss(output_dict[\"pred\"], label_dict[\"label\"].y)}\n</code></pre> <p>\u6700\u540e\u6784\u5efa\u76d1\u7763\u7ea6\u675f\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> airfoilcylinder <pre><code># set constraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    output_expr={\"pred\": lambda out: out[\"pred\"]},\n    loss=ppsci.loss.FunctionalLoss(train_mse_func),\n    name=\"Sup\",\n)\n# wrap constraints together\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre> <pre><code># set constraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    output_expr={\"pred\": lambda out: out[\"pred\"]},\n    loss=ppsci.loss.FunctionalLoss(train_mse_func),\n    name=\"Sup\",\n)\n# wrap constraints together\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#34","title":"3.4 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u8bbe\u7f6e\u8bad\u7ec3\u8f6e\u6570\u7b49\u53c2\u6570\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> airfoilcylinder <pre><code>  output_keys: [\"pred\"]\n  input_dim: 5\n  output_dim: 3\n  latent_dim: 128\n  num_layers: 2\n  message_passing_aggregator: \"sum\"\n  message_passing_steps: 6\n  speed: \"norm\"\n\n# training settings\nTRAIN:\n</code></pre> <pre><code>  output_keys: [\"pred\"]\n  input_dim: 4\n  output_dim: 3\n  latent_dim: 128\n  num_layers: 2\n  message_passing_aggregator: \"sum\"\n  message_passing_steps: 6\n  speed: \"norm\"\n\n# training settings\nTRAIN:\n</code></pre>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u4f7f\u7528\u56fa\u5b9a\u7684 <code>5e-4</code> \u4f5c\u4e3a\u5b66\u4e60\u7387\u3002</p> airfoilcylinder <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#36","title":"3.6 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6(\u6d4b\u8bd5\u96c6)\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\uff0c\u6784\u5efa\u8fc7\u7a0b\u4e0e \u7ea6\u675f\u6784\u5efa \u7c7b\u4f3c\uff0c\u53ea\u9700\u628a\u6570\u636e\u76ee\u5f55\u6539\u4e3a\u6d4b\u8bd5\u96c6\u7684\u76ee\u5f55\uff0c\u5e76\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e <code>EVAL.batch_size=1</code> \u5373\u53ef\u3002</p> airfoilcylinder <pre><code># set validator\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"MeshAirfoilDataset\",\n        \"input_keys\": (\"input\",),\n        \"label_keys\": (\"label\",),\n        \"data_dir\": cfg.EVAL_DATA_DIR,\n        \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n}\nrmse_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    loss=ppsci.loss.FunctionalLoss(train_mse_func),\n    output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n    metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n    name=\"RMSE_validator\",\n)\nvalidator = {rmse_validator.name: rmse_validator}\n</code></pre> <pre><code># set validator\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"MeshCylinderDataset\",\n        \"input_keys\": (\"input\",),\n        \"label_keys\": (\"label\",),\n        \"data_dir\": cfg.EVAL_DATA_DIR,\n        \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n}\nrmse_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    loss=ppsci.loss.FunctionalLoss(train_mse_func),\n    output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n    metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n    name=\"RMSE_validator\",\n)\nvalidator = {rmse_validator.name: rmse_validator}\n</code></pre> <p>\u8bc4\u4f30\u6307\u6807\u4e3a\u9884\u6d4b\u7ed3\u679c\u548c\u771f\u5b9e\u7ed3\u679c\u7684 RMSE \u503c\uff0c\u56e0\u6b64\u9700\u81ea\u5b9a\u4e49\u6307\u6807\u8ba1\u7b97\u51fd\u6570\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> airfoilcylinder <pre><code>def eval_rmse_func(\n    output_dict: Dict[str, List[\"paddle.Tensor\"]],\n    label_dict: Dict[str, List[\"pgl.Graph\"]],\n    *args,\n) -&gt; Dict[str, paddle.Tensor]:\n    mse_losses = [\n        F.mse_loss(pred, label.y)\n        for (pred, label) in zip(output_dict[\"pred\"], label_dict[\"label\"])\n    ]\n    return {\"RMSE\": (sum(mse_losses) / len(mse_losses)) ** 0.5}\n</code></pre> <pre><code>def eval_rmse_func(\n    output_dict: Dict[str, List[\"paddle.Tensor\"]],\n    label_dict: Dict[str, List[\"pgl.Graph\"]],\n    *args,\n) -&gt; Dict[str, paddle.Tensor]:\n    mse_losses = [\n        F.mse_loss(pred, label.y)\n        for (pred, label) in zip(output_dict[\"pred\"], label_dict[\"label\"])\n    ]\n    return {\"RMSE\": (sum(mse_losses) / len(mse_losses)) ** 0.5}\n</code></pre>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3002</p> airfoilcylinder <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    validator=validator,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n# train model\nsolver.train()\n</code></pre> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    validator=validator,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n# train model\nsolver.train()\n</code></pre>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#38","title":"3.8 \u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\u7a0b\u5e8f\u4f1a\u5bf9\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u4ee5\u56fe\u7247\u7684\u5f62\u5f0f\u5bf9\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> airfoilcylinder <pre><code>    # visualize prediction\n    logger.message(\"Now visualizing prediction, please wait...\")\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"airfoil\",\n            )\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set airfoil model\n    model = ppsci.arch.AMGNet(**cfg.MODEL)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshAirfoilDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.EVAL_DATA_DIR,\n            \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    rmse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n        metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n        name=\"RMSE_validator\",\n    )\n    validator = {rmse_validator.name: rmse_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate model\n    solver.eval()\n\n    # visualize prediction\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"airfoil\",\n            )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"amgnet_airfoil.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>    # visualize prediction\n    logger.message(\"Now visualizing prediction, please wait...\")\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"cylinder\",\n            )\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set airfoil model\n    model = ppsci.arch.AMGNet(**cfg.MODEL)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshCylinderDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.EVAL_DATA_DIR,\n            \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    rmse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n        metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n        name=\"RMSE_validator\",\n    )\n    validator = {rmse_validator.name: rmse_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate model\n    solver.eval()\n\n    # visualize prediction\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"cylinder\",\n            )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"amgnet_cylinder.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"airfoilcylinder amgnet_airfoil.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nfrom os import path as osp\nfrom typing import TYPE_CHECKING\nfrom typing import Dict\nfrom typing import List\n\nimport hydra\nimport utils\nfrom omegaconf import DictConfig\nfrom paddle.nn import functional as F\n\nimport ppsci\nfrom ppsci.utils import logger\n\nif TYPE_CHECKING:\n    import paddle\n    import pgl\n\n\ndef train_mse_func(\n    output_dict: Dict[str, \"paddle.Tensor\"],\n    label_dict: Dict[str, \"pgl.Graph\"],\n    *args,\n) -&gt; paddle.Tensor:\n    return {\"pred\": F.mse_loss(output_dict[\"pred\"], label_dict[\"label\"].y)}\n\n\ndef eval_rmse_func(\n    output_dict: Dict[str, List[\"paddle.Tensor\"]],\n    label_dict: Dict[str, List[\"pgl.Graph\"]],\n    *args,\n) -&gt; Dict[str, paddle.Tensor]:\n    mse_losses = [\n        F.mse_loss(pred, label.y)\n        for (pred, label) in zip(output_dict[\"pred\"], label_dict[\"label\"])\n    ]\n    return {\"RMSE\": (sum(mse_losses) / len(mse_losses)) ** 0.5}\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # set airfoil model\n    model = ppsci.arch.AMGNet(**cfg.MODEL)\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshAirfoilDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.TRAIN_DATA_DIR,\n            \"mesh_graph_path\": cfg.TRAIN_MESH_GRAPH_PATH,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n        \"num_workers\": 1,\n    }\n\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        output_expr={\"pred\": lambda out: out[\"pred\"]},\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        name=\"Sup\",\n    )\n    # wrap constraints together\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshAirfoilDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.EVAL_DATA_DIR,\n            \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    rmse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n        metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n        name=\"RMSE_validator\",\n    )\n    validator = {rmse_validator.name: rmse_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        validator=validator,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n\n    # visualize prediction\n    logger.message(\"Now visualizing prediction, please wait...\")\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"airfoil\",\n            )\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set airfoil model\n    model = ppsci.arch.AMGNet(**cfg.MODEL)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshAirfoilDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.EVAL_DATA_DIR,\n            \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    rmse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n        metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n        name=\"RMSE_validator\",\n    )\n    validator = {rmse_validator.name: rmse_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate model\n    solver.eval()\n\n    # visualize prediction\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"airfoil\",\n            )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"amgnet_airfoil.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> amgnet_airfoil.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nfrom os import path as osp\nfrom typing import TYPE_CHECKING\nfrom typing import Dict\nfrom typing import List\n\nimport hydra\nimport utils\nfrom omegaconf import DictConfig\nfrom paddle.nn import functional as F\n\nimport ppsci\nfrom ppsci.utils import logger\n\nif TYPE_CHECKING:\n    import paddle\n    import pgl\n\n\ndef train_mse_func(\n    output_dict: Dict[str, \"paddle.Tensor\"],\n    label_dict: Dict[str, \"pgl.Graph\"],\n    *args,\n) -&gt; paddle.Tensor:\n    return {\"pred\": F.mse_loss(output_dict[\"pred\"], label_dict[\"label\"].y)}\n\n\ndef eval_rmse_func(\n    output_dict: Dict[str, List[\"paddle.Tensor\"]],\n    label_dict: Dict[str, List[\"pgl.Graph\"]],\n    *args,\n) -&gt; Dict[str, paddle.Tensor]:\n    mse_losses = [\n        F.mse_loss(pred, label.y)\n        for (pred, label) in zip(output_dict[\"pred\"], label_dict[\"label\"])\n    ]\n    return {\"RMSE\": (sum(mse_losses) / len(mse_losses)) ** 0.5}\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # set cylinder model\n    model = ppsci.arch.AMGNet(**cfg.MODEL)\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshCylinderDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.TRAIN_DATA_DIR,\n            \"mesh_graph_path\": cfg.TRAIN_MESH_GRAPH_PATH,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n        \"num_workers\": 1,\n    }\n\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        output_expr={\"pred\": lambda out: out[\"pred\"]},\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        name=\"Sup\",\n    )\n    # wrap constraints together\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshCylinderDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.EVAL_DATA_DIR,\n            \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    rmse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n        metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n        name=\"RMSE_validator\",\n    )\n    validator = {rmse_validator.name: rmse_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        validator=validator,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n\n    # visualize prediction\n    logger.message(\"Now visualizing prediction, please wait...\")\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"cylinder\",\n            )\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set airfoil model\n    model = ppsci.arch.AMGNet(**cfg.MODEL)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshCylinderDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.EVAL_DATA_DIR,\n            \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    rmse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n        metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n        name=\"RMSE_validator\",\n    )\n    validator = {rmse_validator.name: rmse_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate model\n    solver.eval()\n\n    # visualize prediction\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"cylinder\",\n            )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"amgnet_cylinder.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u65b9\u5c55\u793a\u4e86\u6a21\u578b\u5bf9\u8ba1\u7b97\u57df\u4e2d\u6bcf\u4e2a\u70b9\u7684\u538b\u529b\\(p(x,y)\\)\u3001x(\u6c34\u5e73)\u65b9\u5411\u6d41\u901f\\(u(x,y)\\)\u3001y(\u5782\u76f4)\u65b9\u5411\u6d41\u901f\\(v(x,y)\\)\u7684\u9884\u6d4b\u7ed3\u679c\u4e0e\u53c2\u8003\u7ed3\u679c\u3002</p> airfoilcylinder <p> \u5de6\uff1a\u9884\u6d4b x \u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1a\u5b9e\u9645 x \u65b9\u5411\u6d41\u901f \u5de6\uff1a\u9884\u6d4b\u538b\u529b p\uff0c\u53f3\uff1a\u5b9e\u9645\u538b\u529b p \u5de6\uff1a\u9884\u6d4by\u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1a\u5b9e\u9645 y \u65b9\u5411\u6d41\u901f </p> <p> \u5de6\uff1a\u9884\u6d4b x \u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1a\u5b9e\u9645 x \u65b9\u5411\u6d41\u901f \u5de6\uff1a\u9884\u6d4b\u538b\u529b p\uff0c\u53f3\uff1a\u5b9e\u9645\u538b\u529b p \u5de6\uff1a\u9884\u6d4b y \u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1a\u5b9e\u9645 y \u65b9\u5411\u6d41\u901f </p> <p>\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/amgnet/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li>AMGNET: multi-scale graph neural networks for flow field prediction</li> <li>AMGNet - Github</li> <li>AMGNet - AIStudio</li> </ul>","tags":["\u7a00\u758f\u8ba1\u7b97"]},{"location":"zh/examples/aneurysm/","title":"Aneurysm","text":""},{"location":"zh/examples/aneurysm/#aneurysm","title":"Aneurysm","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar -o aneurysm_dataset.tar\n# unzip it\ntar -xvf aneurysm_dataset.tar\npython aneurysm.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar -o aneurysm_dataset.tar\n# unzip it\ntar -xvf aneurysm_dataset.tar\npython aneurysm.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/aneurysm/aneurysm_pretrained.pdparams\n</code></pre> <pre><code>python aneurysm.py mode=export\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar -o aneurysm_dataset.tar\n# unzip it\ntar -xvf aneurysm_dataset.tar\npython aneurysm.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 aneurysm_pretrained.pdparams loss(ref_u_v_w_p): 0.01488MSE.p(ref_u_v_w_p): 0.01412MSE.u(ref_u_v_w_p): 0.00021MSE.v(ref_u_v_w_p): 0.00024MSE.w(ref_u_v_w_p): 0.00032"},{"location":"zh/examples/aneurysm/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u5904\u7406\u8840\u7ba1\u7624\u95ee\u9898\uff0c\u5176\u4e2d\u5305\u62ec\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u8111\u8840\u7ba1\u7624\u7684\u538b\u529b\u5efa\u6a21\uff0c\u4ee5\u9884\u6d4b\u548c\u8bc4\u4f30\u8840\u7ba1\u7624\u7834\u88c2\u7684\u98ce\u9669\u3002</p> <p>\u9488\u5bf9\u5982\u4e0b\u8840\u7ba1\u7624\u51e0\u4f55\u6a21\u578b\uff0c\u672c\u6848\u4f8b\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u65b9\u5f0f\uff0c\u5728\u5185\u90e8\u548c\u8fb9\u754c\u65bd\u52a0\u9002\u5f53\u7684\u7269\u7406\u65b9\u7a0b\u7ea6\u675f\uff0c\u4ee5\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\u5bf9\u7ba1\u58c1\u538b\u529b\u8fdb\u884c\u5efa\u6a21\u3002</p> <p></p>"},{"location":"zh/examples/aneurysm/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u5047\u8bbe\u8840\u7ba1\u7624\u6a21\u578b\u4e2d\uff0c\u5728\u5165\u53e3 inlet \u90e8\u5206\uff0c\u4e2d\u5fc3\u70b9\u7684\u6d41\u901f\u4e3a 1.5\uff0c\u5e76\u5411\u56db\u5468\u9010\u6e10\u51cf\u5c0f\uff1b\u5728\u51fa\u53e3 outlet \u533a\u57df\uff0c\u538b\u529b\u6052\u4e3a 0\uff1b\u5728\u8fb9\u754c\u4e0a\u65e0\u6ed1\u79fb\uff0c\u6d41\u901f\u4e3a 0\uff1b\u8840\u7ba1\u5185\u90e8\u5219\u7b26\u5408 N-S \u65b9\u7a0b\u8fd0\u52a8\u89c4\u5f8b\uff0c\u4e2d\u95f4\u6bb5\u7684\u5e73\u5747\u6d41\u91cf\u4e3a\u8d1f\uff08\u6d41\u5165\uff09\uff0c\u51fa\u53e3\u6bb5\u7684\u5e73\u5747\u6d41\u91cf\u4e3a\u6b63\uff08\u6d41\u51fa\uff09\u3002</p>"},{"location":"zh/examples/aneurysm/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/aneurysm/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 aneurysm \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y, z)\\) \u90fd\u6709\u5bf9\u5e94\u7684\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf \\((u, v, w, p)\\)\uff08\u901f\u5ea6\u548c\u538b\u529b\uff09 \uff0c\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((x, y, z)\\) \u5230 \\((u, v, w, p)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^3 \\to \\mathbb{R}^4\\) \uff0c\u5373\uff1a</p> \\[ (u, v, w, p) = f(x, y, z) \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"x\", \"y\", \"z\")</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>(\"u\", \"v\", \"w\", \"p\")</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\uff0c\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 6 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 512 \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\uff0c\u4f7f\u7528 <code>silu</code> \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u4f7f\u7528 <code>WeightNorm</code> \u6743\u91cd\u5f52\u4e00\u5316\u3002</p>"},{"location":"zh/examples/aneurysm/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u8840\u7ba1\u7624\u6a21\u578b\u6d89\u53ca\u5230 2 \u4e2a\u65b9\u7a0b\uff0c\u4e00\u662f\u6d41\u4f53 N-S \u65b9\u7a0b\uff0c\u4e8c\u662f\u6d41\u91cf\u8ba1\u7b97\u65b9\u7a0b\uff0c\u56e0\u6b64\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>NavierStokes</code> \u548c <code>NormalDotVec</code> \u5373\u53ef\u3002</p> <pre><code># set equation\nequation = {\n    \"NavierStokes\": ppsci.equation.NavierStokes(\n        cfg.NU * cfg.SCALE, cfg.RHO, cfg.DIM, False\n    ),\n    \"NormalDotVec\": ppsci.equation.NormalDotVec((\"u\", \"v\", \"w\")),\n}\n</code></pre>"},{"location":"zh/examples/aneurysm/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u7684\u51e0\u4f55\u533a\u57df\u7531 stl \u6587\u4ef6\u6307\u5b9a\uff0c\u6309\u7167\u4e0b\u65b9\u547d\u4ee4\uff0c\u4e0b\u8f7d\u5e76\u89e3\u538b\u5230 <code>aneurysm/</code> \u6587\u4ef6\u5939\u4e0b\u3002</p> <p>\u6ce8\uff1a\u6570\u636e\u96c6\u4e2d\u7684 stl \u6587\u4ef6\u548c\u6d4b\u8bd5\u96c6\u6570\u636e\uff08\u4f7f\u7528OpenFOAM\u751f\u6210\uff09\u5747\u6765\u81ea Aneurysm - NVIDIA Modulus\u3002</p> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar\n\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/aneurysm/aneurysm_dataset.tar -o aneurysm_dataset.tar\n\n# unzip it\ntar -xvf aneurysm_dataset.tar\n</code></pre> <p>\u89e3\u538b\u5b8c\u6bd5\u4e4b\u540e\uff0c<code>aneurysm/stl</code> \u6587\u4ef6\u5939\u4e0b\u5373\u5b58\u653e\u4e86\u8ba1\u7b97\u57df\u6784\u5efa\u6240\u9700\u7684 stl \u51e0\u4f55\u6587\u4ef6\u3002</p> \u6ce8\u610f <p>\u4f7f\u7528 <code>Mesh</code> \u7c7b\u4e4b\u524d\uff0c\u5fc5\u987b\u5148\u6309\u71671.4.2 \u989d\u5916\u4f9d\u8d56\u5b89\u88c5[\u53ef\u9009]\u6587\u6863\uff0c\u5b89\u88c5\u597d open3d\u3001pysdf\u3001PyMesh 3 \u4e2a\u51e0\u4f55\u4f9d\u8d56\u5305\u3002</p> <p>\u7136\u540e\u901a\u8fc7 PaddleScience \u5185\u7f6e\u7684 STL \u51e0\u4f55\u7c7b <code>Mesh</code> \u6765\u8bfb\u53d6\u3001\u89e3\u6790\u8fd9\u4e9b\u51e0\u4f55\u6587\u4ef6\uff0c\u5e76\u4e14\u901a\u8fc7\u5e03\u5c14\u8fd0\u7b97\uff0c\u7ec4\u5408\u51fa\u5404\u4e2a\u8ba1\u7b97\u57df\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set geometry\ninlet_geo = ppsci.geometry.Mesh(cfg.INLET_STL_PATH)\noutlet_geo = ppsci.geometry.Mesh(cfg.OUTLET_STL_PATH)\nnoslip_geo = ppsci.geometry.Mesh(cfg.NOSLIP_STL_PATH)\nintegral_geo = ppsci.geometry.Mesh(cfg.INTEGRAL_STL_PATH)\ninterior_geo = ppsci.geometry.Mesh(cfg.INTERIOR_STL_PATH)\n</code></pre> <p>\u5728\u6b64\u4e4b\u540e\u53ef\u4ee5\u5bf9\u51e0\u4f55\u57df\u8fdb\u884c\u7f29\u653e\u548c\u5e73\u79fb\uff0c\u4ee5\u7f29\u653e\u8f93\u5165\u6570\u636e\u7684\u5750\u6807\u8303\u56f4\uff0c\u4fc3\u8fdb\u6a21\u578b\u8bad\u7ec3\u6536\u655b\u3002</p> <pre><code># normalize meshes\ninlet_geo = inlet_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\noutlet_geo = outlet_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\nnoslip_geo = noslip_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\nintegral_geo = integral_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\ninterior_geo = interior_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\ngeom = {\n    \"inlet_geo\": inlet_geo,\n    \"outlet_geo\": outlet_geo,\n    \"noslip_geo\": noslip_geo,\n    \"integral_geo\": integral_geo,\n    \"interior_geo\": interior_geo,\n}\n</code></pre>"},{"location":"zh/examples/aneurysm/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u5171\u6d89\u53ca\u5230 6 \u4e2a\u7ea6\u675f\uff0c\u5728\u5177\u4f53\u7ea6\u675f\u6784\u5efa\u4e4b\u524d\uff0c\u53ef\u4ee5\u5148\u6784\u5efa\u6570\u636e\u8bfb\u53d6\u914d\u7f6e\uff0c\u4ee5\u4fbf\u540e\u7eed\u6784\u5efa\u591a\u4e2a\u7ea6\u675f\u65f6\u590d\u7528\u8be5\u914d\u7f6e\u3002</p> <pre><code># set dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": \"NamedArrayDataset\",\n    \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"num_workers\": 1,\n}\n</code></pre>"},{"location":"zh/examples/aneurysm/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>pde = ppsci.constraint.InteriorConstraint(\n    equation[\"NavierStokes\"].equations,\n    {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0, \"momentum_z\": 0},\n    geom[\"interior_geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.pde},\n    ppsci.loss.MSELoss(\"sum\"),\n    name=\"interior\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\uff08\u7ec4\uff09\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"NavierStokes\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u5e0c\u671b\u4e0e N-S \u65b9\u7a0b\u76f8\u5173\u7684\u56db\u4e2a\u503c <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code>, <code>momentum_z</code> \u5747\u88ab\u4f18\u5316\u81f3 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"interior_geo\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u8bbe\u7f6e <code>batch_size</code> \u4e3a <code>6000</code>\u3002</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u9009\u7528\u5e38\u7528\u7684 MSE \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"sum\"</code>\uff0c\u5373\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"interior\" \u5373\u53ef\u3002</p>"},{"location":"zh/examples/aneurysm/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u63a5\u7740\u9700\u8981\u5bf9\u8840\u7ba1\u5165\u53e3\u3001\u51fa\u53e3\u3001\u8840\u7ba1\u58c1\u8fd9\u4e09\u4e2a\u8868\u9762\u65bd\u52a0\u7ea6\u675f\uff0c\u5305\u62ec\u5165\u53e3\u901f\u5ea6\u7ea6\u675f\u3001\u51fa\u53e3\u538b\u529b\u7ea6\u675f\u3001\u8840\u7ba1\u58c1\u65e0\u6ed1\u79fb\u7ea6\u675f\u3002 \u5728 <code>bc_inlet</code> \u7ea6\u675f\u4e2d\uff0c\u5165\u53e3\u5904\u7684\u6d41\u901f\u6ee1\u8db3\u4ece\u4e2d\u5fc3\u70b9\u5f00\u59cb\u5411\u5468\u56f4\u5448\u4e8c\u6b21\u629b\u7269\u7ebf\u8870\u51cf\uff0c\u6b64\u5904\u4f7f\u7528\u629b\u7269\u7ebf\u51fd\u6570\u8868\u793a\u901f\u5ea6\u968f\u7740\u8fdc\u79bb\u5706\u5fc3\u800c\u8870\u51cf\uff0c\u518d\u5c06\u5176\u4f5c\u4e3a <code>BoundaryConstraint</code> \u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570(\u5b57\u5178)\u7684 value\u3002</p> <pre><code>def _compute_parabola(_in):\n    centered_x = _in[\"x\"] - cfg.INLET_CENTER[0]\n    centered_y = _in[\"y\"] - cfg.INLET_CENTER[1]\n    centered_z = _in[\"z\"] - cfg.INLET_CENTER[2]\n    distance = np.sqrt(centered_x**2 + centered_y**2 + centered_z**2)\n    parabola = cfg.INLET_VEL * np.maximum((1 - (distance / INLET_RADIUS) ** 2), 0)\n    return parabola\n\ndef inlet_u_ref_func(_in):\n    return cfg.INLET_NORMAL[0] * _compute_parabola(_in)\n\ndef inlet_v_ref_func(_in):\n    return cfg.INLET_NORMAL[1] * _compute_parabola(_in)\n\ndef inlet_w_ref_func(_in):\n    return cfg.INLET_NORMAL[2] * _compute_parabola(_in)\n\nbc_inlet = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"w\": lambda d: d[\"w\"]},\n    {\"u\": inlet_u_ref_func, \"v\": inlet_v_ref_func, \"w\": inlet_w_ref_func},\n    geom[\"inlet_geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_inlet},\n    ppsci.loss.MSELoss(\"sum\"),\n    name=\"inlet\",\n)\n</code></pre> <p>\u8840\u7ba1\u51fa\u53e3\u3001\u8840\u7ba1\u58c1\u7684\u65e0\u6ed1\u79fb\u7ea6\u675f\u6784\u5efa\u65b9\u6cd5\u7c7b\u4f3c\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>bc_outlet = ppsci.constraint.BoundaryConstraint(\n    {\"p\": lambda d: d[\"p\"]},\n    {\"p\": 0},\n    geom[\"outlet_geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_outlet},\n    ppsci.loss.MSELoss(\"sum\"),\n    name=\"outlet\",\n)\nbc_noslip = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"w\": lambda d: d[\"w\"]},\n    {\"u\": 0, \"v\": 0, \"w\": 0},\n    geom[\"noslip_geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_noslip},\n    ppsci.loss.MSELoss(\"sum\"),\n    name=\"no_slip\",\n)\n</code></pre>"},{"location":"zh/examples/aneurysm/#343","title":"3.4.3 \u79ef\u5206\u8fb9\u754c\u7ea6\u675f","text":"<p>\u5bf9\u4e8e\u8840\u7ba1\u5165\u53e3\u4e0b\u65b9\u7684\u4e00\u6bb5\u533a\u57df\u548c\u51fa\u53e3\u533a\u57df\uff08\u9762\uff09\uff0c\u9700\u989d\u5916\u65bd\u52a0\u6d41\u5165\u548c\u6d41\u51fa\u7684\u6d41\u91cf\u7ea6\u675f\uff0c\u7531\u4e8e\u6d41\u91cf\u8ba1\u7b97\u6d89\u53ca\u5230\u5177\u4f53\u9762\u79ef\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528\u79bb\u6563\u79ef\u5206\u7684\u65b9\u5f0f\u8fdb\u884c\u8ba1\u7b97\uff0c\u8fd9\u4e9b\u8fc7\u7a0b\u5df2\u7ecf\u5185\u7f6e\u5728\u4e86 <code>IntegralConstraint</code> \u8fd9\u4e00\u7ea6\u675f\u6761\u4ef6\u4e2d\u3002\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>igc_outlet = ppsci.constraint.IntegralConstraint(\n    equation[\"NormalDotVec\"].equations,\n    {\"normal_dot_vec\": 2.54},\n    geom[\"outlet_geo\"],\n    {\n        **train_dataloader_cfg,\n        \"iters_per_epoch\": cfg.TRAIN.iters_integral.igc_outlet,\n        \"batch_size\": cfg.TRAIN.batch_size.igc_outlet,\n        \"integral_batch_size\": cfg.TRAIN.integral_batch_size.igc_outlet,\n    },\n    ppsci.loss.IntegralLoss(\"sum\"),\n    weight_dict=cfg.TRAIN.weight.igc_outlet,\n    name=\"igc_outlet\",\n)\nigc_integral = ppsci.constraint.IntegralConstraint(\n    equation[\"NormalDotVec\"].equations,\n    {\"normal_dot_vec\": -2.54},\n    geom[\"integral_geo\"],\n    {\n        **train_dataloader_cfg,\n        \"iters_per_epoch\": cfg.TRAIN.iters_integral.igc_integral,\n        \"batch_size\": cfg.TRAIN.batch_size.igc_integral,\n        \"integral_batch_size\": cfg.TRAIN.integral_batch_size.igc_integral,\n    },\n    ppsci.loss.IntegralLoss(\"sum\"),\n    weight_dict=cfg.TRAIN.weight.igc_integral,\n    name=\"igc_integral\",\n)\n</code></pre> <p>\u5bf9\u5e94\u7684\u6d41\u91cf\u8ba1\u7b97\u516c\u5f0f\uff1a</p> \\[ flow_i = \\sum_{i=1}^{M}{s_{i} (\\mathbf{u_i} \\cdot \\mathbf{n_i})} \\] <p>\u5176\u4e2d\\(M\\)\u8868\u793a\u79bb\u6563\u79ef\u5206\u70b9\u4e2a\u6570\uff0c\\(s_i\\)\u8868\u793a\u67d0\u4e00\u4e2a\u70b9\u7684\uff08\u8fd1\u4f3c\uff09\u9762\u79ef\uff0c\\(\\mathbf{u_i}\\)\u8868\u793a\u67d0\u4e00\u4e2a\u70b9\u7684\u901f\u5ea6\u77e2\u91cf\uff0c\\(\\mathbf{n_i}\\)\u8868\u793a\u67d0\u4e00\u4e2a\u70b9\u7684\u5916\u6cd5\u5411\u77e2\u91cf\u3002</p> <p>\u9664\u524d\u9762\u7ae0\u8282\u6240\u8ff0\u7684\u5171\u540c\u53c2\u6570\u5916\uff0c\u6b64\u5904\u989d\u5916\u589e\u52a0\u4e86 <code>integral_batch_size</code> \u53c2\u6570\uff0c\u8fd9\u8868\u793a\u7528\u4e8e\u79bb\u6563\u79ef\u5206\u7684\u91c7\u6837\u70b9\u6570\u91cf\uff0c\u6b64\u5904\u4f7f\u7528 310 \u4e2a\u79bb\u6563\u70b9\u6765\u8fd1\u4f3c\u79ef\u5206\u8ba1\u7b97\uff1b\u540c\u65f6\u6307\u5b9a\u635f\u5931\u51fd\u6570\u4e3a <code>IntegralLoss</code>\uff0c\u8868\u793a\u8ba1\u7b97\u635f\u5931\u6240\u7528\u7684\u6700\u7ec8\u9884\u6d4b\u503c\u7531\u591a\u4e2a\u79bb\u6563\u70b9\u8fd1\u4f3c\u79ef\u5206\uff0c\u518d\u4e0e\u6807\u7b7e\u503c\u8ba1\u7b97\u635f\u5931\u3002</p> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u3001\u8fb9\u754c\u7ea6\u675f\u3001\u521d\u503c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    bc_inlet.name: bc_inlet,\n    bc_outlet.name: bc_outlet,\n    bc_noslip.name: bc_noslip,\n    pde.name: pde,\n    igc_outlet.name: igc_outlet,\n    igc_integral.name: igc_integral,\n}\n</code></pre>"},{"location":"zh/examples/aneurysm/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 1500 \u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c0.001 \u7684\u521d\u59cb\u5b66\u4e60\u7387\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 1500\n  iters_per_epoch: 1000\n  iters_integral:\n    igc_outlet: 100\n    igc_integral: 100\n  save_freq: 20\n  eval_during_train: true\n  eval_freq: 20\n  lr_scheduler:\n    epochs: ${TRAIN.epochs}\n    iters_per_epoch: ${TRAIN.iters_per_epoch}\n    learning_rate: 0.001\n    gamma: 0.95\n    decay_steps: 15000\n    by_epoch: false\n</code></pre>"},{"location":"zh/examples/aneurysm/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u914d\u5408\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u5e38\u7528\u7684 ExponentialDecay \u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u3002</p> <pre><code># set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n    **cfg.TRAIN.lr_scheduler\n)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>"},{"location":"zh/examples/aneurysm/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.GeometryValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\neval_data_dict = reader.load_csv_file(\n    cfg.EVAL_CSV_PATH,\n    (\"x\", \"y\", \"z\", \"u\", \"v\", \"w\", \"p\"),\n    {\n        \"x\": \"Points:0\",\n        \"y\": \"Points:1\",\n        \"z\": \"Points:2\",\n        \"u\": \"U:0\",\n        \"v\": \"U:1\",\n        \"w\": \"U:2\",\n        \"p\": \"p\",\n    },\n)\ninput_dict = {\n    \"x\": (eval_data_dict[\"x\"] - cfg.CENTER[0]) * cfg.SCALE,\n    \"y\": (eval_data_dict[\"y\"] - cfg.CENTER[1]) * cfg.SCALE,\n    \"z\": (eval_data_dict[\"z\"] - cfg.CENTER[2]) * cfg.SCALE,\n}\nif \"area\" in input_dict.keys():\n    input_dict[\"area\"] *= cfg.SCALE ** (equation[\"NavierStokes\"].dim)\n\nlabel_dict = {\n    \"p\": eval_data_dict[\"p\"],\n    \"u\": eval_data_dict[\"u\"],\n    \"v\": eval_data_dict[\"v\"],\n    \"w\": eval_data_dict[\"w\"],\n}\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"NamedArrayDataset\",\n        \"input\": input_dict,\n        \"label\": label_dict,\n    },\n    \"sampler\": {\"name\": \"BatchSampler\"},\n    \"num_workers\": 1,\n}\nsup_validator = ppsci.validate.SupervisedValidator(\n    {**eval_dataloader_cfg, \"batch_size\": cfg.EVAL.batch_size.sup_validator},\n    ppsci.loss.MSELoss(\"mean\"),\n    {\n        \"p\": lambda out: out[\"p\"],\n        \"u\": lambda out: out[\"u\"],\n        \"v\": lambda out: out[\"v\"],\n        \"w\": lambda out: out[\"w\"],\n    },\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"ref_u_v_w_p\",\n)\nvalidator = {sup_validator.name: sup_validator}\n\n# set visualizer(optional)\nvisualizer = {\n    \"visualize_u_v_w_p\": ppsci.visualize.VisualizerVtu(\n        input_dict,\n        {\n            \"p\": lambda out: out[\"p\"],\n            \"u\": lambda out: out[\"u\"],\n            \"v\": lambda out: out[\"v\"],\n            \"w\": lambda out: out[\"w\"],\n        },\n        batch_size=cfg.EVAL.batch_size.sup_validator,\n        prefix=\"result_u_v_w_p\",\n    ),\n}\n</code></pre>"},{"location":"zh/examples/aneurysm/#38","title":"3.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u672c\u6587\u4e2d\u7684\u8f93\u51fa\u6570\u636e\u662f\u4e00\u4e2a\u533a\u57df\u5185\u7684\u4e09\u7ef4\u70b9\u96c6\uff0c\u56e0\u6b64\u53ea\u9700\u8981\u5c06\u8bc4\u4f30\u7684\u8f93\u51fa\u6570\u636e\u4fdd\u5b58\u6210 vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u6700\u540e\u7528\u53ef\u89c6\u5316\u8f6f\u4ef6\u6253\u5f00\u67e5\u770b\u5373\u53ef\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set visualizer(optional)\nvisualizer = {\n    \"visualize_u_v_w_p\": ppsci.visualize.VisualizerVtu(\n        input_dict,\n        {\n            \"p\": lambda out: out[\"p\"],\n            \"u\": lambda out: out[\"u\"],\n            \"v\": lambda out: out[\"v\"],\n            \"w\": lambda out: out[\"w\"],\n        },\n        batch_size=cfg.EVAL.batch_size.sup_validator,\n        prefix=\"result_u_v_w_p\",\n    ),\n}\n</code></pre>"},{"location":"zh/examples/aneurysm/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    log_freq=cfg.log_freq,\n    eval_during_train=True,\n    eval_freq=cfg.TRAIN.eval_freq,\n    seed=cfg.seed,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    visualizer=visualizer,\n    pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>"},{"location":"zh/examples/aneurysm/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"aneurysm.py<pre><code>\"\"\"\nReference: https://docs.nvidia.com/deeplearning/modulus/modulus-v2209/user_guide/intermediate/adding_stl_files.html\n\"\"\"\n\nimport hydra\nimport numpy as np\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import reader\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(\n            cfg.NU * cfg.SCALE, cfg.RHO, cfg.DIM, False\n        ),\n        \"NormalDotVec\": ppsci.equation.NormalDotVec((\"u\", \"v\", \"w\")),\n    }\n\n    # set geometry\n    inlet_geo = ppsci.geometry.Mesh(cfg.INLET_STL_PATH)\n    outlet_geo = ppsci.geometry.Mesh(cfg.OUTLET_STL_PATH)\n    noslip_geo = ppsci.geometry.Mesh(cfg.NOSLIP_STL_PATH)\n    integral_geo = ppsci.geometry.Mesh(cfg.INTEGRAL_STL_PATH)\n    interior_geo = ppsci.geometry.Mesh(cfg.INTERIOR_STL_PATH)\n\n    # normalize meshes\n    inlet_geo = inlet_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\n    outlet_geo = outlet_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\n    noslip_geo = noslip_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\n    integral_geo = integral_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\n    interior_geo = interior_geo.translate(-np.array(cfg.CENTER)).scale(cfg.SCALE)\n    geom = {\n        \"inlet_geo\": inlet_geo,\n        \"outlet_geo\": outlet_geo,\n        \"noslip_geo\": noslip_geo,\n        \"integral_geo\": integral_geo,\n        \"interior_geo\": interior_geo,\n    }\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"num_workers\": 1,\n    }\n\n    # set constraint\n    INLET_AREA = 21.1284 * (cfg.SCALE**2)\n    INLET_RADIUS = np.sqrt(INLET_AREA / np.pi)\n\n    def _compute_parabola(_in):\n        centered_x = _in[\"x\"] - cfg.INLET_CENTER[0]\n        centered_y = _in[\"y\"] - cfg.INLET_CENTER[1]\n        centered_z = _in[\"z\"] - cfg.INLET_CENTER[2]\n        distance = np.sqrt(centered_x**2 + centered_y**2 + centered_z**2)\n        parabola = cfg.INLET_VEL * np.maximum((1 - (distance / INLET_RADIUS) ** 2), 0)\n        return parabola\n\n    def inlet_u_ref_func(_in):\n        return cfg.INLET_NORMAL[0] * _compute_parabola(_in)\n\n    def inlet_v_ref_func(_in):\n        return cfg.INLET_NORMAL[1] * _compute_parabola(_in)\n\n    def inlet_w_ref_func(_in):\n        return cfg.INLET_NORMAL[2] * _compute_parabola(_in)\n\n    bc_inlet = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"w\": lambda d: d[\"w\"]},\n        {\"u\": inlet_u_ref_func, \"v\": inlet_v_ref_func, \"w\": inlet_w_ref_func},\n        geom[\"inlet_geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_inlet},\n        ppsci.loss.MSELoss(\"sum\"),\n        name=\"inlet\",\n    )\n    bc_outlet = ppsci.constraint.BoundaryConstraint(\n        {\"p\": lambda d: d[\"p\"]},\n        {\"p\": 0},\n        geom[\"outlet_geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_outlet},\n        ppsci.loss.MSELoss(\"sum\"),\n        name=\"outlet\",\n    )\n    bc_noslip = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"w\": lambda d: d[\"w\"]},\n        {\"u\": 0, \"v\": 0, \"w\": 0},\n        geom[\"noslip_geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_noslip},\n        ppsci.loss.MSELoss(\"sum\"),\n        name=\"no_slip\",\n    )\n    pde = ppsci.constraint.InteriorConstraint(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0, \"momentum_z\": 0},\n        geom[\"interior_geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.pde},\n        ppsci.loss.MSELoss(\"sum\"),\n        name=\"interior\",\n    )\n    igc_outlet = ppsci.constraint.IntegralConstraint(\n        equation[\"NormalDotVec\"].equations,\n        {\"normal_dot_vec\": 2.54},\n        geom[\"outlet_geo\"],\n        {\n            **train_dataloader_cfg,\n            \"iters_per_epoch\": cfg.TRAIN.iters_integral.igc_outlet,\n            \"batch_size\": cfg.TRAIN.batch_size.igc_outlet,\n            \"integral_batch_size\": cfg.TRAIN.integral_batch_size.igc_outlet,\n        },\n        ppsci.loss.IntegralLoss(\"sum\"),\n        weight_dict=cfg.TRAIN.weight.igc_outlet,\n        name=\"igc_outlet\",\n    )\n    igc_integral = ppsci.constraint.IntegralConstraint(\n        equation[\"NormalDotVec\"].equations,\n        {\"normal_dot_vec\": -2.54},\n        geom[\"integral_geo\"],\n        {\n            **train_dataloader_cfg,\n            \"iters_per_epoch\": cfg.TRAIN.iters_integral.igc_integral,\n            \"batch_size\": cfg.TRAIN.batch_size.igc_integral,\n            \"integral_batch_size\": cfg.TRAIN.integral_batch_size.igc_integral,\n        },\n        ppsci.loss.IntegralLoss(\"sum\"),\n        weight_dict=cfg.TRAIN.weight.igc_integral,\n        name=\"igc_integral\",\n    )\n    # wrap constraints together\n    constraint = {\n        bc_inlet.name: bc_inlet,\n        bc_outlet.name: bc_outlet,\n        bc_noslip.name: bc_noslip,\n        pde.name: pde,\n        igc_outlet.name: igc_outlet,\n        igc_integral.name: igc_integral,\n    }\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n        **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    # set validator\n    eval_data_dict = reader.load_csv_file(\n        cfg.EVAL_CSV_PATH,\n        (\"x\", \"y\", \"z\", \"u\", \"v\", \"w\", \"p\"),\n        {\n            \"x\": \"Points:0\",\n            \"y\": \"Points:1\",\n            \"z\": \"Points:2\",\n            \"u\": \"U:0\",\n            \"v\": \"U:1\",\n            \"w\": \"U:2\",\n            \"p\": \"p\",\n        },\n    )\n    input_dict = {\n        \"x\": (eval_data_dict[\"x\"] - cfg.CENTER[0]) * cfg.SCALE,\n        \"y\": (eval_data_dict[\"y\"] - cfg.CENTER[1]) * cfg.SCALE,\n        \"z\": (eval_data_dict[\"z\"] - cfg.CENTER[2]) * cfg.SCALE,\n    }\n    if \"area\" in input_dict.keys():\n        input_dict[\"area\"] *= cfg.SCALE ** (equation[\"NavierStokes\"].dim)\n\n    label_dict = {\n        \"p\": eval_data_dict[\"p\"],\n        \"u\": eval_data_dict[\"u\"],\n        \"v\": eval_data_dict[\"v\"],\n        \"w\": eval_data_dict[\"w\"],\n    }\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict,\n            \"label\": label_dict,\n        },\n        \"sampler\": {\"name\": \"BatchSampler\"},\n        \"num_workers\": 1,\n    }\n    sup_validator = ppsci.validate.SupervisedValidator(\n        {**eval_dataloader_cfg, \"batch_size\": cfg.EVAL.batch_size.sup_validator},\n        ppsci.loss.MSELoss(\"mean\"),\n        {\n            \"p\": lambda out: out[\"p\"],\n            \"u\": lambda out: out[\"u\"],\n            \"v\": lambda out: out[\"v\"],\n            \"w\": lambda out: out[\"w\"],\n        },\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"ref_u_v_w_p\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set visualizer(optional)\n    visualizer = {\n        \"visualize_u_v_w_p\": ppsci.visualize.VisualizerVtu(\n            input_dict,\n            {\n                \"p\": lambda out: out[\"p\"],\n                \"u\": lambda out: out[\"u\"],\n                \"v\": lambda out: out[\"v\"],\n                \"w\": lambda out: out[\"w\"],\n            },\n            batch_size=cfg.EVAL.batch_size.sup_validator,\n            prefix=\"result_u_v_w_p\",\n        ),\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        eval_during_train=True,\n        eval_freq=cfg.TRAIN.eval_freq,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set validator\n    eval_data_dict = reader.load_csv_file(\n        cfg.EVAL_CSV_PATH,\n        (\"x\", \"y\", \"z\", \"u\", \"v\", \"w\", \"p\"),\n        {\n            \"x\": \"Points:0\",\n            \"y\": \"Points:1\",\n            \"z\": \"Points:2\",\n            \"u\": \"U:0\",\n            \"v\": \"U:1\",\n            \"w\": \"U:2\",\n            \"p\": \"p\",\n        },\n    )\n    input_dict = {\n        \"x\": (eval_data_dict[\"x\"] - cfg.CENTER[0]) * cfg.SCALE,\n        \"y\": (eval_data_dict[\"y\"] - cfg.CENTER[1]) * cfg.SCALE,\n        \"z\": (eval_data_dict[\"z\"] - cfg.CENTER[2]) * cfg.SCALE,\n    }\n\n    label_dict = {\n        \"p\": eval_data_dict[\"p\"],\n        \"u\": eval_data_dict[\"u\"],\n        \"v\": eval_data_dict[\"v\"],\n        \"w\": eval_data_dict[\"w\"],\n    }\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict,\n            \"label\": label_dict,\n        },\n        \"sampler\": {\"name\": \"BatchSampler\"},\n        \"num_workers\": 1,\n    }\n    sup_validator = ppsci.validate.SupervisedValidator(\n        {**eval_dataloader_cfg, \"batch_size\": cfg.EVAL.batch_size.sup_validator},\n        ppsci.loss.MSELoss(\"mean\"),\n        {\n            \"p\": lambda out: out[\"p\"],\n            \"u\": lambda out: out[\"u\"],\n            \"v\": lambda out: out[\"v\"],\n            \"w\": lambda out: out[\"w\"],\n        },\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"ref_u_v_w_p\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set visualizer\n    visualizer = {\n        \"visualize_u_v_w_p\": ppsci.visualize.VisualizerVtu(\n            input_dict,\n            {\n                \"p\": lambda out: out[\"p\"],\n                \"u\": lambda out: out[\"u\"],\n                \"v\": lambda out: out[\"v\"],\n                \"w\": lambda out: out[\"w\"],\n            },\n            batch_size=cfg.EVAL.batch_size.sup_validator,\n            prefix=\"result_u_v_w_p\",\n        ),\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate\n    solver.eval()\n    # visualize prediction\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path, with_onnx=False)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n    eval_data_dict = reader.load_csv_file(\n        cfg.EVAL_CSV_PATH,\n        (\"x\", \"y\", \"z\", \"u\", \"v\", \"w\", \"p\"),\n        {\n            \"x\": \"Points:0\",\n            \"y\": \"Points:1\",\n            \"z\": \"Points:2\",\n            \"u\": \"U:0\",\n            \"v\": \"U:1\",\n            \"w\": \"U:2\",\n            \"p\": \"p\",\n        },\n    )\n    input_dict = {\n        \"x\": (eval_data_dict[\"x\"] - cfg.CENTER[0]) * cfg.SCALE,\n        \"y\": (eval_data_dict[\"y\"] - cfg.CENTER[1]) * cfg.SCALE,\n        \"z\": (eval_data_dict[\"z\"] - cfg.CENTER[2]) * cfg.SCALE,\n    }\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n\n    ppsci.visualize.save_vtu_from_dict(\n        \"./aneurysm_pred.vtu\",\n        {**input_dict, **output_dict},\n        input_dict.keys(),\n        cfg.MODEL.output_keys,\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"aneurysm.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/aneurysm/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u5bf9\u4e8e\u8840\u7ba1\u7624\u6d4b\u8bd5\u96c6\uff08\u5171 2,962,708 \u4e2a\u4e09\u7ef4\u5750\u6807\u70b9\uff09\uff0c\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u5982\u4e0b\u6240\u793a\u3002</p> <p> </p>  \u5de6\u4fa7\u4e3aPaddleScience\u9884\u6d4b\u7ed3\u679c\uff0c\u4e2d\u95f4\u4e3aOpenFOAM\u6c42\u89e3\u5668\u9884\u6d4b\u7ed3\u679c\uff0c\u53f3\u4fa7\u4e3a\u4e24\u8005\u7684\u5dee\u503c <p>\u53ef\u4ee5\u770b\u5230\u5bf9\u4e8e\u7ba1\u58c1\u538b\u529b\\(p(x,y,z)\\)\uff0c\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u548c OpenFOAM \u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p>"},{"location":"zh/examples/aneurysm/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>Aneurysm - NVIDIA Modulus</li> </ul>"},{"location":"zh/examples/biharmonic2d/","title":"Biharmonic2D","text":"","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#2d-biharmonic","title":"2D-Biharmonic","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code>python biharmonic2d.py\n</code></pre> <pre><code>python biharmonic2d.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/biharmonic2d/biharmonic2d_pretrained.pdparams\n</code></pre> <pre><code>python biharmonic2d.py mode=export\n</code></pre> <pre><code>python biharmonic2d.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 biharmonic2d_pretrained.pdparams l2_error: 0.02774","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u53cc\u8c03\u548c\u65b9\u7a0b\uff08Biharmonic Equation\uff09\u662f\u4e00\u79cd\u8868\u5f81\u5e94\u529b\u3001\u5e94\u53d8\u548c\u8f7d\u8377\u4e4b\u95f4\u5173\u7cfb\u7684\u65b9\u7a0b\uff0c\u5b83\u662f\u4e00\u79cd\u56db\u9636\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u56e0\u6b64\u5728\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u4e2d\u96be\u4ee5\u89e3\u51b3\u3002\u672c\u6848\u4f8b\u5c1d\u8bd5\u4f7f\u7528 PINNs(Physics Informed Neural Networks) \u65b9\u6cd5\u89e3\u51b3 Biharmonic \u65b9\u7a0b\u5728 2D \u77e9\u5f62\u5e73\u677f\u4e0a\u7684\u5e94\u7528\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6839\u636e\u7ebf\u5f39\u6027\u7b49\u65b9\u7a0b\u8fdb\u884c\u6c42\u89e3\u3002</p>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u6848\u4f8b\u7ed3\u6784\u4e3a\u4e00\u4e2a\u957f\u3001\u5bbd\u548c\u539a\u5206\u522b\u4e3a 2 m\u30013 m \u548c 0.01 m \u7684\u77e9\u5f62\u5e73\u677f\uff0c\u5e73\u677f\u56db\u5468\u56fa\u5b9a\uff0c\u8868\u9762\u5219\u88ab\u65bd\u52a0\u4e00\u4e2a\u6b63\u5f26\u5206\u5e03\u8f7d\u8377 \\(q=q_0sin(\\dfrac{\\pi x}{a})sin(\\dfrac{\\pi x}{b})\\)\uff0c\u5176\u4e2d \\(q_0=980 Pa\\)\u3002PDE \u65b9\u7a0b\u4e3a 2D \u4e0b\u7684 Biharmonic \u65b9\u7a0b\uff0c\u516c\u5f0f\u4e3a\uff1a</p> \\[\\nabla^4w=(\\dfrac{\\partial^2}{\\partial x^2}+\\dfrac{\\partial^2}{\\partial y^2})(\\dfrac{\\partial^2}{\\partial x^2}+\\dfrac{\\partial^2}{\\partial y^2})w=\\dfrac{q}{D}\\] <p>\u5176\u4e2d \\(w\\) \u4e3a\u5e73\u677f\u6320\u5ea6\uff0c\\(D\\) \u4e3a\u6297\u5f2f\u521a\u5ea6\uff0c\u53ef\u8ba1\u7b97\u5982\u4e0b\uff1a</p> \\[D=\\dfrac{Et^3}{12(1-\\nu^2)}\\] <p>\u5176\u4e2d \\(E=201880.0e+6 Pa\\) \u4e3a\u5f39\u6027\u6768\u6c0f\u6a21\u91cf\uff0c\\(\\nu=0.25\\) \u4e3a\u6cca\u677e\u6bd4\u3002</p> <p>\u6839\u636e\u5e73\u677f\u6320\u5ea6\\(w\\)\uff0c\u53ef\u8ba1\u7b97\u626d\u77e9\u548c\u526a\u5207\u529b\u5982\u4e0b\uff1a</p> \\[ \\begin{cases}   M_x=-D(\\dfrac{\\partial^2w}{\\partial x^2}+\\nu\\dfrac{\\partial^2w}{\\partial y^2}) \\\\   M_y=-D(\\dfrac{\\partial^2w}{\\partial y^2}+\\nu\\dfrac{\\partial^2w}{\\partial x^2}) \\\\   M_{xy}=D(1-\\nu\\dfrac{\\partial^2w}{\\partial x y}) \\\\   Q_x=-D\\dfrac{\\partial}{\\partial x}(\\dfrac{\\partial^2w}{\\partial x^2}+\\dfrac{\\partial^2w}{\\partial y^2}) \\\\   Q_y=-D\\dfrac{\\partial}{\\partial y}(\\dfrac{\\partial^2w}{\\partial x^2}+\\dfrac{\\partial^2w}{\\partial y^2}) \\\\ \\end{cases} \\] <p>\u7531\u4e8e\u5e73\u677f\u56db\u5468\u56fa\u5b9a\uff0c\u5728 \\(x=0\\) \u548c \\(x=x_{max}\\) \u4e0a\uff0c\u6320\u5ea6 \\(w\\) \u548c \\(y\\) \u65b9\u5411\u7684\u529b\u77e9 \\(M_y\\) \u4e3a 0\uff0c\u5728 \\(y=0\\) \u548c \\(y=y_{max}\\) \u4e0a\uff0c \u6320\u5ea6 \\(w\\) \u548c \\(x\\) \u65b9\u5411\u7684\u529b\u77e9 \\(M_x\\) \u4e3a 0\uff0c\u5373\uff1a</p> \\[ \\begin{cases}   w|_{x=0\\ |\\ x=\\ a}=0 \\\\   M_y|_{x=0\\ |\\ x=\\ a}=0 \\\\   w|_{y=0\\ |\\ y=\\ b}=0 \\\\   M_x|_{y=0\\ |\\ y=\\ b}=0 \\\\ \\end{cases} \\] <p>\u76ee\u6807\u6c42\u89e3\u8be5\u5e73\u677f\u8868\u9762\u6bcf\u4e2a\u70b9\u7684\u6320\u5ea6 \\(w\\)\uff0c\u5e76\u4ee5\u6b64\u8ba1\u7b97\u51fa\u529b\u77e9\u548c\u526a\u5207\u529b \\(M_x\\)\u3001\\(M_y\\)\u3001\\(M_{xy}\\)\u3001\\(Q_x\\)\u3001\\(Q_y\\) \u5171 6 \u4e2a\u7269\u7406\u91cf\u3002\u5e38\u91cf\u5b9a\u4e49\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>log_freq: 20\n\n# set working condition\nE: 201880.0e+6  # Pa = N/m2\nNU: 0.25\nQ_0: 980     # Pa = N/m2\nLENGTH: 2        # m\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 biharmonic2d \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y)\\) \u90fd\u6709\u5bf9\u5e94\u7684\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff1a\u53d7\u529b\u65b9\u5411\uff08\u5373 z \u65b9\u5411\uff09\u7684\u6320\u5ea6 \\(w\\) \u548c\u529b\u77e9 \\((M_x, M_y, M_{xy})\\) \u3001\u526a\u5207\u529b $(Q_x, Q_y)\uff0c\u4f46\u7531\u4e8e\u529b\u77e9\u548c\u526a\u5207\u529b\u4e3a\u6320\u5ea6\u8ba1\u7b97\u5f97\u5230\uff0c\u5b9e\u9645\u9700\u8981\u6c42\u51fa\u7684\u672a\u77e5\u91cf\u53ea\u6709\u6320\u5ea6 \\(w\\)\uff0c\u56e0\u6b64\u4ec5\u9700\u6784\u5efa\u4e00\u4e2a\u6a21\u578b\uff1a</p> \\[w = f(x,y)\\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a\u6320\u5ea6\u6a21\u578b <code>disp_net</code>\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> <pre><code># set models\ndisp_net = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u5728\u8fd9\u91cc\u6307\u5b9a\u5e94\u53d8\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"x\", \"y\")</code>\uff0c\u4e3a\u4e86\u4e0e PaddleScience \u5185\u7f6e\u65b9\u7a0b API ppsci.equation.Biharmonic \u5339\u914d\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>(\"u\")</code> \u800c\u4e0d\u662f <code>(\"w\")</code> \uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\uff0c\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 5 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 20 \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>disp_net</code>\uff0c\u4f7f\u7528 <code>tanh</code> \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u4f7f\u7528 <code>WeightNorm</code> \u6743\u91cd\u5f52\u4e00\u5316\u3002</p>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u6d89\u53ca\u5230\u53cc\u8c03\u548c\u65b9\u7a0b\uff0c\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>ppsci.equation.Biharmonic</code> \u5373\u53ef\uff0c\u7531\u4e8e\u8f7d\u8377 \\(q\\) \u4e3a\u975e\u5747\u5300\u8f7d\u8377\uff0c\u9700\u8981\u81ea\u5b9a\u4e49\u8f7d\u8377\u5206\u5e03\u51fd\u6570\uff0c\u5e76\u4f20\u5165 API\u3002</p> <pre><code># set equation\nx, y = sp.symbols(\"x y\")\nQ = cfg.Q_0 * sp.sin(np.pi * x / cfg.LENGTH) * sp.sin(np.pi * y / cfg.WIDTH)\nequation = {\n    \"Biharmonic\": ppsci.equation.Biharmonic(\n        dim=2, q=Q, D=cfg.E * (cfg.HEIGHT**3) / (12.0 * (1.0 - cfg.NU**2))\n    ),\n}\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u7531\u4e8e\u5e73\u677f\u7684\u9ad8\u5f88\u5c0f\uff0c\u672c\u95ee\u9898\u7684\u51e0\u4f55\u533a\u57df\u8ba4\u4e3a\u662f\u957f\u4e3a 2 \u5bbd\u4e3a 3 \u7684 2D \u77e9\u5f62\uff0c\u901a\u8fc7 PaddleScience \u5185\u7f6e\u7684 <code>ppsci.geometry.Rectangle</code> API \u6784\u5efa\uff1a</p> <pre><code># set geometry\nplate = ppsci.geometry.Rectangle((0, 0), (cfg.LENGTH, cfg.WIDTH))\ngeom = {\"geo\": plate}\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u5171\u6d89\u53ca\u5230 9 \u4e2a\u7ea6\u675f\uff0c\u5728\u5177\u4f53\u7ea6\u675f\u6784\u5efa\u4e4b\u524d\uff0c\u53ef\u4ee5\u5148\u6784\u5efa\u6570\u636e\u8bfb\u53d6\u914d\u7f6e\uff0c\u4ee5\u4fbf\u540e\u7eed\u6784\u5efa\u591a\u4e2a\u7ea6\u675f\u65f6\u590d\u7528\u8be5\u914d\u7f6e\u3002</p> <pre><code># set dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": \"NamedArrayDataset\",\n    \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n}\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#341","title":"3.4.1 \u5185\u90e8\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u80cc\u677f\u5185\u90e8\u70b9\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>interior = ppsci.constraint.InteriorConstraint(\n    equation[\"Biharmonic\"].equations,\n    {\"biharmonic\": 0},\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.interior},\n    ppsci.loss.MSELoss(),\n    criteria=lambda x, y: ((0 &lt; x) &amp; (x &lt; cfg.LENGTH) &amp; (0 &lt; y) &amp; (y &lt; cfg.WIDTH)),\n    weight_dict={\"biharmonic\": cfg.TRAIN.weight.interior},\n    name=\"INTERIOR\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\uff08\u7ec4\uff09\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"Biharmonic\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u5e0c\u671b\u4e0e Biharmonic \u65b9\u7a0b\u76f8\u5173\u7684 1 \u4e2a\u503c <code>biharmonic</code> \u88ab\u4f18\u5316\u81f3 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"geo\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u8bbe\u7f6e <code>batch_size</code> \u4e3a\uff1a</p> <pre><code>    tolerance_grad: 1.0e-8\n    tolerance_change: 0\nbatch_size:\n</code></pre> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u9009\u7528\u5e38\u7528\u7684 MSE \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u51e0\u4f55\u70b9\u7b5b\u9009\uff0c\u7531\u4e8e\u8fd9\u4e2a\u7ea6\u675f\u53ea\u65bd\u52a0\u5728\u80cc\u677f\u533a\u57df\uff0c\u56e0\u6b64\u9700\u8981\u5bf9 geo \u4e0a\u91c7\u6837\u51fa\u7684\u70b9\u8fdb\u884c\u7b5b\u9009\uff0c\u6b64\u5904\u4f20\u5165\u4e00\u4e2a lambda \u7b5b\u9009\u51fd\u6570\u5373\u53ef\uff0c\u5176\u63a5\u53d7\u70b9\u96c6\u6784\u6210\u7684\u5f20\u91cf <code>x, y</code>\uff0c\u8fd4\u56de\u5e03\u5c14\u503c\u5f20\u4eae\uff0c\u8868\u793a\u6bcf\u4e2a\u70b9\u662f\u5426\u7b26\u5408\u7b5b\u9009\u6761\u4ef6\uff0c\u4e0d\u7b26\u5408\u4e3a <code>False</code>\uff0c\u7b26\u5408\u4e3a <code>True</code>\uff1b</p> <p>\u7b2c\u4e03\u4e2a\u53c2\u6570\u662f\u6bcf\u4e2a\u70b9\u53c2\u4e0e\u635f\u5931\u8ba1\u7b97\u65f6\u7684\u6743\u91cd\uff0c\u6b64\u5904\u8bbe\u7f6e\u4e3a\uff1a</p> <pre><code>  bc: 125\n  interior: 8000\nweight:\n</code></pre> <p>\u7b2c\u516b\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"INTERIOR\" \u5373\u53ef\u3002</p>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u5982 2. \u95ee\u9898\u5b9a\u4e49 \u4e2d\u6240\u8ff0\uff0c\\(x=0\\) \u5904\u7684\u6320\u5ea6 \\(w\\) \u4e3a 0\uff0c\u6709\u5982\u4e0b\u8fb9\u754c\u6761\u4ef6\uff0c\u5176\u4ed6 7 \u4e2a\u8fb9\u754c\u6761\u4ef6\u4e5f\u4e0e\u4e4b\u7c7b\u4f3c\uff1a</p> <pre><code># set constraint\nbc_left = ppsci.constraint.BoundaryConstraint(\n    {\"w\": lambda d: d[\"u\"]},\n    {\"w\": 0},\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n    ppsci.loss.MSELoss(),\n    criteria=lambda x, y: x == 0,\n    weight_dict={\"w\": cfg.TRAIN.weight.bc},\n    name=\"BC_LEFT\",\n)\n</code></pre> <p>\u5728\u65b9\u7a0b\u7ea6\u675f\u3001\u8fb9\u754c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    bc_left.name: bc_left,\n    bc_right.name: bc_right,\n    bc_up.name: bc_up,\n    bc_bottom.name: bc_bottom,\n    bc_left_My.name: bc_left_My,\n    bc_right_My.name: bc_right_My,\n    bc_up_Mx.name: bc_up_Mx,\n    bc_bottom_Mx.name: bc_bottom_Mx,\n    interior.name: interior,\n}\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u4f7f\u7528 <code>Adam</code> \u5148\u8fdb\u884c\u5c11\u91cf\u8bad\u7ec3\u540e\uff0c\u518d\u4f7f\u7528 <code>LBFGS</code> \u4f18\u5316\u5668\u7cbe\u8c03\u3002</p> <pre><code>optimizer_adam = ppsci.optimizer.Adam(**cfg.TRAIN.optimizer.adam)(disp_net)\noptimizer_lbfgs = ppsci.optimizer.LBFGS(**cfg.TRAIN.optimizer.lbfgs)(disp_net)\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#36","title":"3.6 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\u7b49\u4f18\u5316\u5668\u53c2\u6570\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 1000\n  iters_per_epoch: 1\n  optimizer:\n    adam:\n      learning_rate: 1.0e-3\n    lbfgs:\n      learning_rate: 1.0\n      max_iter: 50000\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\uff0c\u6ce8\u610f\u4e24\u4e2a\u4f18\u5316\u8fc7\u7a0b\u9700\u8981\u5206\u522b\u6784\u5efa <code>Solver</code>\u3002</p> <pre><code># initialize adam solver\nsolver_adam = ppsci.solver.Solver(\n    disp_net,\n    constraint,\n    cfg.output_dir,\n    optimizer_adam,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    log_freq=cfg.log_freq,\n    seed=cfg.seed,\n    equation=equation,\n    geom=geom,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n    pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n)\n# train model\nsolver_adam.train()\n# plot loss\nsolver_adam.plot_loss_history(by_epoch=True)\n# initialize lbfgs solver\nsolver_lbfgs = ppsci.solver.Solver(\n    disp_net,\n    constraint,\n    cfg.output_dir,\n    optimizer_lbfgs,\n    None,\n    1,\n    1,\n    save_freq=cfg.TRAIN.save_freq,\n    log_freq=cfg.log_freq,\n    seed=cfg.seed,\n    equation=equation,\n    geom=geom,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n    pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n)\n# evaluate after finished training\nsolver_lbfgs.train()\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#38","title":"3.8 \u6a21\u578b\u8bc4\u4f30\u548c\u53ef\u89c6\u5316","text":"<p>\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u5728 <code>eval</code> \u6a21\u5f0f\u4e2d\u5bf9\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u3002\u7531\u4e8e\u6848\u4f8b\u7684\u7279\u6b8a\u6027\uff0c\u4e0d\u9700\u6784\u5efa\u8bc4\u4f30\u5668\u548c\u53ef\u89c6\u5316\u5668\uff0c\u800c\u662f\u4f7f\u7528\u81ea\u5b9a\u4e49\u4ee3\u7801\u3002</p> <pre><code>def evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set models\n    disp_net = ppsci.arch.MLP(**cfg.MODEL)\n\n    # load pretrained model\n    solver = ppsci.solver.Solver(\n        model=disp_net, pretrained_model_path=cfg.EVAL.pretrained_model_path\n    )\n\n    # generate samples\n    num_x = 201\n    num_y = 301\n    num_cords = num_x * num_y\n    logger.info(f\"num_cords: {num_cords}\")\n    x_grad, y_grad = np.meshgrid(\n        np.linspace(start=0, stop=cfg.LENGTH, num=num_x, endpoint=True),\n        np.linspace(start=0, stop=cfg.WIDTH, num=num_y, endpoint=True),\n    )\n    x_faltten = paddle.to_tensor(\n        x_grad.flatten()[:, None], dtype=paddle.get_default_dtype(), stop_gradient=False\n    )\n    y_faltten = paddle.to_tensor(\n        y_grad.flatten()[:, None], dtype=paddle.get_default_dtype(), stop_gradient=False\n    )\n    outs_pred = solver.predict(\n        {\"x\": x_faltten, \"y\": y_faltten}, batch_size=num_cords, no_grad=False\n    )\n\n    # generate label\n    D = cfg.E * (cfg.HEIGHT**3) / (12.0 * (1.0 - cfg.NU**2))\n    Q = cfg.Q_0 / (\n        (np.pi**4) * D * ((1 / (cfg.LENGTH**2) + 1 / (cfg.WIDTH**2)) ** 2)\n    )\n    outs_label = (\n        paddle.to_tensor(Q, dtype=paddle.get_default_dtype())\n        * paddle.sin(\n            paddle.to_tensor(np.pi / cfg.LENGTH, dtype=paddle.get_default_dtype())\n            * x_faltten,\n        )\n        * paddle.sin(\n            paddle.to_tensor(np.pi / cfg.WIDTH, dtype=paddle.get_default_dtype())\n            * y_faltten,\n        )\n    )\n\n    # eval\n    l2_error = ppsci.metric.L2Rel()(outs_pred, {\"u\": outs_label})[\"u\"]\n    logger.info(f\"l2_error: {float(l2_error)}\")\n\n    # compute other pred outs\n    def compute_outs(w, x, y):\n        D = cfg.E * (cfg.HEIGHT**3) / (12.0 * (1.0 - cfg.NU**2))\n        w_x2 = hessian(w, x)\n        w_y2 = hessian(w, y)\n        w_x_y = jacobian(jacobian(w, x), y)\n        M_x = -(w_x2 + cfg.NU * w_y2) * D\n        M_y = -(cfg.NU * w_x2 + w_y2) * D\n        M_xy = (1 - cfg.NU) * w_x_y * D\n        Q_x = -jacobian((w_x2 + w_y2), x) * D\n        Q_y = -jacobian((w_x2 + w_y2), y) * D\n        return {\"Mx\": M_x, \"Mxy\": M_xy, \"My\": M_y, \"Qx\": Q_x, \"Qy\": Q_y, \"w\": w}\n\n    outs = compute_outs(outs_pred[\"u\"], x_faltten, y_faltten)\n\n    # plotting\n    griddata_points = paddle.concat([x_faltten, y_faltten], axis=-1).numpy()\n    griddata_xi = (x_grad, y_grad)\n    boundary = [0, cfg.LENGTH, 0, cfg.WIDTH]\n    plotting(\n        \"eval_Mx_Mxy_My_Qx_Qy_w\",\n        cfg.output_dir,\n        {k: v.numpy() for k, v in outs.items()},\n        griddata_points,\n        griddata_xi,\n        boundary,\n    )\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"biharmonic2d.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\n\nimport hydra\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport paddle\nimport sympy as sp\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom omegaconf import DictConfig\nfrom scipy.interpolate import griddata\n\nimport ppsci\nfrom ppsci.autodiff import hessian\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\n\n\ndef plotting(figname, output_dir, data, griddata_points, griddata_xi, boundary):\n    plt.clf()\n    fig = plt.figure(figname, figsize=(15, 12))\n    gs = gridspec.GridSpec(2, 3)\n    gs.update(top=0.8, bottom=0.2, left=0.1, right=0.9, wspace=0.5)\n\n    for i, key in enumerate(data):\n        plot_data = griddata(\n            griddata_points,\n            data[key].flatten(),\n            griddata_xi,\n            method=\"cubic\",\n        )\n\n        ax = plt.subplot(gs[i // 3, i % 3])\n        h = ax.imshow(\n            plot_data,\n            interpolation=\"nearest\",\n            cmap=\"jet\",\n            extent=boundary,\n            origin=\"lower\",\n            aspect=\"auto\",\n        )\n        divider = make_axes_locatable(ax)\n        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n        fig.colorbar(h, cax=cax)\n        ax.axis(\"equal\")\n        ax.set_xlim(0, boundary[1])\n        ax.set_ylim(0, boundary[3])\n        ax.set_xlabel(\"$x$\")\n        ax.set_ylabel(\"$y$\")\n        plt.tick_params(labelsize=12)\n        ax.set_title(key, fontsize=10)\n\n    plt.savefig(osp.join(output_dir, figname))\n    plt.close()\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set models\n    disp_net = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set optimizer\n    optimizer_adam = ppsci.optimizer.Adam(**cfg.TRAIN.optimizer.adam)(disp_net)\n    optimizer_lbfgs = ppsci.optimizer.LBFGS(**cfg.TRAIN.optimizer.lbfgs)(disp_net)\n\n    # set equation\n    x, y = sp.symbols(\"x y\")\n    Q = cfg.Q_0 * sp.sin(np.pi * x / cfg.LENGTH) * sp.sin(np.pi * y / cfg.WIDTH)\n    equation = {\n        \"Biharmonic\": ppsci.equation.Biharmonic(\n            dim=2, q=Q, D=cfg.E * (cfg.HEIGHT**3) / (12.0 * (1.0 - cfg.NU**2))\n        ),\n    }\n\n    # set geometry\n    plate = ppsci.geometry.Rectangle((0, 0), (cfg.LENGTH, cfg.WIDTH))\n    geom = {\"geo\": plate}\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n    }\n\n    # set constraint\n    bc_left = ppsci.constraint.BoundaryConstraint(\n        {\"w\": lambda d: d[\"u\"]},\n        {\"w\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n        ppsci.loss.MSELoss(),\n        criteria=lambda x, y: x == 0,\n        weight_dict={\"w\": cfg.TRAIN.weight.bc},\n        name=\"BC_LEFT\",\n    )\n    bc_right = ppsci.constraint.BoundaryConstraint(\n        {\"w\": lambda d: d[\"u\"]},\n        {\"w\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n        ppsci.loss.MSELoss(),\n        criteria=lambda x, y: x == cfg.LENGTH,\n        weight_dict={\"w\": cfg.TRAIN.weight.bc},\n        name=\"BC_RIGHT\",\n    )\n    bc_up = ppsci.constraint.BoundaryConstraint(\n        {\"w\": lambda d: d[\"u\"]},\n        {\"w\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n        ppsci.loss.MSELoss(),\n        criteria=lambda x, y: y == 0,\n        weight_dict={\"w\": cfg.TRAIN.weight.bc},\n        name=\"BC_UP\",\n    )\n    bc_bottom = ppsci.constraint.BoundaryConstraint(\n        {\"w\": lambda d: d[\"u\"]},\n        {\"w\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n        ppsci.loss.MSELoss(),\n        criteria=lambda x, y: y == cfg.WIDTH,\n        weight_dict={\"w\": cfg.TRAIN.weight.bc},\n        name=\"BC_BOTTOM\",\n    )\n    bc_left_My = ppsci.constraint.BoundaryConstraint(\n        {\n            \"M_y\": lambda d: -(\n                cfg.NU * hessian(d[\"u\"], d[\"x\"]) + hessian(d[\"u\"], d[\"y\"])\n            )\n        },\n        {\"M_y\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n        ppsci.loss.MSELoss(),\n        criteria=lambda x, y: x == 0,\n        weight_dict={\"M_y\": cfg.TRAIN.weight.bc},\n        name=\"BC_LEFT_My\",\n    )\n    bc_right_My = ppsci.constraint.BoundaryConstraint(\n        {\n            \"M_y\": lambda d: -(\n                cfg.NU * hessian(d[\"u\"], d[\"x\"]) + hessian(d[\"u\"], d[\"y\"])\n            )\n        },\n        {\"M_y\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n        ppsci.loss.MSELoss(),\n        criteria=lambda x, y: x == cfg.LENGTH,\n        weight_dict={\"M_y\": cfg.TRAIN.weight.bc},\n        name=\"BC_RIGHT_My\",\n    )\n    bc_up_Mx = ppsci.constraint.BoundaryConstraint(\n        {\n            \"M_x\": lambda d: -(\n                hessian(d[\"u\"], d[\"x\"]) + cfg.NU * hessian(d[\"u\"], d[\"y\"])\n            )\n        },\n        {\"M_x\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n        ppsci.loss.MSELoss(),\n        criteria=lambda x, y: y == 0,\n        weight_dict={\"M_x\": cfg.TRAIN.weight.bc},\n        name=\"BC_UP_Mx\",\n    )\n    bc_bottom_Mx = ppsci.constraint.BoundaryConstraint(\n        {\n            \"M_x\": lambda d: -(\n                hessian(d[\"u\"], d[\"x\"]) + cfg.NU * hessian(d[\"u\"], d[\"y\"])\n            )\n        },\n        {\"M_x\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n        ppsci.loss.MSELoss(),\n        criteria=lambda x, y: y == cfg.WIDTH,\n        weight_dict={\"M_x\": cfg.TRAIN.weight.bc},\n        name=\"BC_BOTTOM_Mx\",\n    )\n    interior = ppsci.constraint.InteriorConstraint(\n        equation[\"Biharmonic\"].equations,\n        {\"biharmonic\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.interior},\n        ppsci.loss.MSELoss(),\n        criteria=lambda x, y: ((0 &lt; x) &amp; (x &lt; cfg.LENGTH) &amp; (0 &lt; y) &amp; (y &lt; cfg.WIDTH)),\n        weight_dict={\"biharmonic\": cfg.TRAIN.weight.interior},\n        name=\"INTERIOR\",\n    )\n    # wrap constraints together\n    constraint = {\n        bc_left.name: bc_left,\n        bc_right.name: bc_right,\n        bc_up.name: bc_up,\n        bc_bottom.name: bc_bottom,\n        bc_left_My.name: bc_left_My,\n        bc_right_My.name: bc_right_My,\n        bc_up_Mx.name: bc_up_Mx,\n        bc_bottom_Mx.name: bc_bottom_Mx,\n        interior.name: interior,\n    }\n\n    # initialize adam solver\n    solver_adam = ppsci.solver.Solver(\n        disp_net,\n        constraint,\n        cfg.output_dir,\n        optimizer_adam,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n    )\n    # train model\n    solver_adam.train()\n    # plot loss\n    solver_adam.plot_loss_history(by_epoch=True)\n    # initialize lbfgs solver\n    solver_lbfgs = ppsci.solver.Solver(\n        disp_net,\n        constraint,\n        cfg.output_dir,\n        optimizer_lbfgs,\n        None,\n        1,\n        1,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n    )\n    # evaluate after finished training\n    solver_lbfgs.train()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set models\n    disp_net = ppsci.arch.MLP(**cfg.MODEL)\n\n    # load pretrained model\n    solver = ppsci.solver.Solver(\n        model=disp_net, pretrained_model_path=cfg.EVAL.pretrained_model_path\n    )\n\n    # generate samples\n    num_x = 201\n    num_y = 301\n    num_cords = num_x * num_y\n    logger.info(f\"num_cords: {num_cords}\")\n    x_grad, y_grad = np.meshgrid(\n        np.linspace(start=0, stop=cfg.LENGTH, num=num_x, endpoint=True),\n        np.linspace(start=0, stop=cfg.WIDTH, num=num_y, endpoint=True),\n    )\n    x_faltten = paddle.to_tensor(\n        x_grad.flatten()[:, None], dtype=paddle.get_default_dtype(), stop_gradient=False\n    )\n    y_faltten = paddle.to_tensor(\n        y_grad.flatten()[:, None], dtype=paddle.get_default_dtype(), stop_gradient=False\n    )\n    outs_pred = solver.predict(\n        {\"x\": x_faltten, \"y\": y_faltten}, batch_size=num_cords, no_grad=False\n    )\n\n    # generate label\n    D = cfg.E * (cfg.HEIGHT**3) / (12.0 * (1.0 - cfg.NU**2))\n    Q = cfg.Q_0 / (\n        (np.pi**4) * D * ((1 / (cfg.LENGTH**2) + 1 / (cfg.WIDTH**2)) ** 2)\n    )\n    outs_label = (\n        paddle.to_tensor(Q, dtype=paddle.get_default_dtype())\n        * paddle.sin(\n            paddle.to_tensor(np.pi / cfg.LENGTH, dtype=paddle.get_default_dtype())\n            * x_faltten,\n        )\n        * paddle.sin(\n            paddle.to_tensor(np.pi / cfg.WIDTH, dtype=paddle.get_default_dtype())\n            * y_faltten,\n        )\n    )\n\n    # eval\n    l2_error = ppsci.metric.L2Rel()(outs_pred, {\"u\": outs_label})[\"u\"]\n    logger.info(f\"l2_error: {float(l2_error)}\")\n\n    # compute other pred outs\n    def compute_outs(w, x, y):\n        D = cfg.E * (cfg.HEIGHT**3) / (12.0 * (1.0 - cfg.NU**2))\n        w_x2 = hessian(w, x)\n        w_y2 = hessian(w, y)\n        w_x_y = jacobian(jacobian(w, x), y)\n        M_x = -(w_x2 + cfg.NU * w_y2) * D\n        M_y = -(cfg.NU * w_x2 + w_y2) * D\n        M_xy = (1 - cfg.NU) * w_x_y * D\n        Q_x = -jacobian((w_x2 + w_y2), x) * D\n        Q_y = -jacobian((w_x2 + w_y2), y) * D\n        return {\"Mx\": M_x, \"Mxy\": M_xy, \"My\": M_y, \"Qx\": Q_x, \"Qy\": Q_y, \"w\": w}\n\n    outs = compute_outs(outs_pred[\"u\"], x_faltten, y_faltten)\n\n    # plotting\n    griddata_points = paddle.concat([x_faltten, y_faltten], axis=-1).numpy()\n    griddata_xi = (x_grad, y_grad)\n    boundary = [0, cfg.LENGTH, 0, cfg.WIDTH]\n    plotting(\n        \"eval_Mx_Mxy_My_Qx_Qy_w\",\n        cfg.output_dir,\n        {k: v.numpy() for k, v in outs.items()},\n        griddata_points,\n        griddata_xi,\n        boundary,\n    )\n\n\ndef export(cfg: DictConfig):\n    from paddle import nn\n    from paddle.static import InputSpec\n\n    # set models\n    disp_net = ppsci.arch.MLP(**cfg.MODEL)\n\n    # load pretrained model\n    solver = ppsci.solver.Solver(\n        model=disp_net, pretrained_model_path=cfg.INFER.pretrained_model_path\n    )\n\n    class Wrapped_Model(nn.Layer):\n        def __init__(self, model):\n            super().__init__()\n            self.model = model\n\n        def forward(self, x):\n            model_out = self.model(x)\n            outs = self.compute_outs(model_out[\"u\"], x[\"x\"], x[\"y\"])\n            return outs\n\n        def compute_outs(self, w, x, y):\n            D = cfg.E * (cfg.HEIGHT**3) / (12.0 * (1.0 - cfg.NU**2))\n            w_x2 = hessian(w, x)\n            w_y2 = hessian(w, y)\n            w_x_y = jacobian(jacobian(w, x), y)\n            M_x = -(w_x2 + cfg.NU * w_y2) * D\n            M_y = -(cfg.NU * w_x2 + w_y2) * D\n            M_xy = (1 - cfg.NU) * w_x_y * D\n            Q_x = -jacobian((w_x2 + w_y2), x) * D\n            Q_y = -jacobian((w_x2 + w_y2), y) * D\n            return {\"Mx\": M_x, \"Mxy\": M_xy, \"My\": M_y, \"Qx\": Q_x, \"Qy\": Q_y, \"w\": w}\n\n    solver.model = Wrapped_Model(solver.model)\n\n    # export models\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in disp_net.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    # set model predictor\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # generate samples\n    num_x = 201\n    num_y = 301\n    x_grad, y_grad = np.meshgrid(\n        np.linspace(\n            start=0, stop=cfg.LENGTH, num=num_x, endpoint=True, dtype=np.float32\n        ),\n        np.linspace(\n            start=0, stop=cfg.WIDTH, num=num_y, endpoint=True, dtype=np.float32\n        ),\n    )\n    x_faltten = x_grad.reshape(-1, 1)\n    y_faltten = y_grad.reshape(-1, 1)\n\n    output_dict = predictor.predict(\n        {\"x\": x_faltten, \"y\": y_faltten}, cfg.INFER.batch_size\n    )\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.INFER.output_keys, output_dict.keys())\n    }\n\n    # plotting\n    griddata_points = np.concatenate([x_faltten, y_faltten], axis=-1)\n    griddata_xi = (x_grad, y_grad)\n    boundary = [0, cfg.LENGTH, 0, cfg.WIDTH]\n    plotting(\n        \"eval_Mx_Mxy_My_Qx_Qy_w\",\n        cfg.output_dir,\n        output_dict,\n        griddata_points,\n        griddata_xi,\n        boundary,\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"biharmonic2d.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u9762\u5c55\u793a\u4e86\u6320\u5ea6 \\(w\\) \u4ee5\u53ca\u529b\u77e9 \\(M_x, M_y, M_{xy}\\) \u548c\u526a\u5207\u529b \\(Q_x, Q_y\\) \u7684\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u548c\u7406\u8bba\u89e3\u7ed3\u679c\u3002</p> <p> </p> \u529b\u77e9 Mx, My, Mxy\u3001\u526a\u5207\u529b Qx, Qy \u548c\u6320\u5ea6 w \u7684\u6a21\u578b\u9884\u6d4b\u7ed3\u679c <p> </p> \u529b\u77e9 Mx, My, Mxy \u7684\u7406\u8bba\u89e3\u7ed3\u679c <p> </p> \u526a\u5207\u529b Qx, Qy \u548c\u6320\u5ea6 w \u7684\u7406\u8bba\u89e3\u7ed3\u679c <p>\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u679c\u4e0e\u7406\u8bba\u89e3\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/biharmonic2d/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<p>\u53c2\u8003\u6587\u732e\uff1aA Physics Informed Neural Network Approach to Solution and Identification of Biharmonic Equations of Elasticity</p>","tags":["\u6982\u7387\u7edf\u8ba1","\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Biharmonic\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/","title":"Bracket","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#bracket","title":"Bracket","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/bracket/bracket_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/bracket/bracket_dataset.tar -o bracket_dataset.tar\n# unzip it\ntar -xvf bracket_dataset.tar\npython bracket.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/bracket/bracket_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/bracket/bracket_dataset.tar -o bracket_dataset.tar\n# unzip it\ntar -xvf bracket_dataset.tar\npython bracket.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/bracket/bracket_pretrained.pdparams\n</code></pre> <pre><code>python bracket.py mode=export\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/bracket/bracket_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/bracket/bracket_dataset.tar -o bracket_dataset.tar\n# unzip it\ntar -xvf bracket_dataset.tar\npython bracket.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 bracket_pretrained.pdparams loss(commercial_ref_u_v_w_sigmas): 32.28704MSE.u(commercial_ref_u_v_w_sigmas): 0.00005MSE.v(commercial_ref_u_v_w_sigmas): 0.00000MSE.w(commercial_ref_u_v_w_sigmas): 0.00734MSE.sigma_xx(commercial_ref_u_v_w_sigmas): 27.64751MSE.sigma_yy(commercial_ref_u_v_w_sigmas): 1.23101MSE.sigma_zz(commercial_ref_u_v_w_sigmas): 0.89106MSE.sigma_xy(commercial_ref_u_v_w_sigmas): 0.84370MSE.sigma_xz(commercial_ref_u_v_w_sigmas): 1.42126MSE.sigma_yz(commercial_ref_u_v_w_sigmas): 0.24510","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u7ebf\u5f39\u6027\u65b9\u7a0b\u5728\u5f62\u53d8\u5206\u6790\u4e2d\u8d77\u7740\u6838\u5fc3\u7684\u4f5c\u7528\u3002\u5728\u7269\u7406\u548c\u5de5\u7a0b\u9886\u57df\uff0c\u5f62\u53d8\u5206\u6790\u662f\u7814\u7a76\u7269\u4f53\u5728\u5916\u529b\u4f5c\u7528\u4e0b\u7684\u5f62\u72b6\u548c\u5c3a\u5bf8\u53d8\u5316\u7684\u65b9\u6cd5\u3002\u7ebf\u5f39\u6027\u65b9\u7a0b\u662f\u63cf\u8ff0\u7269\u4f53\u5728\u53d7\u529b\u540e\u6062\u590d\u539f\u72b6\u7684\u80fd\u529b\u7684\u6570\u5b66\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7ebf\u5f39\u6027\u65b9\u7a0b\u901a\u5e38\u662f\u6307\u5e94\u529b\u548c\u5e94\u53d8\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5e94\u529b\u662f\u4e00\u4e2a\u7269\u7406\u91cf\uff0c\u7528\u4e8e\u63cf\u8ff0\u7269\u4f53\u5185\u90e8\u7531\u4e8e\u5916\u529b\u800c\u4ea7\u751f\u7684\u5355\u4f4d\u9762\u79ef\u4e0a\u7684\u529b\u3002\u5e94\u53d8\u5219\u63cf\u8ff0\u4e86\u7269\u4f53\u7684\u5f62\u72b6\u548c\u5c3a\u5bf8\u7684\u53d8\u5316\u3002\u7ebf\u5f39\u6027\u65b9\u7a0b\u901a\u5e38\u53ef\u4ee5\u8868\u793a\u4e3a\u5e94\u529b\u548c\u5e94\u53d8\u4e4b\u95f4\u7684\u7ebf\u6027\u5173\u7cfb\uff0c\u5373\u5e94\u529b\u548c\u5e94\u53d8\u662f\u6210\u6bd4\u4f8b\u7684\u3002\u8fd9\u79cd\u5173\u7cfb\u53ef\u4ee5\u7528\u4e00\u4e2a\u7ebf\u6027\u65b9\u7a0b\u6765\u8868\u793a\uff0c\u5176\u4e2d\u7cfb\u6570\u88ab\u79f0\u4e3a\u5f39\u6027\u6a21\u91cf\uff08\u6216\u6768\u6c0f\u6a21\u91cf\uff09\u3002\u8fd9\u79cd\u6a21\u578b\u5047\u8bbe\u7269\u4f53\u5728\u53d7\u529b\u540e\u80fd\u591f\u5b8c\u5168\u6062\u590d\u539f\u72b6\uff0c\u5373\u6ca1\u6709\u6c38\u4e45\u53d8\u5f62\u3002\u8fd9\u79cd\u5047\u8bbe\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u662f\u5408\u7406\u7684\uff0c\u4f8b\u5982\u5728\u7814\u7a76\u91d1\u5c5e\u7684\u529b\u5b66\u884c\u4e3a\u65f6\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u67d0\u4e9b\u6750\u6599\uff08\u5982\u5851\u6599\u6216\u6a61\u80f6\uff09\uff0c\u8fd9\u79cd\u5047\u8bbe\u53ef\u80fd\u4e0d\u51c6\u786e\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u53d7\u529b\u540e\u53ef\u80fd\u4f1a\u4ea7\u751f\u6c38\u4e45\u53d8\u5f62\u3002\u7ebf\u5f39\u6027\u65b9\u7a0b\u53ea\u662f\u5f62\u53d8\u5206\u6790\u4e2d\u7684\u4e00\u90e8\u5206\u3002\u8981\u5168\u9762\u7406\u89e3\u5f62\u53d8\uff0c\u8fd8\u9700\u8981\u8003\u8651\u5176\u4ed6\u56e0\u7d20\uff0c\u4f8b\u5982\u7269\u4f53\u7684\u521d\u59cb\u5f62\u72b6\u548c\u5c3a\u5bf8\u3001\u5916\u529b\u7684\u5386\u53f2\u3001\u6750\u6599\u7684\u5176\u4ed6\u7269\u7406\u6027\u8d28\uff08\u5982\u70ed\u81a8\u80c0\u7cfb\u6570\u548c\u5bc6\u5ea6\uff09\u7b49\u3002\u7136\u800c\uff0c\u7ebf\u5f39\u6027\u65b9\u7a0b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u672c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0\u548c\u7406\u89e3\u7269\u4f53\u5728\u53d7\u529b\u540e\u7684\u884c\u4e3a\u3002</p> <p>\u672c\u6848\u4f8b\u4e3b\u8981\u7814\u7a76\u5982\u4e0b\u91d1\u5c5e\u8fde\u63a5\u4ef6\u5728\u7ed9\u5b9a\u8f7d\u8377\u4e0b\u7684\u5f62\u53d8\u60c5\u51b5\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6839\u636e\u7ebf\u5f39\u6027\u7b49\u65b9\u7a0b\u8fdb\u884c\u6c42\u89e3\uff0c\u8fde\u63a5\u4ef6\u5982\u4e0b\u6240\u793a\uff08\u53c2\u8003 Matlab deflection-analysis-of-a-bracket\uff09\u3002</p> <p> </p> Bracket \u91d1\u5c5e\u4ef6\u8f7d\u8377\u793a\u610f\u56fe\uff0c\u7ea2\u8272\u533a\u57df\u8868\u793a\u8f7d\u8377\u9762","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u4e0a\u8ff0\u8fde\u63a5\u4ef6\u5305\u62ec\u4e00\u4e2a\u5782\u76f4\u4e8e x \u8f74\u7684\u80cc\u677f\u548c\u4e0e\u4e4b\u8fde\u63a5\u7684\u5782\u76f4\u4e8e z \u8f74\u7684\u5e26\u5b54\u5e73\u677f\u3002\u5176\u4e2d\u80cc\u677f\u5904\u4e8e\u56fa\u5b9a\u72b6\u6001\uff0c\u5e26\u5b54\u5e73\u677f\u7684\u6700\u53f3\u4fa7\u8868\u9762\uff08\u7ea2\u8272\u533a\u57df\uff09\u53d7\u5230 z \u8f74\u8d1f\u65b9\u5411\uff0c\u5355\u4f4d\u9762\u79ef\u5927\u5c0f\u4e3a \\(4 \\times 10^4 Pa\\) \u7684\u5e94\u529b\uff1b\u9664\u6b64\u4e4b\u5916\uff0c\u5176\u4ed6\u53c2\u6570\u5305\u62ec\u5f39\u6027\u6a21\u91cf \\(E=10^{11} Pa\\)\uff0c\u6cca\u677e\u6bd4 \\(\\nu=0.3\\)\u3002\u901a\u8fc7\u8bbe\u7f6e\u7279\u5f81\u957f\u5ea6 \\(L=1m\\)\uff0c\u7279\u5f81\u4f4d\u79fb \\(U=0.0001m\\)\uff0c\u65e0\u91cf\u7eb2\u526a\u5207\u6a21\u91cf \\(0.01\\mu\\)\uff0c\u76ee\u6807\u6c42\u89e3\u8be5\u91d1\u5c5e\u4ef6\u8868\u9762\u6bcf\u4e2a\u70b9\u7684 \\(u\\)\u3001\\(v\\)\u3001\\(w\\)\u3001\\(\\sigma_{xx}\\)\u3001\\(\\sigma_{yy}\\)\u3001\\(\\sigma_{zz}\\)\u3001\\(\\sigma_{xy}\\)\u3001\\(\\sigma_{xz}\\)\u3001\\(\\sigma_{yz}\\) \u5171 9 \u4e2a\u7269\u7406\u91cf\u3002\u5e38\u91cf\u5b9a\u4e49\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># specify parameters\nLAMBDA_ = cfg.NU * cfg.E / ((1 + cfg.NU) * (1 - 2 * cfg.NU))\nMU = cfg.E / (2 * (1 + cfg.NU))\nMU_C = 0.01 * MU\nLAMBDA_ = LAMBDA_ / MU_C\nMU = MU / MU_C\nSIGMA_NORMALIZATION = cfg.CHARACTERISTIC_LENGTH / (\n    cfg.CHARACTERISTIC_DISPLACEMENT * MU_C\n)\nT = -4.0e4 * SIGMA_NORMALIZATION\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 bracket \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y, z)\\) \u90fd\u6709\u5bf9\u5e94\u7684\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff1a\u4e09\u4e2a\u65b9\u5411\u7684\u5e94\u53d8 \\((u, v, w)\\) \u548c\u5e94\u529b \\((\\sigma_{xx}, \\sigma_{yy}, \\sigma_{zz}, \\sigma_{xy}, \\sigma_{xz}, \\sigma_{yz})\\)\u3002</p> <p>\u8fd9\u91cc\u8003\u8651\u5230\u4e24\u7ec4\u7269\u7406\u91cf\u5bf9\u5e94\u7740\u4e0d\u540c\u7684\u65b9\u7a0b\uff0c\u56e0\u6b64\u4f7f\u7528\u4e24\u4e2a\u6a21\u578b\u6765\u5206\u522b\u9884\u6d4b\u8fd9\u4e24\u7ec4\u7269\u7406\u91cf\uff1a</p> \\[ \\begin{cases} u, v, w = f(x,y,z) \\\\ \\sigma_{xx}, \\sigma_{yy}, \\sigma_{zz}, \\sigma_{xy}, \\sigma_{xz}, \\sigma_{yz} = g(x,y,z) \\end{cases} \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a\u5e94\u53d8\u6a21\u578b <code>disp_net</code>\uff0c\\(g\\) \u4e3a\u5e94\u529b\u6a21\u578b <code>stress_net</code>\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> <pre><code># set model\ndisp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\nstress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n# wrap to a model_list\nmodel = ppsci.arch.ModelList((disp_net, stress_net))\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u5728\u8fd9\u91cc\u6307\u5b9a\u5e94\u53d8\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"x\", \"y\", \"z\")</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>(\"u\", \"v\", \"w\")</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\uff08\u5e94\u529b\u6a21\u578b\u540c\u7406\uff09\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\uff0c\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 6 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 512 \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>disp_net</code>\uff0c\u4f7f\u7528 <code>silu</code> \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u4f7f\u7528 <code>WeightNorm</code> \u6743\u91cd\u5f52\u4e00\u5316\uff08\u5e94\u529b\u6a21\u578b <code>stress_net</code> \u540c\u7406\uff09\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>Bracket \u6848\u4f8b\u6d89\u53ca\u5230\u4ee5\u4e0b\u7ebf\u5f39\u6027\u65b9\u7a0b\uff0c\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>LinearElasticity</code> \u5373\u53ef\u3002</p> <p>$$ \\begin{cases}     stress_disp_{xx} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial u}{\\partial x} - \\sigma_{xx} \\     stress_disp_{yy} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial v}{\\partial y} - \\sigma_{yy} \\     stress_disp_{zz} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial w}{\\partial z} - \\sigma_{zz} \\     stress_disp_{xy} = \\mu(\\dfrac{\\partial u}{\\partial y} + \\dfrac{\\partial v}{\\partial x}) - \\sigma_{xy} \\     stress_disp_{xz} = \\mu(\\dfrac{\\partial u}{\\partial z} + \\dfrac{\\partial w}{\\partial x}) - \\sigma_{xz} \\     stress_disp_{yz} = \\mu(\\dfrac{\\partial v}{\\partial z} + \\dfrac{\\partial w}{\\partial y}) - \\sigma_{yz} \\     equilibrium_{x} = \\rho \\dfrac{\\partial^2 u}{\\partial t^2} - (\\dfrac{\\partial \\sigma_{xx}}{\\partial x} + \\dfrac{\\partial \\sigma_{xy}}{\\partial y} + \\dfrac{\\partial \\sigma_{xz}}{\\partial z}) \\     equilibrium_{y} = \\rho \\dfrac{\\partial^2 u}{\\partial t^2} - (\\dfrac{\\partial \\sigma_{xy}}{\\partial x} + \\dfrac{\\partial \\sigma_{yy}}{\\partial y} + \\dfrac{\\partial \\sigma_{yz}}{\\partial z}) \\     equilibrium_{z} = \\rho \\dfrac{\\partial^2 u}{\\partial t^2} - (\\dfrac{\\partial \\sigma_{xz}}{\\partial x} + \\dfrac{\\partial \\sigma_{yz}}{\\partial y} + \\dfrac{\\partial \\sigma_{zz}}{\\partial z}) \\ \\end{cases}</p> <p>\u5bf9\u5e94\u7684\u65b9\u7a0b\u5b9e\u4f8b\u5316\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set equation\nequation = {\n    \"LinearElasticity\": ppsci.equation.LinearElasticity(\n        lambda_=LAMBDA_, mu=MU, dim=3\n    )\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u7684\u51e0\u4f55\u533a\u57df\u7531 stl \u6587\u4ef6\u6307\u5b9a\uff0c\u6309\u7167\u4e0b\u65b9\u547d\u4ee4\uff0c\u4e0b\u8f7d\u5e76\u89e3\u538b\u5230 <code>bracket/</code> \u6587\u4ef6\u5939\u4e0b\u3002</p> <p>\u6ce8\uff1a\u6570\u636e\u96c6\u4e2d\u7684 stl \u6587\u4ef6\u548c\u6d4b\u8bd5\u96c6\u6570\u636e\u5747\u6765\u81ea Bracket - NVIDIA Modulus\u3002</p> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/bracket/bracket_dataset.tar\n\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/bracket/bracket_dataset.tar -o bracket_dataset.tar\n\n# unzip it\ntar -xvf bracket_dataset.tar\n</code></pre> <p>\u89e3\u538b\u5b8c\u6bd5\u4e4b\u540e\uff0c<code>bracket/stl</code> \u6587\u4ef6\u5939\u4e0b\u5373\u5b58\u653e\u4e86\u8ba1\u7b97\u57df\u6784\u5efa\u6240\u9700\u7684 stl \u51e0\u4f55\u6587\u4ef6\u3002</p> \u6ce8\u610f <p>\u4f7f\u7528 <code>Mesh</code> \u7c7b\u4e4b\u524d\uff0c\u5fc5\u987b\u5148\u6309\u71671.4.2 \u989d\u5916\u4f9d\u8d56\u5b89\u88c5[\u53ef\u9009]\u6587\u6863\uff0c\u5b89\u88c5\u597d open3d\u3001pysdf\u3001PyMesh 3 \u4e2a\u51e0\u4f55\u4f9d\u8d56\u5305\u3002</p> <p>\u7136\u540e\u901a\u8fc7 PaddleScience \u5185\u7f6e\u7684 STL \u51e0\u4f55\u7c7b <code>Mesh</code> \u6765\u8bfb\u53d6\u3001\u89e3\u6790\u8fd9\u4e9b\u51e0\u4f55\u6587\u4ef6\uff0c\u5e76\u4e14\u901a\u8fc7\u5e03\u5c14\u8fd0\u7b97\uff0c\u7ec4\u5408\u51fa\u5404\u4e2a\u8ba1\u7b97\u57df\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set geometry\nsupport = ppsci.geometry.Mesh(cfg.SUPPORT_PATH)\nbracket = ppsci.geometry.Mesh(cfg.BRACKET_PATH)\naux_lower = ppsci.geometry.Mesh(cfg.AUX_LOWER_PATH)\naux_upper = ppsci.geometry.Mesh(cfg.AUX_UPPER_PATH)\ncylinder_hole = ppsci.geometry.Mesh(cfg.CYLINDER_HOLE_PATH)\ncylinder_lower = ppsci.geometry.Mesh(cfg.CYLINDER_LOWER_PATH)\ncylinder_upper = ppsci.geometry.Mesh(cfg.CYLINDER_UPPER_PATH)\n# geometry bool operation\ncurve_lower = aux_lower - cylinder_lower\ncurve_upper = aux_upper - cylinder_upper\ngeo = support + bracket + curve_lower + curve_upper - cylinder_hole\ngeom = {\"geo\": geo}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u5171\u6d89\u53ca\u5230 5 \u4e2a\u7ea6\u675f\uff0c\u5728\u5177\u4f53\u7ea6\u675f\u6784\u5efa\u4e4b\u524d\uff0c\u53ef\u4ee5\u5148\u6784\u5efa\u6570\u636e\u8bfb\u53d6\u914d\u7f6e\uff0c\u4ee5\u4fbf\u540e\u7eed\u6784\u5efa\u591a\u4e2a\u7ea6\u675f\u65f6\u590d\u7528\u8be5\u914d\u7f6e\u3002</p> <pre><code># set dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": \"NamedArrayDataset\",\n    \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"num_workers\": 1,\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u80cc\u677f\u5185\u90e8\u70b9\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>support_interior = ppsci.constraint.InteriorConstraint(\n    equation[\"LinearElasticity\"].equations,\n    {\n        \"stress_disp_xx\": 0,\n        \"stress_disp_yy\": 0,\n        \"stress_disp_zz\": 0,\n        \"stress_disp_xy\": 0,\n        \"stress_disp_xz\": 0,\n        \"stress_disp_yz\": 0,\n        \"equilibrium_x\": 0,\n        \"equilibrium_y\": 0,\n        \"equilibrium_z\": 0,\n    },\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.support_interior},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: (\n        (BOUNDS_SUPPORT_X[0] &lt; x)\n        &amp; (x &lt; BOUNDS_SUPPORT_X[1])\n        &amp; (BOUNDS_SUPPORT_Y[0] &lt; y)\n        &amp; (y &lt; BOUNDS_SUPPORT_Y[1])\n        &amp; (BOUNDS_SUPPORT_Z[0] &lt; z)\n        &amp; (z &lt; BOUNDS_SUPPORT_Z[1])\n    ),\n    weight_dict={\n        \"stress_disp_xx\": \"sdf\",\n        \"stress_disp_yy\": \"sdf\",\n        \"stress_disp_zz\": \"sdf\",\n        \"stress_disp_xy\": \"sdf\",\n        \"stress_disp_xz\": \"sdf\",\n        \"stress_disp_yz\": \"sdf\",\n        \"equilibrium_x\": \"sdf\",\n        \"equilibrium_y\": \"sdf\",\n        \"equilibrium_z\": \"sdf\",\n    },\n    name=\"SUPPORT_INTERIOR\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\uff08\u7ec4\uff09\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"LinearElasticity\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u5e0c\u671b\u4e0e LinearElasticity \u65b9\u7a0b\u76f8\u5173\u7684 9 \u4e2a\u503c <code>equilibrium_x</code>, <code>equilibrium_y</code>, <code>equilibrium_z</code>, <code>stress_disp_xx</code>, <code>stress_disp_yy</code>, <code>stress_disp_zz</code>, <code>stress_disp_xy</code>, <code>stress_disp_xz</code>, <code>stress_disp_yz</code> \u5747\u88ab\u4f18\u5316\u81f3 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"geo\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u8bbe\u7f6e <code>batch_size</code> \u4e3a <code>2048</code>\u3002</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u9009\u7528\u5e38\u7528\u7684 MSE \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"sum\"</code>\uff0c\u5373\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u51e0\u4f55\u70b9\u7b5b\u9009\uff0c\u7531\u4e8e\u8fd9\u4e2a\u7ea6\u675f\u53ea\u65bd\u52a0\u5728\u80cc\u677f\u533a\u57df\uff0c\u56e0\u6b64\u9700\u8981\u5bf9 geo \u4e0a\u91c7\u6837\u51fa\u7684\u70b9\u8fdb\u884c\u7b5b\u9009\uff0c\u6b64\u5904\u4f20\u5165\u4e00\u4e2a lambda \u7b5b\u9009\u51fd\u6570\u5373\u53ef\uff0c\u5176\u63a5\u53d7\u70b9\u96c6\u6784\u6210\u7684\u5f20\u91cf <code>x, y, z</code>\uff0c\u8fd4\u56de\u5e03\u5c14\u503c\u5f20\u4eae\uff0c\u8868\u793a\u6bcf\u4e2a\u70b9\u662f\u5426\u7b26\u5408\u7b5b\u9009\u6761\u4ef6\uff0c\u4e0d\u7b26\u5408\u4e3a <code>False</code>\uff0c\u7b26\u5408\u4e3a <code>True</code>\uff1b</p> <p>\u7b2c\u4e03\u4e2a\u53c2\u6570\u662f\u6bcf\u4e2a\u70b9\u53c2\u4e0e\u635f\u5931\u8ba1\u7b97\u65f6\u7684\u6743\u91cd\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528 <code>\"sdf\"</code> \u8868\u793a\u4f7f\u7528\u6bcf\u4e2a\u70b9\u5230\u8fb9\u754c\u7684\u6700\u77ed\u8ddd\u79bb\uff08\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u503c\uff09\u6765\u4f5c\u4e3a\u6743\u91cd\uff0c\u8fd9\u79cd sdf \u52a0\u6743\u7684\u65b9\u6cd5\u53ef\u4ee5\u52a0\u5927\u8fdc\u79bb\u8fb9\u754c\uff08\u96be\u6837\u672c\uff09\u70b9\u7684\u6743\u91cd\uff0c\u51cf\u5c11\u9760\u8fd1\u8fb9\u754c\u7684\uff08\u7b80\u5355\u6837\u672c\uff09\u70b9\u7684\u6743\u91cd\uff0c\u6709\u5229\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u3002</p> <p>\u7b2c\u516b\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"support_interior\" \u5373\u53ef\u3002</p> <p>\u53e6\u4e00\u4e2a\u4f5c\u7528\u5728\u5e26\u5b54\u5e73\u677f\u4e0a\u7684\u7ea6\u675f\u6761\u4ef6\u5219\u4e0e\u4e4b\u7c7b\u4f3c\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>bracket_interior = ppsci.constraint.InteriorConstraint(\n    equation[\"LinearElasticity\"].equations,\n    {\n        \"stress_disp_xx\": 0,\n        \"stress_disp_yy\": 0,\n        \"stress_disp_zz\": 0,\n        \"stress_disp_xy\": 0,\n        \"stress_disp_xz\": 0,\n        \"stress_disp_yz\": 0,\n        \"equilibrium_x\": 0,\n        \"equilibrium_y\": 0,\n        \"equilibrium_z\": 0,\n    },\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bracket_interior},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: (\n        (BOUNDS_BRACKET_X[0] &lt; x)\n        &amp; (x &lt; BOUNDS_BRACKET_X[1])\n        &amp; (BOUNDS_BRACKET_Y[0] &lt; y)\n        &amp; (y &lt; BOUNDS_BRACKET_Y[1])\n        &amp; (BOUNDS_BRACKET_Z[0] &lt; z)\n        &amp; (z &lt; BOUNDS_BRACKET_Z[1])\n    ),\n    weight_dict={\n        \"stress_disp_xx\": \"sdf\",\n        \"stress_disp_yy\": \"sdf\",\n        \"stress_disp_zz\": \"sdf\",\n        \"stress_disp_xy\": \"sdf\",\n        \"stress_disp_xz\": \"sdf\",\n        \"stress_disp_yz\": \"sdf\",\n        \"equilibrium_x\": \"sdf\",\n        \"equilibrium_y\": \"sdf\",\n        \"equilibrium_z\": \"sdf\",\n    },\n    name=\"BRACKET_INTERIOR\",\n)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u5bf9\u4e8e\u80cc\u677f\u540e\u8868\u9762\uff0c\u7531\u4e8e\u88ab\u56fa\u5b9a\uff0c\u6240\u4ee5\u5176\u4e0a\u7684\u70b9\u5728\u4e09\u4e2a\u65b9\u5411\u7684\u5f62\u53d8\u5747\u4e3a 0\uff0c\u56e0\u6b64\u6709\u5982\u4e0b\u7684\u8fb9\u754c\u7ea6\u675f\u6761\u4ef6\uff1a</p> <pre><code>bc_back = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"w\": lambda d: d[\"w\"]},\n    {\"u\": 0, \"v\": 0, \"w\": 0},\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_back},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: x == SUPPORT_ORIGIN[0],\n    weight_dict=cfg.TRAIN.weight.bc_back,\n    name=\"BC_BACK\",\n)\n</code></pre> <p>\u5bf9\u4e8e\u5e26\u5b54\u5e73\u677f\u53f3\u4fa7\u957f\u65b9\u5f62\u8f7d\u8377\u9762\uff0c\u5176\u4e0a\u7684\u6bcf\u4e2a\u70b9\u53ea\u53d7 z \u6b63\u65b9\u5411\u7684\u8f7d\u8377\uff0c\u5927\u5c0f\u4e3a \\(T\\)\uff0c\u5176\u4f59\u65b9\u5411\u5e94\u529b\u4e3a 0\uff0c\u6709\u5982\u4e0b\u8fb9\u754c\u6761\u4ef6\u7ea6\u675f\uff1a</p> <pre><code>bc_front = ppsci.constraint.BoundaryConstraint(\n    equation[\"LinearElasticity\"].equations,\n    {\"traction_x\": 0, \"traction_y\": 0, \"traction_z\": T},\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_front},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: x == BRACKET_ORIGIN[0] + BRACKET_DIM[0],\n    name=\"BC_FRONT\",\n)\n</code></pre> <p>\u5bf9\u4e8e\u9664\u80cc\u677f\u540e\u9762\u3001\u5e26\u5b54\u5e73\u677f\u53f3\u4fa7\u957f\u65b9\u5f62\u8f7d\u8377\u9762\u5916\u7684\u8868\u9762\uff0c\u4e0d\u53d7\u4efb\u4f55\u8f7d\u8377\uff0c\u5373\u4e09\u4e2a\u65b9\u5411\u7684\u5185\u529b\u5e73\u8861\uff0c\u5408\u529b\u4e3a 0\uff0c\u6709\u5982\u4e0b\u8fb9\u754c\u6761\u4ef6\u7ea6\u675f\uff1a</p> <pre><code>bc_surface = ppsci.constraint.BoundaryConstraint(\n    equation[\"LinearElasticity\"].equations,\n    {\"traction_x\": 0, \"traction_y\": 0, \"traction_z\": 0},\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_surface},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: np.logical_and(\n        x &gt; SUPPORT_ORIGIN[0] + 1e-7, x &lt; BRACKET_ORIGIN[0] + BRACKET_DIM[0] - 1e-7\n    ),\n    name=\"BC_SURFACE\",\n)\n</code></pre> <p>\u5728\u65b9\u7a0b\u7ea6\u675f\u3001\u8fb9\u754c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    bc_back.name: bc_back,\n    bc_front.name: bc_front,\n    bc_surface.name: bc_surface,\n    support_interior.name: support_interior,\n    bracket_interior.name: bracket_interior,\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 2000 \u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u6bcf\u8f6e\u8fdb\u884c 1000 \u6b65\u4f18\u5316\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 2000\n  iters_per_epoch: 1000\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u914d\u5408\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u5e38\u7528\u7684 ExponentialDecay \u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u3002</p> <pre><code># set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n    **cfg.TRAIN.lr_scheduler\n)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u800c\u9a8c\u8bc1\u96c6\u7684\u6570\u636e\u6765\u81ea\u5916\u90e8 txt \u6587\u4ef6\uff0c\u56e0\u6b64\u9996\u5148\u4f7f\u7528 <code>ppsci.utils.reader</code> \u6a21\u5757\u4ece txt \u6587\u4ef6\u4e2d\u8bfb\u53d6\u9a8c\u8bc1\u70b9\u96c6\uff1a</p> <pre><code># set validator\nref_xyzu = ppsci.utils.reader.load_csv_file(\n    cfg.DEFORMATION_X_PATH,\n    (\"x\", \"y\", \"z\", \"u\"),\n    {\n        \"x\": \"X Location (m)\",\n        \"y\": \"Y Location (m)\",\n        \"z\": \"Z Location (m)\",\n        \"u\": \"Directional Deformation (m)\",\n    },\n    \"\\t\",\n)\nref_v = ppsci.utils.reader.load_csv_file(\n    cfg.DEFORMATION_Y_PATH,\n    (\"v\",),\n    {\"v\": \"Directional Deformation (m)\"},\n    \"\\t\",\n)\nref_w = ppsci.utils.reader.load_csv_file(\n    cfg.DEFORMATION_Z_PATH,\n    (\"w\",),\n    {\"w\": \"Directional Deformation (m)\"},\n    \"\\t\",\n)\n\nref_sxx = ppsci.utils.reader.load_csv_file(\n    cfg.NORMAL_X_PATH,\n    (\"sigma_xx\",),\n    {\"sigma_xx\": \"Normal Stress (Pa)\"},\n    \"\\t\",\n)\nref_syy = ppsci.utils.reader.load_csv_file(\n    cfg.NORMAL_Y_PATH,\n    (\"sigma_yy\",),\n    {\"sigma_yy\": \"Normal Stress (Pa)\"},\n    \"\\t\",\n)\nref_szz = ppsci.utils.reader.load_csv_file(\n    cfg.NORMAL_Z_PATH,\n    (\"sigma_zz\",),\n    {\"sigma_zz\": \"Normal Stress (Pa)\"},\n    \"\\t\",\n)\n\nref_sxy = ppsci.utils.reader.load_csv_file(\n    cfg.SHEAR_XY_PATH,\n    (\"sigma_xy\",),\n    {\"sigma_xy\": \"Shear Stress (Pa)\"},\n    \"\\t\",\n)\nref_sxz = ppsci.utils.reader.load_csv_file(\n    cfg.SHEAR_XZ_PATH,\n    (\"sigma_xz\",),\n    {\"sigma_xz\": \"Shear Stress (Pa)\"},\n    \"\\t\",\n)\nref_syz = ppsci.utils.reader.load_csv_file(\n    cfg.SHEAR_YZ_PATH,\n    (\"sigma_yz\",),\n    {\"sigma_yz\": \"Shear Stress (Pa)\"},\n    \"\\t\",\n)\n</code></pre> <p>\u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3a\u5b57\u5178\u5e76\u8fdb\u884c\u65e0\u91cf\u7eb2\u5316\u548c\u5f52\u4e00\u5316\uff0c\u518d\u5c06\u5176\u5305\u88c5\u6210\u5b57\u5178\u548c <code>eval_dataloader_cfg</code>\uff08\u9a8c\u8bc1\u96c6dataloader\u914d\u7f6e\uff0c\u6784\u9020\u65b9\u5f0f\u4e0e <code>train_dataloader_cfg</code> \u7c7b\u4f3c\uff09\u4e00\u8d77\u4f20\u9012\u7ed9 <code>ppsci.validate.SupervisedValidator</code> \u6784\u9020\u8bc4\u4f30\u5668\u3002</p> <pre><code>input_dict = {\n    \"x\": ref_xyzu[\"x\"],\n    \"y\": ref_xyzu[\"y\"],\n    \"z\": ref_xyzu[\"z\"],\n}\nlabel_dict = {\n    \"u\": ref_xyzu[\"u\"] / cfg.CHARACTERISTIC_DISPLACEMENT,\n    \"v\": ref_v[\"v\"] / cfg.CHARACTERISTIC_DISPLACEMENT,\n    \"w\": ref_w[\"w\"] / cfg.CHARACTERISTIC_DISPLACEMENT,\n    \"sigma_xx\": ref_sxx[\"sigma_xx\"] * SIGMA_NORMALIZATION,\n    \"sigma_yy\": ref_syy[\"sigma_yy\"] * SIGMA_NORMALIZATION,\n    \"sigma_zz\": ref_szz[\"sigma_zz\"] * SIGMA_NORMALIZATION,\n    \"sigma_xy\": ref_sxy[\"sigma_xy\"] * SIGMA_NORMALIZATION,\n    \"sigma_xz\": ref_sxz[\"sigma_xz\"] * SIGMA_NORMALIZATION,\n    \"sigma_yz\": ref_syz[\"sigma_yz\"] * SIGMA_NORMALIZATION,\n}\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"NamedArrayDataset\",\n        \"input\": input_dict,\n        \"label\": label_dict,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n}\nsup_validator = ppsci.validate.SupervisedValidator(\n    {**eval_dataloader_cfg, \"batch_size\": cfg.EVAL.batch_size.sup_validator},\n    ppsci.loss.MSELoss(\"mean\"),\n    {\n        \"u\": lambda out: out[\"u\"],\n        \"v\": lambda out: out[\"v\"],\n        \"w\": lambda out: out[\"w\"],\n        \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n        \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n        \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n        \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n        \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n        \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n    },\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"commercial_ref_u_v_w_sigmas\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#38","title":"3.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u672c\u6587\u4e2d\u7684\u8f93\u5165\u6570\u636e\u662f\u8bc4\u4f30\u5668\u6784\u5efa\u4e2d\u51c6\u5907\u597d\u7684\u8f93\u5165\u5b57\u5178 <code>input_dict</code>\uff0c\u8f93\u51fa\u6570\u636e\u662f\u5bf9\u5e94\u7684 9 \u4e2a\u9884\u6d4b\u7684\u7269\u7406\u91cf\uff0c\u56e0\u6b64\u53ea\u9700\u8981\u5c06\u8bc4\u4f30\u7684\u8f93\u51fa\u6570\u636e\u4fdd\u5b58\u6210 vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u6700\u540e\u7528\u53ef\u89c6\u5316\u8f6f\u4ef6\u6253\u5f00\u67e5\u770b\u5373\u53ef\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set visualizer(optional)\nvisualizer = {\n    \"visualize_u_v_w_sigmas\": ppsci.visualize.VisualizerVtu(\n        input_dict,\n        {\n            \"u\": lambda out: out[\"u\"],\n            \"v\": lambda out: out[\"v\"],\n            \"w\": lambda out: out[\"w\"],\n            \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n            \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n            \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n            \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n            \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n            \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n        },\n        prefix=\"result_u_v_w_sigmas\",\n    )\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    log_freq=cfg.log_freq,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    seed=cfg.seed,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    visualizer=visualizer,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n# train model\nsolver.train()\n\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"bracket.py<pre><code>\"\"\"\nReference: https://docs.nvidia.com/deeplearning/modulus/modulus-v2209/user_guide/foundational/linear_elasticity.html\nSTL data files download link: https://paddle-org.bj.bcebos.com/paddlescience/datasets/bracket/bracket_dataset.tar\npretrained model download link: https://paddle-org.bj.bcebos.com/paddlescience/models/bracket/bracket_pretrained.pdparams\n\"\"\"\n\nimport hydra\nimport numpy as np\nfrom omegaconf import DictConfig\n\nimport ppsci\n\n\ndef train(cfg: DictConfig):\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    # wrap to a model_list\n    model = ppsci.arch.ModelList((disp_net, stress_net))\n\n    # specify parameters\n    LAMBDA_ = cfg.NU * cfg.E / ((1 + cfg.NU) * (1 - 2 * cfg.NU))\n    MU = cfg.E / (2 * (1 + cfg.NU))\n    MU_C = 0.01 * MU\n    LAMBDA_ = LAMBDA_ / MU_C\n    MU = MU / MU_C\n    SIGMA_NORMALIZATION = cfg.CHARACTERISTIC_LENGTH / (\n        cfg.CHARACTERISTIC_DISPLACEMENT * MU_C\n    )\n    T = -4.0e4 * SIGMA_NORMALIZATION\n\n    # set equation\n    equation = {\n        \"LinearElasticity\": ppsci.equation.LinearElasticity(\n            lambda_=LAMBDA_, mu=MU, dim=3\n        )\n    }\n\n    # set geometry\n    support = ppsci.geometry.Mesh(cfg.SUPPORT_PATH)\n    bracket = ppsci.geometry.Mesh(cfg.BRACKET_PATH)\n    aux_lower = ppsci.geometry.Mesh(cfg.AUX_LOWER_PATH)\n    aux_upper = ppsci.geometry.Mesh(cfg.AUX_UPPER_PATH)\n    cylinder_hole = ppsci.geometry.Mesh(cfg.CYLINDER_HOLE_PATH)\n    cylinder_lower = ppsci.geometry.Mesh(cfg.CYLINDER_LOWER_PATH)\n    cylinder_upper = ppsci.geometry.Mesh(cfg.CYLINDER_UPPER_PATH)\n    # geometry bool operation\n    curve_lower = aux_lower - cylinder_lower\n    curve_upper = aux_upper - cylinder_upper\n    geo = support + bracket + curve_lower + curve_upper - cylinder_hole\n    geom = {\"geo\": geo}\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"num_workers\": 1,\n    }\n\n    # set constraint\n    SUPPORT_ORIGIN = (-1, -1, -1)\n    BRACKET_ORIGIN = (-0.75, -1, -0.1)\n    BRACKET_DIM = (1.75, 2, 0.2)\n    BOUNDS_SUPPORT_X = (-1, -0.65)\n    BOUNDS_SUPPORT_Y = (-1, 1)\n    BOUNDS_SUPPORT_Z = (-1, 1)\n    BOUNDS_BRACKET_X = (-0.65, 1)\n    BOUNDS_BRACKET_Y = (-1, 1)\n    BOUNDS_BRACKET_Z = (-0.1, 0.1)\n\n    bc_back = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"w\": lambda d: d[\"w\"]},\n        {\"u\": 0, \"v\": 0, \"w\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_back},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: x == SUPPORT_ORIGIN[0],\n        weight_dict=cfg.TRAIN.weight.bc_back,\n        name=\"BC_BACK\",\n    )\n    bc_front = ppsci.constraint.BoundaryConstraint(\n        equation[\"LinearElasticity\"].equations,\n        {\"traction_x\": 0, \"traction_y\": 0, \"traction_z\": T},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_front},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: x == BRACKET_ORIGIN[0] + BRACKET_DIM[0],\n        name=\"BC_FRONT\",\n    )\n    bc_surface = ppsci.constraint.BoundaryConstraint(\n        equation[\"LinearElasticity\"].equations,\n        {\"traction_x\": 0, \"traction_y\": 0, \"traction_z\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc_surface},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: np.logical_and(\n            x &gt; SUPPORT_ORIGIN[0] + 1e-7, x &lt; BRACKET_ORIGIN[0] + BRACKET_DIM[0] - 1e-7\n        ),\n        name=\"BC_SURFACE\",\n    )\n    support_interior = ppsci.constraint.InteriorConstraint(\n        equation[\"LinearElasticity\"].equations,\n        {\n            \"stress_disp_xx\": 0,\n            \"stress_disp_yy\": 0,\n            \"stress_disp_zz\": 0,\n            \"stress_disp_xy\": 0,\n            \"stress_disp_xz\": 0,\n            \"stress_disp_yz\": 0,\n            \"equilibrium_x\": 0,\n            \"equilibrium_y\": 0,\n            \"equilibrium_z\": 0,\n        },\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.support_interior},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: (\n            (BOUNDS_SUPPORT_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_SUPPORT_X[1])\n            &amp; (BOUNDS_SUPPORT_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_SUPPORT_Y[1])\n            &amp; (BOUNDS_SUPPORT_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_SUPPORT_Z[1])\n        ),\n        weight_dict={\n            \"stress_disp_xx\": \"sdf\",\n            \"stress_disp_yy\": \"sdf\",\n            \"stress_disp_zz\": \"sdf\",\n            \"stress_disp_xy\": \"sdf\",\n            \"stress_disp_xz\": \"sdf\",\n            \"stress_disp_yz\": \"sdf\",\n            \"equilibrium_x\": \"sdf\",\n            \"equilibrium_y\": \"sdf\",\n            \"equilibrium_z\": \"sdf\",\n        },\n        name=\"SUPPORT_INTERIOR\",\n    )\n    bracket_interior = ppsci.constraint.InteriorConstraint(\n        equation[\"LinearElasticity\"].equations,\n        {\n            \"stress_disp_xx\": 0,\n            \"stress_disp_yy\": 0,\n            \"stress_disp_zz\": 0,\n            \"stress_disp_xy\": 0,\n            \"stress_disp_xz\": 0,\n            \"stress_disp_yz\": 0,\n            \"equilibrium_x\": 0,\n            \"equilibrium_y\": 0,\n            \"equilibrium_z\": 0,\n        },\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bracket_interior},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: (\n            (BOUNDS_BRACKET_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_BRACKET_X[1])\n            &amp; (BOUNDS_BRACKET_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_BRACKET_Y[1])\n            &amp; (BOUNDS_BRACKET_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_BRACKET_Z[1])\n        ),\n        weight_dict={\n            \"stress_disp_xx\": \"sdf\",\n            \"stress_disp_yy\": \"sdf\",\n            \"stress_disp_zz\": \"sdf\",\n            \"stress_disp_xy\": \"sdf\",\n            \"stress_disp_xz\": \"sdf\",\n            \"stress_disp_yz\": \"sdf\",\n            \"equilibrium_x\": \"sdf\",\n            \"equilibrium_y\": \"sdf\",\n            \"equilibrium_z\": \"sdf\",\n        },\n        name=\"BRACKET_INTERIOR\",\n    )\n    # wrap constraints together\n    constraint = {\n        bc_back.name: bc_back,\n        bc_front.name: bc_front,\n        bc_surface.name: bc_surface,\n        support_interior.name: support_interior,\n        bracket_interior.name: bracket_interior,\n    }\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n        **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    # set validator\n    ref_xyzu = ppsci.utils.reader.load_csv_file(\n        cfg.DEFORMATION_X_PATH,\n        (\"x\", \"y\", \"z\", \"u\"),\n        {\n            \"x\": \"X Location (m)\",\n            \"y\": \"Y Location (m)\",\n            \"z\": \"Z Location (m)\",\n            \"u\": \"Directional Deformation (m)\",\n        },\n        \"\\t\",\n    )\n    ref_v = ppsci.utils.reader.load_csv_file(\n        cfg.DEFORMATION_Y_PATH,\n        (\"v\",),\n        {\"v\": \"Directional Deformation (m)\"},\n        \"\\t\",\n    )\n    ref_w = ppsci.utils.reader.load_csv_file(\n        cfg.DEFORMATION_Z_PATH,\n        (\"w\",),\n        {\"w\": \"Directional Deformation (m)\"},\n        \"\\t\",\n    )\n\n    ref_sxx = ppsci.utils.reader.load_csv_file(\n        cfg.NORMAL_X_PATH,\n        (\"sigma_xx\",),\n        {\"sigma_xx\": \"Normal Stress (Pa)\"},\n        \"\\t\",\n    )\n    ref_syy = ppsci.utils.reader.load_csv_file(\n        cfg.NORMAL_Y_PATH,\n        (\"sigma_yy\",),\n        {\"sigma_yy\": \"Normal Stress (Pa)\"},\n        \"\\t\",\n    )\n    ref_szz = ppsci.utils.reader.load_csv_file(\n        cfg.NORMAL_Z_PATH,\n        (\"sigma_zz\",),\n        {\"sigma_zz\": \"Normal Stress (Pa)\"},\n        \"\\t\",\n    )\n\n    ref_sxy = ppsci.utils.reader.load_csv_file(\n        cfg.SHEAR_XY_PATH,\n        (\"sigma_xy\",),\n        {\"sigma_xy\": \"Shear Stress (Pa)\"},\n        \"\\t\",\n    )\n    ref_sxz = ppsci.utils.reader.load_csv_file(\n        cfg.SHEAR_XZ_PATH,\n        (\"sigma_xz\",),\n        {\"sigma_xz\": \"Shear Stress (Pa)\"},\n        \"\\t\",\n    )\n    ref_syz = ppsci.utils.reader.load_csv_file(\n        cfg.SHEAR_YZ_PATH,\n        (\"sigma_yz\",),\n        {\"sigma_yz\": \"Shear Stress (Pa)\"},\n        \"\\t\",\n    )\n\n    input_dict = {\n        \"x\": ref_xyzu[\"x\"],\n        \"y\": ref_xyzu[\"y\"],\n        \"z\": ref_xyzu[\"z\"],\n    }\n    label_dict = {\n        \"u\": ref_xyzu[\"u\"] / cfg.CHARACTERISTIC_DISPLACEMENT,\n        \"v\": ref_v[\"v\"] / cfg.CHARACTERISTIC_DISPLACEMENT,\n        \"w\": ref_w[\"w\"] / cfg.CHARACTERISTIC_DISPLACEMENT,\n        \"sigma_xx\": ref_sxx[\"sigma_xx\"] * SIGMA_NORMALIZATION,\n        \"sigma_yy\": ref_syy[\"sigma_yy\"] * SIGMA_NORMALIZATION,\n        \"sigma_zz\": ref_szz[\"sigma_zz\"] * SIGMA_NORMALIZATION,\n        \"sigma_xy\": ref_sxy[\"sigma_xy\"] * SIGMA_NORMALIZATION,\n        \"sigma_xz\": ref_sxz[\"sigma_xz\"] * SIGMA_NORMALIZATION,\n        \"sigma_yz\": ref_syz[\"sigma_yz\"] * SIGMA_NORMALIZATION,\n    }\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict,\n            \"label\": label_dict,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    sup_validator = ppsci.validate.SupervisedValidator(\n        {**eval_dataloader_cfg, \"batch_size\": cfg.EVAL.batch_size.sup_validator},\n        ppsci.loss.MSELoss(\"mean\"),\n        {\n            \"u\": lambda out: out[\"u\"],\n            \"v\": lambda out: out[\"v\"],\n            \"w\": lambda out: out[\"w\"],\n            \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n            \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n            \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n            \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n            \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n            \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n        },\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"commercial_ref_u_v_w_sigmas\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set visualizer(optional)\n    visualizer = {\n        \"visualize_u_v_w_sigmas\": ppsci.visualize.VisualizerVtu(\n            input_dict,\n            {\n                \"u\": lambda out: out[\"u\"],\n                \"v\": lambda out: out[\"v\"],\n                \"w\": lambda out: out[\"w\"],\n                \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n                \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n                \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n                \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n                \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n                \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n            },\n            prefix=\"result_u_v_w_sigmas\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    # wrap to a model_list\n    model = ppsci.arch.ModelList((disp_net, stress_net))\n\n    # Specify parameters\n    LAMBDA_ = cfg.NU * cfg.E / ((1 + cfg.NU) * (1 - 2 * cfg.NU))\n    MU = cfg.E / (2 * (1 + cfg.NU))\n    MU_C = 0.01 * MU\n    LAMBDA_ = LAMBDA_ / MU_C\n    MU = MU / MU_C\n    SIGMA_NORMALIZATION = cfg.CHARACTERISTIC_LENGTH / (\n        cfg.CHARACTERISTIC_DISPLACEMENT * MU_C\n    )\n\n    # set validator\n    ref_xyzu = ppsci.utils.reader.load_csv_file(\n        cfg.DEFORMATION_X_PATH,\n        (\"x\", \"y\", \"z\", \"u\"),\n        {\n            \"x\": \"X Location (m)\",\n            \"y\": \"Y Location (m)\",\n            \"z\": \"Z Location (m)\",\n            \"u\": \"Directional Deformation (m)\",\n        },\n        \"\\t\",\n    )\n    ref_v = ppsci.utils.reader.load_csv_file(\n        cfg.DEFORMATION_Y_PATH,\n        (\"v\",),\n        {\"v\": \"Directional Deformation (m)\"},\n        \"\\t\",\n    )\n    ref_w = ppsci.utils.reader.load_csv_file(\n        cfg.DEFORMATION_Z_PATH,\n        (\"w\",),\n        {\"w\": \"Directional Deformation (m)\"},\n        \"\\t\",\n    )\n\n    ref_sxx = ppsci.utils.reader.load_csv_file(\n        cfg.NORMAL_X_PATH,\n        (\"sigma_xx\",),\n        {\"sigma_xx\": \"Normal Stress (Pa)\"},\n        \"\\t\",\n    )\n    ref_syy = ppsci.utils.reader.load_csv_file(\n        cfg.NORMAL_Y_PATH,\n        (\"sigma_yy\",),\n        {\"sigma_yy\": \"Normal Stress (Pa)\"},\n        \"\\t\",\n    )\n    ref_szz = ppsci.utils.reader.load_csv_file(\n        cfg.NORMAL_Z_PATH,\n        (\"sigma_zz\",),\n        {\"sigma_zz\": \"Normal Stress (Pa)\"},\n        \"\\t\",\n    )\n\n    ref_sxy = ppsci.utils.reader.load_csv_file(\n        cfg.SHEAR_XY_PATH,\n        (\"sigma_xy\",),\n        {\"sigma_xy\": \"Shear Stress (Pa)\"},\n        \"\\t\",\n    )\n    ref_sxz = ppsci.utils.reader.load_csv_file(\n        cfg.SHEAR_XZ_PATH,\n        (\"sigma_xz\",),\n        {\"sigma_xz\": \"Shear Stress (Pa)\"},\n        \"\\t\",\n    )\n    ref_syz = ppsci.utils.reader.load_csv_file(\n        cfg.SHEAR_YZ_PATH,\n        (\"sigma_yz\",),\n        {\"sigma_yz\": \"Shear Stress (Pa)\"},\n        \"\\t\",\n    )\n\n    input_dict = {\n        \"x\": ref_xyzu[\"x\"],\n        \"y\": ref_xyzu[\"y\"],\n        \"z\": ref_xyzu[\"z\"],\n    }\n    label_dict = {\n        \"u\": ref_xyzu[\"u\"] / cfg.CHARACTERISTIC_DISPLACEMENT,\n        \"v\": ref_v[\"v\"] / cfg.CHARACTERISTIC_DISPLACEMENT,\n        \"w\": ref_w[\"w\"] / cfg.CHARACTERISTIC_DISPLACEMENT,\n        \"sigma_xx\": ref_sxx[\"sigma_xx\"] * SIGMA_NORMALIZATION,\n        \"sigma_yy\": ref_syy[\"sigma_yy\"] * SIGMA_NORMALIZATION,\n        \"sigma_zz\": ref_szz[\"sigma_zz\"] * SIGMA_NORMALIZATION,\n        \"sigma_xy\": ref_sxy[\"sigma_xy\"] * SIGMA_NORMALIZATION,\n        \"sigma_xz\": ref_sxz[\"sigma_xz\"] * SIGMA_NORMALIZATION,\n        \"sigma_yz\": ref_syz[\"sigma_yz\"] * SIGMA_NORMALIZATION,\n    }\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict,\n            \"label\": label_dict,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    sup_validator = ppsci.validate.SupervisedValidator(\n        {**eval_dataloader_cfg, \"batch_size\": cfg.EVAL.batch_size.sup_validator},\n        ppsci.loss.MSELoss(\"mean\"),\n        {\n            \"u\": lambda out: out[\"u\"],\n            \"v\": lambda out: out[\"v\"],\n            \"w\": lambda out: out[\"w\"],\n            \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n            \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n            \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n            \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n            \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n            \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n        },\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"commercial_ref_u_v_w_sigmas\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set visualizer(optional)\n    visualizer = {\n        \"visualize_u_v_w_sigmas\": ppsci.visualize.VisualizerVtu(\n            input_dict,\n            {\n                \"u\": lambda out: out[\"u\"],\n                \"v\": lambda out: out[\"v\"],\n                \"w\": lambda out: out[\"w\"],\n                \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n                \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n                \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n                \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n                \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n                \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n            },\n            prefix=\"result_u_v_w_sigmas\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate\n    solver.eval()\n    # visualize prediction\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    # wrap to a model_list\n    model = ppsci.arch.ModelList((disp_net, stress_net))\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n    ref_xyzu = ppsci.utils.reader.load_csv_file(\n        cfg.DEFORMATION_X_PATH,\n        (\"x\", \"y\", \"z\", \"u\"),\n        {\n            \"x\": \"X Location (m)\",\n            \"y\": \"Y Location (m)\",\n            \"z\": \"Z Location (m)\",\n            \"u\": \"Directional Deformation (m)\",\n        },\n        \"\\t\",\n    )\n    input_dict = {\n        \"x\": ref_xyzu[\"x\"],\n        \"y\": ref_xyzu[\"y\"],\n        \"z\": ref_xyzu[\"z\"],\n    }\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_keys = cfg.MODEL.disp_net.output_keys + cfg.MODEL.stress_net.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(output_keys, output_dict.keys())\n    }\n\n    ppsci.visualize.save_vtu_from_dict(\n        \"./bracket_pred\",\n        {**input_dict, **output_dict},\n        input_dict.keys(),\n        output_keys,\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"bracket.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u9762\u5c55\u793a\u4e86\u5728\u6d4b\u8bd5\u70b9\u96c6\u4e0a\uff0c3 \u4e2a\u65b9\u5411\u7684\u6320\u5ea6 \\(u, v, w\\) \u4ee5\u53ca 6 \u4e2a\u5e94\u529b \\(\\sigma_{xx}, \\sigma_{yy}, \\sigma_{zz}, \\sigma_{xy}, \\sigma_{xz}, \\sigma_{yz}\\) \u7684\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u3001\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7ed3\u679c\u4ee5\u53ca\u4e24\u8005\u7684\u5dee\u503c\u3002</p> <p> </p> \u5de6\u4fa7\u4e3a\u91d1\u5c5e\u4ef6\u8868\u9762\u9884\u6d4b\u7684\u6320\u5ea6 u\uff1b\u4e2d\u95f4\u8868\u793a\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7684\u6320\u5ea6 u\uff1b\u53f3\u4fa7\u8868\u793a\u4e24\u8005\u5dee\u503c <p> </p> \u5de6\u4fa7\u4e3a\u91d1\u5c5e\u4ef6\u8868\u9762\u9884\u6d4b\u7684\u6320\u5ea6 v\uff1b\u4e2d\u95f4\u8868\u793a\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7684\u6320\u5ea6 v\uff1b\u53f3\u4fa7\u8868\u793a\u4e24\u8005\u5dee\u503c <p> </p> \u5de6\u4fa7\u4e3a\u91d1\u5c5e\u4ef6\u8868\u9762\u9884\u6d4b\u7684\u6320\u5ea6 w\uff1b\u4e2d\u95f4\u8868\u793a\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7684\u6320\u5ea6 w\uff1b\u53f3\u4fa7\u8868\u793a\u4e24\u8005\u5dee\u503c <p> </p> \u5de6\u4fa7\u4e3a\u91d1\u5c5e\u4ef6\u8868\u9762\u9884\u6d4b\u7684\u5e94\u529b sigma_xx\uff1b\u4e2d\u95f4\u8868\u793a\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7684\u5e94\u529b sigma_xx\uff1b\u53f3\u4fa7\u8868\u793a\u4e24\u8005\u5dee\u503c <p> </p> \u5de6\u4fa7\u4e3a\u91d1\u5c5e\u4ef6\u8868\u9762\u9884\u6d4b\u7684\u5e94\u529b sigma_xy\uff1b\u4e2d\u95f4\u8868\u793a\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7684\u5e94\u529b sigma_xy\uff1b\u53f3\u4fa7\u8868\u793a\u4e24\u8005\u5dee\u503c <p> </p> \u5de6\u4fa7\u4e3a\u91d1\u5c5e\u4ef6\u8868\u9762\u9884\u6d4b\u7684\u5e94\u529b sigma_xz\uff1b\u4e2d\u95f4\u8868\u793a\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7684\u5e94\u529b sigma_xz\uff1b\u53f3\u4fa7\u8868\u793a\u4e24\u8005\u5dee\u503c <p> </p> \u5de6\u4fa7\u4e3a\u91d1\u5c5e\u4ef6\u8868\u9762\u9884\u6d4b\u7684\u5e94\u529b sigma_yy\uff1b\u4e2d\u95f4\u8868\u793a\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7684\u5e94\u529b sigma_yy\uff1b\u53f3\u4fa7\u8868\u793a\u4e24\u8005\u5dee\u503c <p> </p> \u5de6\u4fa7\u4e3a\u91d1\u5c5e\u4ef6\u8868\u9762\u9884\u6d4b\u7684\u5e94\u529bsigma_yz\uff1b\u4e2d\u95f4\u8868\u793a\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7684\u5e94\u529bsigma_yz\uff1b\u53f3\u4fa7\u8868\u793a\u4e24\u8005\u5dee\u503c <p> </p> \u5de6\u4fa7\u4e3a\u91d1\u5c5e\u4ef6\u8868\u9762\u9884\u6d4b\u7684\u5e94\u529bsigma_zz\uff1b\u4e2d\u95f4\u8868\u793a\u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7684\u5e94\u529bsigma_zz\uff1b\u53f3\u4fa7\u8868\u793a\u4e24\u8005\u5dee\u503c <p>\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u679c\u4e0e \u4f20\u7edf\u7b97\u6cd5\u6c42\u89e3\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bracket/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>Bracket - NVIDIA Modulus</li> <li>Scaling of Differential Equations</li> <li>Matlab PDE toolbox</li> </ul>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","LinearElasticity\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/bubble/","title":"BubbleNet","text":""},{"location":"zh/examples/bubble/#bubble_flow","title":"Bubble_flow","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/BubbleNet/bubble.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/BubbleNet/bubble.mat -o bubble.mat\npython bubble.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/BubbleNet/bubble.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/BubbleNet/bubble.mat -o bubble.mat\npython bubble.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/bubble/bubble_pretrained.pdparams\n</code></pre> <pre><code>python bubble.py mode=export\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/BubbleNet/bubble.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/BubbleNet/bubble.mat -o bubble.mat\npython bubble.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 bubble_pretrained.pdparams loss(bubble_mse): 0.00558MSE.u(bubble_mse): 0.00090MSE.v(bubble_mse): 0.00322MSE.p(bubble_mse): 0.00066MSE.phil(bubble_mse): 0.00079"},{"location":"zh/examples/bubble/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":""},{"location":"zh/examples/bubble/#11","title":"1.1 \u6c14\u6ce1\u6d41","text":"<p>\u6c14\u6ce1\u6d41\u662f\u591a\u76f8\u6d41\u7684\u5178\u578b\u4ee3\u8868\u3002\u800c\u591a\u76f8\u6d41\u662f\u7814\u7a76\u4e24\u79cd\u6216\u4ee5\u4e0a\u4e0d\u540c\u76f8\u6001\u6216\u4e0d\u540c\u7ec4\u5206\u7684\u7269\u8d28\u5171\u5b58\u5e76\u6709\u660e\u786e\u5206\u754c\u9762\u7684\u591a\u76f8\u6d41\u4f53\u6d41\u52a8\u529b\u5b66\u3001\u70ed\u529b\u5b66\u3001\u4f20\u70ed\u4f20\u8d28\u5b66\u3001\u71c3\u70e7\u5b66\u3001\u5316\u5b66\u548c\u751f\u7269\u53cd\u5e94\u4ee5\u53ca\u76f8\u5173\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u7684\u5171\u6027\u79d1\u5b66\u95ee\u9898\u3002\u8fd9\u662f\u4e00\u95e8\u4ece\u4f20\u7edf\u80fd\u6e90\u8f6c\u5316\u4e0e\u5229\u7528\u9886\u57df\u9010\u6e10\u53d1\u5c55\u8d77\u6765\u7684\u65b0\u5174\u4ea4\u53c9\u79d1\u5b66\uff0c\u6d89\u53ca\u5230\u80fd\u6e90\u3001\u52a8\u529b\u3001\u6838\u53cd\u5e94\u5806\u3001\u5316\u5de5\u3001\u77f3\u6cb9\u3001\u5236\u51b7\u3001\u4f4e\u6e29\u3001\u53ef\u518d\u751f\u80fd\u6e90\u5f00\u53d1\u5229\u7528\u3001\u822a\u7a7a\u822a\u5929\u3001\u73af\u5883\u4fdd\u62a4\u3001\u751f\u547d\u79d1\u5b66\u7b49\u8bb8\u591a\u9886\u57df\uff0c\u5728\u56fd\u6c11\u7ecf\u6d4e\u7684\u57fa\u7840\u4e0e\u652f\u67f1\u4ea7\u4e1a\u53ca\u56fd\u9632\u79d1\u5b66\u6280\u672f\u53d1\u5c55\u4e2d\u6709\u4e0d\u53ef\u66ff\u4ee3\u7684\u5de8\u5927\u4f5c\u7528\u3002\u5728\u591a\u76f8\u6d41\u4e2d\uff0c\u5404\u76f8\u4e4b\u95f4\u6709\u660e\u663e\u7684\u754c\u9762\uff0c\u5e76\u4e14\u5404\u81ea\u4fdd\u6301\u76f8\u5bf9\u72ec\u7acb\u7684\u7269\u8d28\u7279\u6027\uff0c\u5982\u6c14-\u6db2\u3001\u6c14-\u56fa\u3001\u6db2-\u6db2\u3001\u6db2-\u56fa\u7b49\u3002\u672c\u6587\u6211\u4eec\u4e3b\u8981\u7814\u7a76\u6c14\u6ce1\u6d41\uff0c\u5f53\u7136\u672c\u6587\u4ecb\u7ecd\u7684\u90e8\u5206\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff08Semi-physics-informed neural networks\uff0cSemi-PINNs\uff09\u540c\u6837\u9002\u7528\u4e8e\u591a\u76f8\u6d41\u95ee\u9898\u3002</p> <p>\u6c14\u6ce1\u6d41\u662f\u4e00\u79cd\u6d41\u4f53\u529b\u5b66\u73b0\u8c61\uff0c\u53d1\u751f\u5728\u6c14\u6db2\u4e24\u76f8\u6df7\u5408\u7269\u5728\u7ba1\u4e2d\u53d7\u5230\u529b\u7684\u4f5c\u7528\u800c\u6d41\u52a8\u4e14\u6df7\u5408\u7269\u4e2d\u542b\u6c14\u91cf\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u3002\u6b64\u65f6\uff0c\u6c14\u76f8\u4ee5\u5206\u6563\u7684\u5c0f\u6c14\u6ce1\u5206\u5e03\u4e8e\u6db2\u76f8\u4e2d\uff0c\u5728\u7ba1\u5b50\u4e2d\u592e\u7684\u6c14\u6ce1\u8f83\u591a\uff0c\u9760\u8fd1\u7ba1\u58c1\u7684\u6c14\u6ce1\u8f83\u5c11\u3002\u8fd9\u4e9b\u5c0f\u6c14\u6ce1\u8fd1\u4f3c\u7403\u5f62\uff0c\u5e76\u4e14\u5176\u8fd0\u52a8\u901f\u5ea6\u5927\u4e8e\u6db2\u4f53\u6d41\u901f\uff0c\u8fd9\u79cd\u6d41\u6001\u88ab\u79f0\u4e3a\u6c14\u6ce1\u6d41\u3002\u6c14\u6ce1\u6d41\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u9886\u57df\uff0c\u4f8b\u5982</p> <ul> <li>\u5316\u5de5\u8fc7\u7a0b\uff1a\u5728\u5316\u5de5\u8fc7\u7a0b\u4e2d\uff0c\u6c14\u6ce1\u6d41\u5e38\u5e38\u53d1\u751f\u5728\u6db2\u4f53\u4e2d\u5b58\u5728\u5927\u91cf\u6c14\u4f53\u7684\u573a\u5408\uff0c\u4f8b\u5982\u5728\u6c14\u4f53\u5438\u6536\u3001\u89e3\u5438\u3001\u8403\u53d6\u3001\u4e73\u5316\u7b49\u8fc7\u7a0b\u4e2d\u3002\u6c14\u6ce1\u6d41\u7684\u7279\u6027\u5bf9\u4e8e\u5de5\u827a\u6d41\u7a0b\u7684\u4f18\u5316\u548c\u8bbe\u5907\u7684\u9009\u62e9\u5177\u6709\u91cd\u8981\u7684\u5f71\u54cd\u3002</li> <li>\u751f\u7269\u533b\u5b66\uff1a\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\uff0c\u6c14\u6ce1\u6d41\u5e38\u5e38\u88ab\u7528\u4e8e\u836f\u7269\u7684\u4f20\u9012\u3001\u7ec6\u80de\u7684\u5206\u79bb\u3001\u751f\u7269\u53cd\u5e94\u5668\u7684\u8bbe\u8ba1\u7b49\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u63a7\u5236\u6c14\u6ce1\u7684\u5927\u5c0f\u548c\u6d41\u901f\uff0c\u53ef\u4ee5\u5c06\u836f\u7269\u7cbe\u786e\u5730\u4f20\u9012\u5230\u76ee\u6807\u90e8\u4f4d\u3002</li> <li>\u73af\u5883\u5de5\u7a0b\uff1a\u5728\u73af\u5883\u5de5\u7a0b\u4e2d\uff0c\u6c14\u6ce1\u6d41\u53ef\u7528\u4e8e\u6c34\u5904\u7406\u3001\u5e9f\u6c34\u5904\u7406\u3001\u6c14\u4f53\u51c0\u5316\u7b49\u8fc7\u7a0b\u4e2d\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u6c14\u6ce1\u6d41\u53ef\u4ee5\u5c06\u6c27\u6c14\u5f15\u5165\u6c61\u6c34\u4e2d\uff0c\u4fc3\u8fdb\u5fae\u751f\u7269\u7684\u751f\u957f\uff0c\u4ece\u800c\u52a0\u901f\u6709\u673a\u5e9f\u7269\u7684\u5206\u89e3\u3002</li> <li>\u98df\u54c1\u5de5\u4e1a\uff1a\u5728\u98df\u54c1\u5de5\u4e1a\u4e2d\uff0c\u6c14\u6ce1\u6d41\u4e5f\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u4f8b\u5982\uff0c\u5728\u5236\u4f5c\u9762\u5305\u3001\u86cb\u7cd5\u7b49\u98df\u54c1\u65f6\uff0c\u901a\u8fc7\u6c14\u6ce1\u6d41\u7684\u7279\u6027\u53ef\u4ee5\u63a7\u5236\u9762\u56e2\u7684\u53d1\u9175\u8fc7\u7a0b\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u597d\u7684\u53e3\u611f\u548c\u8d28\u5730\u3002</li> <li>\u822a\u7a7a\u822a\u5929\uff1a\u5728\u822a\u7a7a\u822a\u5929\u9886\u57df\uff0c\u6c14\u6ce1\u6d41\u7684\u7814\u7a76\u53ef\u4ee5\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u66f4\u597d\u5730\u7406\u89e3\u98de\u673a\u3001\u706b\u7bad\u7b49\u590d\u6742\u6d41\u4f53\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u6d41\u52a8\u7279\u6027\uff0c\u4ece\u800c\u4f18\u5316\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u6027\u80fd\u3002</li> <li>\u77f3\u6cb9\u5de5\u4e1a\uff1a\u5728\u77f3\u6cb9\u5de5\u4e1a\u4e2d\uff0c\u6c14\u6ce1\u6d41\u5e38\u5e38\u51fa\u73b0\u5728\u91c7\u6cb9\u3001\u8f93\u6cb9\u3001\u70bc\u6cb9\u7b49\u8fc7\u7a0b\u4e2d\u3002\u901a\u8fc7\u6c14\u6ce1\u6d41\u53ef\u4ee5\u589e\u52a0\u6cb9\u6c34\u754c\u9762\u7684\u5f20\u529b\uff0c\u63d0\u9ad8\u91c7\u6cb9\u6548\u7387\u3002</li> </ul> <p>\u6c14\u6ce1\u6d41\u7684\u7814\u7a76\u548c\u5e94\u7528\u5bf9\u4e8e\u8bb8\u591a\u9886\u57df\u90fd\u5177\u6709\u91cd\u8981\u7684\u610f\u4e49\uff0c\u4e0d\u4ec5\u6709\u52a9\u4e8e\u6211\u4eec\u6df1\u5165\u7406\u89e3\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u57fa\u672c\u539f\u7406\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u5b9e\u9645\u751f\u4ea7\u548c\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u6709\u76ca\u7684\u6307\u5bfc\u548c\u5e2e\u52a9\u3002\u540c\u65f6\u7531\u4e8e\u6c14\u6ce1\u6d41\u662f\u4e00\u79cd\u5177\u6709\u9ad8\u5bc6\u5ea6\u68af\u5ea6\u7684\u7ecf\u5178\u6d41\u4f53\u529b\u5b66\u95ee\u9898\uff0c\u56e0\u6b64\u7ecf\u5e38\u88ab\u7528\u4e8e\u6d4b\u8bd5\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002</p>"},{"location":"zh/examples/bubble/#12-semi-pinns","title":"1.2 Semi-PINNs\u65b9\u6cd5","text":"<p>\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08Physics-informed Neural Networks\uff0cPINNs\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u7269\u7406\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u6709\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\uff0c\u540c\u65f6\u5c0a\u91cd\u7531\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u63cf\u8ff0\u7684\u7269\u7406\u89c4\u5f8b\u3002\u8fd9\u79cd\u7f51\u7edc\u4e0d\u4ec5\u5b66\u4e60\u5230\u8bad\u7ec3\u6570\u636e\u6837\u672c\u7684\u5206\u5e03\u89c4\u5f8b\uff0c\u8fd8\u80fd\u5b66\u4e60\u5230\u6570\u5b66\u65b9\u7a0b\u63cf\u8ff0\u7684\u7269\u7406\u5b9a\u5f8b\u3002</p> <p>PINNs \u7684\u80cc\u666f\u6e90\u4e8e\u5bf9\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u548c\u7269\u7406\u6a21\u578b\u7684\u7ed3\u5408\u3002\u5728\u8bb8\u591a\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u91c7\u96c6\u96be\u5ea6\u9ad8\u548c\u590d\u6742\u6027\uff0c\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u53d6\u5f97\u7406\u60f3\u7684\u6548\u679c\u3002\u540c\u65f6\uff0c\u4f20\u7edf\u7684\u7269\u7406\u6a21\u578b\u867d\u7136\u80fd\u591f\u51c6\u786e\u5730\u63cf\u8ff0\u81ea\u7136\u73b0\u8c61\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6240\u6709\u53ef\u7528\u7684\u6570\u636e\u3002\u56e0\u6b64\uff0cPINN\u4f5c\u4e3a\u4e00\u79cd\u7ed3\u5408\u4e86\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u5229\u7528\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002</p> <p>PINNs \u7684\u539f\u7406\u662f\u5c06\u7269\u7406\u65b9\u7a0b\u4f5c\u4e3a\u6b63\u5219\u5668\uff0c\u4ee5\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u6c42\u89e3\u5668\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7684\u7ed3\u679c\u4e0e\u5b9e\u9645\u89c2\u6d4b\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u66f4\u65b0\u795e\u7ecf\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u4ee5\u51cf\u5c0f\u9884\u6d4b\u8bef\u5dee\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8003\u8651\u4e86\u7269\u7406\u7ea6\u675f\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u7cfb\u7edf\u7684\u52a8\u6001\u884c\u4e3a\u3002</p> <p>\u5c3d\u7ba1PINN\u5177\u6709\u8bb8\u591a\u4f18\u70b9\uff0c\u5982\u80fd\u591f\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u548c\u89e3\u51b3\u53cd\u95ee\u9898\u7b49\uff0c\u4f46\u5b83\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u635f\u5931\u51fd\u6570\u4e2d\u7269\u7406\u65b9\u7a0b\u7684\u8003\u8651\u901a\u5e38\u9700\u8981\u7269\u7406\u91cf\u7684\u9ad8\u9636\u5fae\u5206\u3002\u7279\u522b\u662f\u5728\u4e24\u76f8\u6d41\u4e2d\uff0c\u4e0d\u540c\u6d41\u4f53\u754c\u9762\u5904\u7684\u76f8\u4f4d\u503c\u5448\u73b0\u51fa\u4ece 0 \u5230 1 \u7684\u5267\u70c8\u53d8\u5316\uff0c\u4f7f\u5f97\u68af\u5ea6\u7684\u8ba1\u7b97\u53d8\u5f97\u975e\u5e38\u56f0\u96be\u3002 \u56e0\u6b64\uff0c\u5bf9\u4e8e\u5177\u6709\u9ad8\u68af\u5ea6\u7684\u53d8\u91cf\uff0c\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\u5c06\u662f\u7b97\u6cd5\u6210\u529f\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u7136\u800c\u8fd9\u5c06\u5927\u5927\u589e\u52a0\u6df1\u5ea6\u5b66\u4e60\u7684\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u8bb8\u591a\u5b9e\u9a8c\u4e5f\u5f88\u96be\u83b7\u5f97\u9ad8\u5206\u8fa8\u7387\u7684\u6570\u636e\u3002</p> <p>\u4e3a\u6b64\uff0c\u6211\u4eec\u4e0d\u91c7\u7528\u5b8c\u6574\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u65b9\u7a0b\u6765\u76d1\u7763\u8bad\u7ec3\u6c14\u6ce1\u6d41\u52a8\u7684\u8fc7\u7a0b\uff0c\u800c\u662f\u57fa\u4e8e\u90e8\u5206\u7269\u7406\u4fe1\u606f\u4ee5\u83b7\u5f97\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ec5\u5c06\u6d41\u4f53\u8fde\u7eed\u6027\u6761\u4ef6\uff08 \\(\\nabla \\mathbf{u} =0\\) \uff09\u548c\u538b\u529b\u6cca\u677e\u65b9\u7a0b\uff08\u8868\u793a\u4e3a \\(\\mathcal{P}\\)\u2060 ) \u4ee3\u5165\u635f\u5931\u51fd\u6570\uff0c\u53ef\u4ee5\u5c06\u5176\u63cf\u8ff0\u4e3a\u5177\u6709\u90e8\u5206\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7f51\u7edc\u2014\u2014 Semi-PINNs\u3002</p>"},{"location":"zh/examples/bubble/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":""},{"location":"zh/examples/bubble/#21","title":"2.1 \u6c14\u6ce1\u6d41\u63a7\u5236\u65b9\u7a0b","text":"<p>\u6c14\u6ce1\u6d41\u6a21\u578b\u4e00\u822c\u7531 Navier\u2013Stokes \u65b9\u7a0b\u8fdb\u884c\u63cf\u8ff0\uff0c</p> \\[ \\begin{cases} \\begin{aligned}   &amp;\\nabla \\cdot \\mathbf{u}=0, \\\\   &amp;\\rho\\left(\\frac{\\partial \\mathbf{u}}{\\partial t}+(\\mathbf{u} \\cdot \\nabla) \\mathbf{u}\\right)=-\\nabla p+\\mu \\nabla^2 \\mathbf{u}, \\end{aligned} \\end{cases} \\] <p>\u5176\u4e2d \\(\\rho\\) \u662f\u6d41\u4f53\u5bc6\u5ea6\uff0c\\(\\mathbf{u} = ( u , v )\\) \u662f\u4e8c\u7ef4\u901f\u5ea6\u77e2\u91cf\uff0c\\(p\\) \u662f\u6d41\u4f53\u538b\u529b\uff0c\\(\\mu\\) \u662f\u52a8\u529b\u7c98\u5ea6\u3002\u7a7a\u6c14\u548c\u6c34\u4e4b\u95f4\u7684\u754c\u9762\u7531\u67d0\u4e2a\u6c34\u5e73\u96c6\u6216\u5168\u5c40\u5b9a\u4e49\u51fd\u6570\u7684\u7b49\u503c\u7ebf\u8868\u793a\uff0c\u5373\u5b9a\u4e49\u4e8c\u7ef4\u7a7a\u95f4\u4e2d\u7684\u6c34\u5e73\u96c6\u51fd\u6570 \\(\\phi = \\phi ( x , y , t )\\)\u3002\u5bf9\u4e8e\u6c34\u6765\u8bf4 \\(\\phi=0\\)\uff0c\u5bf9\u4e8e\u7a7a\u6c14\u6765\u8bf4 \\(\\phi=1\\)\u3002\u5728\u4e24\u8005\u754c\u9762\u9644\u8fd1\uff0c\u6709\u4ece 0 \u5230 1 \u7684\u5149\u6ed1\u8fc7\u6e21\uff0c\u540c\u65f6\u8bbe\u754c\u9762\u7684\u6c34\u5e73\u96c6 \\(\\phi= 0.5\\)\u3002</p> <p>\u8bbe \\(\\rho_l\\) \u4e3a\u6c34\uff08\u6db2\u4f53\uff09\u5bc6\u5ea6\uff0c\\(\\rho_g\\) \u4e3a\u7a7a\u6c14\uff08\u6c14\u4f53\uff09\u5bc6\u5ea6\uff0c\\(\\mu_l\\) \u4e3a\u6c34\u7c98\u5ea6\uff0c\\(\\mu_g\\) \u4e3a\u7a7a\u6c14\u7c98\u5ea6\uff0c\u5219\u6d41\u4f53\u4e2d\u7684\u5bc6\u5ea6\u548c\u7c98\u5ea6\u53ef\u4ee5\u901a\u8fc7\u6c34\u5e73\u96c6\u51fd\u6570\u8868\u793a\u4e3a\uff1a</p> \\[\\begin{aligned} \\rho &amp; =\\rho_l+\\phi\\left(\\rho_g-\\rho_l\\right), \\\\ \\mu &amp; =\\mu_l+\\phi\\left(\\mu_g-\\mu_l\\right) . \\end{aligned}\\] <p>\u4e3a\u4e86\u6a21\u62df\u6db2\u4f53\u548c\u6c14\u4f53\u4e4b\u95f4\u7684\u754c\u9762\uff0c\u6c34\u5e73\u96c6\u51fd\u6570 \\(\\phi = \\phi ( x , y , t )\\) \u5b9a\u4e49\u4e3a</p> \\[\\frac{\\partial \\phi}{\\partial t}+\\mathbf{u} \\cdot \\nabla \\phi=\\gamma \\nabla \\cdot\\left(\\epsilon_{l s} \\nabla \\phi-\\phi(1-\\phi) \\frac{\\nabla \\phi}{|\\nabla \\phi|}\\right),\\] <p>\u5176\u4e2d\u7b49\u53f7\u5de6\u7aef\u63cf\u8ff0\u4e86\u754c\u9762\u7684\u8fd0\u52a8\uff0c\u800c\u53f3\u7aef\u65e8\u5728\u4fdd\u6301\u754c\u9762\u7d27\u51d1\uff0c\u4f7f\u6570\u503c\u7a33\u5b9a\u3002\\(\\gamma\\) \u662f\u521d\u59cb\u5316\u53c2\u6570\uff0c\u51b3\u5b9a\u4e86\u6c34\u5e73\u96c6\u51fd\u6570\u7684\u91cd\u65b0\u521d\u59cb\u5316\u6216\u7a33\u5b9a\u91cf\uff0c\\(\\epsilon_{l s}\\) \u662f\u63a7\u5236\u754c\u9762\u539a\u5ea6\u7684\u53c2\u6570\uff0c\u5b83\u7b49\u4e8e\u7f51\u683c\u6700\u5927\u5c3a\u5bf8\u3002</p>"},{"location":"zh/examples/bubble/#22-bubblenetsemi-pinns","title":"2.2 BubbleNet\uff08Semi-PINNs\u65b9\u6cd5\uff09","text":"<p>\u6c14\u6ce1\u6d41\u95ee\u9898\u7684\u7814\u7a76\u901a\u5e38\u53ef\u4ee5\u5206\u4e3a\u5355\u6c14\u6ce1\u6d41(\u56fe A )\u548c\u591a\u6c14\u6ce1\u6d41(\u56fe B )\u3002</p> <p></p> <p>\u672c\u6587\u6211\u4eec\u4e3b\u8981\u8003\u8651\u5355\u6c14\u6ce1\u6d41(\u56fe A )\uff0c\u5f53\u7136\u5bf9\u4e8e\u591a\u6c14\u6ce1\u6d41\u95ee\u9898\u540c\u6837\u9002\u7528\u3002\u5bf9\u4e8e\u5355\u6c14\u6ce1\u60c5\u51b5\uff0c\u6c14\u6ce1\u521d\u59cb\u76f4\u5f84\u8bbe\u7f6e\u4e3a \\(d =  4~\u03bcm\\)\uff0c\u5fae\u901a\u9053\u957f\u5ea6\u4e3a \\(15~\u03bcm\\)\uff0c\u5bbd\u5ea6\u4e3a \\(5~\u03bcm\\)\u3002\u6cbf\u8f74\u5411\u65bd\u52a0\u538b\u529b\u5dee \\(\\Delta p = 10~Pa\\) \u6765\u9a71\u52a8\u6c14\u6ce1\u6d41\u52a8\uff0c\u901a\u9053\u672b\u7aef\u7684\u538b\u529b\u4fdd\u6301\u4e3a\u6052\u5b9a\u538b\u529b \\(p_0 = 799.932~Pa(6~mmHg)\\)\uff0c\u5bf9\u5e94\u4e8e\u4eba\u8111\u548c\u6dcb\u5df4\u6db2\u6d41\u52a8\u4e2d\u95f4\u8d28\u6db2\u7684\u538b\u529b\u3002\u521d\u59cb\u6761\u4ef6 (IC) \u5373\u8bbe\u7f6e\u4e3a \\(p=p_0\\)\uff0c\u5ba4\u6e29\u4e3a \\(293.15~K\\)\uff0c\u5982\u56fe A \u6240\u793a\u3002\u8be5\u6570\u503c\u8bbe\u7f6e\u65e8\u5728\u6a21\u62df\u8111\u8840\u7ba1\u4e2d\u7684\u6c14\u6ce1\u4f20\u8f93\uff0c\u4ee5\u7814\u7a76\u8840\u8111\u5c4f\u969c\u3002\u540c\u65f6\u6211\u4eec\u8bbe \\(\\gamma=1\\) \u548c \\(\\epsilon_{l s}=0.430\\)\u3002</p> <p>\u672c\u6587\u7684\u7b97\u6cd5 BubbleNet \u7684\u4e3b\u8981\u5185\u5bb9\u5982\u4e0b\uff1a</p> <ul> <li>\u91c7\u7528\u65f6\u95f4\u79bb\u6563\u5f52\u4e00\u5316\uff08TDN\uff09\uff0c\u5373</li> </ul> \\[\\mathcal{W}=\\frac{\\mathcal{U}-\\mathcal{U}_{\\min }}{\\mathcal{U}_{\\max }-\\mathcal{U}_{\\min }},\\] <p>\u5176\u4e2d \\(\\mathcal{U}=(u, v, p, \\phi)\\) \u662f\u4ece\u6c14\u6ce1\u6d41\u6a21\u62df\u4e2d\u83b7\u5f97\u7684\u7c97\u5316\u6570\u636e\uff0c\\(\\mathcal{U}_{\\min },~\\mathcal{U}_{\\max }\\) \u5206\u522b\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7c97\u5316CFD\u6570\u636e\u7684\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u3002\u5982\u6b64\u5904\u7406\u7684\u539f\u56e0\u5728\u4e8e\u6d41\u573a\u4e2d\u7684\u7269\u7406\u91cf\u53d8\u5316\u8f83\u5927\uff0cTDN \u5c06\u6709\u52a9\u4e8e\u6d88\u9664\u7269\u7406\u91cf\u53d8\u5316\u9020\u6210\u7684\u4e0d\u51c6\u786e\u6027\u3002</p> <ul> <li>\u5f15\u5165\u6d41\u51fd\u6570 \\(\\psi\\) \u7528\u4e8e\u9884\u6d4b\u901f\u5ea6\u573a \\(u\\)\u3001\\(v\\)\uff0c\u5373</li> </ul> \\[u=\\frac{\\partial \\psi}{\\partial y},\\quad  v=-\\frac{\\partial \\psi}{\\partial x},\\] <p>\u6d41\u51fd\u6570\u7684\u5f15\u5165\u907f\u514d\u4e86\u635f\u5931\u51fd\u6570\u4e2d\u901f\u5ea6\u5411\u91cf\u7684\u68af\u5ea6\u8ba1\u7b97\uff0c\u63d0\u9ad8\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u6548\u7387\u3002</p> <ul> <li>\u5f15\u5165\u538b\u529b\u6cca\u677e\u65b9\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5373\u5bf9\u52a8\u91cf\u65b9\u7a0b\u7b49\u53f7\u4e24\u7aef\u540c\u65f6\u6c42\u6563\u5ea6\uff1a</li> </ul> \\[\\nabla^2 p=\\rho \\frac{\\nabla \\cdot \\mathbf{u}}{\\Delta t}-\\rho \\nabla \\cdot(\\mathbf{u} \\cdot \\nabla \\mathbf{u})+\\mu \\nabla^2(\\nabla \\cdot \\mathbf{u}) = -\\rho \\nabla \\cdot(\\mathbf{u} \\cdot \\nabla \\mathbf{u}).\\] <ul> <li>\u4f7f\u7528\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u6765\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u4e2d\u9884\u6d4b\u548c\u8bad\u7ec3\u6570\u636e\u7684\u504f\u5dee\uff0c\u635f\u5931\u51fd\u6570\u8868\u793a\u4e3a</li> </ul> \\[\\mathcal{L}=\\frac{1}{m} \\sum_{i=1}^m\\left(\\mathcal{W}_{\\text {pred }(i)}-\\mathcal{W}_{\\text {train }(i)}\\right)^2+\\frac{1}{m} \\sum_{i=1}^m\\left(\\nabla^2 p_{(i)}\\right)^2,\\] <p>\u5176\u4e2d \\(\\mathcal{W}=(u, v, p, \\phi)\\) \u8868\u793a\u5f52\u4e00\u5316\u540e\u7684\u6570\u636e\u96c6\u3002</p> <p>\u5bf9\u4e8e\u5355\u6c14\u6ce1\u6d41\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u7684 BubbleNet\uff08Semi-PINNs\u65b9\u6cd5\uff09\u7684\u6a21\u578b\u7ed3\u6784\u56fe\u4e3a\uff1a</p> <p></p> <p>\u5176\u4e2d\u6211\u4eec\u4f7f\u7528\u4e09\u4e2a\u5b50\u7f51\uff1a\\(Net_{\\psi},~Net_p\\) \u548c \\(Net_{\\phi}\\) \u6765\u5206\u522b\u9884\u6d4b \\(\\psi\\)\u3001\\(p\\) \u548c \\(\\phi\\)\uff0c\u5e76\u901a\u8fc7\u5bf9 \\(\\psi\\) \u81ea\u52a8\u5fae\u5206\u6765\u8ba1\u7b97\u901f\u5ea6 \\(u,~v\\)\u3002</p>"},{"location":"zh/examples/bubble/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u5355\u6c14\u6ce1\u6d41\u95ee\u9898\u3002\u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u7ea6\u675f\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003API\u6587\u6863\u3002</p>"},{"location":"zh/examples/bubble/#31","title":"3.1 \u6570\u636e\u5904\u7406","text":"<p>\u6570\u636e\u96c6\u662f\u901a\u8fc7\u5728\u7ec6\u7f51\u683c\u4e0b\u7684 CFD \u7ed3\u679c\u83b7\u5f97\u7684\uff0c\u5305\u542b\u672a\u5f52\u4e00\u5316\u7684 \\(x,~y,~t,~u,~v,~p,~\\phi\\)\uff0c\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b58\u50a8\u5728 <code>.mat</code> \u6587\u4ef6\u4e2d\u3002\u8fd0\u884c\u672c\u95ee\u9898\u4ee3\u7801\u524d\u8bf7\u4e0b\u8f7d\u6570\u636e\u96c6\u3002</p> <p>\u4e0b\u8f7d\u540e\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u65f6\u95f4\u79bb\u6563\u5f52\u4e00\u5316\uff08TDN\uff09\u5904\u7406\uff0c\u540c\u65f6\u6784\u9020\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u3002</p> <pre><code># load Data\ndata = scipy.io.loadmat(cfg.DATA_PATH)\n# normalize data\np_max = data[\"p\"].max(axis=0)\np_min = data[\"p\"].min(axis=0)\np_norm = (data[\"p\"] - p_min) / (p_max - p_min)\nu_max = data[\"u\"].max(axis=0)\nu_min = data[\"u\"].min(axis=0)\nu_norm = (data[\"u\"] - u_min) / (u_max - u_min)\nv_max = data[\"v\"].max(axis=0)\nv_min = data[\"v\"].min(axis=0)\nv_norm = (data[\"v\"] - v_min) / (v_max - v_min)\n\nu_star = u_norm  # N x T\nv_star = v_norm  # N x T\np_star = p_norm  # N x T\nphil_star = data[\"phil\"]  # N x T\nt_star = data[\"t\"]  # T x 1\nx_star = data[\"X\"]  # N x 2\n\nN = x_star.shape[0]\nT = t_star.shape[0]\n\n# rearrange data\nxx = np.tile(x_star[:, 0:1], (1, T))  # N x T\nyy = np.tile(x_star[:, 1:2], (1, T))  # N x T\ntt = np.tile(t_star, (1, N)).T  # N x T\n\nx = xx.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\ny = yy.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\nt = tt.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n\nu = u_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\nv = v_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\np = p_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\nphil = phil_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n\nidx = np.random.choice(N * T, int(N * T * 0.75), replace=False)\n# train data\ntrain_input = {\"x\": x[idx, :], \"y\": y[idx, :], \"t\": t[idx, :]}\ntrain_label = {\"u\": u[idx, :], \"v\": v[idx, :], \"p\": p[idx, :], \"phil\": phil[idx, :]}\n\n# eval data\ntest_input = {\"x\": x, \"y\": y, \"t\": t}\ntest_label = {\"u\": u, \"v\": v, \"p\": p, \"phil\": phil}\n</code></pre>"},{"location":"zh/examples/bubble/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u6c14\u6ce1\u6d41\u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((t, x, y)\\) \u90fd\u6709\u81ea\u8eab\u7684\u6d41\u51fd\u6570 \\(\\psi\\)\u3001\u538b\u529b \\(p\\) \u548c \u6c34\u5e73\u96c6\u51fd\u6570 \\(\\phi\\) \u4e09\u4e2a\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528 3 \u4e2a\u5e76\u884c\u7684\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((t, x, y)\\) \u5206\u522b\u5230 \\((\\psi, p, \\phi)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f_i: \\mathbb{R}^3 \\to \\mathbb{R}\\)\uff0c\u5373\uff1a</p> \\[ \\begin{aligned} \\psi &amp;= f_1(t, x, y),\\\\ p &amp;= f_2(t, x, y),\\\\ \\phi &amp;= f_3(t, x, y). \\end{aligned} \\] <p>\u4e0a\u5f0f\u4e2d \\(f_1,f_2,f_3\\) \u5747\u4e3a MLP \u6a21\u578b\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel_psi = ppsci.arch.MLP(**cfg.MODEL.psi_net)\nmodel_p = ppsci.arch.MLP(**cfg.MODEL.p_net)\nmodel_phil = ppsci.arch.MLP(**cfg.MODEL.phil_net)\n</code></pre> <p>\u4f7f\u7528  <code>transform_out</code> \u51fd\u6570\u5b9e\u73b0\u6d41\u51fd\u6570 \\(\\psi\\) \u5230\u901f\u5ea6 \\(u,~v\\) \u7684\u53d8\u6362\uff0c\u4ee3\u7801\u5982\u4e0b</p> <pre><code># transform\ndef transform_out(in_, out):\n    psi_y = out[\"psi\"]\n    y = in_[\"y\"]\n    x = in_[\"x\"]\n    u = jacobian(psi_y, y)\n    v = -jacobian(psi_y, x)\n    return {\"u\": u, \"v\": v}\n</code></pre> <p>\u540c\u65f6\u9700\u8981\u5bf9\u6a21\u578b <code>model_psi</code> \u6ce8\u518c\u76f8\u5e94\u7684 transform \uff0c\u7136\u540e\u5c06 3 \u4e2a MLP \u6a21\u578b\u7ec4\u6210 <code>Model_List</code></p> <pre><code># register transform\nmodel_psi.register_output_transform(transform_out)\nmodel_list = ppsci.arch.ModelList((model_psi, model_p, model_phil))\n</code></pre> <p>\u8fd9\u6837\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 3 \u4e2a MLP \u6a21\u578b\uff0c\u6bcf\u4e2a MLP \u5305\u542b 9 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 30\uff0c\u4f7f\u7528 \"tanh\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5305\u542b\u8f93\u51fa transform \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model_list</code>\u3002</p>"},{"location":"zh/examples/bubble/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d\u5355\u6c14\u6ce1\u6d41\u7684\u8bad\u7ec3\u533a\u57df\u7531\u5b57\u5178 <code>train_input</code> \u50a8\u5b58\u7684\u70b9\u4e91\u6784\u6210\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u70b9\u4e91\u51e0\u4f55 <code>PointCloud</code> \u8bfb\u5165\u6570\u636e\uff0c\u7ec4\u5408\u6210\u65f6\u95f4-\u7a7a\u95f4\u7684\u8ba1\u7b97\u57df\u3002</p> <p>\u540c\u65f6\u6784\u9020\u53ef\u89c6\u5316\u533a\u57df\uff0c\u5373\u4ee5 [0, 0], [15, 5] \u4e3a\u5bf9\u89d2\u7ebf\u7684\u4e8c\u7ef4\u77e9\u5f62\u533a\u57df\uff0c\u4e14\u65f6\u95f4\u57df\u4e3a 126 \u4e2a\u65f6\u523b [1, 2,..., 125, 126]\uff0c\u8be5\u533a\u57df\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u7a7a\u95f4\u51e0\u4f55 <code>Rectangle</code> \u548c\u65f6\u95f4\u57df <code>TimeDomain</code>\uff0c\u7ec4\u5408\u6210\u65f6\u95f4-\u7a7a\u95f4\u7684 <code>TimeXGeometry</code> \u8ba1\u7b97\u57df\u3002\u4ee3\u7801\u5982\u4e0b</p> <pre><code># set time-geometry\n# set timestamps(including initial t0)\ntimestamps = np.linspace(0, 126, 127, endpoint=True)\ngeom = {\n    \"time_rect\": ppsci.geometry.PointCloud(\n        train_input,\n        (\"t\", \"x\", \"y\"),\n    ),\n    \"time_rect_visu\": ppsci.geometry.TimeXGeometry(\n        ppsci.geometry.TimeDomain(1, 126, timestamps=timestamps),\n        ppsci.geometry.Rectangle((0, 0), (15, 5)),\n    ),\n}\n</code></pre> \u63d0\u793a <p><code>Rectangle</code> \u548c <code>TimeDomain</code> \u662f\u4e24\u79cd\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\u7684 <code>Geometry</code> \u6d3e\u751f\u7c7b\u3002</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e8e\u4e8c\u7ef4\u77e9\u5f62\u51e0\u4f55\u57df\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.Rectangle(...)</code> \u521b\u5efa\u7a7a\u95f4\u51e0\u4f55\u57df\u5bf9\u8c61\uff1b</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e00\u7ef4\u65f6\u95f4\u57df\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.TimeDomain(...)</code> \u6784\u5efa\u65f6\u95f4\u57df\u5bf9\u8c61\u3002</p>"},{"location":"zh/examples/bubble/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u6839\u636e 2.2 BubbleNet\uff08Semi-PINNs\u65b9\u6cd5\uff09 \u4e2d\u5b9a\u4e49\u7684\u635f\u5931\u51fd\u6570\u8868\u8fbe\u5f0f\uff0c\u5bf9\u5e94\u4e86\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u7684\u4e24\u4e2a\u7ea6\u675f\u6761\u4ef6\uff0c\u63a5\u4e0b\u6765\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>InteriorConstraint</code> \u548c <code>SupervisedConstraint</code> \u6784\u5efa\u4e0a\u8ff0\u4e24\u79cd\u7ea6\u675f\u6761\u4ef6\u3002</p>"},{"location":"zh/examples/bubble/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u6211\u4eec\u4ee5\u5185\u90e8\u70b9\u7ea6\u675f <code>InteriorConstraint</code> \u6765\u5b9e\u73b0\u5728\u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u538b\u529b\u6cca\u677e\u65b9\u7a0b\u7684\u7ea6\u675f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set constraint\npde_constraint = ppsci.constraint.InteriorConstraint(\n    {\n        \"pressure_Poisson\": lambda out: hessian(out[\"p\"], out[\"x\"])\n        + hessian(out[\"p\"], out[\"y\"])\n    },\n    {\"pressure_Poisson\": 0},\n    geom[\"time_rect\"],\n    {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"batch_size\": cfg.TRAIN.batch_size.pde_constraint,\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"EQ\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u8ba1\u7b97\u65b9\u7a0b\u7ed3\u679c\uff0c\u6b64\u5904\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u8868\u8fbe\u5f0f\u7b2c\u4e8c\u9879\u4e2d \\(\\nabla^2 p_{(i)}\\)\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u6211\u4eec\u5e0c\u671b \\(\\nabla^2 p_{(i)}\\) \u7684\u7ed3\u679c\u88ab\u4f18\u5316\u81f3 0\uff0c\u56e0\u6b64\u5c06\u76ee\u6807\u503c\u8bbe\u4e3a 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"time_rect\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528\u5168\u91cf\u6570\u636e\u70b9\u8bad\u7ec3\uff0c\u56e0\u6b64 <code>dataset</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e3a \"IterableNamedArrayDataset\" \u4e14 <code>iters_per_epoch</code> \u4e5f\u8bbe\u7f6e\u4e3a 1\uff0c\u91c7\u6837\u70b9\u6570 <code>batch_size</code> \u8bbe\u4e3a 228595\uff1b</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u7684\u5e73\u5747\u503c\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"EQ\" \u5373\u53ef\u3002</p>"},{"location":"zh/examples/bubble/#342","title":"3.4.2 \u76d1\u7763\u7ea6\u675f","text":"<p>\u540c\u65f6\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u4ee5\u76d1\u7763\u5b66\u4e60\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u6b64\u5904\u91c7\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code>\uff1a</p> <pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": train_input,\n            \"label\": train_label,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size.sup_constraint,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"Sup\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u5176\u4e2d <code>\u201cdataset\u201d</code> \u5b57\u6bb5\u8868\u793a\u4f7f\u7528\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4fe1\u606f\uff0c\u5404\u4e2a\u5b57\u6bb5\u5206\u522b\u8868\u793a\uff1a</p> <ol> <li><code>name</code>\uff1a \u6570\u636e\u96c6\u7c7b\u578b\uff0c\u6b64\u5904 <code>\"NamedArrayDataset\"</code> \u8868\u793a\u5206 batch \u987a\u5e8f\u8bfb\u53d6\u7684 <code>.mat</code> \u7c7b\u578b\u7684\u6570\u636e\u96c6\uff1b</li> <li><code>input_keys</code>\uff1a \u8f93\u5165\u53d8\u91cf\u540d\uff1b</li> <li><code>label_keys</code>\uff1a \u6807\u7b7e\u53d8\u91cf\u540d\u3002</li> </ol> <p>\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u53e6\u5916\u8fd8\u6307\u5b9a\u4e86\u8be5\u7c7b\u521d\u59cb\u5316\u65f6\u53c2\u6570 <code>drop_last</code> \u4e3a <code>False</code>\u3001<code>shuffle</code> \u4e3a <code>True</code>\u3002</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684 MSE \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\u53d6\u5e73\u5747\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"Sup\" \u5373\u53ef\u3002</p> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u548c\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    sup_constraint.name: sup_constraint,\n    pde_constraint.name: pde_constraint,\n}\n</code></pre>"},{"location":"zh/examples/bubble/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528\u4e00\u4e07\u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u8bc4\u4f30\u95f4\u9694\u4e3a\u4e00\u5343\u8f6e\uff0c\u5b66\u4e60\u7387\u8bbe\u4e3a 0.001\u3002</p> <pre><code>TRAIN:\n  epochs: 10000\n  iters_per_epoch: 1\n  eval_during_train: true\n  eval_freq: 1000\n</code></pre>"},{"location":"zh/examples/bubble/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model_list)\n</code></pre>"},{"location":"zh/examples/bubble/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\nmse_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": test_input,\n            \"label\": test_label,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size.mse_validator,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"bubble_mse\",\n)\nvalidator = {\n    mse_validator.name: mse_validator,\n}\n</code></pre> <p>\u914d\u7f6e\u4e0e 3.4 \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>"},{"location":"zh/examples/bubble/#38","title":"3.8 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> <pre><code>solver = ppsci.solver.Solver(\n    model_list,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    geom=geom,\n    validator=validator,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/bubble/#39","title":"3.9 \u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u6700\u540e\u5728\u7ed9\u5b9a\u7684\u53ef\u89c6\u5316\u533a\u57df\u4e0a\u8fdb\u884c\u9884\u6d4b\u5e76\u53ef\u89c6\u5316\uff0c\u53ef\u89c6\u5316\u6570\u636e\u662f\u533a\u57df\u5185\u7684\u4e8c\u7ef4\u70b9\u96c6\uff0c\u6bcf\u4e2a\u65f6\u523b \\(t\\) \u7684\u5750\u6807\u662f \\((x^t_i, y^t_i)\\)\uff0c\u5bf9\u5e94\u503c\u662f \\((u^t_i, v^t_i, p^t_i,\\phi^t_i)\\)\uff0c\u5728\u6b64\u6211\u4eec\u5bf9\u9884\u6d4b\u5f97\u5230\u7684 \\((u^t_i, v^t_i, p^t_i,\\phi^t_i)\\) \u8fdb\u884c\u53cd\u5f52\u4e00\u5316\uff0c\u6211\u4eec\u5c06\u53cd\u5f52\u4e00\u5316\u540e\u7684\u6570\u636e\u6309\u65f6\u523b\u4fdd\u5b58\u6210 126 \u4e2a vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u6700\u540e\u7528\u53ef\u89c6\u5316\u8f6f\u4ef6\u6253\u5f00\u67e5\u770b\u5373\u53ef\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>pred_norm = solver.predict(visu_mat, None, 4096, no_grad=False, return_numpy=True)\n# inverse normalization\np_pred = pred_norm[\"p\"].reshape([NTIME_PDE, NPOINT_PDE]).T\nu_pred = pred_norm[\"u\"].reshape([NTIME_PDE, NPOINT_PDE]).T\nv_pred = pred_norm[\"v\"].reshape([NTIME_PDE, NPOINT_PDE]).T\npred = {\n    \"p\": (p_pred * (p_max - p_min) + p_min).T.reshape([-1, 1]),\n    \"u\": (u_pred * (u_max - u_min) + u_min).T.reshape([-1, 1]),\n    \"v\": (v_pred * (v_max - v_min) + v_min).T.reshape([-1, 1]),\n    \"phil\": pred_norm[\"phil\"],\n}\nlogger.message(\"Now saving visual result to: visual/result.vtu, please wait...\")\nppsci.visualize.save_vtu_from_dict(\n    osp.join(cfg.output_dir, \"visual/result.vtu\"),\n    {\n        \"t\": visu_mat[\"t\"],\n        \"x\": visu_mat[\"x\"],\n        \"y\": visu_mat[\"y\"],\n        \"u\": pred[\"u\"],\n        \"v\": pred[\"v\"],\n        \"p\": pred[\"p\"],\n        \"phil\": pred[\"phil\"],\n    },\n    (\"t\", \"x\", \"y\"),\n    (\"u\", \"v\", \"p\", \"phil\"),\n    NTIME_PDE,\n)\n</code></pre>"},{"location":"zh/examples/bubble/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"bubble.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nReference: https://github.com/hanfengzhai/BubbleNet\nBubble data files download link: https://paddle-org.bj.bcebos.com/paddlescience/datasets/BubbleNet/bubble.mat\n\"\"\"\n\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle\nimport scipy\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.autodiff import hessian\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # load Data\n    data = scipy.io.loadmat(cfg.DATA_PATH)\n    # normalize data\n    p_max = data[\"p\"].max(axis=0)\n    p_min = data[\"p\"].min(axis=0)\n    p_norm = (data[\"p\"] - p_min) / (p_max - p_min)\n    u_max = data[\"u\"].max(axis=0)\n    u_min = data[\"u\"].min(axis=0)\n    u_norm = (data[\"u\"] - u_min) / (u_max - u_min)\n    v_max = data[\"v\"].max(axis=0)\n    v_min = data[\"v\"].min(axis=0)\n    v_norm = (data[\"v\"] - v_min) / (v_max - v_min)\n\n    u_star = u_norm  # N x T\n    v_star = v_norm  # N x T\n    p_star = p_norm  # N x T\n    phil_star = data[\"phil\"]  # N x T\n    t_star = data[\"t\"]  # T x 1\n    x_star = data[\"X\"]  # N x 2\n\n    N = x_star.shape[0]\n    T = t_star.shape[0]\n\n    # rearrange data\n    xx = np.tile(x_star[:, 0:1], (1, T))  # N x T\n    yy = np.tile(x_star[:, 1:2], (1, T))  # N x T\n    tt = np.tile(t_star, (1, N)).T  # N x T\n\n    x = xx.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    y = yy.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    t = tt.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n\n    u = u_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    v = v_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    p = p_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    phil = phil_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n\n    idx = np.random.choice(N * T, int(N * T * 0.75), replace=False)\n    # train data\n    train_input = {\"x\": x[idx, :], \"y\": y[idx, :], \"t\": t[idx, :]}\n    train_label = {\"u\": u[idx, :], \"v\": v[idx, :], \"p\": p[idx, :], \"phil\": phil[idx, :]}\n\n    # eval data\n    test_input = {\"x\": x, \"y\": y, \"t\": t}\n    test_label = {\"u\": u, \"v\": v, \"p\": p, \"phil\": phil}\n\n    # set model\n    model_psi = ppsci.arch.MLP(**cfg.MODEL.psi_net)\n    model_p = ppsci.arch.MLP(**cfg.MODEL.p_net)\n    model_phil = ppsci.arch.MLP(**cfg.MODEL.phil_net)\n\n    # transform\n    def transform_out(in_, out):\n        psi_y = out[\"psi\"]\n        y = in_[\"y\"]\n        x = in_[\"x\"]\n        u = jacobian(psi_y, y)\n        v = -jacobian(psi_y, x)\n        return {\"u\": u, \"v\": v}\n\n    # register transform\n    model_psi.register_output_transform(transform_out)\n    model_list = ppsci.arch.ModelList((model_psi, model_p, model_phil))\n\n    # set time-geometry\n    # set timestamps(including initial t0)\n    timestamps = np.linspace(0, 126, 127, endpoint=True)\n    geom = {\n        \"time_rect\": ppsci.geometry.PointCloud(\n            train_input,\n            (\"t\", \"x\", \"y\"),\n        ),\n        \"time_rect_visu\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(1, 126, timestamps=timestamps),\n            ppsci.geometry.Rectangle((0, 0), (15, 5)),\n        ),\n    }\n\n    NTIME_ALL = len(timestamps)\n    NPOINT_PDE, NTIME_PDE = 300 * 100, NTIME_ALL - 1\n\n    # set constraint\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        {\n            \"pressure_Poisson\": lambda out: hessian(out[\"p\"], out[\"x\"])\n            + hessian(out[\"p\"], out[\"y\"])\n        },\n        {\"pressure_Poisson\": 0},\n        geom[\"time_rect\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"batch_size\": cfg.TRAIN.batch_size.pde_constraint,\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"EQ\",\n    )\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": train_input,\n                \"label\": train_label,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size.sup_constraint,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"Sup\",\n    )\n\n    # wrap constraints together\n    constraint = {\n        sup_constraint.name: sup_constraint,\n        pde_constraint.name: pde_constraint,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model_list)\n\n    # set validator\n    mse_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_input,\n                \"label\": test_label,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size.mse_validator,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"bubble_mse\",\n    )\n    validator = {\n        mse_validator.name: mse_validator,\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        geom=geom,\n        validator=validator,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n    # visualize prediction after finished training\n    visu_mat = geom[\"time_rect_visu\"].sample_interior(\n        NPOINT_PDE * NTIME_PDE, evenly=True\n    )\n    # transform\n    def transform_out(in_, out):\n        psi_y = out[\"psi\"]\n        y = in_[\"y\"]\n        x = in_[\"x\"]\n        u = jacobian(psi_y, y, create_graph=False)\n        v = -jacobian(psi_y, x, create_graph=False)\n        return {\"u\": u, \"v\": v}\n\n    model_psi.register_output_transform(transform_out)\n\n    pred_norm = solver.predict(visu_mat, None, 4096, no_grad=False, return_numpy=True)\n    # inverse normalization\n    p_pred = pred_norm[\"p\"].reshape([NTIME_PDE, NPOINT_PDE]).T\n    u_pred = pred_norm[\"u\"].reshape([NTIME_PDE, NPOINT_PDE]).T\n    v_pred = pred_norm[\"v\"].reshape([NTIME_PDE, NPOINT_PDE]).T\n    pred = {\n        \"p\": (p_pred * (p_max - p_min) + p_min).T.reshape([-1, 1]),\n        \"u\": (u_pred * (u_max - u_min) + u_min).T.reshape([-1, 1]),\n        \"v\": (v_pred * (v_max - v_min) + v_min).T.reshape([-1, 1]),\n        \"phil\": pred_norm[\"phil\"],\n    }\n    logger.message(\"Now saving visual result to: visual/result.vtu, please wait...\")\n    ppsci.visualize.save_vtu_from_dict(\n        osp.join(cfg.output_dir, \"visual/result.vtu\"),\n        {\n            \"t\": visu_mat[\"t\"],\n            \"x\": visu_mat[\"x\"],\n            \"y\": visu_mat[\"y\"],\n            \"u\": pred[\"u\"],\n            \"v\": pred[\"v\"],\n            \"p\": pred[\"p\"],\n            \"phil\": pred[\"phil\"],\n        },\n        (\"t\", \"x\", \"y\"),\n        (\"u\", \"v\", \"p\", \"phil\"),\n        NTIME_PDE,\n    )\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # load Data\n    data = scipy.io.loadmat(cfg.DATA_PATH)\n    # normalize data\n    p_max = data[\"p\"].max(axis=0)\n    p_min = data[\"p\"].min(axis=0)\n    p_norm = (data[\"p\"] - p_min) / (p_max - p_min)\n    u_max = data[\"u\"].max(axis=0)\n    u_min = data[\"u\"].min(axis=0)\n    u_norm = (data[\"u\"] - u_min) / (u_max - u_min)\n    v_max = data[\"v\"].max(axis=0)\n    v_min = data[\"v\"].min(axis=0)\n    v_norm = (data[\"v\"] - v_min) / (v_max - v_min)\n\n    u_star = u_norm  # N x T\n    v_star = v_norm  # N x T\n    p_star = p_norm  # N x T\n    phil_star = data[\"phil\"]  # N x T\n    t_star = data[\"t\"]  # T x 1\n    x_star = data[\"X\"]  # N x 2\n\n    N = x_star.shape[0]\n    T = t_star.shape[0]\n\n    # rearrange data\n    xx = np.tile(x_star[:, 0:1], (1, T))  # N x T\n    yy = np.tile(x_star[:, 1:2], (1, T))  # N x T\n    tt = np.tile(t_star, (1, N)).T  # N x T\n\n    x = xx.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    y = yy.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    t = tt.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n\n    u = u_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    v = v_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    p = p_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n    phil = phil_star.flatten()[:, None].astype(paddle.get_default_dtype())  # NT x 1\n\n    idx = np.random.choice(N * T, int(N * T * 0.75), replace=False)\n    # train data\n    train_input = {\"x\": x[idx, :], \"y\": y[idx, :], \"t\": t[idx, :]}\n\n    # eval data\n    test_input = {\"x\": x, \"y\": y, \"t\": t}\n    test_label = {\"u\": u, \"v\": v, \"p\": p, \"phil\": phil}\n\n    # set model\n    model_psi = ppsci.arch.MLP(**cfg.MODEL.psi_net)\n    model_p = ppsci.arch.MLP(**cfg.MODEL.p_net)\n    model_phil = ppsci.arch.MLP(**cfg.MODEL.phil_net)\n\n    # transform\n    def transform_out(in_, out):\n        psi_y = out[\"psi\"]\n        y = in_[\"y\"]\n        x = in_[\"x\"]\n        u = jacobian(psi_y, y, create_graph=False)\n        v = -jacobian(psi_y, x, create_graph=False)\n        return {\"u\": u, \"v\": v}\n\n    # register transform\n    model_psi.register_output_transform(transform_out)\n    model_list = ppsci.arch.ModelList((model_psi, model_p, model_phil))\n\n    # set time-geometry\n    # set timestamps(including initial t0)\n    timestamps = np.linspace(0, 126, 127, endpoint=True)\n    geom = {\n        \"time_rect\": ppsci.geometry.PointCloud(\n            train_input,\n            (\"t\", \"x\", \"y\"),\n        ),\n        \"time_rect_visu\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(1, 126, timestamps=timestamps),\n            ppsci.geometry.Rectangle((0, 0), (15, 5)),\n        ),\n    }\n\n    NTIME_ALL = len(timestamps)\n    NPOINT_PDE, NTIME_PDE = 300 * 100, NTIME_ALL - 1\n\n    # set validator\n    mse_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_input,\n                \"label\": test_label,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size.mse_validator,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"bubble_mse\",\n    )\n    validator = {\n        mse_validator.name: mse_validator,\n    }\n\n    # directly evaluate pretrained model(optional)\n    solver = ppsci.solver.Solver(\n        model_list,\n        output_dir=cfg.output_dir,\n        geom=geom,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n\n    # visualize prediction\n    visu_mat = geom[\"time_rect_visu\"].sample_interior(\n        NPOINT_PDE * NTIME_PDE, evenly=True\n    )\n\n    pred_norm = solver.predict(\n        visu_mat, None, 4096 * 2, no_grad=False, return_numpy=True\n    )\n    # inverse normalization\n    p_pred = pred_norm[\"p\"].reshape([NTIME_PDE, NPOINT_PDE]).T\n    u_pred = pred_norm[\"u\"].reshape([NTIME_PDE, NPOINT_PDE]).T\n    v_pred = pred_norm[\"v\"].reshape([NTIME_PDE, NPOINT_PDE]).T\n    pred = {\n        \"p\": (p_pred * (p_max - p_min) + p_min).T.reshape([-1, 1]),\n        \"u\": (u_pred * (u_max - u_min) + u_min).T.reshape([-1, 1]),\n        \"v\": (v_pred * (v_max - v_min) + v_min).T.reshape([-1, 1]),\n        \"phil\": pred_norm[\"phil\"],\n    }\n    logger.message(\"Now saving visual result to: visual/result.vtu, please wait...\")\n    ppsci.visualize.save_vtu_from_dict(\n        osp.join(cfg.output_dir, \"visual/result.vtu\"),\n        {\n            \"t\": visu_mat[\"t\"],\n            \"x\": visu_mat[\"x\"],\n            \"y\": visu_mat[\"y\"],\n            \"u\": pred[\"u\"],\n            \"v\": pred[\"v\"],\n            \"p\": pred[\"p\"],\n            \"phil\": pred[\"phil\"],\n        },\n        (\"t\", \"x\", \"y\"),\n        (\"u\", \"v\", \"p\", \"phil\"),\n        NTIME_PDE,\n    )\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model_psi = ppsci.arch.MLP(**cfg.MODEL.psi_net)\n    model_p = ppsci.arch.MLP(**cfg.MODEL.p_net)\n    model_phil = ppsci.arch.MLP(**cfg.MODEL.phil_net)\n\n    # transform\n    def transform_out(in_, out):\n        psi_y = out[\"psi\"]\n        y = in_[\"y\"]\n        x = in_[\"x\"]\n        u = jacobian(psi_y, y, create_graph=False)\n        v = -jacobian(psi_y, x, create_graph=False)\n        return {\"u\": u, \"v\": v}\n\n    # register transform\n    model_psi.register_output_transform(transform_out)\n    model_list = ppsci.arch.ModelList((model_psi, model_p, model_phil))\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {\n            key: InputSpec([None, 1], \"float32\", name=key)\n            for key in model_list.input_keys\n        },\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    # load Data\n    data = scipy.io.loadmat(cfg.DATA_PATH)\n    # normalize data\n    p_max = data[\"p\"].max(axis=0)\n    p_min = data[\"p\"].min(axis=0)\n    u_max = data[\"u\"].max(axis=0)\n    u_min = data[\"u\"].min(axis=0)\n    v_max = data[\"v\"].max(axis=0)\n    v_min = data[\"v\"].min(axis=0)\n\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n    # set time-geometry\n    timestamps = np.linspace(0, 126, 127, endpoint=True)\n    geom = {\n        \"time_rect_visu\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(1, 126, timestamps=timestamps),\n            ppsci.geometry.Rectangle((0, 0), (15, 5)),\n        ),\n    }\n    NTIME_ALL = len(timestamps)\n    NPOINT_PDE, NTIME_PDE = 300 * 100, NTIME_ALL - 1\n    input_dict = geom[\"time_rect_visu\"].sample_interior(\n        NPOINT_PDE * NTIME_PDE, evenly=True\n    )\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n\n    # inverse normalization\n    p_pred = output_dict[\"p\"].reshape([NTIME_PDE, NPOINT_PDE]).T\n    u_pred = output_dict[\"u\"].reshape([NTIME_PDE, NPOINT_PDE]).T\n    v_pred = output_dict[\"v\"].reshape([NTIME_PDE, NPOINT_PDE]).T\n    pred = {\n        \"p\": (p_pred * (p_max - p_min) + p_min).T.reshape([-1, 1]),\n        \"u\": (u_pred * (u_max - u_min) + u_min).T.reshape([-1, 1]),\n        \"v\": (v_pred * (v_max - v_min) + v_min).T.reshape([-1, 1]),\n        \"phil\": output_dict[\"phil\"],\n    }\n    ppsci.visualize.save_vtu_from_dict(\n        \"./visual/bubble_pred.vtu\",\n        {\n            \"t\": input_dict[\"t\"],\n            \"x\": input_dict[\"x\"],\n            \"y\": input_dict[\"y\"],\n            \"u\": pred[\"u\"],\n            \"v\": pred[\"v\"],\n            \"p\": pred[\"p\"],\n            \"phil\": pred[\"phil\"],\n        },\n        (\"t\", \"x\", \"y\"),\n        (\"u\", \"v\", \"p\", \"phil\"),\n        NTIME_PDE,\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"bubble.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/bubble/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u6211\u4eec\u4f7f\u7528 paraview \u6253\u5f00\u4fdd\u5b58\u7684 126 \u4e2a vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u5173\u4e8e\u901f\u5ea6 \\(u,~v\\)\uff0c\u538b\u529b \\(p\\)\uff0c\u4ee5\u53ca\u6c34\u5e73\u96c6\u51fd\u6570\uff08\u6c14\u6ce1\u7684\u5f62\u72b6\uff09 \\(\\phi\\) \u968f\u65f6\u95f4\u7684\u52a8\u6001\u53d8\u5316\u56fe\u50cf</p> <p> </p>\u901f\u5ea6 u \u968f\u65f6\u95f4\u7684\u52a8\u6001\u53d8\u5316\u56fe\u50cf \u901f\u5ea6 v \u968f\u65f6\u95f4\u7684\u52a8\u6001\u53d8\u5316\u56fe\u50cf \u538b\u529b p \u968f\u65f6\u95f4\u7684\u52a8\u6001\u53d8\u5316\u56fe\u50cf \u6c34\u5e73\u96c6\u51fd\u6570\uff08\u6c14\u6ce1\u7684\u5f62\u72b6\uff09 phi \u968f\u65f6\u95f4\u7684\u52a8\u6001\u53d8\u5316\u56fe\u50cf <p>\u4ece\u52a8\u6001\u53d8\u5316\u56fe\u50cf\u53ef\u4ee5\u5f97\u51fa\u4ee5\u4e0b\u7ed3\u8bba\uff1a</p> <ul> <li>\u4ece\u6c34\u5e73\u96c6\u51fd\u6570\uff08\u6c14\u6ce1\u7684\u5f62\u72b6\uff09 \\(\\phi\\) \u968f\u65f6\u95f4\u7684\u52a8\u6001\u53d8\u5316\u56fe\u50cf\uff0c\u53ef\u4ee5\u770b\u51fa\u8be5\u6a21\u578b\u53ef\u4ee5\u5f88\u597d\u7684\u9884\u6d4b\u6c14\u6ce1\u5728\u6db2\u4f53\u7ba1\u4e2d\u7684\u53d8\u5316\u8fc7\u7a0b\uff0c\u5177\u6709\u826f\u597d\u7684\u7cbe\u5ea6\uff1b</li> <li>\u4ece\u901f\u5ea6 \\(u,~v\\)  \u968f\u65f6\u95f4\u7684\u52a8\u6001\u53d8\u5316\u56fe\u50cf\uff0c\u53ef\u4ee5\u770b\u51fa\u8be5\u6a21\u578b\u53ef\u4ee5\u5f88\u597d\u7684\u9884\u6d4b\u6c14\u6ce1\u5728\u6db2\u4f53\u7ba1\u4e2d\u53d8\u5316\u65f6\u7684\u901f\u5ea6\u5927\u5c0f\uff0c\u540c\u65f6\u5bf9\u901f\u5ea6\u7684\u9884\u6d4b\u4f18\u4e8e\u4f20\u7edf DNN \u65b9\u6cd5\uff0c\u5177\u4f53\u6bd4\u8f83\u53ef\u4ee5\u53c2\u8003\u6587\u7ae0\uff1b</li> <li>\u7136\u800c\u89c2\u5bdf\u538b\u529b \\(p\\)  \u968f\u65f6\u95f4\u7684\u52a8\u6001\u53d8\u5316\u56fe\u50cf\uff0c\u51e0\u4e4e\u4e0d\u600e\u4e48\u53d8\u5316\uff0c \u5e76\u6ca1\u6709\u6210\u529f\u6355\u83b7\u538b\u529b\u573a\u4e2d\u7684\u6c14\u6ce1\u5f62\u72b6\u7279\u5f81\u7ec6\u8282\uff0c\u8fd9\u662f\u56e0\u4e3a\uff0c\u4e0e\u5927\u538b\u529b\u8303\u56f4(\u52a8\u6001\u53d8\u5316\u56fe\u50cf\u4e2d\u538b\u529b \\(p\\) \u7684\u8303\u56f4[800, 810] )\u76f8\u6bd4\uff0c\u63cf\u7ed8\u6c14\u6ce1\u5f62\u72b6\u7684\u538b\u529b\u5927\u5c0f\u7684\u7ec6\u5fae\u5dee\u522b\u592a\u5c0f\u3002</li> </ul> <p>\u7efc\u4e0a\u6240\u8ff0\uff0c\u7269\u7406\u4fe1\u606f\u4e0e\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u76f8\u7ed3\u5408\u7684 Semi-PINNs \u65b9\u6cd5\u53ef\u4ee5\u7075\u6d3b\u5730\u6784\u5efa\u7f51\u7edc\u6846\u67b6\uff0c\u83b7\u5f97\u6ee1\u8db3\u5de5\u7a0b\u9700\u6c42\u7684\u6ee1\u610f\u7ed3\u679c\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u6613\u83b7\u53d6\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u65f6\u662f\u975e\u5e38\u6709\u6548\u7684\u3002\u867d\u7136\u7f16\u7801\u5b8c\u6574\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u65b9\u7a0b\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u9884\u6d4b\u4e0a\u53ef\u80fd\u66f4\u51c6\u786e\uff0c\u4f46\u76ee\u524d\u7684 BubbleNet \u672c\u8d28\u4e0a\u662f\u9762\u5411\u5de5\u7a0b\u7684 Semi-PINNs \u65b9\u6cd5\uff0c\u5177\u6709\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u548c\u7075\u6d3b\u6027\u7684\u4f18\u52bf\u3002\u8fd9\u5c31\u63d0\u51fa\u4e86\u4e00\u4e2a\u503c\u5f97\u672a\u6765\u7ee7\u7eed\u7814\u7a76\u7684\u6709\u8da3\u95ee\u9898\uff0c\u5373\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9\u6027\u5730\u5c06\u7269\u7406\u4fe1\u606f\u5f15\u5165\u795e\u7ecf\u7f51\u7edc\u6765\u4f18\u5316\u7f51\u7edc\u6027\u80fd\uff0c\u66f4\u591a\u76f8\u5173\u5185\u5bb9\u53ca\u7ed3\u8bba\u8bf7\u53c2\u8003\u6587\u7ae0\u3002</p>"},{"location":"zh/examples/bubble/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<p>\u53c2\u8003\u6587\u732e\uff1a Predicting micro-bubble dynamics with semi-physics-informed deep learning</p> <p>\u53c2\u8003\u4ee3\u7801\uff1a BubbleNet(Semi-PINNs)</p>"},{"location":"zh/examples/cfdgcn/","title":"CFDGCN","text":""},{"location":"zh/examples/cfdgcn/#combining-differentiable-pde-solvers-and-graph-neural-networks-for-fluid-flow-prediction","title":"Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4 <pre><code># only linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/CFDGCN/data.zip\nunzip data.zip\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/CFDGCN/meshes.tar\ntar -xvf meshes.tar\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/CFDGCN/SU2Bin.tgz\ntar -zxvf SU2Bin.tgz\n\n# set BATCH_SIZE = number of cpu cores\nexport BATCH_SIZE=4\n\n# prediction experiments\nmpirun -np $((BATCH_SIZE+1)) python cfdgcn.py \\\n  TRAIN.batch_size=$((BATCH_SIZE)) &gt; /dev/null\n\n# generalization experiments\nmpirun -np $((BATCH_SIZE+1)) python cfdgcn.py \\\n  TRAIN.batch_size=$((BATCH_SIZE)) \\\n  TRAIN_DATA_DIR=\"./data/NACA0012_machsplit_noshock/outputs_train\" \\  \n  TRAIN_MESH_GRAPH_PATH=\"./data/NACA0012_machsplit_noshock/mesh_fine. su2\" \\\n  EVAL_DATA_DIR=\"./data/NACA0012_machsplit_noshock/outputs_test\" \\\n  EVAL_MESH_GRAPH_PATH=\"./data/NACA0012_machsplit_noshock/mesh_fine.su2\" \\\n  &gt; /dev/null\n</code></pre>"},{"location":"zh/examples/cfdgcn/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u7684\u6210\u529f\u5e94\u7528\uff0c\u4fc3\u4f7f\u4eba\u4eec\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66(CFD)\u9886\u57df\u7684\u5e94\u7528\u3002</p> <p>\u6d41\u4f53\u662f\u975e\u5e38\u590d\u6742\u7684\u7269\u7406\u7cfb\u7edf\uff0c\u6d41\u4f53\u7684\u884c\u4e3a\u7531 Navier-Stokes \u65b9\u7a0b\u63a7\u5236\u3002\u57fa\u4e8e\u7f51\u683c\u7684\u6709\u9650\u4f53\u79ef\u6216\u6709\u9650\u5143\u6a21\u62df\u65b9\u6cd5\u662f CFD \u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u503c\u65b9\u6cd5\u3002\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u7814\u7a76\u7684\u7269\u7406\u95ee\u9898\u5f80\u5f80\u975e\u5e38\u590d\u6742\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u624d\u80fd\u6c42\u51fa\u95ee\u9898\u7684\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u5728\u6c42\u89e3\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u4e3a\u4e86\u8fdb\u884c\u6570\u503c\u6a21\u62df\uff0c\u8ba1\u7b97\u57df\u901a\u5e38\u88ab\u7f51\u683c\u79bb\u6563\u5316\uff0c\u7531\u4e8e\u7f51\u683c\u5177\u6709\u826f\u597d\u7684\u51e0\u4f55\u548c\u7269\u7406\u95ee\u9898\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u548c\u56fe\u7ed3\u6784\u76f8\u5951\u5408\uff0c\u6240\u4ee5\u8fd9\u7bc7\u6587\u7ae0\u7684\u4f5c\u8005\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u8bad\u7ec3 CFD \u4eff\u771f\u6570\u636e\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u6a21\u578b\u6765\u8fdb\u884c\u6d41\u573a\u9884\u6d4b\u3002</p>"},{"location":"zh/examples/cfdgcn/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684 CFD \u8ba1\u7b97\u6a21\u578b\uff0c\u79f0\u4e3a CFD-GCN (Computational fluid dynamics - Graph convolution network)\uff0c\u8be5\u6a21\u578b\u662f\u4e00\u79cd\u6df7\u5408\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5c06\u4f20\u7edf\u7684\u56fe\u5377\u79ef\u7f51\u7edc\u4e0e\u7c97\u5206\u8fa8\u7387\u7684 CFD \u6a21\u62df\u5668\u76f8\u7ed3\u5408\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u5927\u5e45\u5ea6\u52a0\u901f CFD \u9884\u6d4b\uff0c\u8fd8\u53ef\u4ee5\u5f88\u597d\u5730\u6cdb\u5316\u5230\u65b0\u7684\u573a\u666f\uff0c\u4e0e\u6b64\u540c\u65f6\uff0c\u6a21\u578b\u7684\u9884\u6d4b\u6548\u679c\u8fdc\u8fdc\u4f18\u4e8e\u5355\u72ec\u7684\u7c97\u5206\u8fa8\u7387 CFD \u7684\u6a21\u62df\u6548\u679c\u3002</p> <p>\u4e0b\u56fe\u4e3a\u8be5\u65b9\u6cd5\u7684\u7f51\u7edc\u7ed3\u6784\u56fe\uff0c\u7f51\u7edc\u6709\u4e24\u4e2a\u4e3b\u8981\u90e8\u4ef6\uff1aGCN \u56fe\u795e\u7ecf\u7f51\u7edc\u4ee5\u53ca SU2 \u6d41\u4f53\u6a21\u62df\u5668\u3002\u7f51\u7edc\u5728\u4e24\u4e2a\uf967\u540c\u7684\u56fe\u4e0a\u8fd0\ufa08\uff0c\u4e24\u4e2a\u56fe\u5206\u522b\u662f\u7ec6\u7f51\u683c\u7684\u56fe\u548c\u7c97\u7f51\u683c\u7684\u56fe\u3002\u7f51\u7edc\u9996\u5148\u5728\u7c97\u7f51\u683c\u4e0a\u8fd0\ufa08 CFD \u6a21\u62df\uff0c\u540c\u65f6\u4f7f\u7528 GCN \u5904\uf9e4\u7ec6\u7f51\u683c\u7684\u56fe\u3002\u7136\u540e\uff0c\u5bf9\u6a21\u62df\u7ed3\u679c\u8fdb\ufa08\u4e0a\u91c7\u6837\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e GCN \u7684\u4e2d\u95f4\u8f93\u51fa\u8fde\u63a5\u8d77\u6765\u3002\u6700\u540e\uff0c\u6a21\u578b\u5c06\u989d\u5916\u7684 GCN \u5c42\u5e94\u7528\u4e8e\u8fd9\u4e9b\u8fde\u63a5\u7279\u5f81\uff0c\u9884\u6d4b\u6240\u9700\u7684\u8f93\u51fa\u503c\u3002</p> <p></p>"},{"location":"zh/examples/cfdgcn/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p> <p>\u6ce8\u610f\u4e8b\u9879</p> <p>\u672c\u6848\u4f8b\u8fd0\u884c\u524d\u9700\u901a\u8fc7 <code>pip install pgl==2.2.6 mpi4py</code> \u547d\u4ee4\uff0c\u5b89\u88c5 Paddle Graph Learning \u56fe\u5b66\u4e60\u5de5\u5177\u548c Mpi4py MPI python\u63a5\u53e3\u5e93\u3002</p> <p>\u7531\u4e8e\u65b0\u7248\u672c\u7684 Paddle \u4f9d\u8d56\u7684 python \u7248\u672c\u8f83\u9ad8\uff0c<code>pgl</code> \u4e0e <code>mpi4py</code> \u7684\u5b89\u88c5\u53ef\u80fd\u4f1a\u51fa\u73b0\u95ee\u9898\uff0c\u5efa\u8bae\u4f7f\u7528AI Studio\u5feb\u901f\u4f53\u9a8c\uff0c\u9879\u76ee\u4e2d\u5df2\u7ecf\u914d\u7f6e\u597d\u8fd0\u884c\u73af\u5883\u3002</p>"},{"location":"zh/examples/cfdgcn/#31","title":"3.1 \u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>\u8be5\u6848\u4f8b\u4f7f\u7528\u7684\u673a\u7ffc\u6570\u636e\u96c6 Airfoil\u6765\u81ea de Avila Belbute-Peres \u7b49\u4eba\uff0c\u5176\u4e2d\u7ffc\u578b\u6570\u636e\u96c6\u91c7\u7528 NACA0012 \u7ffc\u578b\uff0c\u5305\u62ec train, test \u4ee5\u53ca\u5bf9\u5e94\u7684\u7f51\u683c\u6570\u636e mesh_fine\uff1b\u5706\u67f1\u6570\u636e\u96c6\u662f\u539f\u4f5c\u8005\u5229\u7528\u8f6f\u4ef6\u8ba1\u7b97\u7684 CFD \u7b97\u4f8b\u3002</p> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7d\u5e76\u89e3\u538b\u6570\u636e\u96c6\u3002</p> <pre><code>wget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/CFDGCN/data.zip\nunzip data.zip\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/CFDGCN/meshes.tar\ntar -xvf meshes.tar\n</code></pre>"},{"location":"zh/examples/cfdgcn/#32-su2","title":"3.2 SU2 \u9884\u7f16\u8bd1\u5e93\u5b89\u88c5","text":"<p>SU2 \u6d41\u4f53\u6a21\u62df\u5668\u4ee5\u9884\u7f16\u8bd1\u5e93\u7684\u5f62\u5f0f\u5d4c\u5165\u5728\u7f51\u7edc\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u4e0b\u8f7d\u5e76\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u3002</p> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7d\u5e76\u89e3\u538b\u9884\u7f16\u8bd1\u5e93\u3002</p> <pre><code>wget -nc -P https://paddle-org.bj.bcebos.com/paddlescience/datasets/CFDGCN/SU2Bin.tgz\ntar -zxvf SU2Bin.tgz\n</code></pre> <p>\u9884\u7f16\u8bd1\u5e93\u4e0b\u8f7d\u5b8c\u6210\u540e\uff0c\u8bbe\u7f6e SU2 \u7684\u73af\u5883\u53d8\u91cf\u3002</p> <pre><code>export SU2_RUN=/absolute_path/to/SU2Bin/\nexport SU2_HOME=/absolute_path/to/SU2Bin/\nexport PATH=$PATH:$SU2_RUN\nexport PYTHONPATH=$PYTHONPATH:$SU2_RUN\n</code></pre>"},{"location":"zh/examples/cfdgcn/#33","title":"3.3 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u672c\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc <code>CFDGCN</code> \u4f5c\u4e3a\u6a21\u578b\uff0c\u5176\u63a5\u6536\u56fe\u7ed3\u6784\u6570\u636e\uff0c\u8f93\u51fa\u9884\u6d4b\u7ed3\u679c\u3002</p> <pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    output_expr={\"pred\": lambda out: out[\"pred\"]},\n    loss=ppsci.loss.FunctionalLoss(train_mse_func),\n    name=\"Sup\",\n)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"input\", )</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>(\"pred\", )</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p>"},{"location":"zh/examples/cfdgcn/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u76d1\u7763\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u6307\u5b9a\u6570\u636e\u96c6\u7684\u8def\u5f84\u7b49\u76f8\u5173\u914d\u7f6e\uff0c\u5c06\u8fd9\u4e9b\u4fe1\u606f\u5b58\u653e\u5230\u5bf9\u5e94\u7684 YAML \u6587\u4ef6\u4e2d\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code># set training data path\nTRAIN_DATA_DIR: \"./data/NACA0012_interpolate/outputs_train\"\nTRAIN_MESH_GRAPH_PATH: \"./data/NACA0012_interpolate/mesh_fine.su2\"\n\n# set evaluate data path\nEVAL_DATA_DIR: \"./data/NACA0012_interpolate/outputs_test\"\nEVAL_MESH_GRAPH_PATH: \"./data/NACA0012_interpolate/mesh_fine.su2\"\n</code></pre> <p>\u63a5\u7740\u5b9a\u4e49\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>def train_mse_func(\n    output_dict: Dict[str, \"paddle.Tensor\"],\n    label_dict: Dict[str, \"pgl.Graph\"],\n    *args,\n) -&gt; paddle.Tensor:\n    return {\"pred\": F.mse_loss(output_dict[\"pred\"], label_dict[\"label\"].y)}\n</code></pre> <p>\u6700\u540e\u6784\u5efa\u76d1\u7763\u7ea6\u675f\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>train_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"MeshAirfoilDataset\",\n        \"input_keys\": (\"input\",),\n        \"label_keys\": (\"label\",),\n        \"data_dir\": cfg.TRAIN_DATA_DIR,\n        \"mesh_graph_path\": cfg.TRAIN_MESH_GRAPH_PATH,\n        \"transpose_edges\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": True,\n    },\n    \"num_workers\": 1,\n}\n\n# set constraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    output_expr={\"pred\": lambda out: out[\"pred\"]},\n    loss=ppsci.loss.FunctionalLoss(train_mse_func),\n    name=\"Sup\",\n)\n# wrap constraints together\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre>"},{"location":"zh/examples/cfdgcn/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u8bbe\u7f6e\u8bad\u7ec3\u8f6e\u6570\u7b49\u53c2\u6570\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>epochs: 500\niters_per_epoch: 42\nsave_freq: 50\neval_during_train: true\neval_freq: 50\nlearning_rate: 5.0e-4\nbatch_size: 4\n</code></pre>"},{"location":"zh/examples/cfdgcn/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u4f7f\u7528\u56fa\u5b9a\u7684 <code>5e-4</code> \u4f5c\u4e3a\u5b66\u4e60\u7387\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>"},{"location":"zh/examples/cfdgcn/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6(\u6d4b\u8bd5\u96c6)\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\uff0c\u6784\u5efa\u8fc7\u7a0b\u4e0e \u7ea6\u675f\u6784\u5efa \u7c7b\u4f3c\uff0c\u53ea\u9700\u628a\u6570\u636e\u76ee\u5f55\u6539\u4e3a\u6d4b\u8bd5\u96c6\u7684\u76ee\u5f55\uff0c\u5e76\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e <code>EVAL.batch_size=1</code> \u5373\u53ef\u3002</p> <pre><code>eval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"MeshAirfoilDataset\",\n        \"input_keys\": (\"input\",),\n        \"label_keys\": (\"label\",),\n        \"data_dir\": cfg.EVAL_DATA_DIR,\n        \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n        \"transpose_edges\": True,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n}\nrmse_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    loss=ppsci.loss.FunctionalLoss(train_mse_func),\n    output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n    metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n    name=\"RMSE_validator\",\n)\nvalidator = {rmse_validator.name: rmse_validator}\n</code></pre> <p>\u8bc4\u4f30\u6307\u6807\u4e3a\u9884\u6d4b\u7ed3\u679c\u548c\u771f\u5b9e\u7ed3\u679c\u7684 RMSE \u503c\uff0c\u56e0\u6b64\u9700\u81ea\u5b9a\u4e49\u6307\u6807\u8ba1\u7b97\u51fd\u6570\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>def eval_rmse_func(\n    output_dict: Dict[str, List[\"paddle.Tensor\"]],\n    label_dict: Dict[str, List[\"pgl.Graph\"]],\n    *args,\n) -&gt; Dict[str, paddle.Tensor]:\n    mse_losses = [\n        F.mse_loss(pred, label.y)\n        for (pred, label) in zip(output_dict[\"pred\"], label_dict[\"label\"])\n    ]\n    return {\"RMSE\": (sum(mse_losses) / len(mse_losses)) ** 0.5}\n</code></pre> <p>\u8bc4\u4f30\u6307\u6807\u4e3a\u9884\u6d4b\u7ed3\u679c\u548c\u771f\u5b9e\u7ed3\u679c\u7684 RMSE \u503c\uff0c\u56e0\u6b64\u9700\u81ea\u5b9a\u4e49\u6307\u6807\u8ba1\u7b97\u51fd\u6570\uff0c\u5982\u4e0b\u6240\u793a\u3002</p>"},{"location":"zh/examples/cfdgcn/#38","title":"3.8 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    validator=validator,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n)\n</code></pre>"},{"location":"zh/examples/cfdgcn/#39","title":"3.9 \u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\u7a0b\u5e8f\u4f1a\u5bf9\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u4ee5\u56fe\u7247\u7684\u5f62\u5f0f\u5bf9\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code># visualize prediction\nwith solver.no_grad_context_manager(True):\n    for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n        truefield = label[\"label\"].y\n        prefield = model(input_)\n        utils.log_images(\n            input_[\"input\"].pos,\n            prefield[\"pred\"],\n            truefield,\n            rmse_validator.data_loader.dataset.elems_list,\n            index,\n            \"cylinder\",\n        )\n</code></pre>"},{"location":"zh/examples/cfdgcn/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"cfdgcn.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom typing import Dict\nfrom typing import List\n\nimport hydra\nimport paddle\nimport pgl\nimport su2paddle\nimport utils\nfrom omegaconf import DictConfig\nfrom paddle.nn import functional as F\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train_mse_func(\n    output_dict: Dict[str, \"paddle.Tensor\"],\n    label_dict: Dict[str, \"pgl.Graph\"],\n    *args,\n) -&gt; paddle.Tensor:\n    return {\"pred\": F.mse_loss(output_dict[\"pred\"], label_dict[\"label\"].y)}\n\n\ndef eval_rmse_func(\n    output_dict: Dict[str, List[\"paddle.Tensor\"]],\n    label_dict: Dict[str, List[\"pgl.Graph\"]],\n    *args,\n) -&gt; Dict[str, paddle.Tensor]:\n    mse_losses = [\n        F.mse_loss(pred, label.y)\n        for (pred, label) in zip(output_dict[\"pred\"], label_dict[\"label\"])\n    ]\n    return {\"RMSE\": (sum(mse_losses) / len(mse_losses)) ** 0.5}\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", os.path.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshAirfoilDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.TRAIN_DATA_DIR,\n            \"mesh_graph_path\": cfg.TRAIN_MESH_GRAPH_PATH,\n            \"transpose_edges\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n        \"num_workers\": 1,\n    }\n\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        output_expr={\"pred\": lambda out: out[\"pred\"]},\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        name=\"Sup\",\n    )\n    # wrap constraints together\n    constraint = {sup_constraint.name: sup_constraint}\n    process_sim = sup_constraint.data_loader.dataset._preprocess\n    fine_marker_dict = sup_constraint.data_loader.dataset.marker_dict\n\n    # set model\n    model = ppsci.arch.CFDGCN(\n        **cfg.MODEL,\n        process_sim=process_sim,\n        fine_marker_dict=fine_marker_dict,\n        su2_module=su2paddle.SU2Module,\n    )\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshAirfoilDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.EVAL_DATA_DIR,\n            \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n            \"transpose_edges\": True,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    rmse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n        metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n        name=\"RMSE_validator\",\n    )\n    validator = {rmse_validator.name: rmse_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        validator=validator,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n    )\n\n    # train model\n    solver.train()\n\n    # visualize prediction\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"cylinder\",\n            )\n\n\ndef evaluate(cfg: DictConfig):\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshAirfoilDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.TRAIN_DATA_DIR,\n            \"mesh_graph_path\": cfg.TRAIN_MESH_GRAPH_PATH,\n            \"transpose_edges\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n        \"num_workers\": 1,\n    }\n\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        output_expr={\"pred\": lambda out: out[\"pred\"]},\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        name=\"Sup\",\n    )\n\n    process_sim = sup_constraint.data_loader.dataset._preprocess\n    fine_marker_dict = sup_constraint.data_loader.dataset.marker_dict\n\n    # set airfoil model\n    model = ppsci.arch.CFDGCN(\n        **cfg.MODEL,\n        process_sim=process_sim,\n        fine_marker_dict=fine_marker_dict,\n        su2_module=su2paddle.SU2Module,\n    )\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"MeshAirfoilDataset\",\n            \"input_keys\": (\"input\",),\n            \"label_keys\": (\"label\",),\n            \"data_dir\": cfg.EVAL_DATA_DIR,\n            \"mesh_graph_path\": cfg.EVAL_MESH_GRAPH_PATH,\n            \"transpose_edges\": True,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    rmse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(train_mse_func),\n        output_expr={\"pred\": lambda out: out[\"pred\"].unsqueeze(0)},\n        metric={\"RMSE\": ppsci.metric.FunctionalMetric(eval_rmse_func)},\n        name=\"RMSE_validator\",\n    )\n    validator = {rmse_validator.name: rmse_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n\n    # evaluate model\n    solver.eval()\n\n    # visualize prediction\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(rmse_validator.data_loader):\n            truefield = label[\"label\"].y\n            prefield = model(input_)\n            utils.log_images(\n                input_[\"input\"].pos,\n                prefield[\"pred\"],\n                truefield,\n                rmse_validator.data_loader.dataset.elems_list,\n                index,\n                \"cylinder\",\n            )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"cfdgcn.yaml\")\ndef main(cfg: DictConfig):\n    su2paddle.activate_su2_mpi(remove_temp_files=True)\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/cfdgcn/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u65b9\u5c55\u793a\u4e86\u6a21\u578b\u5bf9\u8ba1\u7b97\u57df\u4e2d\u6bcf\u4e2a\u70b9\u7684\u538b\u529b\\(p(x,y)\\)\u3001x(\u6c34\u5e73)\u65b9\u5411\u6d41\u901f\\(u(x,y)\\)\u3001y(\u5782\u76f4)\u65b9\u5411\u6d41\u901f\\(v(x,y)\\)\u7684\u9884\u6d4b\u7ed3\u679c\u4e0e\u53c2\u8003\u7ed3\u679c\u3002</p> \u9884\u6d4b\u5b9e\u9a8c\u6cdb\u5316\u5b9e\u9a8c <p> \u5de6\uff1a\u9884\u6d4b x \u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1a\u5b9e\u9645 x \u65b9\u5411\u6d41\u901f \u5de6\uff1a\u9884\u6d4b\u538b\u529b p\uff0c\u53f3\uff1a\u5b9e\u9645\u538b\u529b p \u5de6\uff1a\u9884\u6d4by\u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1a\u5b9e\u9645 y \u65b9\u5411\u6d41\u901f </p> <p> \u5de6\uff1a\u9884\u6d4b x \u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1a\u5b9e\u9645 x \u65b9\u5411\u6d41\u901f \u5de6\uff1a\u9884\u6d4b\u538b\u529b p\uff0c\u53f3\uff1a\u5b9e\u9645\u538b\u529b p \u5de6\uff1a\u9884\u6d4by\u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1a\u5b9e\u9645 y \u65b9\u5411\u6d41\u901f </p> <p>\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\uff0c\u6a21\u578b\u6cdb\u5316\u6548\u679c\u826f\u597d\u3002</p>"},{"location":"zh/examples/cfdgcn/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li>Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction</li> <li>locuslab/cfd-gcnCFDGCN</li> <li>CFDGCN - AIStudio</li> </ul>"},{"location":"zh/examples/chip_heat/","title":"Chip_heat","text":""},{"location":"zh/examples/chip_heat/#chip-heat-simulation","title":"Chip Heat Simulation","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <pre><code>python chip_heat.py\n</code></pre> <pre><code>python chip_heat.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/ChipHeat/chip_heat_pretrained.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 chip_heat_pretrained.pdparams MSE.chip(down_mse): 0.04177MSE.chip(left_mse): 0.01783MSE.chip(right_mse): 0.03767MSE.chip(top_mse): 0.05034"},{"location":"zh/examples/chip_heat/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u82af\u7247\u70ed\u4eff\u771f\u7814\u7a76\u4e3b\u8981\u805a\u7126\u4e8e\u9884\u6d4b\u548c\u5206\u6790\u96c6\u6210\u7535\u8def\uff08IC\uff09\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u6e29\u5ea6\u5206\u5e03\uff0c\u4ee5\u53ca\u70ed\u6548\u5e94\u5bf9\u82af\u7247\u6027\u80fd\u3001\u529f\u8017\u3001\u53ef\u9760\u6027\u548c\u5bff\u547d\u7684\u5f71\u54cd\u3002\u968f\u7740\u7535\u5b50\u8bbe\u5907\u5411\u66f4\u9ad8\u6027\u80fd\u3001\u66f4\u9ad8\u5bc6\u5ea6\u548c\u66f4\u5c0f\u5c3a\u5bf8\u53d1\u5c55\uff0c\u70ed\u7ba1\u7406\u6210\u4e3a\u82af\u7247\u8bbe\u8ba1\u548c\u5236\u9020\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002</p> <p>\u82af\u7247\u70ed\u4eff\u771f\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u89e3\u51b3\u82af\u7247\u70ed\u7ba1\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u5bf9\u4e8e\u63d0\u9ad8\u82af\u7247\u7684\u6027\u80fd\u3001\u964d\u4f4e\u529f\u8017\u3001\u4fdd\u8bc1\u53ef\u9760\u6027\u548c\u5ef6\u957f\u5bff\u547d\u6709\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u968f\u7740\u7535\u5b50\u8bbe\u5907\u671d\u7740\u66f4\u9ad8\u6027\u80fd\u548c\u66f4\u7d27\u51d1\u7684\u65b9\u5411\u53d1\u5c55\uff0c\u70ed\u4eff\u771f\u7814\u7a76\u7684\u91cd\u8981\u6027\u5c06\u4f1a\u8fdb\u4e00\u6b65\u589e\u52a0\u3002</p> <p>\u82af\u7247\u70ed\u4eff\u771f\u5728\u5de5\u7a0b\u548c\u79d1\u5b66\u9886\u57df\u5177\u6709\u591a\u65b9\u9762\u7684\u91cd\u8981\u6027\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a</p> <ul> <li>\u8bbe\u8ba1\u4f18\u5316\u548c\u9a8c\u8bc1\uff1a \u82af\u7247\u70ed\u4eff\u771f\u53ef\u4ee5\u5e2e\u52a9\u5de5\u7a0b\u5e08\u548c\u79d1\u5b66\u5bb6\u5728\u8bbe\u8ba1\u521d\u671f\u8bc4\u4f30\u4e0d\u540c\u7ed3\u6784\u548c\u6750\u6599\u7684\u70ed\u7279\u6027\uff0c\u4ee5\u4f18\u5316\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u3002\u901a\u8fc7\u4eff\u771f\u6a21\u62df\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u6e29\u5ea6\u5206\u5e03\u548c\u70ed\u4f20\u5bfc\u6548\u5e94\uff0c\u53ef\u4ee5\u63d0\u524d\u53d1\u73b0\u6f5c\u5728\u7684\u70ed\u95ee\u9898\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u7684\u6539\u8fdb\uff0c\u4ece\u800c\u964d\u4f4e\u540e\u671f\u5f00\u53d1\u6210\u672c\u548c\u98ce\u9669\u3002</li> <li>\u70ed\u7ba1\u7406\u548c\u6563\u70ed\u8bbe\u8ba1\uff1a \u82af\u7247\u70ed\u4eff\u771f\u53ef\u4ee5\u5e2e\u52a9\u8bbe\u8ba1\u6709\u6548\u7684\u70ed\u7ba1\u7406\u7cfb\u7edf\u548c\u6563\u70ed\u65b9\u6848\uff0c\u4ee5\u786e\u4fdd\u82af\u7247\u5728\u957f\u65f6\u95f4\u9ad8\u8d1f\u8f7d\u8fd0\u884c\u65f6\u4fdd\u6301\u5728\u5b89\u5168\u7684\u5de5\u4f5c\u6e29\u5ea6\u8303\u56f4\u5185\u3002\u901a\u8fc7\u5206\u6790\u82af\u7247\u5468\u56f4\u7684\u6563\u70ed\u7ed3\u6784\u3001\u98ce\u6247\u914d\u7f6e\u3001\u6563\u70ed\u7247\u8bbe\u8ba1\u7b49\u56e0\u7d20\uff0c\u53ef\u4ee5\u4f18\u5316\u70ed\u4f20\u5bfc\u548c\u6563\u70ed\u6548\u7387\uff0c\u63d0\u9ad8\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002</li> <li>\u6027\u80fd\u9884\u6d4b\u548c\u4f18\u5316\uff1a \u6e29\u5ea6\u5bf9\u82af\u7247\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u6709\u91cd\u8981\u5f71\u54cd\u3002\u82af\u7247\u70ed\u4eff\u771f\u53ef\u4ee5\u5e2e\u52a9\u9884\u6d4b\u82af\u7247\u5728\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5305\u62ec\u5904\u7406\u5668\u901f\u5ea6\u3001\u529f\u8017\u548c\u7535\u5b50\u5668\u4ef6\u7684\u5bff\u547d\u7b49\u65b9\u9762\u3002\u901a\u8fc7\u5bf9\u70ed\u6548\u5e94\u7684\u5efa\u6a21\u548c\u5206\u6790\uff0c\u53ef\u4ee5\u4f18\u5316\u82af\u7247\u7684\u8bbe\u8ba1\u548c\u5de5\u4f5c\u6761\u4ef6\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002</li> <li>\u8282\u80fd\u548c\u73af\u4fdd\uff1a \u6709\u6548\u7684\u70ed\u7ba1\u7406\u548c\u6563\u70ed\u8bbe\u8ba1\u53ef\u4ee5\u964d\u4f4e\u7cfb\u7edf\u80fd\u8017\uff0c\u63d0\u9ad8\u80fd\u6e90\u5229\u7528\u6548\u7387\uff0c\u4ece\u800c\u5b9e\u73b0\u8282\u80fd\u548c\u73af\u4fdd\u7684\u76ee\u6807\u3002\u901a\u8fc7\u51cf\u5c11\u7cfb\u7edf\u4e2d\u70ed\u91cf\u7684\u635f\u5931\u548c\u6d6a\u8d39\uff0c\u53ef\u4ee5\u964d\u4f4e\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\uff0c\u51cf\u5c11\u5bf9\u73af\u5883\u7684\u8d1f\u9762\u5f71\u54cd\u3002</li> </ul> <p>\u7efc\u4e0a\u6240\u8ff0\uff0c\u82af\u7247\u70ed\u4eff\u771f\u5728\u5de5\u7a0b\u548c\u79d1\u5b66\u9886\u57df\u4e2d\u5177\u6709\u91cd\u8981\u7684\u4f5c\u7528\u548c\u4ef7\u503c\uff0c\u53ef\u4ee5\u5e2e\u52a9\u4f18\u5316\u8bbe\u8ba1\u3001\u63d0\u9ad8\u6027\u80fd\u3001\u964d\u4f4e\u6210\u672c\u3001\u4fdd\u62a4\u73af\u5883\u7b49\u65b9\u9762\u53d6\u5f97\u79ef\u6781\u7684\u6548\u679c\u3002</p>"},{"location":"zh/examples/chip_heat/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":""},{"location":"zh/examples/chip_heat/#21","title":"2.1 \u95ee\u9898\u63cf\u8ff0","text":"<p>\u4e3a\u4e86\u642d\u5efa\u901a\u7528\u7684\u70ed\u4eff\u771f\u6a21\u578b\uff0c\u6211\u4eec\u9996\u5148\u5bf9\u4e00\u822c\u60c5\u51b5\u4e0b\u70ed\u4eff\u771f\u95ee\u9898\u8fdb\u884c\u7b80\u8981\u63cf\u8ff0\uff0c\u70ed\u4eff\u771f\u65e8\u5728\u901a\u8fc7\u5168\u5c40\u6c42\u89e3\u70ed\u4f20\u5bfc\u65b9\u7a0b\u6765\u9884\u6d4b\u7ed9\u5b9a\u7269\u4f53\u7684\u6e29\u5ea6\u573a\uff0c\u901a\u5e38\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u63a7\u5236\u65b9\u7a0b\u6765\u8fdb\u884c\u8868\u793a\uff1a</p> \\[ k \\Delta T(x,t) + S(x,t) = \\rho c_p \\dfrac{\\partial T(x,t)}{\\partial t},\\quad \\text { in } \\Omega\\times (0,t_{*}), \\] <p>\u5176\u4e2d \\(\\Omega\\subset \\mathbb{R}^{n},~n=1,2,3\\) \u4e3a\u7ed9\u5b9a\u7269\u4f53\u6750\u6599\u7684\u6a21\u62df\u533a\u57df\uff0c\u5982\u56fe\u6240\u793a\u4e3a\u4e00\u4e2a\u5177\u6709\u968f\u673a\u70ed\u6e90\u5206\u5e03\u76842D\u82af\u7247\u6a21\u62df\u533a\u57df\u3002\\(T(x,t),~S(x,t)\\) \u5206\u522b\u8868\u793a\u5728\u4efb\u610f\u65f6\u7a7a\u4f4d\u7f6e \\((x,t)\\) \u5904\u6e29\u5ea6\u548c\u70ed\u6e90\u5206\u5e03\uff0c\\(t_*\\) \u4e3a\u6e29\u5ea6\u9608\u503c\u3002\u8fd9\u91cc \\(k\\)\u3001\\(\\rho\\)\u3001\\(c_p\\) \u5747\u4e3a\u7ed9\u5b9a\u7269\u4f53\u7684\u6750\u6599\u7279\u6027\uff0c\u5206\u522b\u8868\u793a\u6750\u6599\u4f20\u70ed\u7cfb\u6570\u3001\u8d28\u91cf\u5bc6\u5ea6\u548c\u6bd4\u70ed\u5bb9\u3002\u4e3a\u4e86\u65b9\u4fbf\uff0c\u6211\u4eec\u5173\u6ce8\u7ed9\u5b9a\u7269\u4f53\u6750\u6599\u7684\u9759\u6001\u6e29\u5ea6\u573a\uff0c\u5e76\u901a\u8fc7\u8bbe\u7f6e \\(\\frac{dT}{dt}=0\\) \u6765\u7b80\u5316\u65b9\u7a0b:</p> \\[ \\tag{1} k \\Delta T(x) + S(x) = 0,\\quad \\text { in } \\Omega. \\] <p> </p>  \u5185\u90e8\u5177\u6709\u968f\u673a\u70ed\u6e90\u5206\u5e03\u7684 2D \u82af\u7247\u6a21\u62df\u533a\u57df\uff0c\u8fb9\u754c\u4e0a\u53ef\u4ee5\u4e3a\u4efb\u610f\u7684\u8fb9\u754c\u6761\u4ef6\u3002 <p>\u5bf9\u4e8e\u7ed9\u5b9a\u7269\u4f53\u6750\u6599\u7684\u901a\u7528\u70ed\u4eff\u771f\u6a21\u578b\uff0c\u9664\u4e86\u8981\u6ee1\u8db3\u63a7\u5236\u65b9\u7a0b(1)\uff0c\u5176\u6e29\u5ea6\u573a\u8fd8\u53d6\u51b3\u4e8e\u4e00\u4e9b\u5173\u952e\u7684 PDE \u914d\u7f6e\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u6750\u6599\u7279\u6027\u548c\u51e0\u4f55\u53c2\u6570\u7b49\u3002</p> <p>\u7b2c\u4e00\u7c7b PDE \u914d\u7f6e\u662f\u7ed9\u5b9a\u7269\u4f53\u6750\u6599\u7684\u8fb9\u754c\u6761\u4ef6:</p> <ul> <li>Dirichlet\u8fb9\u754c\u6761\u4ef6: \u8868\u9762\u4e0a\u7684\u6e29\u5ea6\u573a\u56fa\u5b9a\u4e3a \\(q_d\\)\uff1a</li> </ul> \\[ T = q_d. \\] <ul> <li>Neumann\u8fb9\u754c\u6761\u4ef6: \u8868\u9762\u4e0a\u7684\u6e29\u5ea6\u901a\u91cf\u662f\u56fa\u5b9a\u4e3a \\(q_n\\)\uff0c\u5f53 \\(q_n =0\\) \u65f6\uff0c\u8868\u660e\u8868\u9762\u5b8c\u5168\u7edd\u7f18\uff0c\u79f0\u4e3a\u7edd\u70ed\u8fb9\u754c\u6761\u4ef6\u3002</li> </ul> \\[ \\tag{2} -k \\dfrac{\\partial T}{\\partial n} = q_n. \\] <ul> <li>\u5bf9\u6d41\u8fb9\u754c\u6761\u4ef6\uff1a\u4e5f\u79f0\u4e3a\u725b\u987f\u8fb9\u754c\u6761\u4ef6\uff0c\u8be5\u8fb9\u754c\u6761\u4ef6\u5bf9\u5e94\u4e8e\u8868\u9762\u76f8\u540c\u65b9\u5411\u4e0a\u7684\u70ed\u4f20\u5bfc\u548c\u5bf9\u6d41\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5176\u4e2d \\(h\\) \u548c \\(T_{amb}\\) \u4ee3\u8868\u8868\u9762\u7684\u5bf9\u6d41\u7cfb\u6570\u548c\u73af\u5883\u6e29\u5ea6\u3002</li> </ul> \\[ -k \\dfrac{\\partial T}{\\partial n} = h(T-T_{amb}). \\] <ul> <li>\u8f90\u5c04\u8fb9\u754c\u6761\u4ef6\uff1a\u8be5\u8fb9\u754c\u6761\u4ef6\u5bf9\u5e94\u4e8e\u8868\u9762\u4e0a\u7531\u6e29\u5dee\u4ea7\u751f\u7684\u7535\u78c1\u6ce2\u8f90\u5c04\uff0c\u5176\u4e2d \\(\\epsilon\\) \u548c \\(\\sigma\\) \u5206\u522b\u4ee3\u8868\u70ed\u8f90\u5c04\u7cfb\u6570\u548cStefan-Boltzmann\u7cfb\u6570\u3002</li> </ul> \\[ -k \\dfrac{\\partial T}{\\partial n} = \\epsilon \\sigma (T^4-T_{amb}^4). \\] <p>\u7b2c\u4e8c\u7c7bPDE\u914d\u7f6e\u662f\u7ed9\u5b9a\u7269\u4f53\u6750\u6599\u7684\u8fb9\u754c\u6216\u5185\u90e8\u70ed\u6e90\u7684\u4f4d\u7f6e\u548c\u5f3a\u5ea6\u3002\u672c\u5de5\u4f5c\u8003\u8651\u4e86\u4ee5\u4e0b\u4e24\u79cd\u7c7b\u578b\u7684\u70ed\u6e90\uff1a</p> <ul> <li>\u8fb9\u754c\u968f\u673a\u70ed\u6e90\uff1a\u7531 Neumann \u8fb9\u754c\u6761\u4ef6(2)\u5b9a\u4e49\uff0c\u6b64\u65f6 \\(q_n\\) \u4e3a\u5173\u4e8e \\(x\\) \u7684\u51fd\u6570\uff0c\u5373\u4efb\u610f\u7ed9\u5b9a\u7684\u6e29\u5ea6\u901a\u91cf\u5206\u5e03\uff1b</li> <li>\u5185\u90e8\u968f\u673a\u70ed\u6e90\uff1a\u7531\u63a7\u5236\u65b9\u7a0b(1)\u5b9a\u4e49\uff0c\u6b64\u65f6 \\(S(x)\\) \u4e3a\u5173\u4e8e \\(x\\) \u7684\u51fd\u6570\uff0c\u5373\u4efb\u610f\u7ed9\u5b9a\u7684\u70ed\u6e90\u5206\u5e03\u3002</li> </ul> <p>\u6211\u4eec\u7684\u76ee\u7684\u662f\uff0c\u5728\u7ed9\u5b9a\u7684\u7269\u4f53\u6750\u6599\u7684\u901a\u7528\u70ed\u4eff\u771f\u6a21\u578b\u4e0a\uff0c\u8f93\u5165\u4efb\u610f\u7684\u7b2c\u4e00\u7c7b\u6216\u7b2c\u4e8c\u7c7b\u8bbe\u8ba1\u914d\u7f6e\uff0c\u6211\u4eec\u5747\u53ef\u4ee5\u5f97\u5230\u5bf9\u5e94\u7684\u6e29\u5ea6\u573a\u5206\u5e03\u60c5\u51b5\uff0c\u5728\u8fb9\u754c\u4e0a\u6211\u4eec\u4efb\u610f\u6307\u5b9a\u8fb9\u754c\u7c7b\u578b\u548c\u53c2\u6570\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e2d\u5f00\u53d1\u7684\u901a\u7528\u70ed\u4eff\u771f\u7684 PI-DeepONet \u65b9\u6cd5\u5e76\u4e0d\u9650\u4e8e \u7b2c\u4e00\u7c7b\u6216\u7b2c\u4e8c\u7c7b\u8bbe\u8ba1\u914d\u7f6e \u6761\u4ef6\u548c\u89c4\u5219\u7684\u51e0\u4f55\u5f62\u72b6\u3002\u901a\u8fc7\u8d85\u51fa\u5f53\u524d\u5de5\u4f5c\u8303\u56f4\u7684\u8fdb\u4e00\u6b65\u4ee3\u7801\u4fee\u6539\uff0c\u5b83\u4eec\u53ef\u4ee5\u5e94\u7528\u4e8e\u5404\u79cd\u8f7d\u8377\u3001\u6750\u6599\u5c5e\u6027\uff0c\u751a\u81f3\u5404\u79cd\u4e0d\u89c4\u5219\u7684\u51e0\u4f55\u5f62\u72b6\u3002</p>"},{"location":"zh/examples/chip_heat/#22-pi-deeponet","title":"2.2 PI-DeepONet\u6a21\u578b","text":"<p>PI-DeepONet\u6a21\u578b\uff0c\u5c06 DeepONet \u548c PINN \u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u7269\u7406\u4fe1\u606f\u548c\u7b97\u5b50\u5b66\u4e60\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u8fd9\u79cd\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u65b9\u7a0b\u7684\u7269\u7406\u4fe1\u606f\u6765\u589e\u5f3a DeepONet \u6a21\u578b\uff0c\u540c\u65f6\u53ef\u4ee5\u5c06\u4e0d\u540c\u7684 PDE \u914d\u7f6e\u5206\u522b\u4f5c\u4e3a\u4e0d\u540c\u7684\u5206\u652f\u7f51\u7edc\u7684\u8f93\u5165\u6570\u636e\uff0c\u4ece\u800c\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u5728\u5404\u79cd\uff08\u53c2\u6570\u548c\u975e\u53c2\u6570\uff09PDE \u914d\u7f6e\u4e0b\u8fdb\u884c\u8d85\u5feb\u901f\u7684\u6a21\u578b\u9884\u6d4b\u3002</p> <p>\u5bf9\u4e8e\u82af\u7247\u70ed\u4eff\u771f\u95ee\u9898\uff0cPI-DeepONet \u6a21\u578b\u53ef\u4ee5\u8868\u793a\u4e3a\u5982\u56fe\u6240\u793a\u7684\u6a21\u578b\u7ed3\u6784\uff1a</p> <p></p> <p>\u5982\u56fe\u6240\u793a\uff0c\u6211\u4eec\u4e00\u5171\u4f7f\u7528\u4e86 3 \u4e2a\u5206\u652f\u7f51\u7edc\u548c\u4e00\u4e2a\u4e3b\u5e72\u7f51\u7edc\uff0c\u5206\u652f\u7f51\u7edc\u5206\u522b\u8f93\u5165\u8fb9\u754c\u7c7b\u578b\u6307\u6807\u3001\u968f\u673a\u70ed\u6e90\u5206\u5e03 \\(S(x, y)\\) \u548c\u8fb9\u754c\u51fd\u6570 \\(Q(x, y)\\)\uff0c\u4e3b\u5e72\u7f51\u7edc\u8f93\u5165\u4e8c\u7ef4\u5750\u6807\u70b9\u5750\u6807\u4fe1\u606f\u3002\u6bcf\u4e2a\u5206\u652f\u7f51\u548c\u4e3b\u5e72\u7f51\u5747\u8f93\u51fa \\(q\\) \u7ef4\u7279\u5f81\u5411\u91cf\uff0c\u901a\u8fc7 Hadamard\uff08\u9010\u5143\u7d20\uff09\u4e58\u79ef\u7ec4\u5408\u6240\u6709\u8fd9\u4e9b\u8f93\u51fa\u7279\u5f81\uff0c\u7136\u540e\u5c06\u6240\u5f97\u5411\u91cf\u76f8\u52a0\u4e3a\u9884\u6d4b\u6e29\u5ea6\u573a\u7684\u6807\u91cf\u8f93\u51fa\u3002</p>"},{"location":"zh/examples/chip_heat/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u8be5\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u6362\u70ed\u5668\u70ed\u4eff\u771f\u95ee\u9898\u3002\u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u7ea6\u675f\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003API\u6587\u6863\u3002</p>"},{"location":"zh/examples/chip_heat/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u82af\u7247\u70ed\u4eff\u771f\u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y)\\) \u548c\u6bcf\u4e00\u7ec4\u8fb9\u754c\u7c7b\u578b \\(bt\\)\u3001\u968f\u673a\u70ed\u6e90\u5206\u5e03 \\(S(x, y)\\) \u4ee5\u53ca\u8fb9\u754c\u51fd\u6570 \\(Q(x, y)\\) \u90fd\u5bf9\u5e94\u4e00\u7ec4\u82af\u7247\u7684\u6e29\u5ea6\u5206\u5e03 \\(T\\)\uff0c\u4e00\u4e2a\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\u3002\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528 3 \u4e2a\u5206\u652f\u7f51\u7edc\u548c\u4e00\u4e2a\u4e3b\u5e72\u7f51\u7edc\uff0c4 \u4e2a\u7f51\u7edc\u5747\u4e3a MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u3002 3 \u4e2a\u5206\u652f\u7f51\u7edc\u5206\u522b\u8868\u793a \\((bt, S, Q)\\) \u5230\u8f93\u51fa\u51fd\u6570 \\((b_1, b_2, b_3)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f_1,f_2,f_3: \\mathbb{R}^3 \\to \\mathbb{R}^{q}\\)\uff0c\u5373\uff1a</p> \\[ \\begin{aligned} b_1 &amp;= f_1(bt),\\\\ b_2 &amp;= f_2(S),\\\\ b_3 &amp;= f_3(Q). \\end{aligned} \\] <p>\u4e0a\u5f0f\u4e2d \\(f_1, f_2, f_3\\) \u5747\u4e3a MLP \u6a21\u578b\uff0c\\((b_1,b_2,b_3)\\) \u5206\u522b\u4e3a\u4e09\u4e2a\u5206\u652f\u7f51\u7edc\u7684\u8f93\u51fa\u51fd\u6570\uff0c\\(q\\) \u4e3a\u8f93\u51fa\u51fd\u6570\u7684\u7ef4\u6570\u3002\u4e3b\u5e72\u7f51\u7edc\u8868\u793a \\((x, y)\\) \u5230\u8f93\u51fa\u51fd\u6570 \\(t_0\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f_4: \\mathbb{R} \\to \\mathbb{R}^{q}\\)\uff0c\u5373\uff1a</p> \\[ \\begin{aligned} t_0 &amp;= f_4(x, y). \\end{aligned} \\] <p>\u4e0a\u5f0f\u4e2d \\(f_4\\) \u4e3a MLP \u6a21\u578b\uff0c\\((t_0)\\) \u4e3a\u4e3b\u652f\u7f51\u7edc\u7684\u8f93\u51fa\u51fd\u6570\uff0c\\(q\\) \u4e3a\u8f93\u51fa\u51fd\u6570\u7684\u7ef4\u6570\u3002\u6211\u4eec\u53ef\u4ee5\u5c06\u4e09\u4e2a\u5206\u652f\u7f51\u7edc\u548c\u4e3b\u5e72\u7f51\u7edc\u7684\u8f93\u51fa\u51fd\u6570 \\((b_1, b_2, b_3, t_0)\\) \u8fdb\u884c Hadamard\uff08\u9010\u5143\u7d20\uff09\u4e58\u79ef\u518d\u76f8\u52a0\u5f97\u5230\u6807\u91cf\u6e29\u5ea6\u573a\uff0c\u5373\uff1a</p> \\[ T = \\sum_{i=1}^q b_1^ib_2^ib_3^it_0^i. \\] <p>\u6211\u4eec\u5b9a\u4e49 PaddleScience \u5185\u7f6e\u7684 ChipHeats \u6a21\u578b\u7c7b\uff0c\u5e76\u8c03\u7528\uff0cPaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.ChipDeepONets(**cfg.MODEL)\n</code></pre> <p>\u8fd9\u6837\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 4 \u4e2a MLP \u6a21\u578b\u7684 ChipHeats \u6a21\u578b\uff0c\u6bcf\u4e2a\u5206\u652f\u7f51\u7edc\u5305\u542b 9 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 256\uff0c\u4e3b\u5e72\u7f51\u7edc\u5305\u542b 6 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 128\uff0c\u4f7f\u7528 \"Swish\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u8f93\u51fa\u51fd\u6570 \\(T\\) \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002\u66f4\u591a\u76f8\u5173\u5185\u5bb9\u8bf7\u53c2\u8003\u6587\u732e A fast general thermal simulation model based on MultiBranch Physics-Informed deep operator neural network\u3002</p>"},{"location":"zh/examples/chip_heat/#32","title":"3.2 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u5bf9\u672c\u6587\u4e2d\u82af\u7247\u70ed\u4eff\u771f\u95ee\u9898\u6784\u9020\u8bad\u7ec3\u533a\u57df\uff0c\u5373\u4ee5 \\([0, 1]\\times[0, 1]\\) \u7684\u4e8c\u7ef4\u533a\u57df\uff0c\u8be5\u533a\u57df\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u7a7a\u95f4\u51e0\u4f55 <code>Rectangle</code>\u6765\u6784\u9020\u8ba1\u7b97\u57df\u3002\u4ee3\u7801\u5982\u4e0b</p> <pre><code># set geometry\nNPOINT = cfg.NL * cfg.NW\ngeom = {\"rect\": ppsci.geometry.Rectangle((0, 0), (cfg.DL, cfg.DW))}\n</code></pre> \u63d0\u793a <p><code>Rectangle</code> \u548c <code>TimeDomain</code> \u662f\u4e24\u79cd\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\u7684 <code>Geometry</code> \u6d3e\u751f\u7c7b\u3002</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e8e\u4e8c\u7ef4\u77e9\u5f62\u51e0\u4f55\u57df\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.Rectangle(...)</code> \u521b\u5efa\u7a7a\u95f4\u51e0\u4f55\u57df\u5bf9\u8c61\uff1b</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e00\u7ef4\u65f6\u95f4\u57df\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.TimeDomain(...)</code> \u6784\u5efa\u65f6\u95f4\u57df\u5bf9\u8c61\u3002</p>"},{"location":"zh/examples/chip_heat/#33","title":"3.3 \u8f93\u5165\u6570\u636e\u6784\u5efa","text":"<p>\u4f7f\u7528\u4e8c\u7ef4\u76f8\u5173\u4e14\u5c3a\u5ea6\u4e0d\u53d8\u7684\u9ad8\u65af\u968f\u673a\u573a\u6765\u751f\u6210\u968f\u673a\u70ed\u6e90\u5206\u5e03 \\(S(x)\\) \u548c\u8fb9\u754c\u51fd\u6570 \\(Q(x)\\)\u3002\u6211\u4eec\u53c2\u8003 gaussian-random-fields \u4e2d\u63cf\u8ff0\u7684Python\u5b9e\u73b0\uff0c\u5176\u4e2d\u76f8\u5173\u6027\u7531\u65e0\u6807\u5ea6\u8c31\u6765\u89e3\u91ca\uff0c\u5373</p> \\[ P(k) \\sim \\dfrac{1}{|k|^{\\alpha/2}}. \\] <p>\u91c7\u6837\u51fd\u6570\u7684\u5e73\u6ed1\u5ea6\u7531\u957f\u5ea6\u5c3a\u5ea6\u7cfb\u6570 \\(\\alpha\\) \u51b3\u5b9a\uff0c\\(\\alpha\\) \u503c\u8d8a\u5927\uff0c\u5f97\u5230\u7684\u968f\u673a\u70ed\u6e90\u5206\u5e03 \\(S(x)\\) \u548c\u8fb9\u754c\u51fd\u6570 \\(Q(x)\\) \u8d8a\u5e73\u6ed1\u3002\u5728\u672c\u6587\u6211\u4eec\u91c7\u7528 \\(\\alpha = 4\\)\u3002\u8fd8\u53ef\u4ee5\u8c03\u6574\u8be5\u53c2\u6570\u4ee5\u751f\u6210\u7c7b\u4f3c\u4e8e\u7279\u5b9a\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u70ed\u6e90\u5206\u5e03 \\(S(x)\\) \u548c\u8fb9\u754c\u51fd\u6570 \\(Q(x)\\)\u3002</p> <p>\u901a\u8fc7\u9ad8\u65af\u968f\u673a\u573a\u6765\u751f\u6210\u968f\u673a\u70ed\u6e90\u5206\u5e03 \\(S(x)\\) \u548c\u8fb9\u754c\u51fd\u6570 \\(Q(x)\\)\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u8f93\u5165\u6570\u636e\u3002\u4ee3\u7801\u5982\u4e0b</p> <pre><code># generate training data and validation data\ndata_u = np.ones([1, (cfg.NL - 2) * (cfg.NW - 2)])\ndata_BC = np.ones([1, NPOINT])\ndata_u = np.vstack((data_u, np.zeros([1, (cfg.NL - 2) * (cfg.NW - 2)])))\ndata_BC = np.vstack((data_BC, np.zeros([1, NPOINT])))\nfor i in range(cfg.NU - 2):\n    data_u = np.vstack((data_u, GRF(alpha=cfg.GRF.alpha, size=cfg.NL - 2)))\nfor i in range(cfg.NBC - 2):\n    data_BC = np.vstack((data_BC, GRF(alpha=cfg.GRF.alpha, size=cfg.NL)))\ndata_u = data_u.astype(\"float32\")\ndata_BC = data_BC.astype(\"float32\")\ntest_u = GRF(alpha=4, size=cfg.NL).astype(\"float32\")[0]\n</code></pre> <p>\u7136\u540e\u5bf9\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\u6309\u7167\u7a7a\u95f4\u5750\u6807\u8fdb\u884c\u5206\u7c7b\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u7c7b\u6210\u5de6\u8fb9\u3001\u53f3\u8fb9\u3001\u4e0a\u8fb9\u3001\u4e0b\u8fb9\u4ee5\u53ca\u5185\u90e8\u6570\u636e\u3002\u4ee3\u7801\u5982\u4e0b</p> <pre><code>boundary_indices = np.where(\n    (\n        (points[\"x\"] == 0)\n        | (points[\"x\"] == cfg.DW)\n        | (points[\"y\"] == 0)\n        | (points[\"y\"] == cfg.DL)\n    )\n)\ninterior_indices = np.where(\n    (\n        (points[\"x\"] != 0)\n        &amp; (points[\"x\"] != cfg.DW)\n        &amp; (points[\"y\"] != 0)\n        &amp; (points[\"y\"] != cfg.DL)\n    )\n)\n\npoints[\"u\"] = np.tile(test_u[interior_indices[0]], (NPOINT, 1))\npoints[\"u_one\"] = test_u.T.reshape([-1, 1])\npoints[\"bc_data\"] = np.tile(test_u[boundary_indices[0]], (NPOINT, 1))\npoints[\"bc\"] = np.zeros((NPOINT, 1), dtype=\"float32\")\n\ntop_indices = np.where(points[\"x\"] == cfg.DW)\ndown_indices = np.where(points[\"x\"] == 0)\nleft_indices = np.where(\n    (points[\"y\"] == 0) &amp; (points[\"x\"] != 0) &amp; (points[\"x\"] != cfg.DW)\n)\nright_indices = np.where(\n    ((points[\"y\"] == cfg.DL) &amp; (points[\"x\"] != 0) &amp; (points[\"x\"] != cfg.DW))\n)\n\n# generate validation data\n(\n    test_top_data,\n    test_down_data,\n    test_left_data,\n    test_right_data,\n    test_interior_data,\n) = [\n    {\n        \"x\": points[\"x\"][indices_[0]],\n        \"y\": points[\"y\"][indices_[0]],\n        \"u\": points[\"u\"][indices_[0]],\n        \"u_one\": points[\"u_one\"][indices_[0]],\n        \"bc\": points[\"bc\"][indices_[0]],\n        \"bc_data\": points[\"bc_data\"][indices_[0]],\n    }\n    for indices_ in (\n        top_indices,\n        down_indices,\n        left_indices,\n        right_indices,\n        interior_indices,\n    )\n]\n# generate train data\ntop_data = {\n    \"x\": test_top_data[\"x\"],\n    \"y\": test_top_data[\"y\"],\n    \"u\": data_u,\n    \"u_one\": data_BC[:, top_indices[0]].T.reshape([-1, 1]),\n    \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n    \"bc_data\": data_BC[:, boundary_indices[0]],\n}\ndown_data = {\n    \"x\": test_down_data[\"x\"],\n    \"y\": test_down_data[\"y\"],\n    \"u\": data_u,\n    \"u_one\": data_BC[:, down_indices[0]].T.reshape([-1, 1]),\n    \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n    \"bc_data\": data_BC[:, boundary_indices[0]],\n}\nleft_data = {\n    \"x\": test_left_data[\"x\"],\n    \"y\": test_left_data[\"y\"],\n    \"u\": data_u,\n    \"u_one\": data_BC[:, left_indices[0]].T.reshape([-1, 1]),\n    \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n    \"bc_data\": data_BC[:, boundary_indices[0]],\n}\nright_data = {\n    \"x\": test_right_data[\"x\"],\n    \"y\": test_right_data[\"y\"],\n    \"u\": data_u,\n    \"u_one\": data_BC[:, right_indices[0]].T.reshape([-1, 1]),\n    \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n    \"bc_data\": data_BC[:, boundary_indices[0]],\n}\ninterior_data = {\n    \"x\": test_interior_data[\"x\"],\n    \"y\": test_interior_data[\"y\"],\n    \"u\": data_u,\n    \"u_one\": data_u.T.reshape([-1, 1]),\n    \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n    \"bc_data\": data_BC[:, boundary_indices[0]],\n}\n</code></pre>"},{"location":"zh/examples/chip_heat/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u5728\u6784\u5efa\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u5148\u4ecb\u7ecd\u4e00\u4e0b<code>ChipHeatDataset</code>\uff0c\u5b83\u7ee7\u627f\u81ea <code>Dataset</code> \u7c7b\uff0c\u53ef\u4ee5\u8fed\u4ee3\u7684\u8bfb\u53d6\u7531\u4e0d\u540c <code>numpy.ndarray</code> \u7ec4\u6210\u7684\u6570\u7ec4\u6570\u636e\u96c6\u3002\u7531\u4e8e\u6240\u7528\u7684\u6a21\u578b\u5206\u652f\u7f51\u6570\u76ee\u8f83\u591a\uff0c\u6240\u7528\u7684\u6570\u636e\u91cf\u8f83\u5927\u3002\u82e5\u5148\u5bf9\u6570\u636e\u8fdb\u884c\u7ec4\u5408\uff0c\u5c06\u5bfc\u81f4\u8f93\u5165\u6570\u636e\u5360\u7528\u7684\u5185\u5b58\u5f88\u5927\uff0c\u56e0\u6b64\u91c7\u7528 <code>ChipHeatDataset</code> \u8fed\u4ee3\u8bfb\u53d6\u6570\u636e\u3002</p> <p>\u82af\u7247\u70ed\u4eff\u771f\u95ee\u9898\u7531 2.1 \u95ee\u9898\u63cf\u8ff0 \u4e2d\u63cf\u8ff0\u7684\u65b9\u7a0b\u7ec4\u6210\uff0c\u6b64\u65f6\u6211\u4eec\u5bf9\u5de6\u8fb9\u3001\u53f3\u8fb9\u3001\u4e0a\u8fb9\u3001\u4e0b\u8fb9\u4ee5\u53ca\u5185\u90e8\u6570\u636e\u5206\u522b\u8bbe\u7f6e\u4e94\u4e2a\u7ea6\u675f\u6761\u4ef6\uff0c\u63a5\u4e0b\u6765\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u4e0a\u8ff0\u56db\u79cd\u7ea6\u675f\u6761\u4ef6\uff0c\u4ee3\u7801\u5982\u4e0b</p> <pre><code># set constraint\nindex = (\"x\", \"u\", \"bc\", \"bc_data\")\nlabel = {\"chip\": np.array([0], dtype=\"float32\")}\nweight = {\"chip\": np.array([cfg.TRAIN.weight], dtype=\"float32\")}\ntop_sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"ChipHeatDataset\",\n            \"input\": top_data,\n            \"label\": label,\n            \"index\": index,\n            \"data_type\": \"bc_data\",\n            \"weight\": weight,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\n        \"chip\": lambda out: paddle.where(\n            out[\"bc\"] == 1,\n            jacobian(out[\"T\"], out[\"x\"]) - out[\"u_one\"],\n            paddle.where(\n                out[\"bc\"] == 0,\n                out[\"T\"] - out[\"u_one\"],\n                paddle.where(\n                    out[\"bc\"] == 2,\n                    jacobian(out[\"T\"], out[\"x\"]) + out[\"u_one\"] * (out[\"T\"] - 1),\n                    jacobian(out[\"T\"], out[\"x\"])\n                    + out[\"u_one\"]\n                    * (out[\"T\"] ** 2 - 1)\n                    * (out[\"T\"] ** 2 + 1)\n                    * 5.6\n                    / 50000,\n                ),\n            ),\n        )\n    },\n    name=\"top_sup\",\n)\ndown_sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"ChipHeatDataset\",\n            \"input\": down_data,\n            \"label\": label,\n            \"index\": index,\n            \"data_type\": \"bc_data\",\n            \"weight\": weight,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\n        \"chip\": lambda out: paddle.where(\n            out[\"bc\"] == 1,\n            jacobian(out[\"T\"], out[\"x\"]) - out[\"u_one\"],\n            paddle.where(\n                out[\"bc\"] == 0,\n                out[\"T\"] - out[\"u_one\"],\n                paddle.where(\n                    out[\"bc\"] == 2,\n                    jacobian(out[\"T\"], out[\"x\"]) + out[\"u_one\"] * (out[\"T\"] - 1),\n                    jacobian(out[\"T\"], out[\"x\"])\n                    + out[\"u_one\"]\n                    * (out[\"T\"] ** 2 - 1)\n                    * (out[\"T\"] ** 2 + 1)\n                    * 5.6\n                    / 50000,\n                ),\n            ),\n        )\n    },\n    name=\"down_sup\",\n)\nleft_sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"ChipHeatDataset\",\n            \"input\": left_data,\n            \"label\": label,\n            \"index\": index,\n            \"data_type\": \"bc_data\",\n            \"weight\": weight,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\n        \"chip\": lambda out: paddle.where(\n            out[\"bc\"] == 1,\n            jacobian(out[\"T\"], out[\"y\"]) - out[\"u_one\"],\n            paddle.where(\n                out[\"bc\"] == 0,\n                out[\"T\"] - out[\"u_one\"],\n                paddle.where(\n                    out[\"bc\"] == 2,\n                    jacobian(out[\"T\"], out[\"y\"]) + out[\"u_one\"] * (out[\"T\"] - 1),\n                    jacobian(out[\"T\"], out[\"y\"])\n                    + out[\"u_one\"]\n                    * (out[\"T\"] ** 2 - 1)\n                    * (out[\"T\"] ** 2 + 1)\n                    * 5.6\n                    / 50000,\n                ),\n            ),\n        )\n    },\n    name=\"left_sup\",\n)\nright_sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"ChipHeatDataset\",\n            \"input\": right_data,\n            \"label\": label,\n            \"index\": index,\n            \"data_type\": \"bc_data\",\n            \"weight\": weight,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\n        \"chip\": lambda out: paddle.where(\n            out[\"bc\"] == 1,\n            jacobian(out[\"T\"], out[\"y\"]) - out[\"u_one\"],\n            paddle.where(\n                out[\"bc\"] == 0,\n                out[\"T\"] - out[\"u_one\"],\n                paddle.where(\n                    out[\"bc\"] == 2,\n                    jacobian(out[\"T\"], out[\"y\"]) + out[\"u_one\"] * (out[\"T\"] - 1),\n                    jacobian(out[\"T\"], out[\"y\"])\n                    + out[\"u_one\"]\n                    * (out[\"T\"] ** 2 - 1)\n                    * (out[\"T\"] ** 2 + 1)\n                    * 5.6\n                    / 50000,\n                ),\n            ),\n        )\n    },\n    name=\"right_sup\",\n)\ninterior_sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"ChipHeatDataset\",\n            \"input\": interior_data,\n            \"label\": label,\n            \"index\": index,\n            \"data_type\": \"u\",\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\n        \"chip\": lambda out: hessian(out[\"T\"], out[\"x\"])\n        + hessian(out[\"T\"], out[\"y\"])\n        + 100 * out[\"u_one\"]\n    },\n    name=\"interior_sup\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u5176\u4e2d <code>\u201cdataset\u201d</code> \u5b57\u6bb5\u8868\u793a\u4f7f\u7528\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4fe1\u606f\uff0c\u5404\u4e2a\u5b57\u6bb5\u5206\u522b\u8868\u793a\uff1a</p> <ol> <li><code>name</code>\uff1a \u6570\u636e\u96c6\u7c7b\u578b\uff0c\u6b64\u5904 <code>ChipHeatDataset</code> \u8868\u793a\u5206 batch \u987a\u5e8f\u8fed\u4ee3\u7684\u8bfb\u53d6\u6570\u636e\uff1b</li> <li><code>input</code>\uff1a \u8f93\u5165\u53d8\u91cf\u540d\uff1b</li> <li><code>label</code>\uff1a \u6807\u7b7e\u53d8\u91cf\u540d\uff1b</li> <li><code>index</code>\uff1a \u8f93\u5165\u6570\u636e\u96c6\u7684\u7d22\u5f15\uff1b</li> <li><code>data_type</code>\uff1a \u8f93\u5165\u6570\u636e\u7684\u7c7b\u578b\uff1b</li> <li><code>weight</code>\uff1a \u6743\u91cd\u5927\u5c0f\u3002</li> </ol> <p>\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u53e6\u5916\u8fd8\u6307\u5b9a\u4e86\u8be5\u7c7b\u521d\u59cb\u5316\u65f6\u53c2\u6570 <code>drop_last</code> \u4e3a <code>False</code>\u3001<code>shuffle</code> \u4e3a <code>True</code>\u3002</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684 MSE \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\u53d6\u5e73\u5747\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u6807\u7b7e\u8868\u8fbe\u5f0f\u5217\u8868\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528\u4e0e\u5de6\u8fb9\u3001\u53f3\u8fb9\u3001\u4e0a\u8fb9\u3001\u4e0b\u8fb9\u4ee5\u53ca\u5185\u90e8\u533a\u57df\u76f8\u5bf9\u5e94\u7684\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u540c\u65f6\u6211\u4eec\u5206\u522b\u7528 \\(0,1,2,3\\) \u4ee3\u8868Dirichlet\u8fb9\u754c\u3001Neumann \u8fb9\u754c\u3001\u5bf9\u6d41\u8fb9\u754c\u4ee5\u53ca\u8f90\u5c04\u8fb9\u754c\uff0c\u5bf9\u4e0e\u4e0d\u540c\u7684\u8fb9\u754c\u7c7b\u578b\uff0c\u8bbe\u7f6e\u4e0d\u540c\u7684\u8fb9\u754c\u6761\u4ef6\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002</p> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u548c\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    down_sup_constraint.name: down_sup_constraint,\n    left_sup_constraint.name: left_sup_constraint,\n    right_sup_constraint.name: right_sup_constraint,\n    interior_sup_constraint.name: interior_sup_constraint,\n    top_sup_constraint.name: top_sup_constraint,\n}\n</code></pre>"},{"location":"zh/examples/chip_heat/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u5b66\u4e60\u7387\uff0c\u5b66\u4e60\u7387\u8bbe\u4e3a 0.001\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>"},{"location":"zh/examples/chip_heat/#36","title":"3.6 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u6211\u4eec\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\ntop_down_label = {\"chip\": np.zeros([cfg.NL, 1], dtype=\"float32\")}\nleft_right_label = {\"chip\": np.zeros([(cfg.NL - 2), 1], dtype=\"float32\")}\ninterior_label = {\n    \"thermal_condution\": np.zeros(\n        [test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"\n    )\n}\ntop_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": test_top_data,\n            \"label\": top_down_label,\n            \"weight\": {\n                \"chip\": np.full([cfg.NL, 1], cfg.TRAIN.weight, dtype=\"float32\")\n            },\n        },\n        \"batch_size\": cfg.NL,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"top_mse\",\n)\ndown_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": test_down_data,\n            \"label\": top_down_label,\n            \"weight\": {\n                \"chip\": np.full([cfg.NL, 1], cfg.TRAIN.weight, dtype=\"float32\")\n            },\n        },\n        \"batch_size\": cfg.NL,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"down_mse\",\n)\nleft_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": test_left_data,\n            \"label\": left_right_label,\n            \"weight\": {\n                \"chip\": np.full([cfg.NL - 2, 1], cfg.TRAIN.weight, dtype=\"float32\")\n            },\n        },\n        \"batch_size\": (cfg.NL - 2),\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"left_mse\",\n)\nright_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": test_right_data,\n            \"label\": left_right_label,\n            \"weight\": {\n                \"chip\": np.full([cfg.NL - 2, 1], cfg.TRAIN.weight, dtype=\"float32\")\n            },\n        },\n        \"batch_size\": (cfg.NL - 2),\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"right_mse\",\n)\ninterior_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": test_interior_data,\n            \"label\": interior_label,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\n        \"thermal_condution\": lambda out: (\n            hessian(out[\"T\"], out[\"x\"]) + hessian(out[\"T\"], out[\"y\"])\n        )\n        + 100 * out[\"u_one\"]\n    },\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"interior_mse\",\n)\nvalidator = {\n    down_validator.name: down_validator,\n    left_validator.name: left_validator,\n    right_validator.name: right_validator,\n    top_validator.name: top_validator,\n    interior_validator.name: interior_validator,\n}\n</code></pre> <p>\u914d\u7f6e\u4e0e 3.4 \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8e\u8bc4\u4f30\u6240\u7528\u7684\u6570\u636e\u91cf\u4e0d\u662f\u5f88\u591a\uff0c\u56e0\u6b64\u6211\u4eec\u4e0d\u9700\u8981\u4f7f\u7528<code>ChipHeatDataset</code> \u8fed\u4ee3\u7684\u8bfb\u53d6\u6570\u636e\uff0c\u5728\u8fd9\u91cc\u4f7f\u7528<code>NamedArrayDataset</code> \u8bfb\u53d6\u6570\u636e\u3002</p>"},{"location":"zh/examples/chip_heat/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    validator=validator,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/chip_heat/#38","title":"3.8 \u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u6700\u540e\u5728\u7ed9\u5b9a\u7684\u53ef\u89c6\u5316\u533a\u57df\u4e0a\u8fdb\u884c\u9884\u6d4b\u5e76\u53ef\u89c6\u5316\uff0c\u53ef\u89c6\u5316\u6570\u636e\u662f\u533a\u57df\u5185\u7684\u4e8c\u7ef4\u70b9\u96c6\uff0c\u6bcf\u4e2a\u5750\u6807 \\((x, y)\\) \u5904\uff0c\u5bf9\u5e94\u7684\u6e29\u5ea6\u503c \\(T\\)\uff0c\u5728\u6b64\u6211\u4eec\u753b\u51fa \\(T\\) \u5728\u533a\u57df\u4e0a\u7684\u53d8\u5316\u56fe\u50cf\u3002\u540c\u65f6\u53ef\u4ee5\u6839\u636e\u9700\u8981\uff0c\u8bbe\u7f6e\u4e0d\u540c\u7684\u8fb9\u754c\u7c7b\u578b\u3001\u968f\u673a\u70ed\u6e90\u5206\u5e03 \\(S(x)\\) \u548c\u8fb9\u754c\u51fd\u6570 \\(Q(x)\\)\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># visualize prediction after finished training\npred_points = geom[\"rect\"].sample_interior(NPOINT, evenly=True)\npred_points[\"u\"] = points[\"u\"]\npred_points[\"bc_data\"] = np.zeros_like(points[\"bc_data\"])\npred_points[\"bc\"] = np.repeat(\n    np.array([[cfg.EVAL.bc_type]], dtype=\"float32\"), NPOINT, axis=0\n)\npred = solver.predict(pred_points)\nlogger.message(\"Now saving visual result to: visual/result.vtu, please wait...\")\nppsci.visualize.save_vtu_from_dict(\n    osp.join(cfg.output_dir, \"visual/result.vtu\"),\n    {\n        \"x\": pred_points[\"x\"],\n        \"y\": pred_points[\"y\"],\n        \"T\": pred[\"T\"],\n    },\n    (\n        \"x\",\n        \"y\",\n    ),\n    (\"T\"),\n)\n</code></pre>"},{"location":"zh/examples/chip_heat/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"chip_heat.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle\nimport scipy.fftpack\nimport scipy.io\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.autodiff import hessian\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\n\n\ndef fftind(size):\n    \"\"\"\n    Returns the momentum indices for the 2D Fast Fourier Transform (FFT).\n\n    Args:\n        size (int): Size of the 2D array.\n\n    Returns:\n        numpy.ndarray: Array of momentum indices for the 2D FFT.\n    \"\"\"\n    k_ind = np.mgrid[:size, :size] - int((size + 1) / 2)\n    k_ind = scipy.fftpack.fftshift(k_ind)\n    return k_ind\n\n\ndef GRF(alpha=3.0, size=128, flag_normalize=True):\n    \"\"\"\n    Generates a Gaussian random field(GRF) with a power law amplitude spectrum.\n\n    Args:\n        alpha (float, optional): Power law exponent. Defaults to 3.0.\n        size (int, optional): Size of the output field. Defaults to 128.\n        flag_normalize (bool, optional): Flag indicating whether to normalize the field. Defaults to True.\n\n    Returns:\n        numpy.ndarray: Generated Gaussian random field.\n    \"\"\"\n    # Defines momentum indices\n    k_idx = fftind(size)\n    # Defines the amplitude as a power law 1/|k|^(alpha/2)\n    amplitude = np.power(k_idx[0] ** 2 + k_idx[1] ** 2 + 1e-10, -alpha / 4.0)\n    amplitude[0, 0] = 0\n    # Draws a complex gaussian random noise with normal\n    # (circular) distribution\n    noise = np.random.normal(size=(size, size)) + 1j * np.random.normal(\n        size=(size, size)\n    )\n    # To real space\n    gfield = np.fft.ifft2(noise * amplitude).real\n    # Sets the standard deviation to one\n    if flag_normalize:\n        gfield = gfield - np.mean(gfield)\n        gfield = gfield / np.std(gfield)\n    return gfield.reshape([1, -1])\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.ChipDeepONets(**cfg.MODEL)\n    # set geometry\n    NPOINT = cfg.NL * cfg.NW\n    geom = {\"rect\": ppsci.geometry.Rectangle((0, 0), (cfg.DL, cfg.DW))}\n    points = geom[\"rect\"].sample_interior(NPOINT, evenly=True)\n\n    # generate training data and validation data\n    data_u = np.ones([1, (cfg.NL - 2) * (cfg.NW - 2)])\n    data_BC = np.ones([1, NPOINT])\n    data_u = np.vstack((data_u, np.zeros([1, (cfg.NL - 2) * (cfg.NW - 2)])))\n    data_BC = np.vstack((data_BC, np.zeros([1, NPOINT])))\n    for i in range(cfg.NU - 2):\n        data_u = np.vstack((data_u, GRF(alpha=cfg.GRF.alpha, size=cfg.NL - 2)))\n    for i in range(cfg.NBC - 2):\n        data_BC = np.vstack((data_BC, GRF(alpha=cfg.GRF.alpha, size=cfg.NL)))\n    data_u = data_u.astype(\"float32\")\n    data_BC = data_BC.astype(\"float32\")\n    test_u = GRF(alpha=4, size=cfg.NL).astype(\"float32\")[0]\n\n    boundary_indices = np.where(\n        (\n            (points[\"x\"] == 0)\n            | (points[\"x\"] == cfg.DW)\n            | (points[\"y\"] == 0)\n            | (points[\"y\"] == cfg.DL)\n        )\n    )\n    interior_indices = np.where(\n        (\n            (points[\"x\"] != 0)\n            &amp; (points[\"x\"] != cfg.DW)\n            &amp; (points[\"y\"] != 0)\n            &amp; (points[\"y\"] != cfg.DL)\n        )\n    )\n\n    points[\"u\"] = np.tile(test_u[interior_indices[0]], (NPOINT, 1))\n    points[\"u_one\"] = test_u.T.reshape([-1, 1])\n    points[\"bc_data\"] = np.tile(test_u[boundary_indices[0]], (NPOINT, 1))\n    points[\"bc\"] = np.zeros((NPOINT, 1), dtype=\"float32\")\n\n    top_indices = np.where(points[\"x\"] == cfg.DW)\n    down_indices = np.where(points[\"x\"] == 0)\n    left_indices = np.where(\n        (points[\"y\"] == 0) &amp; (points[\"x\"] != 0) &amp; (points[\"x\"] != cfg.DW)\n    )\n    right_indices = np.where(\n        ((points[\"y\"] == cfg.DL) &amp; (points[\"x\"] != 0) &amp; (points[\"x\"] != cfg.DW))\n    )\n\n    # generate validation data\n    (\n        test_top_data,\n        test_down_data,\n        test_left_data,\n        test_right_data,\n        test_interior_data,\n    ) = [\n        {\n            \"x\": points[\"x\"][indices_[0]],\n            \"y\": points[\"y\"][indices_[0]],\n            \"u\": points[\"u\"][indices_[0]],\n            \"u_one\": points[\"u_one\"][indices_[0]],\n            \"bc\": points[\"bc\"][indices_[0]],\n            \"bc_data\": points[\"bc_data\"][indices_[0]],\n        }\n        for indices_ in (\n            top_indices,\n            down_indices,\n            left_indices,\n            right_indices,\n            interior_indices,\n        )\n    ]\n    # generate train data\n    top_data = {\n        \"x\": test_top_data[\"x\"],\n        \"y\": test_top_data[\"y\"],\n        \"u\": data_u,\n        \"u_one\": data_BC[:, top_indices[0]].T.reshape([-1, 1]),\n        \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n        \"bc_data\": data_BC[:, boundary_indices[0]],\n    }\n    down_data = {\n        \"x\": test_down_data[\"x\"],\n        \"y\": test_down_data[\"y\"],\n        \"u\": data_u,\n        \"u_one\": data_BC[:, down_indices[0]].T.reshape([-1, 1]),\n        \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n        \"bc_data\": data_BC[:, boundary_indices[0]],\n    }\n    left_data = {\n        \"x\": test_left_data[\"x\"],\n        \"y\": test_left_data[\"y\"],\n        \"u\": data_u,\n        \"u_one\": data_BC[:, left_indices[0]].T.reshape([-1, 1]),\n        \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n        \"bc_data\": data_BC[:, boundary_indices[0]],\n    }\n    right_data = {\n        \"x\": test_right_data[\"x\"],\n        \"y\": test_right_data[\"y\"],\n        \"u\": data_u,\n        \"u_one\": data_BC[:, right_indices[0]].T.reshape([-1, 1]),\n        \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n        \"bc_data\": data_BC[:, boundary_indices[0]],\n    }\n    interior_data = {\n        \"x\": test_interior_data[\"x\"],\n        \"y\": test_interior_data[\"y\"],\n        \"u\": data_u,\n        \"u_one\": data_u.T.reshape([-1, 1]),\n        \"bc\": np.array([[0], [1], [2], [3]], dtype=\"float32\"),\n        \"bc_data\": data_BC[:, boundary_indices[0]],\n    }\n\n    # set constraint\n    index = (\"x\", \"u\", \"bc\", \"bc_data\")\n    label = {\"chip\": np.array([0], dtype=\"float32\")}\n    weight = {\"chip\": np.array([cfg.TRAIN.weight], dtype=\"float32\")}\n    top_sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"ChipHeatDataset\",\n                \"input\": top_data,\n                \"label\": label,\n                \"index\": index,\n                \"data_type\": \"bc_data\",\n                \"weight\": weight,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"chip\": lambda out: paddle.where(\n                out[\"bc\"] == 1,\n                jacobian(out[\"T\"], out[\"x\"]) - out[\"u_one\"],\n                paddle.where(\n                    out[\"bc\"] == 0,\n                    out[\"T\"] - out[\"u_one\"],\n                    paddle.where(\n                        out[\"bc\"] == 2,\n                        jacobian(out[\"T\"], out[\"x\"]) + out[\"u_one\"] * (out[\"T\"] - 1),\n                        jacobian(out[\"T\"], out[\"x\"])\n                        + out[\"u_one\"]\n                        * (out[\"T\"] ** 2 - 1)\n                        * (out[\"T\"] ** 2 + 1)\n                        * 5.6\n                        / 50000,\n                    ),\n                ),\n            )\n        },\n        name=\"top_sup\",\n    )\n    down_sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"ChipHeatDataset\",\n                \"input\": down_data,\n                \"label\": label,\n                \"index\": index,\n                \"data_type\": \"bc_data\",\n                \"weight\": weight,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"chip\": lambda out: paddle.where(\n                out[\"bc\"] == 1,\n                jacobian(out[\"T\"], out[\"x\"]) - out[\"u_one\"],\n                paddle.where(\n                    out[\"bc\"] == 0,\n                    out[\"T\"] - out[\"u_one\"],\n                    paddle.where(\n                        out[\"bc\"] == 2,\n                        jacobian(out[\"T\"], out[\"x\"]) + out[\"u_one\"] * (out[\"T\"] - 1),\n                        jacobian(out[\"T\"], out[\"x\"])\n                        + out[\"u_one\"]\n                        * (out[\"T\"] ** 2 - 1)\n                        * (out[\"T\"] ** 2 + 1)\n                        * 5.6\n                        / 50000,\n                    ),\n                ),\n            )\n        },\n        name=\"down_sup\",\n    )\n    left_sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"ChipHeatDataset\",\n                \"input\": left_data,\n                \"label\": label,\n                \"index\": index,\n                \"data_type\": \"bc_data\",\n                \"weight\": weight,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"chip\": lambda out: paddle.where(\n                out[\"bc\"] == 1,\n                jacobian(out[\"T\"], out[\"y\"]) - out[\"u_one\"],\n                paddle.where(\n                    out[\"bc\"] == 0,\n                    out[\"T\"] - out[\"u_one\"],\n                    paddle.where(\n                        out[\"bc\"] == 2,\n                        jacobian(out[\"T\"], out[\"y\"]) + out[\"u_one\"] * (out[\"T\"] - 1),\n                        jacobian(out[\"T\"], out[\"y\"])\n                        + out[\"u_one\"]\n                        * (out[\"T\"] ** 2 - 1)\n                        * (out[\"T\"] ** 2 + 1)\n                        * 5.6\n                        / 50000,\n                    ),\n                ),\n            )\n        },\n        name=\"left_sup\",\n    )\n    right_sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"ChipHeatDataset\",\n                \"input\": right_data,\n                \"label\": label,\n                \"index\": index,\n                \"data_type\": \"bc_data\",\n                \"weight\": weight,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"chip\": lambda out: paddle.where(\n                out[\"bc\"] == 1,\n                jacobian(out[\"T\"], out[\"y\"]) - out[\"u_one\"],\n                paddle.where(\n                    out[\"bc\"] == 0,\n                    out[\"T\"] - out[\"u_one\"],\n                    paddle.where(\n                        out[\"bc\"] == 2,\n                        jacobian(out[\"T\"], out[\"y\"]) + out[\"u_one\"] * (out[\"T\"] - 1),\n                        jacobian(out[\"T\"], out[\"y\"])\n                        + out[\"u_one\"]\n                        * (out[\"T\"] ** 2 - 1)\n                        * (out[\"T\"] ** 2 + 1)\n                        * 5.6\n                        / 50000,\n                    ),\n                ),\n            )\n        },\n        name=\"right_sup\",\n    )\n    interior_sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"ChipHeatDataset\",\n                \"input\": interior_data,\n                \"label\": label,\n                \"index\": index,\n                \"data_type\": \"u\",\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"chip\": lambda out: hessian(out[\"T\"], out[\"x\"])\n            + hessian(out[\"T\"], out[\"y\"])\n            + 100 * out[\"u_one\"]\n        },\n        name=\"interior_sup\",\n    )\n    # wrap constraints together\n    constraint = {\n        down_sup_constraint.name: down_sup_constraint,\n        left_sup_constraint.name: left_sup_constraint,\n        right_sup_constraint.name: right_sup_constraint,\n        interior_sup_constraint.name: interior_sup_constraint,\n        top_sup_constraint.name: top_sup_constraint,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    top_down_label = {\"chip\": np.zeros([cfg.NL, 1], dtype=\"float32\")}\n    left_right_label = {\"chip\": np.zeros([(cfg.NL - 2), 1], dtype=\"float32\")}\n    interior_label = {\n        \"thermal_condution\": np.zeros(\n            [test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"\n        )\n    }\n    top_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_top_data,\n                \"label\": top_down_label,\n                \"weight\": {\n                    \"chip\": np.full([cfg.NL, 1], cfg.TRAIN.weight, dtype=\"float32\")\n                },\n            },\n            \"batch_size\": cfg.NL,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"top_mse\",\n    )\n    down_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_down_data,\n                \"label\": top_down_label,\n                \"weight\": {\n                    \"chip\": np.full([cfg.NL, 1], cfg.TRAIN.weight, dtype=\"float32\")\n                },\n            },\n            \"batch_size\": cfg.NL,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"down_mse\",\n    )\n    left_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_left_data,\n                \"label\": left_right_label,\n                \"weight\": {\n                    \"chip\": np.full([cfg.NL - 2, 1], cfg.TRAIN.weight, dtype=\"float32\")\n                },\n            },\n            \"batch_size\": (cfg.NL - 2),\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"left_mse\",\n    )\n    right_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_right_data,\n                \"label\": left_right_label,\n                \"weight\": {\n                    \"chip\": np.full([cfg.NL - 2, 1], cfg.TRAIN.weight, dtype=\"float32\")\n                },\n            },\n            \"batch_size\": (cfg.NL - 2),\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"right_mse\",\n    )\n    interior_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_interior_data,\n                \"label\": interior_label,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"thermal_condution\": lambda out: (\n                hessian(out[\"T\"], out[\"x\"]) + hessian(out[\"T\"], out[\"y\"])\n            )\n            + 100 * out[\"u_one\"]\n        },\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"interior_mse\",\n    )\n    validator = {\n        down_validator.name: down_validator,\n        left_validator.name: left_validator,\n        right_validator.name: right_validator,\n        top_validator.name: top_validator,\n        interior_validator.name: interior_validator,\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        validator=validator,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    pred_points = geom[\"rect\"].sample_interior(NPOINT, evenly=True)\n    pred_points[\"u\"] = points[\"u\"]\n    pred_points[\"bc_data\"] = np.zeros_like(points[\"bc_data\"])\n    pred_points[\"bc\"] = np.repeat(\n        np.array([[cfg.EVAL.bc_type]], dtype=\"float32\"), NPOINT, axis=0\n    )\n    pred = solver.predict(pred_points)\n    logger.message(\"Now saving visual result to: visual/result.vtu, please wait...\")\n    ppsci.visualize.save_vtu_from_dict(\n        osp.join(cfg.output_dir, \"visual/result.vtu\"),\n        {\n            \"x\": pred_points[\"x\"],\n            \"y\": pred_points[\"y\"],\n            \"T\": pred[\"T\"],\n        },\n        (\n            \"x\",\n            \"y\",\n        ),\n        (\"T\"),\n    )\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.ChipDeepONets(**cfg.MODEL)\n    # set geometry\n    NPOINT = cfg.NL * cfg.NW\n    geom = {\"rect\": ppsci.geometry.Rectangle((0, 0), (cfg.DL, cfg.DW))}\n    points = geom[\"rect\"].sample_interior(NPOINT, evenly=True)\n\n    # generate validation data\n    test_u = GRF(alpha=4, size=cfg.NL).astype(\"float32\")[0]\n\n    boundary_indices = np.where(\n        (\n            (points[\"x\"] == 0)\n            | (points[\"x\"] == cfg.DW)\n            | (points[\"y\"] == 0)\n            | (points[\"y\"] == cfg.DL)\n        )\n    )\n    interior_indices = np.where(\n        (\n            (points[\"x\"] != 0)\n            &amp; (points[\"x\"] != cfg.DW)\n            &amp; (points[\"y\"] != 0)\n            &amp; (points[\"y\"] != cfg.DL)\n        )\n    )\n\n    points[\"u\"] = np.tile(test_u[interior_indices[0]], (NPOINT, 1))\n    points[\"u_one\"] = test_u.T.reshape([-1, 1])\n    points[\"bc_data\"] = np.tile(test_u[boundary_indices[0]], (NPOINT, 1))\n    points[\"bc\"] = np.zeros((NPOINT, 1), dtype=\"float32\")\n\n    top_indices = np.where(points[\"x\"] == cfg.DW)\n    down_indices = np.where(points[\"x\"] == 0)\n    left_indices = np.where(\n        (points[\"y\"] == 0) &amp; (points[\"x\"] != 0) &amp; (points[\"x\"] != cfg.DW)\n    )\n    right_indices = np.where(\n        ((points[\"y\"] == cfg.DL) &amp; (points[\"x\"] != 0) &amp; (points[\"x\"] != cfg.DW))\n    )\n\n    # generate validation data\n    (\n        test_top_data,\n        test_down_data,\n        test_left_data,\n        test_right_data,\n        test_interior_data,\n    ) = [\n        {\n            \"x\": points[\"x\"][indices_[0]],\n            \"y\": points[\"y\"][indices_[0]],\n            \"u\": points[\"u\"][indices_[0]],\n            \"u_one\": points[\"u_one\"][indices_[0]],\n            \"bc\": points[\"bc\"][indices_[0]],\n            \"bc_data\": points[\"bc_data\"][indices_[0]],\n        }\n        for indices_ in (\n            top_indices,\n            down_indices,\n            left_indices,\n            right_indices,\n            interior_indices,\n        )\n    ]\n\n    # set validator\n    top_down_label = {\"chip\": np.zeros([cfg.NL, 1], dtype=\"float32\")}\n    left_right_label = {\"chip\": np.zeros([(cfg.NL - 2), 1], dtype=\"float32\")}\n    interior_label = {\n        \"thermal_condution\": np.zeros(\n            [test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"\n        )\n    }\n    top_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_top_data,\n                \"label\": top_down_label,\n                \"weight\": {\n                    \"chip\": np.full([cfg.NL, 1], cfg.TRAIN.weight, dtype=\"float32\")\n                },\n            },\n            \"batch_size\": cfg.NL,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"top_mse\",\n    )\n    down_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_down_data,\n                \"label\": top_down_label,\n                \"weight\": {\n                    \"chip\": np.full([cfg.NL, 1], cfg.TRAIN.weight, dtype=\"float32\")\n                },\n            },\n            \"batch_size\": cfg.NL,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"down_mse\",\n    )\n    left_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_left_data,\n                \"label\": left_right_label,\n                \"weight\": {\n                    \"chip\": np.full([cfg.NL - 2, 1], cfg.TRAIN.weight, dtype=\"float32\")\n                },\n            },\n            \"batch_size\": (cfg.NL - 2),\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"left_mse\",\n    )\n    right_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_right_data,\n                \"label\": left_right_label,\n                \"weight\": {\n                    \"chip\": np.full([cfg.NL - 2, 1], cfg.TRAIN.weight, dtype=\"float32\")\n                },\n            },\n            \"batch_size\": (cfg.NL - 2),\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"chip\": lambda out: out[\"T\"] - out[\"u_one\"]},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"right_mse\",\n    )\n    interior_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_interior_data,\n                \"label\": interior_label,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"thermal_condution\": lambda out: (\n                hessian(out[\"T\"], out[\"x\"]) + hessian(out[\"T\"], out[\"y\"])\n            )\n            + 100 * out[\"u_one\"]\n        },\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"interior_mse\",\n    )\n    validator = {\n        down_validator.name: down_validator,\n        left_validator.name: left_validator,\n        right_validator.name: right_validator,\n        top_validator.name: top_validator,\n        interior_validator.name: interior_validator,\n    }\n\n    # directly evaluate pretrained model(optional)\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n    # visualize prediction result\n    pred_points = geom[\"rect\"].sample_interior(NPOINT, evenly=True)\n    pred_points[\"u\"] = points[\"u\"]\n    pred_points[\"bc_data\"] = np.zeros_like(points[\"bc_data\"])\n    pred_points[\"bc\"] = np.full((NPOINT, 1), cfg.EVAL.bc_type, dtype=\"float32\")\n    pred = solver.predict(pred_points)\n    logger.message(\"Now saving visual result to: visual/result.vtu, please wait...\")\n    ppsci.visualize.save_vtu_from_dict(\n        osp.join(cfg.output_dir, \"visual/result.vtu\"),\n        {\n            \"x\": pred_points[\"x\"],\n            \"y\": pred_points[\"y\"],\n            \"T\": pred[\"T\"],\n        },\n        (\n            \"x\",\n            \"y\",\n        ),\n        (\"T\"),\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"chip_heat.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/chip_heat/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u901a\u8fc7\u9ad8\u65af\u968f\u673a\u573a\u751f\u6210\u4e09\u7ec4\u968f\u673a\u70ed\u6e90\u5206\u5e03 \\(S(x)\\)\uff0c\u5982\u56fe\u4e2d\u7b2c\u4e00\u884c\u6240\u793a\u3002\u63a5\u4e0b\u6765\u6211\u4eec\u53ef\u4ee5\u8bbe\u7f6e\u7b2c\u4e00\u7c7b PDE \u4e2d\u7684\u4efb\u610f\u8fb9\u754c\u6761\u4ef6\uff0c\u5728\u6b64\u6211\u4eec\u7ed9\u51fa\u4e86\u4e94\u7c7b\u8fb9\u754c\u6761\u4ef6\uff0c\u5982\u56fe\u4e2d\u7b2c\u4e00\u5217\u63a7\u5236\u65b9\u7a0b\u4e2d\u8fb9\u754c\u65b9\u7a0b\u6240\u793a\uff0c\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u8bbe \\(k = 100,~h = 100,~T_{amb} = 1,~\\epsilon\\sigma= 5.6 \\times 10^{-7}\\)\u3002 \u5728\u4e0d\u540c\u968f\u673a\u70ed\u6e90 \\(S(x)\\) \u5206\u5e03\u548c\u4e0d\u540c\u8fb9\u754c\u6761\u4ef6\u4e0b\uff0c\u6211\u4eec\u901a\u8fc7 PI-DeepONet \u6a21\u578b\u6d4b\u8bd5\u7684\u6e29\u5ea6\u573a\u5206\u5e03\u5982\u56fe\u6240\u793a\u3002\u4ece\u56fe\u4e2d\u53ef\u77e5\uff0c\u5c3d\u7ba1\u968f\u673a\u70ed\u6e90\u5206\u5e03 \\(S(x)\\) \u548c\u8fb9\u754c\u6761\u4ef6\u5728\u6d4b\u8bd5\u6837\u672c\u4e4b\u95f4\u5b58\u5728\u7740\u663e\u7740\u5dee\u5f02\uff0c\u4f46 PI-DeepONet \u6a21\u578b\u5747\u53ef\u4ee5\u6b63\u786e\u9884\u6d4b\u7531\u70ed\u4f20\u5bfc\u65b9\u7a0b\u63a7\u5236\u7684\u5185\u90e8\u548c\u8fb9\u754c\u4e0a\u7684\u4e8c\u7ef4\u6269\u6563\u6027\u8d28\u89e3\u3002</p> <p></p>"},{"location":"zh/examples/chip_heat/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<p>\u53c2\u8003\u6587\u732e\uff1a A fast general thermal simulation model based on MultiBranch Physics-Informed deep operator neural network</p> <p>\u53c2\u8003\u4ee3\u7801\uff1a gaussian-random-fields</p>"},{"location":"zh/examples/control_arm/","title":"Control_arm","text":""},{"location":"zh/examples/control_arm/#control-arm","title":"Control arm","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 \u6b63\u95ee\u9898\uff1a\u53d7\u529b\u5206\u6790\u6c42\u89e3\u9006\u95ee\u9898\uff1a\u53c2\u6570\u9006\u63a8\u6c42\u89e3 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl --create-dirs -o ./datasets/control_arm.stl\npython forward_analysis.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl --create-dirs -o ./datasets/control_arm.stl\npython inverse_parameter.py TRAIN.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/control_arm/forward_x_axis_pretrained.pdparams\n</code></pre> \u6b63\u95ee\u9898\uff1a\u53d7\u529b\u5206\u6790\u6c42\u89e3\u9006\u95ee\u9898\uff1a\u53c2\u6570\u9006\u63a8\u6c42\u89e3 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl --create-dirs -o ./datasets/control_arm.stl\npython forward_analysis.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/control_arm/forward_x_axis_pretrained.pdparams\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl --create-dirs -o ./datasets/control_arm.stl\npython inverse_parameter.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/control_arm/inverse_x_axis_pretrained.pdparams\n</code></pre> \u6b63\u95ee\u9898\uff1a\u53d7\u529b\u5206\u6790\u6c42\u89e3\u9006\u95ee\u9898\uff1a\u53c2\u6570\u9006\u63a8\u6c42\u89e3 <pre><code>python forward_analysis.py mode=export\n</code></pre> <pre><code>python inverse_parameter.py mode=export\n</code></pre> \u6b63\u95ee\u9898\uff1a\u53d7\u529b\u5206\u6790\u6c42\u89e3\u9006\u95ee\u9898\uff1a\u53c2\u6570\u9006\u63a8\u6c42\u89e3 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl --create-dirs -o ./datasets/control_arm.stl\npython forward_analysis.py mode=infer\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/control_arm/control_arm.stl --create-dirs -o ./datasets/control_arm.stl\npython inverse_parameter.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 inverse_x_axis_pretrained.pdparams loss(geo_eval): 0.02505L2Rel.lambda_(geo_eval): 0.06025L2Rel.mu(geo_eval): 0.07949"},{"location":"zh/examples/control_arm/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u7ed3\u6784\u53d7\u529b\u5206\u6790\u662f\u5728\u7b26\u5408\u67d0\u4e2a\u8fb9\u754c\u6761\u4ef6\u7684\u7ed3\u6784\u53d7\u5230\u7279\u5b9a\u6761\u4ef6\u7684\u8f7d\u8377\u540e\uff0c\u7ed3\u6784\u4f1a\u4ea7\u751f\u76f8\u5e94\u7684\u5e94\u529b\u5e94\u53d8\uff0c\u6b64\u65f6\u5bf9\u5b83\u4eec\u72b6\u6001\u7684\u5206\u6790\u3002 \u5e94\u529b\u662f\u4e00\u4e2a\u7269\u7406\u91cf\uff0c\u7528\u4e8e\u63cf\u8ff0\u7269\u4f53\u5185\u90e8\u7531\u4e8e\u5916\u529b\u800c\u4ea7\u751f\u7684\u5355\u4f4d\u9762\u79ef\u4e0a\u7684\u529b\u3002\u5e94\u53d8\u5219\u63cf\u8ff0\u4e86\u7269\u4f53\u7684\u5f62\u72b6\u548c\u5c3a\u5bf8\u7684\u53d8\u5316\u3002 \u901a\u5e38\u7ed3\u6784\u529b\u5b66\u95ee\u9898\u5206\u4e3a\u9759\u529b\u5b66\u95ee\u9898\u548c\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u672c\u6848\u4f8b\u7740\u773c\u4e8e\u9759\u529b\u5b66\u5206\u6790\uff0c\u5373\u7ed3\u6784\u8fbe\u5230\u53d7\u529b\u5e73\u8861\u72b6\u6001\u540e\u518d\u8fdb\u884c\u5206\u6790\u3002 \u672c\u95ee\u9898\u5047\u8bbe\u7ed3\u6784\u53d7\u5230\u4e00\u4e2a\u6bd4\u8f83\u5c0f\u7684\u529b\uff0c\u6b64\u65f6\u7ed3\u6784\u5f62\u53d8\u7b26\u5408\u7ebf\u5f39\u6027\u65b9\u7a0b\u3002</p> <p>\u9700\u8981\u6307\u51fa\u7684\u662f\uff0c\u80fd\u591f\u9002\u7528\u7ebf\u5f39\u6027\u65b9\u7a0b\u7684\u7ed3\u6784\u9700\u8981\u6ee1\u8db3\u5728\u53d7\u529b\u540e\u80fd\u591f\u5b8c\u5168\u6062\u590d\u539f\u72b6\uff0c\u5373\u6ca1\u6709\u6c38\u4e45\u53d8\u5f62\u3002 \u8fd9\u79cd\u5047\u8bbe\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\u662f\u5408\u7406\u7684\uff0c\u4f46\u540c\u65f6\u5bf9\u4e8e\u67d0\u4e9b\u53ef\u80fd\u4ea7\u751f\u6c38\u4e45\u5f62\u53d8\u7684\u6750\u6599\uff08\u5982\u5851\u6599\u6216\u6a61\u80f6\uff09\u6765\u8bf4\uff0c\u8fd9\u79cd\u5047\u8bbe\u53ef\u80fd\u4e0d\u51c6\u786e\u3002 \u8981\u5168\u9762\u7406\u89e3\u5f62\u53d8\uff0c\u8fd8\u9700\u8981\u8003\u8651\u5176\u4ed6\u56e0\u7d20\uff0c\u4f8b\u5982\u7269\u4f53\u7684\u521d\u59cb\u5f62\u72b6\u548c\u5c3a\u5bf8\u3001\u5916\u529b\u7684\u5386\u53f2\u3001\u6750\u6599\u7684\u5176\u4ed6\u7269\u7406\u6027\u8d28\uff08\u5982\u70ed\u81a8\u80c0\u7cfb\u6570\u548c\u5bc6\u5ea6\uff09\u7b49\u3002</p> <p>\u6c7d\u8f66\u63a7\u5236\u81c2\uff0c\u4e5f\u79f0\u4e3a\u60ac\u6302\u81c2\u6216\u60ac\u6302\u63a7\u5236\u81c2\uff0c\u662f\u8fde\u63a5\u8f66\u8f6e\u548c\u8f66\u8f86\u5e95\u76d8\u7684\u91cd\u8981\u96f6\u4ef6\u3002\u63a7\u5236\u81c2\u4f5c\u4e3a\u6c7d\u8f66\u60ac\u67b6\u7cfb\u7edf\u7684\u5bfc\u5411\u548c\u4f20\u529b\u5143\u4ef6\uff0c\u5c06\u4f5c\u7528\u5728\u8f66\u8f6e\u4e0a\u7684\u5404\u79cd\u529b\u4f20\u9012\u7ed9\u8f66\u8eab\uff0c\u540c\u65f6\u4fdd\u8bc1\u8f66\u8f6e\u6309\u4e00\u5b9a\u8f68\u8ff9\u8fd0\u52a8\u3002\u63a7\u5236\u81c2\u5206\u522b\u901a\u8fc7\u7403\u94f0\u6216\u8005\u886c\u5957\u628a\u8f66\u8f6e\u548c\u8f66\u8eab\u5f39\u6027\u5730\u8fde\u63a5\u5728\u4e00\u8d77 \uff0c\u63a7\u5236\u81c2\uff08\u5305\u62ec\u4e0e\u4e4b\u76f8\u8fde\u7684\u886c\u5957\u53ca\u7403\u5934\uff09\u5e94\u6709\u8db3\u591f\u7684\u521a\u5ea6\u3001\u5f3a\u5ea6\u548c\u4f7f\u7528\u5bff\u547d\u3002</p> <p>\u672c\u95ee\u9898\u4e3b\u8981\u7814\u7a76\u5982\u4e0b\u6c7d\u8f66\u60ac\u6302\u63a7\u5236\u81c2\u7ed3\u6784\u4e0a\u7684\u53d7\u529b\u5206\u6790\u60c5\u51b5\u4ee5\u53ca\u9a8c\u8bc1\u5728\u4e0d\u7ed9\u5b9a\u9644\u52a0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u53c2\u6570\u9006\u63a8\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6839\u636e\u7ebf\u5f39\u6027\u7b49\u65b9\u7a0b\u8fdb\u884c\u6c42\u89e3\uff0c\u7ed3\u6784\u5982\u4e0b\u6240\u793a\uff0c\u5de6\u4fa7\u5355\u4e00\u5706\u73af\u5185\u8868\u9762\u53d7\u529b\uff0c\u53f3\u4fa7\u4e24\u5706\u73af\u5185\u8868\u9762\u56fa\u5b9a\uff0c\u5171\u7814\u7a76\u4e86\u53d7\u529b\u65b9\u5411\u4e3a\uff1ax \u8f74\u8d1f\u65b9\u5411\u3001z \u8f74\u6b63\u65b9\u5411\u4e24\u79cd\u60c5\u51b5\uff0c\u4e0b\u9762\u4ee5 x \u8f74\u6b63\u65b9\u5411\u53d7\u529b\u4e3a\u4f8b\u8fdb\u884c\u8bf4\u660e\u3002</p> <p> </p> \u63a7\u5236\u81c2\u7ed3\u6784\u793a\u610f\u56fe"},{"location":"zh/examples/control_arm/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u7ebf\u5f39\u6027\u65b9\u7a0b\u662f\u63cf\u8ff0\u7269\u4f53\u5728\u53d7\u529b\u540e\u6062\u590d\u539f\u72b6\u7684\u80fd\u529b\u7684\u6570\u5b66\u6a21\u578b\uff0c\u8868\u5f81\u4e3a\u5e94\u529b\u548c\u5e94\u53d8\u4e4b\u95f4\u7684\u7ebf\u6027\u5173\u7cfb\uff0c\u5176\u4e2d\u7cfb\u6570\u88ab\u79f0\u4e3a\u5f39\u6027\u6a21\u91cf\uff08\u6216\u6768\u6c0f\u6a21\u91cf\uff09\uff0c\u5b83\u7684\u516c\u5f0f\u4e3a\uff1a</p> \\[ \\begin{cases}     stress\\_disp_{xx} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial u}{\\partial x} - \\sigma_{xx} \\\\     stress\\_disp_{yy} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial v}{\\partial y} - \\sigma_{yy} \\\\     stress\\_disp_{zz} = \\lambda(\\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} + \\dfrac{\\partial w}{\\partial z}) + 2\\mu \\dfrac{\\partial w}{\\partial z} - \\sigma_{zz} \\\\     traction_{x} = n_x \\sigma_{xx} + n_y \\sigma_{xy} + n_z \\sigma_{xz} \\\\     traction_{y} = n_y \\sigma_{yx} + n_y \\sigma_{yy} + n_z \\sigma_{yz} \\\\     traction_{z} = n_z \\sigma_{zx} + n_y \\sigma_{zy} + n_z \\sigma_{zz} \\\\ \\end{cases} \\] <p>\u5176\u4e2d \\((x,y,z)\\) \u4e3a\u8f93\u5165\u7684\u4f4d\u7f6e\u5750\u6807\u70b9\uff0c\\((u,v,w)\\) \u4e3a\u5bf9\u5e94\u5750\u6807\u70b9\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5e94\u53d8\uff0c\\((\\sigma_{xx}, \\sigma_{yy}, \\sigma_{zz}, \\sigma_{xy}, \\sigma_{xz}, \\sigma_{yz})\\) \u4e3a \u5bf9\u5e94\u5750\u6807\u70b9\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5e94\u529b\u3002</p> <p>\u7ed3\u6784\u5de6\u4fa7\u5706\u73af\u5185\u8868\u9762\u53d7\u5230\u53d7\u5230\u5747\u5300\u5206\u5e03\u7684\uff0c\u6cbf z \u8f74\u6b63\u65b9\u5411\u5927\u5c0f\u4e3a \\(0.0025\\) \u7684\u5747\u5300\u5e94\u529b\u3002\u5176\u5b83\u53c2\u6570\u5305\u62ec\u5f39\u6027\u6a21\u91cf \\(E=1\\)\uff0c\u6cca\u677e\u6bd4 \\(\\nu=0.3\\)\u3002\u76ee\u6807\u6c42\u89e3\u8be5\u91d1\u5c5e\u4ef6\u8868\u9762\u6bcf\u4e2a\u70b9\u7684 \\(u\\)\u3001\\(v\\)\u3001\\(w\\)\u3001\\(\\sigma_{xx}\\)\u3001\\(\\sigma_{yy}\\)\u3001\\(\\sigma_{zz}\\)\u3001\\(\\sigma_{xy}\\)\u3001\\(\\sigma_{xz}\\)\u3001\\(\\sigma_{yz}\\) \u5171 9 \u4e2a\u7269\u7406\u91cf\u3002\u5e38\u91cf\u5b9a\u4e49\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>log_freq: 100\n\n# set working condition\nNU: 0.3\nE: 1\n</code></pre> <pre><code># specify parameters\nLAMBDA_ = cfg.NU * cfg.E / ((1 + cfg.NU) * (1 - 2 * cfg.NU))\nMU = cfg.E / (2 * (1 + cfg.NU))\n</code></pre>"},{"location":"zh/examples/control_arm/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/control_arm/#31","title":"3.1 \u53d7\u529b\u5206\u6790\u6c42\u89e3","text":""},{"location":"zh/examples/control_arm/#311","title":"3.1.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5982\u4e0a\u6240\u8ff0\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y, z)\\) \u90fd\u6709\u5bf9\u5e94\u7684\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff1a\u4e09\u4e2a\u65b9\u5411\u7684\u5e94\u53d8 \\((u, v, w)\\) \u548c\u5e94\u529b \\((\\sigma_{xx}, \\sigma_{yy}, \\sigma_{zz}, \\sigma_{xy}, \\sigma_{xz}, \\sigma_{yz})\\)\u3002</p> <p>\u8003\u8651\u5230\u4e24\u7ec4\u7269\u7406\u91cf\u5bf9\u5e94\u7740\u4e0d\u540c\u7684\u65b9\u7a0b\uff0c\u56e0\u6b64\u4f7f\u7528\u4e24\u4e2a\u6a21\u578b\u6765\u5206\u522b\u9884\u6d4b\u8fd9\u4e24\u7ec4\u7269\u7406\u91cf\uff1a</p> \\[ \\begin{cases} u, v, w = f(x,y,z) \\\\ \\sigma_{xx}, \\sigma_{yy}, \\sigma_{zz}, \\sigma_{xy}, \\sigma_{xz}, \\sigma_{yz} = g(x,y,z) \\end{cases} \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a\u5e94\u53d8\u6a21\u578b <code>disp_net</code>\uff0c\\(g\\) \u4e3a\u5e94\u529b\u6a21\u578b <code>stress_net</code>\uff0c\u56e0\u4e3a\u4e24\u8005\u5171\u4eab\u8f93\u5165\uff0c\u56e0\u6b64\u5728 PaddleScience \u5206\u522b\u5b9a\u4e49\u8fd9\u4e24\u4e2a\u7f51\u7edc\u6a21\u578b\u540e\uff0c\u518d\u4f7f\u7528 <code>ppsci.arch.ModelList</code> \u8fdb\u884c\u5c01\u88c5\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> <pre><code># set model\ndisp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\nstress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n# wrap to a model_list\nmodel_list = ppsci.arch.ModelList((disp_net, stress_net))\n</code></pre>"},{"location":"zh/examples/control_arm/#312","title":"3.1.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7ebf\u5f39\u6027\u65b9\u7a0b\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>LinearElasticity</code> \u5373\u53ef\u3002</p> <pre><code># set equation\nequation = {\n    \"LinearElasticity\": ppsci.equation.LinearElasticity(\n        E=None, nu=None, lambda_=LAMBDA_, mu=MU, dim=3\n    )\n}\n</code></pre>"},{"location":"zh/examples/control_arm/#313","title":"3.1.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u7684\u51e0\u4f55\u533a\u57df\u7531 stl \u6587\u4ef6\u6307\u5b9a\uff0c\u6309\u7167\u672c\u6587\u6863\u8d77\u59cb\u5904\"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\"\u4e0b\u8f7d\u5e76\u89e3\u538b\u5230 <code>./datasets/</code> \u6587\u4ef6\u5939\u4e0b\u3002</p> <p>\u6ce8\uff1a\u6570\u636e\u96c6\u4e2d\u7684 stl \u6587\u4ef6\u6765\u81ea\u7f51\u7edc\u3002</p> \u6ce8\u610f <p>\u4f7f\u7528 <code>Mesh</code> \u7c7b\u4e4b\u524d\uff0c\u5fc5\u987b\u5148\u6309\u71671.4.2 \u989d\u5916\u4f9d\u8d56\u5b89\u88c5[\u53ef\u9009]\u6587\u6863\uff0c\u5b89\u88c5\u597d open3d\u3001pysdf\u3001PyMesh 3 \u4e2a\u51e0\u4f55\u4f9d\u8d56\u5305\u3002</p> <p>\u7136\u540e\u901a\u8fc7 PaddleScience \u5185\u7f6e\u7684 STL \u51e0\u4f55\u7c7b <code>ppsci.geometry.Mesh</code> \u5373\u53ef\u8bfb\u53d6\u3001\u89e3\u6790\u51e0\u4f55\u6587\u4ef6\uff0c\u5f97\u5230\u8ba1\u7b97\u57df\uff0c\u5e76\u83b7\u53d6\u51e0\u4f55\u7ed3\u6784\u8fb9\u754c\uff1a</p> <pre><code># set geometry\ncontrol_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\ngeom = {\"geo\": control_arm}\n# set bounds\nBOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n</code></pre>"},{"location":"zh/examples/control_arm/#314","title":"3.1.4 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 2000 \u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u6bcf\u8f6e\u8fdb\u884c 1000 \u6b65\u4f18\u5316\u3002</p> <pre><code># training settings\nTRAIN:\n</code></pre>"},{"location":"zh/examples/control_arm/#315","title":"3.1.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u914d\u5408\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u5e38\u7528\u7684 ExponentialDecay \u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u3002</p> <pre><code># set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n    **cfg.TRAIN.lr_scheduler\n)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model_list)\n</code></pre>"},{"location":"zh/examples/control_arm/#316","title":"3.1.6 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u5171\u6d89\u53ca\u5230 4 \u4e2a\u7ea6\u675f\uff0c\u5206\u522b\u4e3a\u5de6\u4fa7\u5706\u73af\u5185\u8868\u9762\u53d7\u529b\u7684\u7ea6\u675f\u3001\u53f3\u4fa7\u4e24\u5706\u73af\u5185\u8868\u9762\u56fa\u5b9a\u7684\u7ea6\u675f\u3001\u7ed3\u6784\u8868\u9762\u8fb9\u754c\u6761\u4ef6\u7684\u7ea6\u675f\u548c\u7ed3\u6784\u5185\u90e8\u70b9\u7684\u7ea6\u675f\u3002\u5728\u5177\u4f53\u7ea6\u675f\u6784\u5efa\u4e4b\u524d\uff0c\u53ef\u4ee5\u5148\u6784\u5efa\u6570\u636e\u8bfb\u53d6\u914d\u7f6e\uff0c\u4ee5\u4fbf\u540e\u7eed\u6784\u5efa\u591a\u4e2a\u7ea6\u675f\u65f6\u590d\u7528\u8be5\u914d\u7f6e\u3002</p> <pre><code># set dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": \"NamedArrayDataset\",\n    \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"num_workers\": 1,\n}\n</code></pre>"},{"location":"zh/examples/control_arm/#3161","title":"3.1.6.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u7ed3\u6784\u5185\u90e8\u70b9\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>arm_interior_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"LinearElasticity\"].equations,\n    {\n        \"equilibrium_x\": 0,\n        \"equilibrium_y\": 0,\n        \"equilibrium_z\": 0,\n        \"stress_disp_xx\": 0,\n        \"stress_disp_yy\": 0,\n        \"stress_disp_zz\": 0,\n        \"stress_disp_xy\": 0,\n        \"stress_disp_xz\": 0,\n        \"stress_disp_yz\": 0,\n    },\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.arm_interior},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: (\n        (BOUNDS_X[0] &lt; x)\n        &amp; (x &lt; BOUNDS_X[1])\n        &amp; (BOUNDS_Y[0] &lt; y)\n        &amp; (y &lt; BOUNDS_Y[1])\n        &amp; (BOUNDS_Z[0] &lt; z)\n        &amp; (z &lt; BOUNDS_Z[1])\n    ),\n    weight_dict={\n        \"equilibrium_x\": \"sdf\",\n        \"equilibrium_y\": \"sdf\",\n        \"equilibrium_z\": \"sdf\",\n        \"stress_disp_xx\": \"sdf\",\n        \"stress_disp_yy\": \"sdf\",\n        \"stress_disp_zz\": \"sdf\",\n        \"stress_disp_xy\": \"sdf\",\n        \"stress_disp_xz\": \"sdf\",\n        \"stress_disp_yz\": \"sdf\",\n    },\n    name=\"INTERIOR\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\uff08\u7ec4\uff09\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.1.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"LinearElasticity\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u5e0c\u671b\u4e0e LinearElasticity \u65b9\u7a0b\u76f8\u5173\u7684 9 \u4e2a\u503c <code>equilibrium_x</code>, <code>equilibrium_y</code>, <code>equilibrium_z</code>, <code>stress_disp_xx</code>, <code>stress_disp_yy</code>, <code>stress_disp_zz</code>, <code>stress_disp_xy</code>, <code>stress_disp_xz</code>, <code>stress_disp_yz</code> \u5747\u88ab\u4f18\u5316\u81f3 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.1.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"geo\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u8bbe\u7f6e <code>batch_size</code> \u4e3a\uff1a</p> <pre><code>arm_right: 256\n</code></pre> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u9009\u7528\u5e38\u7528\u7684 MSE \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"sum\"</code>\uff0c\u5373\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u51e0\u4f55\u70b9\u7b5b\u9009\uff0c\u9700\u8981\u5bf9 geo \u4e0a\u91c7\u6837\u51fa\u7684\u70b9\u8fdb\u884c\u7b5b\u9009\uff0c\u6b64\u5904\u4f20\u5165\u4e00\u4e2a lambda \u7b5b\u9009\u51fd\u6570\u5373\u53ef\uff0c\u5176\u63a5\u53d7\u70b9\u96c6\u6784\u6210\u7684\u5f20\u91cf <code>x, y, z</code>\uff0c\u8fd4\u56de\u5e03\u5c14\u503c\u5f20\u91cf\uff0c\u8868\u793a\u6bcf\u4e2a\u70b9\u662f\u5426\u7b26\u5408\u7b5b\u9009\u6761\u4ef6\uff0c\u4e0d\u7b26\u5408\u4e3a <code>False</code>\uff0c\u7b26\u5408\u4e3a <code>True</code>\uff0c\u56e0\u4e3a\u672c\u6848\u4f8b\u7ed3\u6784\u6765\u6e90\u4e8e\u7f51\u7edc\uff0c\u53c2\u6570\u4e0d\u5b8c\u5168\u7cbe\u786e\uff0c\u56e0\u6b64\u589e\u52a0 <code>1e-1</code> \u4f5c\u4e3a\u53ef\u5bb9\u5fcd\u7684\u91c7\u6837\u8bef\u5dee\u3002</p> <p>\u7b2c\u4e03\u4e2a\u53c2\u6570\u662f\u6bcf\u4e2a\u70b9\u53c2\u4e0e\u635f\u5931\u8ba1\u7b97\u65f6\u7684\u6743\u91cd\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528 <code>\"sdf\"</code> \u8868\u793a\u4f7f\u7528\u6bcf\u4e2a\u70b9\u5230\u8fb9\u754c\u7684\u6700\u77ed\u8ddd\u79bb\uff08\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u503c\uff09\u6765\u4f5c\u4e3a\u6743\u91cd\uff0c\u8fd9\u79cd sdf \u52a0\u6743\u7684\u65b9\u6cd5\u53ef\u4ee5\u52a0\u5927\u8fdc\u79bb\u8fb9\u754c\uff08\u96be\u6837\u672c\uff09\u70b9\u7684\u6743\u91cd\uff0c\u51cf\u5c11\u9760\u8fd1\u8fb9\u754c\u7684\uff08\u7b80\u5355\u6837\u672c\uff09\u70b9\u7684\u6743\u91cd\uff0c\u6709\u5229\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u3002</p> <p>\u7b2c\u516b\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"INTERIOR\" \u5373\u53ef\u3002</p>"},{"location":"zh/examples/control_arm/#3162","title":"3.1.6.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u7ed3\u6784\u5de6\u4fa7\u5706\u73af\u5185\u8868\u9762\u53d7\u529b\uff0c\u5176\u4e0a\u7684\u6bcf\u4e2a\u70b9\u53d7\u5230\u5747\u5300\u5206\u5e03\u7684\u8f7d\u8377\uff0c\u53c2\u7167 2. \u95ee\u9898\u5b9a\u4e49\uff0c\u5927\u5c0f\u5b58\u653e\u5728\u53c2\u6570 \\(T\\) \u4e2d\u3002\u5b9e\u9645\u4e0a\u4e3a x \u8d1f\u65b9\u5411\u7684\u8f7d\u8377\uff0c\u5927\u5c0f\u4e3a \\(0.0025\\)\uff0c\u5176\u4f59\u65b9\u5411\u5e94\u529b\u4e3a 0\uff0c\u6709\u5982\u4e0b\u8fb9\u754c\u6761\u4ef6\u7ea6\u675f\uff1a</p> <pre><code>arm_left_constraint = ppsci.constraint.BoundaryConstraint(\n    equation[\"LinearElasticity\"].equations,\n    {\"traction_x\": cfg.T[0], \"traction_y\": cfg.T[1], \"traction_z\": cfg.T[2]},\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.arm_left},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: np.sqrt(\n        np.square(x - cfg.CIRCLE_LEFT_CENTER_XY[0])\n        + np.square(y - cfg.CIRCLE_LEFT_CENTER_XY[1])\n    )\n    &lt;= cfg.CIRCLE_LEFT_RADIUS + 1e-1,\n    name=\"BC_LEFT\",\n)\n</code></pre> <p>\u7ed3\u6784\u53f3\u4fa7\u4e24\u5706\u73af\u5185\u8868\u9762\u56fa\u5b9a\uff0c\u6240\u4ee5\u5176\u4e0a\u7684\u70b9\u5728\u4e09\u4e2a\u65b9\u5411\u7684\u5f62\u53d8\u5747\u4e3a 0\uff0c\u56e0\u6b64\u6709\u5982\u4e0b\u7684\u8fb9\u754c\u7ea6\u675f\u6761\u4ef6\uff1a</p> <pre><code>arm_right_constraint = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"w\": lambda d: d[\"w\"]},\n    {\"u\": 0, \"v\": 0, \"w\": 0},\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.arm_right},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: np.sqrt(\n        np.square(x - cfg.CIRCLE_RIGHT_CENTER_XZ[0])\n        + np.square(z - cfg.CIRCLE_RIGHT_CENTER_XZ[1])\n    )\n    &lt;= cfg.CIRCLE_RIGHT_RADIUS + 1e-1,\n    weight_dict=cfg.TRAIN.weight.arm_right,\n    name=\"BC_RIGHT\",\n)\n</code></pre> <p>\u7ed3\u6784\u8868\u9762\u4e0d\u53d7\u4efb\u4f55\u8f7d\u8377\uff0c\u5373\u4e09\u4e2a\u65b9\u5411\u7684\u5185\u529b\u5e73\u8861\uff0c\u5408\u529b\u4e3a 0\uff0c\u6709\u5982\u4e0b\u8fb9\u754c\u6761\u4ef6\u7ea6\u675f\uff1a</p> <pre><code>arm_surface_constraint = ppsci.constraint.BoundaryConstraint(\n    equation[\"LinearElasticity\"].equations,\n    {\"traction_x\": 0, \"traction_y\": 0, \"traction_z\": 0},\n    geom[\"geo\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.arm_surface},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: np.sqrt(\n        np.square(x - cfg.CIRCLE_LEFT_CENTER_XY[0])\n        + np.square(y - cfg.CIRCLE_LEFT_CENTER_XY[1])\n    )\n    &gt; cfg.CIRCLE_LEFT_RADIUS + 1e-1,\n    name=\"BC_SURFACE\",\n)\n</code></pre> <p>\u5728\u65b9\u7a0b\u7ea6\u675f\u3001\u8fb9\u754c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints togetherg\nconstraint = {\n    arm_left_constraint.name: arm_left_constraint,\n    arm_right_constraint.name: arm_right_constraint,\n    arm_surface_constraint.name: arm_surface_constraint,\n    arm_interior_constraint.name: arm_interior_constraint,\n}\n</code></pre>"},{"location":"zh/examples/control_arm/#317","title":"3.1.7 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u53ef\u89c6\u5316\u5668\u7684\u8f93\u5165\u6570\u636e\u901a\u8fc7\u8c03\u7528 PaddleScience \u7684 API <code>sample_interior</code> \u4ea7\u751f\uff0c\u8f93\u51fa\u6570\u636e\u662f\u5bf9\u5e94\u7684 9 \u4e2a\u9884\u6d4b\u7684\u7269\u7406\u91cf\uff0c\u901a\u8fc7\u8bbe\u7f6e <code>ppsci.visualize.VisualizerVtu</code> \uff0c\u53ef\u4ee5\u5c06\u8bc4\u4f30\u7684\u8f93\u51fa\u6570\u636e\u4fdd\u5b58\u6210 vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u6700\u540e\u7528\u53ef\u89c6\u5316\u8f6f\u4ef6\u6253\u5f00\u67e5\u770b\u5373\u53ef\u3002</p> <pre><code># set visualizer(optional)\n# add inferencer data\nsamples = geom[\"geo\"].sample_interior(\n    cfg.TRAIN.batch_size.visualizer_vtu,\n    criteria=lambda x, y, z: (\n        (BOUNDS_X[0] &lt; x)\n        &amp; (x &lt; BOUNDS_X[1])\n        &amp; (BOUNDS_Y[0] &lt; y)\n        &amp; (y &lt; BOUNDS_Y[1])\n        &amp; (BOUNDS_Z[0] &lt; z)\n        &amp; (z &lt; BOUNDS_Z[1])\n    ),\n)\npred_input_dict = {\n    k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n}\nvisualizer = {\n    \"visulzie_u_v_w_sigmas\": ppsci.visualize.VisualizerVtu(\n        pred_input_dict,\n        {\n            \"u\": lambda out: out[\"u\"],\n            \"v\": lambda out: out[\"v\"],\n            \"w\": lambda out: out[\"w\"],\n            \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n            \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n            \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n            \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n            \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n            \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n        },\n        prefix=\"vis\",\n    )\n}\n</code></pre>"},{"location":"zh/examples/control_arm/#318","title":"3.1.8 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model_list,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    seed=cfg.seed,\n    equation=equation,\n    geom=geom,\n    save_freq=cfg.TRAIN.save_freq,\n    log_freq=cfg.log_freq,\n    eval_freq=cfg.TRAIN.eval_freq,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_with_no_grad=cfg.TRAIN.eval_with_no_grad,\n    visualizer=visualizer,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n)\n\n# train model\nsolver.train()\n</code></pre> <p>\u8bad\u7ec3\u540e\u8c03\u7528 <code>ppsci.solver.Solver.plot_loss_history</code> \u53ef\u4ee5\u5c06\u8bad\u7ec3\u4e2d\u7684 <code>loss</code> \u753b\u51fa\uff1a</p> <pre><code># plot losses\nsolver.plot_loss_history(by_epoch=True, smooth_step=1)\n</code></pre> <p>\u53e6\u5916\u672c\u6848\u4f8b\u4e2d\u63d0\u4f9b\u4e86\u5e76\u884c\u8bad\u7ec3\u7684\u8bbe\u7f6e\uff0c\u6ce8\u610f\u6253\u5f00\u6570\u636e\u5e76\u884c\u540e\uff0c\u5e94\u8be5\u5c06\u5b66\u4e60\u7387\u5e94\u8be5\u589e\u5927\u4e3a <code>\u539f\u59cb\u5b66\u4e60\u7387*\u5e76\u884c\u5361\u6570</code>\uff0c\u4ee5\u4fdd\u8bc1\u8bad\u7ec3\u6548\u679c\u3002\u5177\u4f53\u7ec6\u8282\u8bf7\u53c2\u8003 \u4f7f\u7528\u6307\u5357 2.1.1 \u6570\u636e\u5e76\u884c\u3002</p> <pre><code># set parallel\nenable_parallel = dist.get_world_size() &gt; 1\n</code></pre> <pre><code># re-assign to cfg.TRAIN.iters_per_epoch\nif enable_parallel:\n    cfg.TRAIN.iters_per_epoch = len(arm_left_constraint.data_loader)\n</code></pre>"},{"location":"zh/examples/control_arm/#319","title":"3.1.9 \u6a21\u578b\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u8bad\u7ec3\u5b8c\u6210\u6216\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u540e\uff0c\u901a\u8fc7\u672c\u6587\u6863\u8d77\u59cb\u5904\u201c\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u201d\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u3002</p> <p>\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u8fc7\u7a0b\u4e0d\u9700\u8981\u8fdb\u884c\u4f18\u5316\u5668\u7b49\u6784\u5efa\uff0c\u4ec5\u9700\u6784\u5efa\u6a21\u578b\u3001\u8ba1\u7b97\u57df\u3001\u8bc4\u4f30\u5668\uff08\u672c\u6848\u4f8b\u4e0d\u5305\u62ec\uff09\u3001\u53ef\u89c6\u5316\u5668\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code> \u542f\u52a8\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u3002</p> <pre><code>def evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    # wrap to a model_list\n    model_list = ppsci.arch.ModelList((disp_net, stress_net))\n\n    # set geometry\n    control_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\n    # geometry bool operation\n    geo = control_arm\n    geom = {\"geo\": geo}\n    # set bounds\n    BOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n\n    # set visualizer(optional)\n    # add inferencer data\n    samples = geom[\"geo\"].sample_interior(\n        cfg.TRAIN.batch_size.visualizer_vtu,\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n    )\n    pred_input_dict = {\n        k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n    }\n    visualizer = {\n        \"visulzie_u_v_w_sigmas\": ppsci.visualize.VisualizerVtu(\n            pred_input_dict,\n            {\n                \"u\": lambda out: out[\"u\"],\n                \"v\": lambda out: out[\"v\"],\n                \"w\": lambda out: out[\"w\"],\n                \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n                \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n                \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n                \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n                \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n                \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n            },\n            prefix=\"vis\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        geom=geom,\n        log_freq=cfg.log_freq,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n\n    # visualize prediction after finished training\n    solver.visualize()\n</code></pre>"},{"location":"zh/examples/control_arm/#32","title":"3.2 \u53c2\u6570\u9006\u63a8\u6c42\u89e3","text":""},{"location":"zh/examples/control_arm/#321","title":"3.2.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u8fdb\u884c\u53c2\u6570\u9006\u63a8\u7684\u524d\u63d0\u662f\u9700\u8981\u77e5\u9053\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y, z)\\) \uff0c\u4ee5\u53ca\u5bf9\u5e94\u7684\u4e09\u4e2a\u65b9\u5411\u7684\u5e94\u53d8 \\((u, v, w)\\) \u548c\u5e94\u529b \\((\\sigma_{xx}, \\sigma_{yy}, \\sigma_{zz}, \\sigma_{xy}, \\sigma_{xz}, \\sigma_{yz})\\)\u3002\u8fd9\u4e9b\u53d8\u91cf\u7684\u6765\u6e90\u53ef\u4ee5\u662f\u771f\u5b9e\u6570\u636e\uff0c\u6216\u6570\u503c\u6a21\u62df\u6570\u636e\uff0c\u6216\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6b63\u95ee\u9898\u6a21\u578b\u3002\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u4e0d\u4f7f\u7528\u4efb\u4f55\u6570\u636e\uff0c\u800c\u662f\u4f7f\u7528 3.1 \u53d7\u529b\u5206\u6790\u6c42\u89e3 \u7ae0\u8282\u4e2d\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u6765\u83b7\u53d6\u8fd9\u4e9b\u53d8\u91cf\uff0c\u56e0\u6b64\u4ecd\u7136\u9700\u8981\u6784\u5efa\u8fd9\u90e8\u5206\u6a21\u578b\uff0c\u5e76\u4e3a <code>disp_net</code> \u548c <code>stress_net</code> \u52a0\u8f7d\u6b63\u95ee\u9898\u6c42\u89e3\u5f97\u5230\u7684\u6743\u91cd\u53c2\u6570\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6ce8\u610f\u5c06\u8fd9\u4e24\u4e2a\u6a21\u578b\u51bb\u7ed3\uff0c\u4ee5\u51cf\u5c11\u53cd\u5411\u4f20\u64ad\u65f6\u95f4\u548c\u5185\u5b58\u5360\u7528\u3002</p> <p>\u53c2\u6570\u9006\u63a8\u4e2d\u9700\u8981\u6c42\u89e3\u4e24\u4e2a\u672a\u77e5\u91cf\uff1a\u7ebf\u5f39\u6027\u65b9\u7a0b\u7684\u53c2\u6570 \\(\\lambda\\) \u548c \\(\\mu\\)\uff0c\u4f7f\u7528\u4e24\u4e2a\u6a21\u578b\u6765\u5206\u522b\u9884\u6d4b\u8fd9\u4e24\u7ec4\u7269\u7406\u91cf\uff1a</p> \\[ \\begin{cases} \\lambda = f(x,y,z) \\\\ \\mu = g(x,y,z) \\end{cases} \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a\u6c42\u89e3 \\(\\lambda\\) \u7684\u6a21\u578b <code>inverse_lambda_net</code>\uff0c\\(g\\) \u4e3a\u6c42\u89e3 \\(\\mu\\) \u6a21\u578b <code>inverse_mu_net</code>\u3002</p> <p>\u56e0\u4e3a\u4e0a\u8ff0\u4e24\u4e2a\u6a21\u578b\u4e0e<code>disp_net</code> \u548c <code>stress_net</code> \u5171\u56db\u4e2a\u6a21\u578b\u5171\u4eab\u8f93\u5165\uff0c\u56e0\u6b64\u5728 PaddleScience \u5206\u522b\u5b9a\u4e49\u8fd9\u56db\u4e2a\u7f51\u7edc\u6a21\u578b\u540e\uff0c\u518d\u4f7f\u7528 <code>ppsci.arch.ModelList</code> \u8fdb\u884c\u5c01\u88c5\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> <pre><code># set model\ndisp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\nstress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\ninverse_lambda_net = ppsci.arch.MLP(**cfg.MODEL.inverse_lambda_net)\ninverse_mu_net = ppsci.arch.MLP(**cfg.MODEL.inverse_mu_net)\n# freeze models\ndisp_net.freeze()\nstress_net.freeze()\n# wrap to a model_list\nmodel = ppsci.arch.ModelList(\n    (disp_net, stress_net, inverse_lambda_net, inverse_mu_net)\n)\n</code></pre>"},{"location":"zh/examples/control_arm/#322","title":"3.2.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7ebf\u5f39\u6027\u65b9\u7a0b\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>LinearElasticity</code> \u5373\u53ef\u3002</p> <pre><code># set equation\nequation = {\n    \"LinearElasticity\": ppsci.equation.LinearElasticity(\n        E=None, nu=None, lambda_=\"lambda_\", mu=\"mu\", dim=3\n    )\n}\n</code></pre>"},{"location":"zh/examples/control_arm/#323","title":"3.2.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u7684\u51e0\u4f55\u533a\u57df\u7531 stl \u6587\u4ef6\u6307\u5b9a\uff0c\u6309\u7167\u672c\u6587\u6863\u8d77\u59cb\u5904\"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\"\u4e0b\u8f7d\u5e76\u89e3\u538b\u5230 <code>./datasets/</code> \u6587\u4ef6\u5939\u4e0b\u3002</p> <p>\u6ce8\uff1a\u6570\u636e\u96c6\u4e2d\u7684 stl \u6587\u4ef6\u6765\u81ea\u7f51\u7edc\u3002</p> \u6ce8\u610f <p>\u4f7f\u7528 <code>Mesh</code> \u7c7b\u4e4b\u524d\uff0c\u5fc5\u987b\u5148\u6309\u71671.4.2 \u989d\u5916\u4f9d\u8d56\u5b89\u88c5[\u53ef\u9009]\u6587\u6863\uff0c\u5b89\u88c5\u597d open3d\u3001pysdf\u3001PyMesh 3 \u4e2a\u51e0\u4f55\u4f9d\u8d56\u5305\u3002</p> <p>\u7136\u540e\u901a\u8fc7 PaddleScience \u5185\u7f6e\u7684 STL \u51e0\u4f55\u7c7b <code>ppsci.geometry.Mesh</code> \u5373\u53ef\u8bfb\u53d6\u3001\u89e3\u6790\u51e0\u4f55\u6587\u4ef6\uff0c\u5f97\u5230\u8ba1\u7b97\u57df\uff0c\u5e76\u83b7\u53d6\u51e0\u4f55\u7ed3\u6784\u8fb9\u754c\uff1a</p> <pre><code># set geometry\ncontrol_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\n# geometry bool operation\ngeo = control_arm\ngeom = {\"geo\": geo}\n# set bounds\nBOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n</code></pre>"},{"location":"zh/examples/control_arm/#324","title":"3.2.4 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 100 \u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u6bcf\u8f6e\u8fdb\u884c 100 \u6b65\u4f18\u5316\u3002</p> <pre><code># training settings\nTRAIN:\n</code></pre>"},{"location":"zh/examples/control_arm/#325","title":"3.2.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u7531\u4e8e <code>disp_net</code> \u548c <code>stress_net</code> \u6a21\u578b\u7684\u4f5c\u7528\u4ec5\u4e3a\u63d0\u4f9b\u4e09\u4e2a\u65b9\u5411\u7684\u5e94\u53d8 \\((u, v, w)\\) \u548c\u5e94\u529b \\((\\sigma_{xx}, \\sigma_{yy}, \\sigma_{zz}, \\sigma_{xy}, \\sigma_{xz}, \\sigma_{yz})\\) \u7684\u503c\uff0c\u5e76\u4e0d\u9700\u8981\u8fdb\u884c\u8bad\u7ec3\uff0c\u56e0\u6b64\u5728\u6784\u5efa\u4f18\u5316\u5668\u65f6\u9700\u8981\u6ce8\u610f\u4e0d\u8981\u4f7f\u7528 3.2.1 \u6a21\u578b\u6784\u5efa \u4e2d\u5c01\u88c5\u7684 <code>ModelList</code> \u4f5c\u4e3a\u53c2\u6570\uff0c\u800c\u662f\u4f7f\u7528 <code>inverse_lambda_net</code> \u548c <code>inverse_mu_net</code> \u7ec4\u6210\u7684\u5143\u7ec4\u4f5c\u4e3a\u53c2\u6570\u3002</p> <p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u914d\u5408\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u5e38\u7528\u7684 <code>ExponentialDecay</code> \u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u3002</p> <pre><code># set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n    **cfg.TRAIN.lr_scheduler\n)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)((inverse_lambda_net, inverse_mu_net))\n</code></pre>"},{"location":"zh/examples/control_arm/#326","title":"3.2.6 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u5171\u6d89\u53ca\u5230 1 \u4e2a\u7ea6\u675f\uff0c\u4e3a\u7ed3\u6784\u5185\u90e8\u70b9\u7684\u7ea6\u675f <code>InteriorConstraint</code>\u3002</p> <pre><code># set dataloader config\ninterior_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"LinearElasticity\"].equations,\n    {\n        \"stress_disp_xx\": 0,\n        \"stress_disp_yy\": 0,\n        \"stress_disp_zz\": 0,\n        \"stress_disp_xy\": 0,\n        \"stress_disp_xz\": 0,\n        \"stress_disp_yz\": 0,\n    },\n    geom[\"geo\"],\n    {\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"num_workers\": 1,\n        \"batch_size\": cfg.TRAIN.batch_size.arm_interior,\n    },\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda x, y, z: (\n        (BOUNDS_X[0] &lt; x)\n        &amp; (x &lt; BOUNDS_X[1])\n        &amp; (BOUNDS_Y[0] &lt; y)\n        &amp; (y &lt; BOUNDS_Y[1])\n        &amp; (BOUNDS_Z[0] &lt; z)\n        &amp; (z &lt; BOUNDS_Z[1])\n    ),\n    name=\"INTERIOR\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\uff08\u7ec4\uff09\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"LinearElasticity\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u5e0c\u671b\u4e0e LinearElasticity \u65b9\u7a0b\u76f8\u5173\u4e14\u9971\u542b\u53c2\u6570 \\(\\lambda\\) \u548c \\(\\mu\\) \u7684 6 \u4e2a\u503c <code>stress_disp_xx</code>, <code>stress_disp_yy</code>, <code>stress_disp_zz</code>, <code>stress_disp_xy</code>, <code>stress_disp_xz</code>, <code>stress_disp_yz</code> \u5747\u88ab\u4f18\u5316\u81f3 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"geo\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u8bbe\u7f6e <code>batch_size</code> \u4e3a\uff1a</p> <pre><code>by_epoch: false\n</code></pre> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u9009\u7528\u5e38\u7528\u7684 MSE \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"sum\"</code>\uff0c\u5373\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u51e0\u4f55\u70b9\u7b5b\u9009\uff0c\u9700\u8981\u5bf9 geo \u4e0a\u91c7\u6837\u51fa\u7684\u70b9\u8fdb\u884c\u7b5b\u9009\uff0c\u6b64\u5904\u4f20\u5165\u4e00\u4e2a lambda \u7b5b\u9009\u51fd\u6570\u5373\u53ef\uff0c\u5176\u63a5\u53d7\u70b9\u96c6\u6784\u6210\u7684\u5f20\u91cf <code>x, y, z</code>\uff0c\u8fd4\u56de\u5e03\u5c14\u503c\u5f20\u91cf\uff0c\u8868\u793a\u6bcf\u4e2a\u70b9\u662f\u5426\u7b26\u5408\u7b5b\u9009\u6761\u4ef6\uff0c\u4e0d\u7b26\u5408\u4e3a <code>False</code>\uff0c\u7b26\u5408\u4e3a <code>True</code>\uff1b</p> <p>\u7b2c\u4e03\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"INTERIOR\" \u5373\u53ef\u3002</p> <p>\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code>constraint = {interior_constraint.name: interior_constraint}\n</code></pre>"},{"location":"zh/examples/control_arm/#327","title":"3.2.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u7531\u4e8e\u6211\u4eec\u4f7f\u7528\u6b63\u95ee\u9898\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u4f9b\u6570\u636e\uff0c\u56e0\u6b64\u5df2\u77e5 <code>label</code> \u7684\u503c\u7ea6\u4e3a \\(\\lambda=0.57692\\) \u548c \\(\\mu=0.38462\\)\u3002\u5c06\u5176\u5305\u88c5\u6210\u5b57\u5178\u4f20\u9012\u7ed9 <code>ppsci.validate.GeometryValidator</code> \u6784\u9020\u8bc4\u4f30\u5668\u5e76\u5c01\u88c5\u3002</p> <pre><code># set validator\nLAMBDA_ = cfg.NU * cfg.E / ((1 + cfg.NU) * (1 - 2 * cfg.NU))  # 0.5769\nMU = cfg.E / (2 * (1 + cfg.NU))  # 0.3846\ngeom_validator = ppsci.validate.GeometryValidator(\n    {\n        \"lambda_\": lambda out: out[\"lambda_\"],\n        \"mu\": lambda out: out[\"mu\"],\n    },\n    {\n        \"lambda_\": LAMBDA_,\n        \"mu\": MU,\n    },\n    geom[\"geo\"],\n    {\n        \"dataset\": \"NamedArrayDataset\",\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"total_size\": cfg.EVAL.total_size.validator,\n        \"batch_size\": cfg.EVAL.batch_size.validator,\n    },\n    ppsci.loss.MSELoss(\"sum\"),\n    metric={\"L2Rel\": ppsci.metric.L2Rel()},\n    name=\"geo_eval\",\n)\nvalidator = {geom_validator.name: geom_validator}\n</code></pre>"},{"location":"zh/examples/control_arm/#328","title":"3.2.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u53ef\u89c6\u5316\u5668\u7684\u8f93\u5165\u6570\u636e\u901a\u8fc7\u8c03\u7528 PaddleScience \u7684 API <code>sample_interior</code> \u4ea7\u751f\uff0c\u8f93\u51fa\u6570\u636e\u662f\\(\\lambda\\) \u548c \\(\\mu\\)\u9884\u6d4b\u7684\u7269\u7406\u91cf\uff0c\u901a\u8fc7\u8bbe\u7f6e <code>ppsci.visualize.VisualizerVtu</code> \uff0c\u53ef\u4ee5\u5c06\u8bc4\u4f30\u7684\u8f93\u51fa\u6570\u636e\u4fdd\u5b58\u6210 vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u6700\u540e\u7528\u53ef\u89c6\u5316\u8f6f\u4ef6\u6253\u5f00\u67e5\u770b\u5373\u53ef\u3002</p> <pre><code># set visualizer(optional)\n# add inferencer data\nsamples = geom[\"geo\"].sample_interior(\n    cfg.TRAIN.batch_size.visualizer_vtu,\n    criteria=lambda x, y, z: (\n        (BOUNDS_X[0] &lt; x)\n        &amp; (x &lt; BOUNDS_X[1])\n        &amp; (BOUNDS_Y[0] &lt; y)\n        &amp; (y &lt; BOUNDS_Y[1])\n        &amp; (BOUNDS_Z[0] &lt; z)\n        &amp; (z &lt; BOUNDS_Z[1])\n    ),\n)\npred_input_dict = {\n    k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n}\nvisualizer = {\n    \"visulzie_lambda_mu\": ppsci.visualize.VisualizerVtu(\n        pred_input_dict,\n        {\n            \"lambda\": lambda out: out[\"lambda_\"],\n            \"mu\": lambda out: out[\"mu\"],\n        },\n        prefix=\"vis\",\n    )\n}\n</code></pre>"},{"location":"zh/examples/control_arm/#329","title":"3.2.9 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    seed=cfg.seed,\n    equation=equation,\n    geom=geom,\n    save_freq=cfg.TRAIN.save_freq,\n    log_freq=cfg.log_freq,\n    eval_freq=cfg.TRAIN.eval_freq,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_with_no_grad=cfg.TRAIN.eval_with_no_grad,\n    validator=validator,\n    visualizer=visualizer,\n    pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n)\n\n# train model\nsolver.train()\n</code></pre> <p>\u8bad\u7ec3\u540e\u8c03\u7528 <code>ppsci.solver.Solver.plot_loss_history</code> \u53ef\u4ee5\u5c06\u8bad\u7ec3\u4e2d\u7684 <code>loss</code> \u753b\u51fa\uff1a</p> <pre><code># plot losses\nsolver.plot_loss_history(by_epoch=False, smooth_step=1, use_semilogy=True)\n</code></pre>"},{"location":"zh/examples/control_arm/#3210","title":"3.2.10 \u6a21\u578b\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u8bad\u7ec3\u5b8c\u6210\u6216\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u540e\uff0c\u901a\u8fc7\u672c\u6587\u6863\u8d77\u59cb\u5904\u201c\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u201d\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u3002</p> <p>\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u8fc7\u7a0b\u4e0d\u9700\u8981\u8fdb\u884c\u4f18\u5316\u5668\u7b49\u6784\u5efa\uff0c\u4ec5\u9700\u6784\u5efa\u6a21\u578b\u3001\u8ba1\u7b97\u57df\u3001\u8bc4\u4f30\u5668\uff08\u672c\u6848\u4f8b\u4e0d\u5305\u62ec\uff09\u3001\u53ef\u89c6\u5316\u5668\uff0c\u7136\u540e\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code> \u542f\u52a8\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u3002</p> <pre><code>def evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    inverse_lambda_net = ppsci.arch.MLP(**cfg.MODEL.inverse_lambda_net)\n    inverse_mu_net = ppsci.arch.MLP(**cfg.MODEL.inverse_mu_net)\n    # wrap to a model_list\n    model = ppsci.arch.ModelList(\n        (disp_net, stress_net, inverse_lambda_net, inverse_mu_net)\n    )\n\n    # set geometry\n    control_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\n    # geometry bool operation\n    geo = control_arm\n    geom = {\"geo\": geo}\n    # set bounds\n    BOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n\n    # set validator\n    LAMBDA_ = cfg.NU * cfg.E / ((1 + cfg.NU) * (1 - 2 * cfg.NU))  # 0.57692\n    MU = cfg.E / (2 * (1 + cfg.NU))  # 0.38462\n    geom_validator = ppsci.validate.GeometryValidator(\n        {\n            \"lambda_\": lambda out: out[\"lambda_\"],\n            \"mu\": lambda out: out[\"mu\"],\n        },\n        {\n            \"lambda_\": LAMBDA_,\n            \"mu\": MU,\n        },\n        geom[\"geo\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n            \"total_size\": cfg.EVAL.total_size.validator,\n            \"batch_size\": cfg.EVAL.batch_size.validator,\n        },\n        ppsci.loss.MSELoss(\"sum\"),\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"geo_eval\",\n    )\n    validator = {geom_validator.name: geom_validator}\n\n    # set visualizer(optional)\n    # add inferencer data\n    samples = geom[\"geo\"].sample_interior(\n        cfg.EVAL.batch_size.visualizer_vtu,\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n    )\n    pred_input_dict = {\n        k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n    }\n    visualizer = {\n        \"visulzie_lambda_mu\": ppsci.visualize.VisualizerVtu(\n            pred_input_dict,\n            {\n                \"lambda\": lambda out: out[\"lambda_\"],\n                \"mu\": lambda out: out[\"mu\"],\n            },\n            prefix=\"vis\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        log_freq=cfg.log_freq,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n</code></pre>"},{"location":"zh/examples/control_arm/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"forward_analysis.py<pre><code>from os import path as osp\n\nimport hydra\nimport numpy as np\nfrom omegaconf import DictConfig\nfrom paddle import distributed as dist\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n    # set parallel\n    enable_parallel = dist.get_world_size() &gt; 1\n\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    # wrap to a model_list\n    model_list = ppsci.arch.ModelList((disp_net, stress_net))\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n        **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model_list)\n\n    # specify parameters\n    LAMBDA_ = cfg.NU * cfg.E / ((1 + cfg.NU) * (1 - 2 * cfg.NU))\n    MU = cfg.E / (2 * (1 + cfg.NU))\n\n    # set equation\n    equation = {\n        \"LinearElasticity\": ppsci.equation.LinearElasticity(\n            E=None, nu=None, lambda_=LAMBDA_, mu=MU, dim=3\n        )\n    }\n\n    # set geometry\n    control_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\n    geom = {\"geo\": control_arm}\n    # set bounds\n    BOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": \"NamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"num_workers\": 1,\n    }\n\n    # set constraint\n    arm_left_constraint = ppsci.constraint.BoundaryConstraint(\n        equation[\"LinearElasticity\"].equations,\n        {\"traction_x\": cfg.T[0], \"traction_y\": cfg.T[1], \"traction_z\": cfg.T[2]},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.arm_left},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: np.sqrt(\n            np.square(x - cfg.CIRCLE_LEFT_CENTER_XY[0])\n            + np.square(y - cfg.CIRCLE_LEFT_CENTER_XY[1])\n        )\n        &lt;= cfg.CIRCLE_LEFT_RADIUS + 1e-1,\n        name=\"BC_LEFT\",\n    )\n    arm_right_constraint = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"w\": lambda d: d[\"w\"]},\n        {\"u\": 0, \"v\": 0, \"w\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.arm_right},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: np.sqrt(\n            np.square(x - cfg.CIRCLE_RIGHT_CENTER_XZ[0])\n            + np.square(z - cfg.CIRCLE_RIGHT_CENTER_XZ[1])\n        )\n        &lt;= cfg.CIRCLE_RIGHT_RADIUS + 1e-1,\n        weight_dict=cfg.TRAIN.weight.arm_right,\n        name=\"BC_RIGHT\",\n    )\n    arm_surface_constraint = ppsci.constraint.BoundaryConstraint(\n        equation[\"LinearElasticity\"].equations,\n        {\"traction_x\": 0, \"traction_y\": 0, \"traction_z\": 0},\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.arm_surface},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: np.sqrt(\n            np.square(x - cfg.CIRCLE_LEFT_CENTER_XY[0])\n            + np.square(y - cfg.CIRCLE_LEFT_CENTER_XY[1])\n        )\n        &gt; cfg.CIRCLE_LEFT_RADIUS + 1e-1,\n        name=\"BC_SURFACE\",\n    )\n    arm_interior_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"LinearElasticity\"].equations,\n        {\n            \"equilibrium_x\": 0,\n            \"equilibrium_y\": 0,\n            \"equilibrium_z\": 0,\n            \"stress_disp_xx\": 0,\n            \"stress_disp_yy\": 0,\n            \"stress_disp_zz\": 0,\n            \"stress_disp_xy\": 0,\n            \"stress_disp_xz\": 0,\n            \"stress_disp_yz\": 0,\n        },\n        geom[\"geo\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.arm_interior},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n        weight_dict={\n            \"equilibrium_x\": \"sdf\",\n            \"equilibrium_y\": \"sdf\",\n            \"equilibrium_z\": \"sdf\",\n            \"stress_disp_xx\": \"sdf\",\n            \"stress_disp_yy\": \"sdf\",\n            \"stress_disp_zz\": \"sdf\",\n            \"stress_disp_xy\": \"sdf\",\n            \"stress_disp_xz\": \"sdf\",\n            \"stress_disp_yz\": \"sdf\",\n        },\n        name=\"INTERIOR\",\n    )\n\n    # re-assign to cfg.TRAIN.iters_per_epoch\n    if enable_parallel:\n        cfg.TRAIN.iters_per_epoch = len(arm_left_constraint.data_loader)\n\n    # wrap constraints togetherg\n    constraint = {\n        arm_left_constraint.name: arm_left_constraint,\n        arm_right_constraint.name: arm_right_constraint,\n        arm_surface_constraint.name: arm_surface_constraint,\n        arm_interior_constraint.name: arm_interior_constraint,\n    }\n\n    # set visualizer(optional)\n    # add inferencer data\n    samples = geom[\"geo\"].sample_interior(\n        cfg.TRAIN.batch_size.visualizer_vtu,\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n    )\n    pred_input_dict = {\n        k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n    }\n    visualizer = {\n        \"visulzie_u_v_w_sigmas\": ppsci.visualize.VisualizerVtu(\n            pred_input_dict,\n            {\n                \"u\": lambda out: out[\"u\"],\n                \"v\": lambda out: out[\"v\"],\n                \"w\": lambda out: out[\"w\"],\n                \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n                \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n                \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n                \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n                \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n                \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n            },\n            prefix=\"vis\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        eval_freq=cfg.TRAIN.eval_freq,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_with_no_grad=cfg.TRAIN.eval_with_no_grad,\n        visualizer=visualizer,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n    )\n\n    # train model\n    solver.train()\n\n    # plot losses\n    solver.plot_loss_history(by_epoch=True, smooth_step=1)\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    # wrap to a model_list\n    model_list = ppsci.arch.ModelList((disp_net, stress_net))\n\n    # set geometry\n    control_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\n    # geometry bool operation\n    geo = control_arm\n    geom = {\"geo\": geo}\n    # set bounds\n    BOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n\n    # set visualizer(optional)\n    # add inferencer data\n    samples = geom[\"geo\"].sample_interior(\n        cfg.TRAIN.batch_size.visualizer_vtu,\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n    )\n    pred_input_dict = {\n        k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n    }\n    visualizer = {\n        \"visulzie_u_v_w_sigmas\": ppsci.visualize.VisualizerVtu(\n            pred_input_dict,\n            {\n                \"u\": lambda out: out[\"u\"],\n                \"v\": lambda out: out[\"v\"],\n                \"w\": lambda out: out[\"w\"],\n                \"sigma_xx\": lambda out: out[\"sigma_xx\"],\n                \"sigma_yy\": lambda out: out[\"sigma_yy\"],\n                \"sigma_zz\": lambda out: out[\"sigma_zz\"],\n                \"sigma_xy\": lambda out: out[\"sigma_xy\"],\n                \"sigma_xz\": lambda out: out[\"sigma_xz\"],\n                \"sigma_yz\": lambda out: out[\"sigma_yz\"],\n            },\n            prefix=\"vis\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        geom=geom,\n        log_freq=cfg.log_freq,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    from paddle.static import InputSpec\n\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    # wrap to a model_list\n    model_list = ppsci.arch.ModelList((disp_net, stress_net))\n\n    # load pretrained model\n    solver = ppsci.solver.Solver(\n        model=model_list, pretrained_model_path=cfg.INFER.pretrained_model_path\n    )\n\n    # export models\n    input_spec = [\n        {\n            key: InputSpec([None, 1], \"float32\", name=key)\n            for key in cfg.MODEL.disp_net.input_keys\n        },\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n    from ppsci.visualize import vtu\n\n    # set model predictor\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # set geometry\n    control_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\n    # geometry bool operation\n    geo = control_arm\n    geom = {\"geo\": geo}\n    # set bounds\n    BOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n\n    # set visualizer(optional)\n    # add inferencer data\n    samples = geom[\"geo\"].sample_interior(\n        cfg.TRAIN.batch_size.visualizer_vtu,\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n    )\n    pred_input_dict = {\n        k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n    }\n\n    output_dict = predictor.predict(pred_input_dict, cfg.INFER.batch_size)\n\n    # mapping data to output_keys\n    output_keys = cfg.MODEL.disp_net.output_keys + cfg.MODEL.stress_net.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(output_keys, output_dict.keys())\n    }\n    output_dict.update(pred_input_dict)\n\n    vtu.save_vtu_from_dict(\n        osp.join(cfg.output_dir, \"vis\"),\n        output_dict,\n        cfg.MODEL.disp_net.input_keys,\n        output_keys,\n        1,\n    )\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"forward_analysis.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> inverse_parameter.py<pre><code>from os import path as osp\n\nimport hydra\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    inverse_lambda_net = ppsci.arch.MLP(**cfg.MODEL.inverse_lambda_net)\n    inverse_mu_net = ppsci.arch.MLP(**cfg.MODEL.inverse_mu_net)\n    # freeze models\n    disp_net.freeze()\n    stress_net.freeze()\n    # wrap to a model_list\n    model = ppsci.arch.ModelList(\n        (disp_net, stress_net, inverse_lambda_net, inverse_mu_net)\n    )\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n        **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)((inverse_lambda_net, inverse_mu_net))\n\n    # set equation\n    equation = {\n        \"LinearElasticity\": ppsci.equation.LinearElasticity(\n            E=None, nu=None, lambda_=\"lambda_\", mu=\"mu\", dim=3\n        )\n    }\n\n    # set geometry\n    control_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\n    # geometry bool operation\n    geo = control_arm\n    geom = {\"geo\": geo}\n    # set bounds\n    BOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n\n    # set dataloader config\n    interior_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"LinearElasticity\"].equations,\n        {\n            \"stress_disp_xx\": 0,\n            \"stress_disp_yy\": 0,\n            \"stress_disp_zz\": 0,\n            \"stress_disp_xy\": 0,\n            \"stress_disp_xz\": 0,\n            \"stress_disp_yz\": 0,\n        },\n        geom[\"geo\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": True,\n                \"shuffle\": True,\n            },\n            \"num_workers\": 1,\n            \"batch_size\": cfg.TRAIN.batch_size.arm_interior,\n        },\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n        name=\"INTERIOR\",\n    )\n    constraint = {interior_constraint.name: interior_constraint}\n\n    # set validator\n    LAMBDA_ = cfg.NU * cfg.E / ((1 + cfg.NU) * (1 - 2 * cfg.NU))  # 0.5769\n    MU = cfg.E / (2 * (1 + cfg.NU))  # 0.3846\n    geom_validator = ppsci.validate.GeometryValidator(\n        {\n            \"lambda_\": lambda out: out[\"lambda_\"],\n            \"mu\": lambda out: out[\"mu\"],\n        },\n        {\n            \"lambda_\": LAMBDA_,\n            \"mu\": MU,\n        },\n        geom[\"geo\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n            \"total_size\": cfg.EVAL.total_size.validator,\n            \"batch_size\": cfg.EVAL.batch_size.validator,\n        },\n        ppsci.loss.MSELoss(\"sum\"),\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"geo_eval\",\n    )\n    validator = {geom_validator.name: geom_validator}\n\n    # set visualizer(optional)\n    # add inferencer data\n    samples = geom[\"geo\"].sample_interior(\n        cfg.TRAIN.batch_size.visualizer_vtu,\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n    )\n    pred_input_dict = {\n        k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n    }\n    visualizer = {\n        \"visulzie_lambda_mu\": ppsci.visualize.VisualizerVtu(\n            pred_input_dict,\n            {\n                \"lambda\": lambda out: out[\"lambda_\"],\n                \"mu\": lambda out: out[\"mu\"],\n            },\n            prefix=\"vis\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        eval_freq=cfg.TRAIN.eval_freq,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_with_no_grad=cfg.TRAIN.eval_with_no_grad,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n    )\n\n    # train model\n    solver.train()\n\n    # plot losses\n    solver.plot_loss_history(by_epoch=False, smooth_step=1, use_semilogy=True)\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    inverse_lambda_net = ppsci.arch.MLP(**cfg.MODEL.inverse_lambda_net)\n    inverse_mu_net = ppsci.arch.MLP(**cfg.MODEL.inverse_mu_net)\n    # wrap to a model_list\n    model = ppsci.arch.ModelList(\n        (disp_net, stress_net, inverse_lambda_net, inverse_mu_net)\n    )\n\n    # set geometry\n    control_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\n    # geometry bool operation\n    geo = control_arm\n    geom = {\"geo\": geo}\n    # set bounds\n    BOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n\n    # set validator\n    LAMBDA_ = cfg.NU * cfg.E / ((1 + cfg.NU) * (1 - 2 * cfg.NU))  # 0.57692\n    MU = cfg.E / (2 * (1 + cfg.NU))  # 0.38462\n    geom_validator = ppsci.validate.GeometryValidator(\n        {\n            \"lambda_\": lambda out: out[\"lambda_\"],\n            \"mu\": lambda out: out[\"mu\"],\n        },\n        {\n            \"lambda_\": LAMBDA_,\n            \"mu\": MU,\n        },\n        geom[\"geo\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n            \"total_size\": cfg.EVAL.total_size.validator,\n            \"batch_size\": cfg.EVAL.batch_size.validator,\n        },\n        ppsci.loss.MSELoss(\"sum\"),\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"geo_eval\",\n    )\n    validator = {geom_validator.name: geom_validator}\n\n    # set visualizer(optional)\n    # add inferencer data\n    samples = geom[\"geo\"].sample_interior(\n        cfg.EVAL.batch_size.visualizer_vtu,\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n    )\n    pred_input_dict = {\n        k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n    }\n    visualizer = {\n        \"visulzie_lambda_mu\": ppsci.visualize.VisualizerVtu(\n            pred_input_dict,\n            {\n                \"lambda\": lambda out: out[\"lambda_\"],\n                \"mu\": lambda out: out[\"mu\"],\n            },\n            prefix=\"vis\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        log_freq=cfg.log_freq,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    from paddle.static import InputSpec\n\n    # set model\n    disp_net = ppsci.arch.MLP(**cfg.MODEL.disp_net)\n    stress_net = ppsci.arch.MLP(**cfg.MODEL.stress_net)\n    inverse_lambda_net = ppsci.arch.MLP(**cfg.MODEL.inverse_lambda_net)\n    inverse_mu_net = ppsci.arch.MLP(**cfg.MODEL.inverse_mu_net)\n    # wrap to a model_list\n    model = ppsci.arch.ModelList(\n        (disp_net, stress_net, inverse_lambda_net, inverse_mu_net)\n    )\n\n    # load pretrained model\n    solver = ppsci.solver.Solver(\n        model=model, pretrained_model_path=cfg.INFER.pretrained_model_path\n    )\n\n    # export models\n    input_spec = [\n        {\n            key: InputSpec([None, 1], \"float32\", name=key)\n            for key in cfg.MODEL.disp_net.input_keys\n        },\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n    from ppsci.visualize import vtu\n\n    # set model predictor\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # set geometry\n    control_arm = ppsci.geometry.Mesh(cfg.GEOM_PATH)\n    # geometry bool operation\n    geo = control_arm\n    geom = {\"geo\": geo}\n    # set bounds\n    BOUNDS_X, BOUNDS_Y, BOUNDS_Z = control_arm.bounds\n    samples = geom[\"geo\"].sample_interior(\n        cfg.EVAL.batch_size.visualizer_vtu,\n        criteria=lambda x, y, z: (\n            (BOUNDS_X[0] &lt; x)\n            &amp; (x &lt; BOUNDS_X[1])\n            &amp; (BOUNDS_Y[0] &lt; y)\n            &amp; (y &lt; BOUNDS_Y[1])\n            &amp; (BOUNDS_Z[0] &lt; z)\n            &amp; (z &lt; BOUNDS_Z[1])\n        ),\n    )\n    pred_input_dict = {\n        k: v for k, v in samples.items() if k in cfg.MODEL.disp_net.input_keys\n    }\n\n    output_dict = predictor.predict(pred_input_dict, cfg.INFER.batch_size)\n\n    # mapping data to output_keys\n    output_keys = (\n        cfg.MODEL.disp_net.output_keys\n        + cfg.MODEL.stress_net.output_keys\n        + cfg.MODEL.inverse_lambda_net.output_keys\n        + cfg.MODEL.inverse_mu_net.output_keys\n    )\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(output_keys, output_dict.keys())\n    }\n    output_dict.update(pred_input_dict)\n    vtu.save_vtu_from_dict(\n        osp.join(cfg.output_dir, \"vis\"),\n        output_dict,\n        cfg.MODEL.disp_net.input_keys,\n        output_keys,\n        1,\n    )\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"inverse_parameter.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/control_arm/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":""},{"location":"zh/examples/control_arm/#51","title":"5.1 \u53d7\u529b\u5206\u6790\u6c42\u89e3","text":"<p>\u4e0b\u9762\u5c55\u793a\u4e86\u5f53\u529b\u7684\u65b9\u5411\u4e3a x \u6b63\u65b9\u5411\u65f6 3 \u4e2a\u65b9\u5411\u7684\u5e94\u53d8 \\(u, v, w\\) \u4ee5\u53ca 6 \u4e2a\u5e94\u529b \\(\\sigma_{xx}, \\sigma_{yy}, \\sigma_{zz}, \\sigma_{xy}, \\sigma_{xz}, \\sigma_{yz}\\) \u7684\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff0c\u7ed3\u679c\u57fa\u672c\u7b26\u5408\u8ba4\u77e5\u3002</p> <p> </p> \u5de6\u4fa7\u4e3a\u9884\u6d4b\u7684\u7ed3\u6784\u5e94\u53d8 u\uff1b\u4e2d\u95f4\u4e3a\u9884\u6d4b\u7684\u7ed3\u6784\u5e94\u53d8 v\uff1b\u53f3\u4fa7\u4e3a\u9884\u6d4b\u7684\u7ed3\u6784\u5e94\u53d8 w <p> </p> \u5de6\u4fa7\u4e3a\u9884\u6d4b\u7684\u7ed3\u6784\u5e94\u529b sigma_xx\uff1b\u4e2d\u95f4\u4e3a\u9884\u6d4b\u7684\u7ed3\u6784\u5e94\u529b sigma_xy\uff1b\u53f3\u4fa7\u4e3a\u9884\u6d4b\u7684\u7ed3\u6784\u5e94\u529b sigma_xz <p> </p> \u5de6\u4fa7\u4e3a\u9884\u6d4b\u7684\u7ed3\u6784\u5e94\u529b sigma_yy\uff1b\u4e2d\u95f4\u4e3a\u9884\u6d4b\u7684\u7ed3\u6784\u5e94\u529b sigma_yz\uff1b\u53f3\u4fa7\u4e3a\u9884\u6d4b\u7684\u7ed3\u6784\u5e94\u529b sigma_zz"},{"location":"zh/examples/control_arm/#52","title":"5.2 \u53c2\u6570\u9006\u63a8\u6c42\u89e3","text":"<p>\u4e0b\u9762\u5c55\u793a\u4e86\u7ebf\u5f39\u6027\u65b9\u7a0b\u53c2\u6570 \\(\\lambda, \\mu\\) \u7684\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff0c\u5728\u7ed3\u6784\u7684\u5927\u90e8\u5206\u533a\u57df\u9884\u6d4b\u8bef\u5dee\u5728 1% \u5de6\u53f3\u3002</p> data lambda mu outs(mean) 0.54950 0.38642 label 0.57692 0.38462 <p> </p> \u5de6\u4fa7\u4e3a\u9884\u6d4b\u7684 lambda\uff1b\u53f3\u4fa7\u4e3a\u9884\u6d4b\u7684 mu"},{"location":"zh/examples/cylinder2d_unsteady/","title":"Cylinder2D_unsteady","text":""},{"location":"zh/examples/cylinder2d_unsteady/#2d-cylinder2d-flow-around-a-cylinder","title":"2D-Cylinder(2D Flow Around a Cylinder)","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/cylinder2d_unsteady_Re100/cylinder2d_unsteady_Re100_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/cylinder2d_unsteady_Re100/cylinder2d_unsteady_Re100_dataset.tar -o cylinder2d_unsteady_Re100_dataset.tar\n# unzip it\ntar -xvf cylinder2d_unsteady_Re100_dataset.tar\npython cylinder2d_unsteady_Re100.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/cylinder2d_unsteady_Re100/cylinder2d_unsteady_Re100_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/cylinder2d_unsteady_Re100/cylinder2d_unsteady_Re100_dataset.tar -o cylinder2d_unsteady_Re100_dataset.tar\n# unzip it\ntar -xvf cylinder2d_unsteady_Re100_dataset.tar\npython cylinder2d_unsteady_Re100.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/cylinder2d_unsteady_Re100/cylinder2d_unsteady_Re100_pretrained.pdparams\n</code></pre> <pre><code>python cylinder2d_unsteady_Re100.py mode=export\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/cylinder2d_unsteady_Re100/cylinder2d_unsteady_Re100_dataset.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/cylinder2d_unsteady_Re100/cylinder2d_unsteady_Re100_dataset.tar -o cylinder2d_unsteady_Re100_dataset.tar\n# unzip it\ntar -xvf cylinder2d_unsteady_Re100_dataset.tar\npython cylinder2d_unsteady_Re100.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 cylinder2d_unsteady_Re100_pretrained.pdparams loss(Residual): 0.00398MSE.continuity(Residual): 0.00126MSE.momentum_x(Residual): 0.00151MSE.momentum_y(Residual): 0.00120"},{"location":"zh/examples/cylinder2d_unsteady/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u5706\u67f1\u7ed5\u6d41\u95ee\u9898\u53ef\u4ee5\u5e94\u7528\u4e8e\u5f88\u591a\u9886\u57df\u3002\u4f8b\u5982\uff0c\u5728\u5de5\u4e1a\u8bbe\u8ba1\u4e2d\uff0c\u5b83\u53ef\u4ee5\u88ab\u7528\u6765\u6a21\u62df\u548c\u4f18\u5316\u6d41\u4f53\u5728\u5404\u79cd\u8bbe\u5907\u4e2d\u7684\u6d41\u52a8\uff0c\u5982\u98ce\u529b\u53d1\u7535\u673a\u3001\u6c7d\u8f66\u548c\u98de\u673a\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u6027\u80fd\u7b49\u3002\u5728\u73af\u4fdd\u9886\u57df\uff0c\u5706\u67f1\u7ed5\u6d41\u95ee\u9898\u4e5f\u6709\u5e94\u7528\uff0c\u5982\u9884\u6d4b\u548c\u63a7\u5236\u6cb3\u6d41\u7684\u6d2a\u6c34\u3001\u7814\u7a76\u6c61\u67d3\u7269\u7684\u6269\u6563\u7b49\u3002\u6b64\u5916\uff0c\u5728\u5de5\u7a0b\u5b9e\u8df5\u4e2d\uff0c\u5982\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u6d41\u4f53\u9759\u529b\u5b66\u3001\u70ed\u4ea4\u6362\u3001\u7a7a\u6c14\u52a8\u529b\u5b66\u7b49\u9886\u57df\uff0c\u5706\u67f1\u7ed5\u6d41\u95ee\u9898\u4e5f\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002</p> <p>2D Flow Around a Cylinder\uff0c\u4e2d\u6587\u540d\u79f0\u53ef\u8bd1\u4f5c\u201c2\u7ef4\u5706\u67f1\u7ed5\u6d41\u201d\uff0c\u662f\u6307\u4e8c\u7ef4\u5706\u67f1\u4f4e\u901f\u5b9a\u5e38\u7ed5\u6d41\u7684\u6d41\u578b\u53ea\u4e0e \\(Re\\) \u6570\u6709\u5173\u3002\u5728 \\(Re \\le 1\\) \u65f6\uff0c\u6d41\u573a\u4e2d\u7684\u60ef\u6027\u529b\u4e0e\u7c98\u6027\u529b\u76f8\u6bd4\u5c45\u6b21\u8981\u5730\u4f4d\uff0c\u5706\u67f1\u4e0a\u4e0b\u6e38\u7684\u6d41\u7ebf\u524d\u540e\u5bf9\u79f0\uff0c\u963b\u529b\u7cfb\u6570\u8fd1\u4f3c\u4e0e \\(Re\\) \u6210\u53cd\u6bd4(\u963b\u529b\u7cfb\u6570\u4e3a 10~60)\uff0c\u6b64 \\(Re\\) \u6570\u8303\u56f4\u7684\u7ed5\u6d41\u79f0\u4e3a\u65af\u6258\u514b\u65af\u533a\uff1b\u968f\u7740 \\(Re\\) \u7684\u589e\u5927\uff0c\u5706\u67f1\u4e0a\u4e0b\u6e38\u7684\u6d41\u7ebf\u9010\u6e10\u5931\u53bb\u5bf9\u79f0\u6027\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial u}{\\partial t} + u\\dfrac{\\partial u}{\\partial x} + v\\dfrac{\\partial u}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial x} + \\nu(\\dfrac{\\partial ^2 u}{\\partial x ^2} + \\dfrac{\\partial ^2 u}{\\partial y ^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial v}{\\partial t} + u\\dfrac{\\partial v}{\\partial x} + v\\dfrac{\\partial v}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial y} + \\nu(\\dfrac{\\partial ^2 v}{\\partial x ^2} + \\dfrac{\\partial ^2 v}{\\partial y ^2}) \\] <p>\u4ee4\uff1a</p> <p>\\(t^* = \\dfrac{L}{U_0}\\)</p> <p>\\(x^*=y^* = L\\)</p> <p>\\(u^*=v^* = U_0\\)</p> <p>\\(p^* = \\rho {U_0}^2\\)</p> <p>\u5b9a\u4e49\uff1a</p> <p>\u65e0\u91cf\u7eb2\u65f6\u95f4 \\(\\tau = \\dfrac{t}{t^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u5750\u6807 \\(x\uff1aX = \\dfrac{x}{x^*}\\)\uff1b\u65e0\u91cf\u7eb2\u5750\u6807 \\(y\uff1aY = \\dfrac{y}{y^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u901f\u5ea6 \\(x\uff1aU = \\dfrac{u}{u^*}\\)\uff1b\u65e0\u91cf\u7eb2\u901f\u5ea6 \\(y\uff1aV = \\dfrac{v}{u^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u538b\u529b \\(P = \\dfrac{p}{p^*}\\)</p> <p>\u96f7\u8bfa\u6570 \\(Re = \\dfrac{L U_0}{\\nu}\\)</p> <p>\u5219\u53ef\u83b7\u5f97\u5982\u4e0b\u65e0\u91cf\u7eb2Navier-Stokes\u65b9\u7a0b\uff0c\u65bd\u52a0\u4e8e\u6d41\u4f53\u57df\u5185\u90e8\uff1a</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial U}{\\partial X} + \\dfrac{\\partial U}{\\partial Y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial U}{\\partial \\tau} + U\\dfrac{\\partial U}{\\partial X} + V\\dfrac{\\partial U}{\\partial Y} = -\\dfrac{\\partial P}{\\partial X} + \\dfrac{1}{Re}(\\dfrac{\\partial ^2 U}{\\partial X^2} + \\dfrac{\\partial ^2 U}{\\partial Y^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial V}{\\partial \\tau} + U\\dfrac{\\partial V}{\\partial X} + V\\dfrac{\\partial V}{\\partial Y} = -\\dfrac{\\partial P}{\\partial Y} + \\dfrac{1}{Re}(\\dfrac{\\partial ^2 V}{\\partial X^2} + \\dfrac{\\partial ^2 V}{\\partial Y^2}) \\] <p>\u5bf9\u4e8e\u6d41\u4f53\u57df\u8fb9\u754c\u548c\u6d41\u4f53\u57df\u5185\u90e8\u5706\u5468\u8fb9\u754c\uff0c\u5219\u9700\u65bd\u52a0 Dirichlet \u8fb9\u754c\u6761\u4ef6\uff1a</p> <p>\u6d41\u4f53\u57df\u5165\u53e3\u8fb9\u754c\uff1a</p> \\[ u=1, v=0 \\] <p>\u5706\u5468\u8fb9\u754c\uff1a</p> \\[ u=0, v=0 \\] <p>\u6d41\u4f53\u57df\u51fa\u53e3\u8fb9\u754c\uff1a</p> \\[ p=0 \\]"},{"location":"zh/examples/cylinder2d_unsteady/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p> <p>\u5728\u5f00\u59cb\u6784\u5efa\u4ee3\u7801\u4e4b\u524d\uff0c\u8bf7\u5148\u6309\u7167\u4e0b\u5217\u547d\u4ee4\u4e0b\u8f7d\u8bad\u7ec3\u3001\u8bc4\u4f30\u6240\u9700\u7684\u6570\u636e\u96c6</p> <pre><code>wget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/cylinder2d_unsteady_Re100/cylinder2d_unsteady_Re100_dataset.tar\ntar -xf cylinder2d_unsteady_Re100_dataset.tar\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 2D-Cylinder \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((t, x, y)\\) \u90fd\u6709\u81ea\u8eab\u7684\u6a2a\u5411\u901f\u5ea6 \\(u\\)\u3001\u7eb5\u5411\u901f\u5ea6 \\(v\\)\u3001\u538b\u529b \\(p\\) \u4e09\u4e2a\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((t, x, y)\\) \u5230 \\((u, v, p)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^3 \\to \\mathbb{R}^3\\) \uff0c\u5373\uff1a</p> \\[ u, v, p = f(t, x, y) \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>[\"t\", \"x\", \"y\"]</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>[\"u\", \"v\", \"p\"]</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\u4ee5\u53ca\u6fc0\u6d3b\u51fd\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 9 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 50\uff0c\u4f7f\u7528 \"tanh\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e 2D-Cylinder \u4f7f\u7528\u7684\u662f Navier-Stokes \u65b9\u7a0b\u76842\u7ef4\u77ac\u6001\u5f62\u5f0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>NavierStokes</code>\u3002</p> <pre><code># set equation\nequation = {\n    \"NavierStokes\": ppsci.equation.NavierStokes(cfg.VISCOSITY, cfg.DENSITY, 2, True)\n}\n</code></pre> <p>\u5728\u5b9e\u4f8b\u5316 <code>NavierStokes</code> \u7c7b\u65f6\u9700\u6307\u5b9a\u5fc5\u8981\u7684\u53c2\u6570\uff1a\u52a8\u529b\u7c98\u5ea6 \\(\\nu=0.02\\), \u6d41\u4f53\u5bc6\u5ea6 \\(\\rho=1.0\\)\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d 2D-Cylinder \u7684\u8ba1\u7b97\u57df\u7531 CSV \u6587\u4ef6\u50a8\u5b58\u7684\u70b9\u4e91\u6784\u6210\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u70b9\u4e91\u51e0\u4f55 <code>PointCloud</code> \u548c\u65f6\u95f4\u57df <code>TimeDomain</code>\uff0c\u7ec4\u5408\u6210\u65f6\u95f4-\u7a7a\u95f4\u7684 <code>TimeXGeometry</code> \u8ba1\u7b97\u57df\u3002</p> <pre><code># set timestamps\ntrain_timestamps = np.linspace(\n    cfg.TIME_START, cfg.TIME_END, cfg.NUM_TIMESTAMPS, endpoint=True\n).astype(\"float32\")\ntrain_timestamps = np.random.choice(train_timestamps, cfg.TRAIN_NUM_TIMESTAMPS)\ntrain_timestamps.sort()\nt0 = np.array([cfg.TIME_START], dtype=\"float32\")\n\nval_timestamps = np.linspace(\n    cfg.TIME_START, cfg.TIME_END, cfg.NUM_TIMESTAMPS, endpoint=True\n).astype(\"float32\")\n\nlogger.message(f\"train_timestamps: {train_timestamps.tolist()}\")\nlogger.message(f\"val_timestamps: {val_timestamps.tolist()}\")\n\n# set time-geometry\ngeom = {\n    \"time_rect\": ppsci.geometry.TimeXGeometry(\n        ppsci.geometry.TimeDomain(\n            cfg.TIME_START,\n            cfg.TIME_END,\n            timestamps=np.concatenate((t0, train_timestamps), axis=0),\n        ),\n        ppsci.geometry.PointCloud(\n            reader.load_csv_file(\n                \"./datasets/domain_train.csv\",\n                (\"x\", \"y\"),\n                alias_dict={\"x\": \"Points:0\", \"y\": \"Points:1\"},\n            ),\n            (\"x\", \"y\"),\n        ),\n    ),\n    \"time_rect_eval\": ppsci.geometry.PointCloud(\n        reader.load_csv_file(\n            \"./datasets/domain_eval.csv\",\n            (\"t\", \"x\", \"y\"),\n        ),\n        (\"t\", \"x\", \"y\"),\n    ),\n}\n</code></pre> <ol> <li>\u8bc4\u4f30\u6570\u636e\u70b9\u5df2\u5305\u542b\u65f6\u95f4\u6233\u4fe1\u606f\uff0c\u56e0\u6b64\u65e0\u9700\u989d\u5916\u518d\u4e0e <code>TimeDomain</code> \u7ec4\u5408\u6210 <code>TimeXGeometry</code>\uff0c\u53ea\u9700\u4f7f\u7528 <code>PointCloud</code> \u8bfb\u5165\u6570\u636e\u5373\u53ef\u3002</li> </ol> \u63d0\u793a <p><code>PointCloud</code> \u548c <code>TimeDomain</code> \u662f\u4e24\u79cd\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\u7684 <code>Geometry</code> \u6d3e\u751f\u7c7b\u3002</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e8e\u70b9\u4e91\u51e0\u4f55\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.PointCloud(...)</code> \u521b\u5efa\u7a7a\u95f4\u51e0\u4f55\u57df\u5bf9\u8c61\uff1b</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e00\u7ef4\u65f6\u95f4\u57df\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.TimeDomain(...)</code> \u6784\u5efa\u65f6\u95f4\u57df\u5bf9\u8c61\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u6839\u636e 2. \u95ee\u9898\u5b9a\u4e49 \u5f97\u5230\u7684\u65e0\u91cf\u7eb2\u516c\u5f0f\u548c\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u5bf9\u5e94\u4e86\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u7684\u4e09\u4e2a\u7ea6\u675f\u6761\u4ef6\uff0c\u5373\uff1a</p> <ol> <li> <p>\u65bd\u52a0\u5728\u6d41\u4f53\u57df\u5185\u90e8\u70b9\u4e0a\u7684\u65e0\u91cf\u7eb2 Navier-Stokes \u65b9\u7a0b\u7ea6\u675f\uff08\u7ecf\u8fc7\u7b80\u5355\u79fb\u9879\uff09</p> \\[ \\dfrac{\\partial U}{\\partial X} + \\dfrac{\\partial U}{\\partial Y} = 0 \\] \\[ \\dfrac{\\partial U}{\\partial \\tau} + U\\dfrac{\\partial U}{\\partial X} + V\\dfrac{\\partial U}{\\partial Y} + \\dfrac{\\partial P}{\\partial X} - \\dfrac{1}{Re}(\\dfrac{\\partial ^2 U}{\\partial X^2} + \\dfrac{\\partial ^2 U}{\\partial Y^2}) = 0 \\] \\[ \\dfrac{\\partial V}{\\partial \\tau} + U\\dfrac{\\partial V}{\\partial X} + V\\dfrac{\\partial V}{\\partial Y} + \\dfrac{\\partial P}{\\partial Y} - \\dfrac{1}{Re}(\\dfrac{\\partial ^2 V}{\\partial X^2} + \\dfrac{\\partial ^2 V}{\\partial Y^2}) = 0 \\] <p>\u4e3a\u4e86\u65b9\u4fbf\u83b7\u53d6\u4e2d\u95f4\u53d8\u91cf\uff0c<code>NavierStokes</code> \u7c7b\u5185\u90e8\u5c06\u4e0a\u5f0f\u5de6\u4fa7\u7684\u7ed3\u679c\u5206\u522b\u547d\u540d\u4e3a <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code>\u3002</p> </li> <li> <p>\u65bd\u52a0\u5728\u6d41\u4f53\u57df\u5165\u53e3\u3001\u5185\u90e8\u5706\u5468\u3001\u6d41\u4f53\u57df\u51fa\u53e3\u7684 Dirichlet \u8fb9\u754c\u6761\u4ef6\u7ea6\u675f</p> <p>\u6d41\u4f53\u57df\u5165\u53e3\u8fb9\u754c\uff1a</p> \\[ u=1, v=0 \\] <p>\u6d41\u4f53\u57df\u51fa\u53e3\u8fb9\u754c\uff1a</p> \\[ p=0 \\] <p>\u5706\u5468\u8fb9\u754c\uff1a</p> \\[ u=0, v=0 \\] </li> <li> <p>\u65bd\u52a0\u5728\u521d\u59cb\u65f6\u523b\u6d41\u4f53\u57df\u5185\u90e8\u70b9\u4e0a\u7684\u521d\u503c\u6761\u4ef6\u7ea6\u675f\uff1a</p> \\[ u=u_{t0}, v=v_{t0}, p=p_{t0} \\] </li> </ol> <p>\u63a5\u4e0b\u6765\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>InteriorConstraint</code> \u548c <code>SupervisedConstraint</code> \u6784\u5efa\u4e0a\u8ff0\u4e24\u79cd\u7ea6\u675f\u6761\u4ef6\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u79cd\u7ea6\u675f\u6307\u5b9a\u91c7\u6837\u70b9\u4e2a\u6570\uff0c\u8868\u793a\u6bcf\u4e00\u79cd\u7ea6\u675f\u5728\u5176\u5bf9\u5e94\u8ba1\u7b97\u57df\u5185\u91c7\u6837\u6570\u636e\u7684\u6570\u91cf\uff0c\u4ee5\u53ca\u901a\u7528\u7684\u91c7\u6837\u914d\u7f6e\u3002</p> <pre><code># pde/bc/sup constraint use t1~tn, initial constraint use t0\nNTIME_PDE = len(train_timestamps)\nALIAS_DICT = {\"x\": \"Points:0\", \"y\": \"Points:1\", \"u\": \"U:0\", \"v\": \"U:1\"}\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u6d41\u4f53\u57df\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set constraint\npde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"NavierStokes\"].equations,\n    {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n    geom[\"time_rect\"],\n    {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"batch_size\": cfg.NPOINT_PDE * NTIME_PDE,\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"EQ\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"NavierStokes\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u6211\u4eec\u5e0c\u671b Navier-Stokes \u65b9\u7a0b\u4ea7\u751f\u7684\u4e09\u4e2a\u4e2d\u95f4\u7ed3\u679c <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code> \u88ab\u4f18\u5316\u81f3 0\uff0c\u56e0\u6b64\u5c06\u5b83\u4eec\u7684\u76ee\u6807\u503c\u5168\u90e8\u8bbe\u4e3a 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"time_rect\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528\u5168\u91cf\u6570\u636e\u70b9\u8bad\u7ec3\uff0c\u56e0\u6b64 <code>dataset</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e3a \"IterableNamedArrayDataset\" \u4e14 <code>iters_per_epoch</code> \u4e5f\u8bbe\u7f6e\u4e3a 1\uff0c\u91c7\u6837\u70b9\u6570 <code>batch_size</code> \u8bbe\u4e3a 9420 * 30(\u8868\u793a\u4e00\u4e2a\u65f6\u523b\u4ea7\u751f 9420 \u4e2a\u6570\u636e\u70b9\uff0c\u5171\u6709 30 \u4e2a\u65f6\u523b)\uff1b</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\u53d6\u5e73\u5747\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"EQ\" \u5373\u53ef\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u540c\u7406\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u6784\u5efa\u6d41\u4f53\u57df\u7684\u6d41\u5165\u8fb9\u754c\u3001\u6d41\u51fa\u8fb9\u754c\u3001\u5706\u5468\u8fb9\u754c\u5171\u4e09\u4e2a\u8fb9\u754c\u7684 Dirichlet \u8fb9\u754c\u7ea6\u675f\u3002\u4ee5 <code>bc_inlet_cylinder</code> \u8fb9\u754c\u7ea6\u675f\u4e3a\u4f8b\uff0c\u7531\u4e8e\u4f5c\u7528\u533a\u57df\u662f\u8fb9\u754c\u4e14\u8fb9\u754c\u4e0a\u7684\u6570\u636e\u7531 CSV \u6587\u4ef6\u8bb0\u5f55\uff0c\u56e0\u6b64\u6211\u4eec\u4f7f\u7528 <code>SupervisedConstraint</code> \u7c7b\uff0c\u5e76\u6309\u7167\u5982\u4e0b\u89c4\u5219\u6307\u5b9a\u7b2c\u4e00\u4e2a\u53c2\u6570 <code>dataloader_cfg</code> \u914d\u7f6e\u5b57\u5178\uff1a</p> <ul> <li> <p>\u8be5\u914d\u7f6e\u5b57\u5178\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3a\u5305\u542b CSV \u6587\u4ef6\u7684\u8def\u5f84 <code>./datasets/domain_inlet_cylinder.csv</code> \u5728\u5185\u7684\u914d\u7f6e\u5b57\u5178\uff1b</p> </li> <li> <p>\u8be5\u914d\u7f6e\u5b57\u5178\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u6307\u5b9a\u6570\u636e\u52a0\u8f7d\u65b9\u5f0f\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528 <code>IterableCSVDataset</code> \u4f5c\u4e3a\u5168\u91cf\u6570\u636e\u52a0\u8f7d\u5668\uff1b</p> </li> <li> <p>\u8be5\u914d\u7f6e\u5b57\u5178\u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\u6307\u5b9a\u6570\u636e\u7684\u52a0\u8f7d\u8def\u5f84\uff0c\u6b64\u5904\u586b\u5199 <code>./datasets/domain_inlet_cylinder.csv</code>\uff1b</p> </li> <li> <p>\u8be5\u914d\u7f6e\u5b57\u5178\u7684\u7b2c\u4e09\u4e2a\u53c2\u6570\u6307\u5b9a\u8981\u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6\u7684\u8f93\u5165\u5217\uff0c\u5bf9\u5e94\u8f6c\u6362\u540e\u5173\u952e\u5b57\uff0c\u6b64\u5904\u586b\u5199\u4e3a <code>(\"x\", \"y\")</code>\uff1b</p> </li> <li> <p>\u8be5\u914d\u7f6e\u5b57\u5178\u7684\u7b2c\u56db\u4e2a\u53c2\u6570\u6307\u5b9a\u8981\u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6\u7684\u6807\u7b7e\u5217\uff0c\u5bf9\u5e94\u8f6c\u6362\u540e\u5173\u952e\u5b57\uff0c\u6b64\u5904\u586b\u5199\u4e3a <code>(\"u\", \"v\")</code>\uff1b</p> </li> <li> <p>\u8003\u8651\u5230\u540c\u4e00\u4e2a\u53d8\u91cf\u5728\u4e0d\u540c CSV \u6587\u4ef6\u4e2d\u53ef\u80fd\u5177\u6709\u4e0d\u540c\u7684\u5b57\u6bb5\u540d\uff0c\u800c\u4e14\u6709\u7684\u5b57\u6bb5\u540d\u8fc7\u957f\u5728\u7f16\u5199\u4ee3\u7801\u65f6\u5bb9\u6613\u5199\u9519\uff0c\u56e0\u6b64\u8be5\u914d\u7f6e\u5b57\u5178\u7684\u7b2c\u4e94\u4e2a\u53c2\u6570\u7528\u4e8e\u6307\u5b9a\u5b57\u6bb5\u5217\u7684\u522b\u540d\uff0c\u6b64\u5904\u586b\u5199\u4e3a <code>{\"x\": \"Points:0\", \"y\": \"Points:1\", \"u\": \"U:0\", \"v\": \"U:1\"}</code>\uff1b</p> </li> <li> <p>\u8be5\u914d\u7f6e\u5b57\u5178\u7684\u7b2c\u516d\u4e2a\u53c2\u6570\u6307\u5b9a\u6bcf\u4e2a\u6807\u7b7e\u5728\u8ba1\u7b97\u635f\u5931\u65f6\u7684\u6743\u91cd\uff0c\u6b64\u5904\u6211\u4eec\u653e\u5927 \"u\" \u548c \"v\" \u7684\u6743\u91cd\u81f3 10\uff0c\u586b\u5199 <code>{\"u\": 10, \"v\": 10}</code>\uff1b</p> </li> <li> <p>\u8be5\u914d\u7f6e\u5b57\u5178\u7684\u7b2c\u4e03\u4e2a\u53c2\u6570\u6307\u5b9a\u6570\u636e\u8bfb\u53d6\u662f\u5426\u6d89\u53ca\u65f6\u95f4\u4fe1\u606f\uff0c\u6b64\u5904\u6211\u4eec\u8bbe\u5b9a\u4e3a\u8bad\u7ec3\u65f6\u95f4\u6233\uff0c\u5373\u586b\u5199 <code>train_timestamps</code>\uff1b</p> </li> </ul> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\u53d6\u5e73\u5747\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"BC_inlet_cylinder\" \u5373\u53ef\u3002</p> <p>\u5269\u4e0b\u7684 <code>bc_outlet</code> \u6309\u7167\u76f8\u540c\u539f\u7406\u6784\u5efa\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>bc_inlet_cylinder = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableCSVDataset\",\n            \"file_path\": cfg.DOMAIN_INLET_CYLINDER_PATH,\n            \"input_keys\": (\"x\", \"y\"),\n            \"label_keys\": (\"u\", \"v\"),\n            \"alias_dict\": ALIAS_DICT,\n            \"weight_dict\": {\"u\": 10, \"v\": 10},\n            \"timestamps\": train_timestamps,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"BC_inlet_cylinder\",\n)\nbc_outlet = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableCSVDataset\",\n            \"file_path\": cfg.DOMAIN_OUTLET_PATH,\n            \"input_keys\": (\"x\", \"y\"),\n            \"label_keys\": (\"p\",),\n            \"alias_dict\": ALIAS_DICT,\n            \"timestamps\": train_timestamps,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"BC_outlet\",\n)\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady/#343","title":"3.4.3 \u521d\u503c\u7ea6\u675f","text":"<p>\u5bf9\u4e8e \\(t=t_0\\) \u65f6\u523b\u7684\u6d41\u4f53\u57df\u5185\u7684\u70b9\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u5bf9 \\(u\\), \\(v\\), \\(p\\) \u65bd\u52a0\u521d\u503c\u7ea6\u675f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>ic = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableCSVDataset\",\n            \"file_path\": cfg.IC0_1_PATH,\n            \"input_keys\": (\"x\", \"y\"),\n            \"label_keys\": (\"u\", \"v\", \"p\"),\n            \"alias_dict\": ALIAS_DICT,\n            \"weight_dict\": {\"u\": 10, \"v\": 10, \"p\": 10},\n            \"timestamps\": t0,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"IC\",\n)\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady/#344","title":"3.4.4 \u76d1\u7763\u7ea6\u675f","text":"<p>\u672c\u6848\u4f8b\u5728\u6d41\u4f53\u57df\u5185\u90e8\u52a0\u5165\u4e86\u4e00\u5b9a\u6570\u91cf\u7684\u76d1\u7763\u70b9\u6765\u4fdd\u8bc1\u6a21\u578b\u6700\u7ec8\u7684\u6536\u655b\u60c5\u51b5\uff0c\u56e0\u6b64\u6700\u540e\u8fd8\u9700\u8981\u52a0\u5165\u4e00\u4e2a\u76d1\u7763\u7ea6\u675f\uff0c\u6570\u636e\u540c\u6837\u6765\u81ea CSV \u6587\u4ef6\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableCSVDataset\",\n            \"file_path\": cfg.PROBE1_50_PATH,\n            \"input_keys\": (\"t\", \"x\", \"y\"),\n            \"label_keys\": (\"u\", \"v\"),\n            \"alias_dict\": ALIAS_DICT,\n            \"weight_dict\": {\"u\": 10, \"v\": 10},\n            \"timestamps\": train_timestamps,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"Sup\",\n)\n</code></pre> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u3001\u8fb9\u754c\u7ea6\u675f\u3001\u521d\u503c\u7ea6\u675f\u3001\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    pde_constraint.name: pde_constraint,\n    bc_inlet_cylinder.name: bc_inlet_cylinder,\n    bc_outlet.name: bc_outlet,\n    ic.name: ic,\n    sup_constraint.name: sup_constraint,\n}\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528\u56db\u4e07\u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u8bc4\u4f30\u95f4\u9694\u4e3a\u56db\u767e\u8f6e\uff0c\u5b66\u4e60\u7387\u8bbe\u4e3a 0.001\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.GeometryValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\nNPOINT_EVAL = (\n    cfg.NPOINT_PDE + cfg.NPOINT_INLET_CYLINDER + cfg.NPOINT_OUTLET\n) * cfg.NUM_TIMESTAMPS\nresidual_validator = ppsci.validate.GeometryValidator(\n    equation[\"NavierStokes\"].equations,\n    {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n    geom[\"time_rect_eval\"],\n    {\n        \"dataset\": \"NamedArrayDataset\",\n        \"total_size\": NPOINT_EVAL,\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\"name\": \"BatchSampler\"},\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"Residual\",\n)\nvalidator = {residual_validator.name: residual_validator}\n</code></pre> <p>\u65b9\u7a0b\u8bbe\u7f6e\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u76f8\u540c\uff0c\u8868\u793a\u5982\u4f55\u8ba1\u7b97\u6240\u9700\u8bc4\u4f30\u7684\u76ee\u6807\u53d8\u91cf\uff1b</p> <p>\u6b64\u5904\u6211\u4eec\u4e3a <code>momentum_x</code>, <code>continuity</code>, <code>momentum_y</code> \u4e09\u4e2a\u76ee\u6807\u53d8\u91cf\u8bbe\u7f6e\u6807\u7b7e\u503c\u4e3a 0\uff1b</p> <p>\u8ba1\u7b97\u57df\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u76f8\u540c\uff0c\u8868\u793a\u5728\u6307\u5b9a\u8ba1\u7b97\u57df\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff1b</p> <p>\u91c7\u6837\u70b9\u914d\u7f6e\u5219\u9700\u8981\u6307\u5b9a\u603b\u7684\u8bc4\u4f30\u70b9\u6570 <code>total_size</code>\uff0c\u6b64\u5904\u6211\u4eec\u8bbe\u7f6e\u4e3a 9662 * 50(9420\u4e2a\u6d41\u4f53\u57df\u5185\u7684\u70b9+161\u4e2a\u6d41\u4f53\u57df\u6d41\u5165\u8fb9\u754c\u70b9+81\u4e2a\u6d41\u4f53\u57df\u6d41\u51fa\u8fb9\u754c\u70b9\uff0c\u5171 50 \u4e2a\u8bc4\u4f30\u65f6\u523b)\uff1b</p> <p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u9009\u62e9 <code>ppsci.metric.MSE</code> \u5373\u53ef\uff1b</p> <p>\u5176\u4f59\u914d\u7f6e\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady/#38","title":"3.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u672c\u6587\u4e2d\u7684\u8f93\u51fa\u6570\u636e\u662f\u4e00\u4e2a\u533a\u57df\u5185\u7684\u4e8c\u7ef4\u70b9\u96c6\uff0c\u6bcf\u4e2a\u65f6\u523b \\(t\\) \u7684\u5750\u6807\u662f \\((x^t_i, y^t_i)\\)\uff0c\u5bf9\u5e94\u503c\u662f \\((u^t_i, v^t_i, p^t_i)\\)\uff0c\u56e0\u6b64\u6211\u4eec\u53ea\u9700\u8981\u5c06\u8bc4\u4f30\u7684\u8f93\u51fa\u6570\u636e\u6309\u65f6\u523b\u4fdd\u5b58\u6210 50 \u4e2a vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u6700\u540e\u7528\u53ef\u89c6\u5316\u8f6f\u4ef6\u6253\u5f00\u67e5\u770b\u5373\u53ef\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set visualizer(optional)\nvis_points = geom[\"time_rect_eval\"].sample_interior(\n    (cfg.NPOINT_PDE + cfg.NPOINT_INLET_CYLINDER + cfg.NPOINT_OUTLET)\n    * cfg.NUM_TIMESTAMPS,\n    evenly=True,\n)\nvisualizer = {\n    \"visualize_u_v_p\": ppsci.visualize.VisualizerVtu(\n        vis_points,\n        {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"p\": lambda d: d[\"p\"]},\n        num_timestamps=cfg.NUM_TIMESTAMPS,\n        prefix=\"result_u_v_p\",\n    )\n}\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    visualizer=visualizer,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"cylinder2d_unsteady_Re100.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\nfrom ppsci.utils import reader\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(cfg.VISCOSITY, cfg.DENSITY, 2, True)\n    }\n\n    # set timestamps\n    train_timestamps = np.linspace(\n        cfg.TIME_START, cfg.TIME_END, cfg.NUM_TIMESTAMPS, endpoint=True\n    ).astype(\"float32\")\n    train_timestamps = np.random.choice(train_timestamps, cfg.TRAIN_NUM_TIMESTAMPS)\n    train_timestamps.sort()\n    t0 = np.array([cfg.TIME_START], dtype=\"float32\")\n\n    val_timestamps = np.linspace(\n        cfg.TIME_START, cfg.TIME_END, cfg.NUM_TIMESTAMPS, endpoint=True\n    ).astype(\"float32\")\n\n    logger.message(f\"train_timestamps: {train_timestamps.tolist()}\")\n    logger.message(f\"val_timestamps: {val_timestamps.tolist()}\")\n\n    # set time-geometry\n    geom = {\n        \"time_rect\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(\n                cfg.TIME_START,\n                cfg.TIME_END,\n                timestamps=np.concatenate((t0, train_timestamps), axis=0),\n            ),\n            ppsci.geometry.PointCloud(\n                reader.load_csv_file(\n                    cfg.DOMAIN_TRAIN_PATH,\n                    (\"x\", \"y\"),\n                    alias_dict={\"x\": \"Points:0\", \"y\": \"Points:1\"},\n                ),\n                (\"x\", \"y\"),\n            ),\n        ),\n        \"time_rect_eval\": ppsci.geometry.PointCloud(\n            reader.load_csv_file(\n                cfg.DOMAIN_EVAL_PATH,\n                (\"t\", \"x\", \"y\"),\n            ),\n            (\"t\", \"x\", \"y\"),\n        ),\n    }\n\n    # pde/bc/sup constraint use t1~tn, initial constraint use t0\n    NTIME_PDE = len(train_timestamps)\n    ALIAS_DICT = {\"x\": \"Points:0\", \"y\": \"Points:1\", \"u\": \"U:0\", \"v\": \"U:1\"}\n\n    # set constraint\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n        geom[\"time_rect\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"batch_size\": cfg.NPOINT_PDE * NTIME_PDE,\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"EQ\",\n    )\n    bc_inlet_cylinder = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableCSVDataset\",\n                \"file_path\": cfg.DOMAIN_INLET_CYLINDER_PATH,\n                \"input_keys\": (\"x\", \"y\"),\n                \"label_keys\": (\"u\", \"v\"),\n                \"alias_dict\": ALIAS_DICT,\n                \"weight_dict\": {\"u\": 10, \"v\": 10},\n                \"timestamps\": train_timestamps,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"BC_inlet_cylinder\",\n    )\n    bc_outlet = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableCSVDataset\",\n                \"file_path\": cfg.DOMAIN_OUTLET_PATH,\n                \"input_keys\": (\"x\", \"y\"),\n                \"label_keys\": (\"p\",),\n                \"alias_dict\": ALIAS_DICT,\n                \"timestamps\": train_timestamps,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"BC_outlet\",\n    )\n    ic = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableCSVDataset\",\n                \"file_path\": cfg.IC0_1_PATH,\n                \"input_keys\": (\"x\", \"y\"),\n                \"label_keys\": (\"u\", \"v\", \"p\"),\n                \"alias_dict\": ALIAS_DICT,\n                \"weight_dict\": {\"u\": 10, \"v\": 10, \"p\": 10},\n                \"timestamps\": t0,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"IC\",\n    )\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableCSVDataset\",\n                \"file_path\": cfg.PROBE1_50_PATH,\n                \"input_keys\": (\"t\", \"x\", \"y\"),\n                \"label_keys\": (\"u\", \"v\"),\n                \"alias_dict\": ALIAS_DICT,\n                \"weight_dict\": {\"u\": 10, \"v\": 10},\n                \"timestamps\": train_timestamps,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"Sup\",\n    )\n\n    # wrap constraints together\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        bc_inlet_cylinder.name: bc_inlet_cylinder,\n        bc_outlet.name: bc_outlet,\n        ic.name: ic,\n        sup_constraint.name: sup_constraint,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    NPOINT_EVAL = (\n        cfg.NPOINT_PDE + cfg.NPOINT_INLET_CYLINDER + cfg.NPOINT_OUTLET\n    ) * cfg.NUM_TIMESTAMPS\n    residual_validator = ppsci.validate.GeometryValidator(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n        geom[\"time_rect_eval\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"total_size\": NPOINT_EVAL,\n            \"batch_size\": cfg.EVAL.batch_size,\n            \"sampler\": {\"name\": \"BatchSampler\"},\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"Residual\",\n    )\n    validator = {residual_validator.name: residual_validator}\n\n    # set visualizer(optional)\n    vis_points = geom[\"time_rect_eval\"].sample_interior(\n        (cfg.NPOINT_PDE + cfg.NPOINT_INLET_CYLINDER + cfg.NPOINT_OUTLET)\n        * cfg.NUM_TIMESTAMPS,\n        evenly=True,\n    )\n    visualizer = {\n        \"visualize_u_v_p\": ppsci.visualize.VisualizerVtu(\n            vis_points,\n            {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"p\": lambda d: d[\"p\"]},\n            num_timestamps=cfg.NUM_TIMESTAMPS,\n            prefix=\"result_u_v_p\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(cfg.VISCOSITY, cfg.DENSITY, 2, True)\n    }\n\n    # set timestamps\n    val_timestamps = np.linspace(\n        cfg.TIME_START, cfg.TIME_END, cfg.NUM_TIMESTAMPS, endpoint=True\n    ).astype(\"float32\")\n\n    logger.message(f\"val_timestamps: {val_timestamps.tolist()}\")\n\n    # set time-geometry\n    geom = {\n        \"time_rect_eval\": ppsci.geometry.PointCloud(\n            reader.load_csv_file(\n                cfg.DOMAIN_EVAL_PATH,\n                (\"t\", \"x\", \"y\"),\n            ),\n            (\"t\", \"x\", \"y\"),\n        ),\n    }\n\n    # set validator\n    NPOINT_EVAL = (\n        cfg.NPOINT_PDE + cfg.NPOINT_INLET_CYLINDER + cfg.NPOINT_OUTLET\n    ) * cfg.NUM_TIMESTAMPS\n    residual_validator = ppsci.validate.GeometryValidator(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n        geom[\"time_rect_eval\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"total_size\": NPOINT_EVAL,\n            \"batch_size\": cfg.EVAL.batch_size,\n            \"sampler\": {\"name\": \"BatchSampler\"},\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"Residual\",\n    )\n    validator = {residual_validator.name: residual_validator}\n\n    # set visualizer(optional)\n    vis_points = geom[\"time_rect_eval\"].sample_interior(\n        (cfg.NPOINT_PDE + cfg.NPOINT_INLET_CYLINDER + cfg.NPOINT_OUTLET)\n        * cfg.NUM_TIMESTAMPS,\n        evenly=True,\n    )\n    visualizer = {\n        \"visualize_u_v_p\": ppsci.visualize.VisualizerVtu(\n            vis_points,\n            {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"p\": lambda d: d[\"p\"]},\n            num_timestamps=cfg.NUM_TIMESTAMPS,\n            prefix=\"result_u_v_p\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        geom=geom,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    # evaluate\n    solver.eval()\n    # visualize prediction\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n    # set time-geometry\n    geom = {\n        \"time_rect_eval\": ppsci.geometry.PointCloud(\n            reader.load_csv_file(\n                cfg.DOMAIN_EVAL_PATH,\n                (\"t\", \"x\", \"y\"),\n            ),\n            (\"t\", \"x\", \"y\"),\n        ),\n    }\n    NPOINT_EVAL = (\n        cfg.NPOINT_PDE + cfg.NPOINT_INLET_CYLINDER + cfg.NPOINT_OUTLET\n    ) * cfg.NUM_TIMESTAMPS\n    input_dict = geom[\"time_rect_eval\"].sample_interior(NPOINT_EVAL, evenly=True)\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n\n    ppsci.visualize.save_vtu_from_dict(\n        \"./cylinder2d_unsteady_Re100_pred.vtu\",\n        {**input_dict, **output_dict},\n        input_dict.keys(),\n        cfg.MODEL.output_keys,\n        cfg.NUM_TIMESTAMPS,\n    )\n\n\n@hydra.main(\n    version_base=None,\n    config_path=\"./conf\",\n    config_name=\"cylinder2d_unsteady_Re100.yaml\",\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u9884\u6d4b\u7ed3\u679c\u5982\u4e0b\u6240\u793a\uff0c\u56fe\u50cf\u7684\u6a2a\u8f74\u662f\u6c34\u5e73\u65b9\u5411\uff0c\u7eb5\u8f74\u4ee3\u8868\u7ad6\u76f4\u65b9\u5411\uff0c\u6d41\u4f53\u6d41\u5411\u4e3a\u4ece\u5de6\u5230\u53f3\uff0c\u56fe\u7247\u4e2d\u5c55\u793a\u7684\u662f\u6a21\u578b\u9884\u6d4b50\u4e2a\u65f6\u523b\u5bf9\u5e94\u6d41\u573a\u7684\u6a2a\u5411\u6d41\u901f\\(u(t,x,y)\\)\u7684\u7ed3\u679c\u3002</p> \u8bf4\u660e <p>\u672c\u6848\u4f8b\u53ea\u4f5c\u4e3ademo\u5c55\u793a\uff0c\u5c1a\u672a\u8fdb\u884c\u5145\u5206\u8c03\u4f18\uff0c\u4e0b\u65b9\u90e8\u5206\u5c55\u793a\u7ed3\u679c\u53ef\u80fd\u4e0e OpenFOAM \u5b58\u5728\u4e00\u5b9a\u5dee\u522b\u3002</p> <p> </p> \u6a2a\u5411\u6d41\u901fu\u7684\u6a21\u578b\u9884\u6d4b\u7ed3\u679c"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/","title":"Cylinder2D_unsteady_transform_physx","text":""},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#2d-cylinder2d-flow-around-a-cylinder","title":"2D-Cylinder(2D Flow Around a Cylinder)","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_training.hdf5 -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_valid.hdf5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_training.hdf5 --create-dirs -o ./datasets/cylinder_training.hdf5\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_valid.hdf5 --create-dirs -o ./datasets/cylinder_valid.hdf5\npython train_enn.py\npython train_transformer.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_training.hdf5 -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_valid.hdf5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_training.hdf5 --create-dirs -o ./datasets/cylinder_training.hdf5\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_valid.hdf5 --create-dirs -o ./datasets/cylinder_valid.hdf5\npython train_enn.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/cylinder/cylinder_pretrained.pdparams\npython train_transformer.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/cylinder/cylinder_transformer_pretrained.pdparams EMBEDDING_MODEL_PATH=https://paddle-org.bj.bcebos.com/paddlescience/models/cylinder/cylinder_pretrained.pdparams\n</code></pre> \u6a21\u578b\u5bfc\u51fa\u547d\u4ee4 <pre><code>python train_transformer.py mode=export EMBEDDING_MODEL_PATH=https://paddle-org.bj.bcebos.com/paddlescience/models/cylinder/cylinder_pretrained.pdparams\n</code></pre> <pre><code>python train_transformer.py mode=export EMBEDDING_MODEL_PATH=https://paddle-org.bj.bcebos.com/paddlescience/models/cylinder/cylinder_pretrained.pdparams\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_training.hdf5 -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_valid.hdf5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_training.hdf5 --create-dirs -o ./datasets/cylinder_training.hdf5\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/cylinder_valid.hdf5 --create-dirs -o ./datasets/cylinder_valid.hdf5\npython train_transformer.py mode=infer\n</code></pre> \u6a21\u578b MSE cylinder_transformer_pretrained.pdparams 1.093"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u5706\u67f1\u7ed5\u6d41\u95ee\u9898\u53ef\u4ee5\u5e94\u7528\u4e8e\u5f88\u591a\u9886\u57df\u3002\u4f8b\u5982\uff0c\u5728\u5de5\u4e1a\u8bbe\u8ba1\u4e2d\uff0c\u5b83\u53ef\u4ee5\u88ab\u7528\u6765\u6a21\u62df\u548c\u4f18\u5316\u6d41\u4f53\u5728\u5404\u79cd\u8bbe\u5907\u4e2d\u7684\u6d41\u52a8\uff0c\u5982\u98ce\u529b\u53d1\u7535\u673a\u3001\u6c7d\u8f66\u548c\u98de\u673a\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u6027\u80fd\u7b49\u3002\u5728\u73af\u4fdd\u9886\u57df\uff0c\u5706\u67f1\u7ed5\u6d41\u95ee\u9898\u4e5f\u6709\u5e94\u7528\uff0c\u5982\u9884\u6d4b\u548c\u63a7\u5236\u6cb3\u6d41\u7684\u6d2a\u6c34\u3001\u7814\u7a76\u6c61\u67d3\u7269\u7684\u6269\u6563\u7b49\u3002\u6b64\u5916\uff0c\u5728\u5de5\u7a0b\u5b9e\u8df5\u4e2d\uff0c\u5982\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u6d41\u4f53\u9759\u529b\u5b66\u3001\u70ed\u4ea4\u6362\u3001\u7a7a\u6c14\u52a8\u529b\u5b66\u7b49\u9886\u57df\uff0c\u5706\u67f1\u7ed5\u6d41\u95ee\u9898\u4e5f\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002</p> <p>2D Flow Around a Cylinder\uff0c\u4e2d\u6587\u540d\u79f0\u53ef\u8bd1\u4f5c\u201c2\u7ef4\u5706\u67f1\u7ed5\u6d41\u201d\uff0c\u662f\u6307\u4e8c\u7ef4\u5706\u67f1\u4f4e\u901f\u5b9a\u5e38\u7ed5\u6d41\u7684\u6d41\u578b\u53ea\u4e0e \\(Re\\) \u6570\u6709\u5173\u3002\u5728 \\(Re \\le 1\\) \u65f6\uff0c\u6d41\u573a\u4e2d\u7684\u60ef\u6027\u529b\u4e0e\u7c98\u6027\u529b\u76f8\u6bd4\u5c45\u6b21\u8981\u5730\u4f4d\uff0c\u5706\u67f1\u4e0a\u4e0b\u6e38\u7684\u6d41\u7ebf\u524d\u540e\u5bf9\u79f0\uff0c\u963b\u529b\u7cfb\u6570\u8fd1\u4f3c\u4e0e \\(Re\\) \u6210\u53cd\u6bd4(\u963b\u529b\u7cfb\u6570\u4e3a 10~60)\uff0c\u6b64 \\(Re\\) \u6570\u8303\u56f4\u7684\u7ed5\u6d41\u79f0\u4e3a\u65af\u6258\u514b\u65af\u533a\uff1b\u968f\u7740 \\(Re\\) \u7684\u589e\u5927\uff0c\u5706\u67f1\u4e0a\u4e0b\u6e38\u7684\u6d41\u7ebf\u9010\u6e10\u5931\u53bb\u5bf9\u79f0\u6027\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\frac{\\partial u}{\\partial t} + u\\frac{\\partial u}{\\partial x} + v\\frac{\\partial u}{\\partial y} = -\\frac{1}{\\rho}\\frac{\\partial p}{\\partial x} + \\nu(\\frac{\\partial ^2 u}{\\partial x ^2} + \\frac{\\partial ^2 u}{\\partial y ^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\frac{\\partial v}{\\partial t} + u\\frac{\\partial v}{\\partial x} + v\\frac{\\partial v}{\\partial y} = -\\frac{1}{\\rho}\\frac{\\partial p}{\\partial y} + \\nu(\\frac{\\partial ^2 v}{\\partial x ^2} + \\frac{\\partial ^2 v}{\\partial y ^2}) \\] <p>\u4ee4\uff1a</p> <p>\\(t^* = \\frac{L}{U_0}\\)</p> <p>\\(x^*=y^* = L\\)</p> <p>\\(u^*=v^* = U_0\\)</p> <p>\\(p^* = \\rho {U_0}^2\\)</p> <p>\u5b9a\u4e49\uff1a</p> <p>\u65e0\u91cf\u7eb2\u65f6\u95f4 \\(\\tau = \\frac{t}{t^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u5750\u6807 \\(x\uff1aX = \\frac{x}{x^*}\\)\uff1b\u65e0\u91cf\u7eb2\u5750\u6807 \\(y\uff1aY = \\frac{y}{y^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u901f\u5ea6 \\(x\uff1aU = \\frac{u}{u^*}\\)\uff1b\u65e0\u91cf\u7eb2\u901f\u5ea6 \\(y\uff1aV = \\frac{v}{u^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u538b\u529b \\(P = \\frac{p}{p^*}\\)</p> <p>\u96f7\u8bfa\u6570 \\(Re = \\frac{L U_0}{\\nu}\\)</p> <p>\u5219\u53ef\u83b7\u5f97\u5982\u4e0b\u65e0\u91cf\u7eb2Navier-Stokes\u65b9\u7a0b\uff0c\u65bd\u52a0\u4e8e\u6d41\u4f53\u57df\u5185\u90e8\uff1a</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\frac{\\partial U}{\\partial X} + \\frac{\\partial U}{\\partial Y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\frac{\\partial U}{\\partial \\tau} + U\\frac{\\partial U}{\\partial X} + V\\frac{\\partial U}{\\partial Y} = -\\frac{\\partial P}{\\partial X} + \\frac{1}{Re}(\\frac{\\partial ^2 U}{\\partial X^2} + \\frac{\\partial ^2 U}{\\partial Y^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\frac{\\partial V}{\\partial \\tau} + U\\frac{\\partial V}{\\partial X} + V\\frac{\\partial V}{\\partial Y} = -\\frac{\\partial P}{\\partial Y} + \\frac{1}{Re}(\\frac{\\partial ^2 V}{\\partial X^2} + \\frac{\\partial ^2 V}{\\partial Y^2}) \\] <p>\u5bf9\u4e8e\u6d41\u4f53\u57df\u8fb9\u754c\u548c\u6d41\u4f53\u57df\u5185\u90e8\u5706\u5468\u8fb9\u754c\uff0c\u5219\u9700\u65bd\u52a0 Dirichlet \u8fb9\u754c\u6761\u4ef6\uff1a</p> <p>\u6d41\u4f53\u57df\u5165\u53e3\u8fb9\u754c\uff1a</p> \\[ u=1, v=0 \\] <p>\u5706\u5468\u8fb9\u754c\uff1a</p> \\[ u=0, v=0 \\] <p>\u6d41\u4f53\u57df\u51fa\u53e3\u8fb9\u754c\uff1a</p> \\[ p=0 \\]"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u57fa\u4e8e PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002\u672c\u6848\u4f8b\u57fa\u4e8e\u8bba\u6587 Transformers for Modeling Physical Systems \u65b9\u6cd5\u8fdb\u884c\u6c42\u89e3\uff0c\u5173\u4e8e\u8be5\u65b9\u6cd5\u7684\u7406\u8bba\u90e8\u5206\u8bf7\u53c2\u8003\u6b64\u6587\u6863\u6216\u539f\u8bba\u6587\u3002\u63a5\u4e0b\u6765\u9996\u5148\u4f1a\u5bf9\u4f7f\u7528\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4ecb\u7ecd\uff0c\u7136\u540e\u5bf9\u8be5\u65b9\u6cd5\u4e24\u4e2a\u8bad\u7ec3\u6b65\u9aa4\uff08Embedding \u6a21\u578b\u8bad\u7ec3\u3001Transformer \u6a21\u578b\u8bad\u7ec3\uff09\u7684\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u3001\u6a21\u578b\u6784\u5efa\u7b49\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u6570\u636e\u96c6\u91c7\u7528\u4e86 Transformer-Physx \u4e2d\u63d0\u4f9b\u7684\u6570\u636e\u3002\u8be5\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u4f7f\u7528 OpenFOAM \u6c42\u5f97\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u5927\u5c0f\u4e3a0.5\uff0c\\(Re\\) \u4ece\u4ee5\u4e0b\u8303\u56f4\u4e2d\u968f\u673a\u9009\u53d6\uff1a</p> \\[Re \\sim(100, 750)\\] <p>\u6570\u636e\u96c6\u7684\u5212\u5206\u5982\u4e0b\uff1a</p> \u6570\u636e\u96c6 \u6d41\u573a\u4eff\u771f\u7684\u6570\u91cf \u65f6\u95f4\u6b65\u7684\u6570\u91cf \u4e0b\u8f7d\u5730\u5740 \u8bad\u7ec3\u96c6 27 400 cylinder_training.hdf5 \u9a8c\u8bc1\u96c6 6 400 cylinder_valid.hdf5 <p>\u6570\u636e\u96c6\u5b98\u7f51\u4e3a\uff1ahttps://zenodo.org/record/5148524#.ZDe77-xByrc</p>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#32-embedding","title":"3.2 Embedding \u6a21\u578b","text":"<p>\u9996\u5148\u5c55\u793a\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u5404\u4e2a\u53c2\u6570\u53d8\u91cf\uff0c\u6bcf\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u4f1a\u5728\u4e0b\u9762\u4f7f\u7528\u5230\u65f6\u8fdb\u884c\u89e3\u91ca\u3002</p> examples/cylinder/2d_unsteady/transformer_physx/train_enn.py<pre><code>weights = (10.0 * (cfg.TRAIN_BLOCK_SIZE - 1), 10.0 * cfg.TRAIN_BLOCK_SIZE)\nregularization_key = \"k_matrix\"\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#321","title":"3.2.1 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_enn.py<pre><code>train_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"CylinderDataset\",\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n        \"stride\": 16,\n        \"weight_dict\": {\n            key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n        },\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 4,\n}\n</code></pre> <p>\u5176\u4e2d\uff0c\"dataset\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Dataset</code> \u7c7b\u540d\u4e3a <code>CylinderDataset</code>\uff0c\u53e6\u5916\u8fd8\u6307\u5b9a\u4e86\u8be5\u7c7b\u521d\u59cb\u5316\u65f6\u53c2\u6570\u7684\u53d6\u503c\uff1a</p> <ol> <li><code>file_path</code>\uff1a\u4ee3\u8868\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6587\u4ef6\u8def\u5f84\uff0c\u6307\u5b9a\u4e3a\u53d8\u91cf <code>train_file_path</code> \u7684\u503c\uff1b</li> <li><code>input_keys</code>\uff1a\u4ee3\u8868\u6a21\u578b\u8f93\u5165\u6570\u636e\u7684\u53d8\u91cf\u540d\u79f0\uff0c\u6b64\u5904\u586b\u5165\u53d8\u91cf <code>input_keys</code>\uff1b</li> <li><code>label_keys</code>\uff1a\u4ee3\u8868\u771f\u5b9e\u6807\u7b7e\u7684\u53d8\u91cf\u540d\u79f0\uff0c\u6b64\u5904\u586b\u5165\u53d8\u91cf <code>output_keys</code>\uff1b</li> <li><code>block_size</code>\uff1a\u4ee3\u8868\u4f7f\u7528\u591a\u957f\u7684\u65f6\u95f4\u6b65\u8fdb\u884c\u8bad\u7ec3\uff0c\u6307\u5b9a\u4e3a\u53d8\u91cf <code>train_block_size</code> \u7684\u503c\uff1b</li> <li><code>stride</code>\uff1a\u4ee3\u8868\u8fde\u7eed\u7684\u4e24\u4e2a\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u7684\u65f6\u95f4\u6b65\u95f4\u9694\uff0c\u6307\u5b9a\u4e3a16\uff1b</li> <li><code>weight_dict</code>\uff1a\u4ee3\u8868\u6a21\u578b\u8f93\u51fa\u5404\u4e2a\u53d8\u91cf\u4e0e\u771f\u5b9e\u6807\u7b7e\u635f\u5931\u51fd\u6570\u7684\u6743\u91cd\uff0c\u6b64\u5904\u4f7f\u7528 <code>output_keys</code>\u3001<code>weights</code> \u751f\u6210\u3002</li> </ol> <p>\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u53e6\u5916\u8fd8\u6307\u5b9a\u4e86\u8be5\u7c7b\u521d\u59cb\u5316\u65f6\u53c2\u6570 <code>drop_last</code>\u3001<code>shuffle</code> \u5747\u4e3a <code>True</code>\u3002</p> <p><code>train_dataloader_cfg</code> \u8fd8\u5b9a\u4e49\u4e86 <code>batch_size</code>\u3001<code>num_workers</code> \u7684\u503c\u3002</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_enn.py<pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.MSELossWithL2Decay(\n        regularization_dict={\n            regularization_key: 1.0e-2 * (cfg.TRAIN_BLOCK_SIZE - 1)\n        }\n    ),\n    {\n        key: lambda out, k=key: out[k]\n        for key in cfg.MODEL.output_keys + (regularization_key,)\n    },\n    name=\"Sup\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u6570\u636e\u7684\u52a0\u8f7d\u65b9\u5f0f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e0a\u6587\u4e2d\u5b9a\u4e49\u7684 <code>train_dataloader_cfg</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u4f7f\u7528\u5e26\u6709 L2Decay \u7684 MSELoss\uff0c\u7c7b\u540d\u4e3a <code>MSELossWithL2Decay</code>\uff0c<code>regularization_dict</code> \u8bbe\u7f6e\u4e86\u6b63\u5219\u5316\u7684\u53d8\u91cf\u540d\u79f0\u548c\u5bf9\u5e94\u7684\u6743\u91cd\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u8868\u793a\u5728\u8bad\u7ec3\u65f6\u5982\u4f55\u8ba1\u7b97\u9700\u8981\u88ab\u7ea6\u675f\u7684\u4e2d\u95f4\u53d8\u91cf\uff0c\u6b64\u5904\u6211\u4eec\u7ea6\u675f\u7684\u53d8\u91cf\u5c31\u662f\u7f51\u7edc\u7684\u8f93\u51fa\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"Sup\"\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#322","title":"3.2.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0cEmbedding \u6a21\u578b\u4f7f\u7528\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0 Embedding \u6a21\u578b\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002</p> <p> </p> Embedding \u7f51\u7edc\u6a21\u578b <p>\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_enn.py<pre><code>model = ppsci.arch.CylinderEmbedding(\n    cfg.MODEL.input_keys,\n    cfg.MODEL.output_keys + (regularization_key,),\n    data_mean,\n    data_std,\n)\n</code></pre> <p>\u5176\u4e2d\uff0c<code>CylinderEmbedding</code> \u7684\u524d\u4e24\u4e2a\u53c2\u6570\u5728\u524d\u6587\u4e2d\u5df2\u6709\u63cf\u8ff0\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\uff0c\u7f51\u7edc\u6a21\u578b\u7684\u7b2c\u4e09\u3001\u56db\u4e2a\u53c2\u6570\u662f\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5747\u503c\u548c\u65b9\u5dee\uff0c\u7528\u4e8e\u5f52\u4e00\u5316\u8f93\u5165\u6570\u636e\u3002\u8ba1\u7b97\u5747\u503c\u3001\u65b9\u5dee\u7684\u7684\u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_enn.py<pre><code>def get_mean_std(data: np.ndarray, visc: np.ndarray):\n    mean = np.asarray(\n        [\n            np.mean(data[:, :, 0]),\n            np.mean(data[:, :, 1]),\n            np.mean(data[:, :, 2]),\n            np.mean(visc),\n        ]\n    ).reshape(1, 4, 1, 1)\n    std = np.asarray(\n        [\n            np.std(data[:, :, 0]),\n            np.std(data[:, :, 1]),\n            np.std(data[:, :, 2]),\n            np.std(visc),\n        ]\n    ).reshape(1, 4, 1, 1)\n    return mean, std\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#323","title":"3.2.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>ExponentialDecay</code>\uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a0.001\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>Adam</code>\uff0c\u68af\u5ea6\u88c1\u526a\u4f7f\u7528\u4e86 Paddle \u5185\u7f6e\u7684 <code>ClipGradByGlobalNorm</code> \u65b9\u6cd5\u3002\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_enn.py<pre><code># init optimizer and lr scheduler\nclip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\nlr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n    iters_per_epoch=ITERS_PER_EPOCH,\n    decay_steps=ITERS_PER_EPOCH,\n    **cfg.TRAIN.lr_scheduler,\n)()\noptimizer = ppsci.optimizer.Adam(\n    lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n)(model)\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#324","title":"3.2.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_enn.py<pre><code>eval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"CylinderDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 32,\n        \"weight_dict\": {\n            key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n        },\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"num_workers\": 4,\n}\n\nmse_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"MSE_Validator\",\n)\nvalidator = {mse_validator.name: mse_validator}\n</code></pre> <p><code>SupervisedValidator</code> \u8bc4\u4f30\u5668\u4e0e <code>SupervisedConstraint</code> \u6bd4\u8f83\u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u8bc4\u4f30\u5668\u9700\u8981\u8bbe\u7f6e\u8bc4\u4ef7\u6307\u6807 <code>metric</code>\uff0c\u5728\u8fd9\u91cc\u4f7f\u7528 <code>ppsci.metric.MSE</code> \u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#325","title":"3.2.5 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/cylinder/2d_unsteady/transformer_physx/train_enn.py<pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=True,\n    eval_freq=50,\n    validator=validator,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#33-transformer","title":"3.3 Transformer \u6a21\u578b","text":"<p>\u4e0a\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u6784\u5efa Embedding \u6a21\u578b\u7684\u8bad\u7ec3\u3001\u8bc4\u4f30\uff0c\u5728\u672c\u8282\u4e2d\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528\u8bad\u7ec3\u597d\u7684 Embedding \u6a21\u578b\u8bad\u7ec3 Transformer \u6a21\u578b\u3002\u56e0\u4e3a\u8bad\u7ec3 Transformer \u6a21\u578b\u7684\u6b65\u9aa4\u4e0e\u8bad\u7ec3 Embedding \u6a21\u578b\u7684\u6b65\u9aa4\u57fa\u672c\u76f8\u4f3c\uff0c\u56e0\u6b64\u672c\u8282\u5728\u4e24\u8005\u7684\u91cd\u590d\u90e8\u5206\u7684\u5404\u4e2a\u53c2\u6570\u4e0d\u518d\u8be6\u7ec6\u4ecb\u7ecd\u3002\u9996\u5148\u5c06\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u5404\u4e2a\u53c2\u6570\u53d8\u91cf\u5c55\u793a\u5982\u4e0b\uff0c\u6bcf\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u4f1a\u5728\u4e0b\u9762\u4f7f\u7528\u5230\u65f6\u8fdb\u884c\u89e3\u91ca\u3002</p> examples/cylinder/2d_unsteady/transformer_physx/conf/transformer.yaml<pre><code># general settings\nmode: train # running mode: train/eval\nseed: 42\noutput_dir: ${hydra:run.dir}\nTRAIN_BLOCK_SIZE: 16\nVALID_BLOCK_SIZE: 256\nTRAIN_FILE_PATH: ./datasets/cylinder_training.hdf5\nVALID_FILE_PATH: ./datasets/cylinder_valid.hdf5\nlog_freq: 20\n\n# set working condition\nEMBEDDING_MODEL_PATH: ./outputs_cylinder2d_unsteady_transformer_physx_enn/checkpoints/latest\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#331","title":"3.3.1 \u7ea6\u675f\u6784\u5efa","text":"<p>Transformer \u6a21\u578b\u540c\u6837\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code>train_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"CylinderDataset\",\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n        \"stride\": 4,\n        \"embedding_model\": embedding_model,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 4,\n}\n</code></pre> <p>\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\u4e0e Embedding \u6a21\u578b\u4e2d\u7684\u57fa\u672c\u4e00\u81f4\uff0c\u4e0d\u518d\u8d58\u8ff0\u3002\u9700\u8981\u8bf4\u660e\u7684\u662f\u7531\u4e8e Transformer \u6a21\u578b\u8bad\u7ec3\u7684\u8f93\u5165\u6570\u636e\u662f Embedding \u6a21\u578b Encoder \u6a21\u5757\u7684\u8f93\u51fa\u6570\u636e\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u8bad\u7ec3\u597d\u7684 Embedding \u6a21\u578b\u4f5c\u4e3a <code>CylinderDataset</code> \u7684\u4e00\u4e2a\u53c2\u6570\uff0c\u5728\u521d\u59cb\u5316\u65f6\u9996\u5148\u5c06\u8bad\u7ec3\u6570\u636e\u6620\u5c04\u5230\u7f16\u7801\u7a7a\u95f4\u3002</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    name=\"Sup\",\n)\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#332","title":"3.3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0cTransformer \u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u90fd\u662f\u7f16\u7801\u7a7a\u95f4\u4e2d\u7684\u5411\u91cf\uff0c\u4f7f\u7528\u7684 Transformer \u7ed3\u6784\u5982\u4e0b\uff1a</p> <p> </p> Transformer \u7f51\u7edc\u6a21\u578b <p>\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code>model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n</code></pre> <p>\u7c7b <code>PhysformerGPT2</code> \u9664\u4e86\u9700\u8981\u586b\u5165 <code>input_keys</code>\u3001<code>output_keys</code> \u5916\uff0c\u8fd8\u9700\u8981\u8bbe\u7f6e Transformer \u6a21\u578b\u7684\u5c42\u6570 <code>num_layers</code>\u3001\u4e0a\u4e0b\u6587\u7684\u5927\u5c0f <code>num_ctx</code>\u3001\u8f93\u5165\u7684 Embedding \u5411\u91cf\u7684\u957f\u5ea6 <code>embed_size</code>\u3001\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u53c2\u6570 <code>num_heads</code>\uff0c\u5728\u8fd9\u91cc\u586b\u5165\u7684\u6570\u503c\u4e3a6\u300116\u3001128\u30014\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#333","title":"3.3.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>CosineWarmRestarts</code>\uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a0.001\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>Adam</code>\uff0c\u68af\u5ea6\u88c1\u526a\u4f7f\u7528\u4e86 Paddle \u5185\u7f6e\u7684 <code>ClipGradByGlobalNorm</code> \u65b9\u6cd5\u3002\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code># init optimizer and lr scheduler\nclip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\nlr_scheduler = ppsci.optimizer.lr_scheduler.CosineWarmRestarts(\n    iters_per_epoch=ITERS_PER_EPOCH, **cfg.TRAIN.lr_scheduler\n)()\noptimizer = ppsci.optimizer.Adam(\n    lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n)(model)\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#334","title":"3.3.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code>eval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"CylinderDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 1024,\n        \"embedding_model\": embedding_model,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"num_workers\": 4,\n}\n\nmse_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"MSE_Validator\",\n)\nvalidator = {mse_validator.name: mse_validator}\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#335","title":"3.3.5 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u53ef\u4ee5\u901a\u8fc7\u6784\u5efa\u53ef\u89c6\u5316\u5668\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\u5c06\u8bc4\u4f30\u7ed3\u679c\u53ef\u89c6\u5316\u51fa\u6765\uff0c\u7531\u4e8e Transformer \u6a21\u578b\u7684\u8f93\u51fa\u6570\u636e\u662f\u9884\u6d4b\u7684\u7f16\u7801\u7a7a\u95f4\u7684\u6570\u636e\u65e0\u6cd5\u76f4\u63a5\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u56e0\u6b64\u9700\u8981\u989d\u5916\u5c06\u8f93\u51fa\u6570\u636e\u4f7f\u7528 Embedding \u7f51\u7edc\u7684 Decoder \u6a21\u5757\u53d8\u6362\u5230\u7269\u7406\u72b6\u6001\u7a7a\u95f4\u3002</p> <p>\u5728\u672c\u6587\u4e2d\u9996\u5148\u5b9a\u4e49\u4e86\u5bf9 Transformer \u6a21\u578b\u8f93\u51fa\u6570\u636e\u53d8\u6362\u5230\u7269\u7406\u72b6\u6001\u7a7a\u95f4\u7684\u4ee3\u7801\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code>def build_embedding_model(embedding_model_path: str) -&gt; ppsci.arch.CylinderEmbedding:\n    input_keys = (\"states\", \"visc\")\n    output_keys = (\"pred_states\", \"recover_states\")\n    regularization_key = \"k_matrix\"\n    model = ppsci.arch.CylinderEmbedding(\n        input_keys, output_keys + (regularization_key,)\n    )\n    save_load.load_pretrain(model, embedding_model_path)\n    return model\n\n\nclass OutputTransform(object):\n    def __init__(self, model: base.Arch):\n        self.model = model\n        self.model.eval()\n\n    def __call__(self, x: Dict[str, paddle.Tensor]) -&gt; Dict[str, paddle.Tensor]:\n        pred_embeds = x[\"pred_embeds\"]\n        pred_states = self.model.decoder(pred_embeds)\n        # pred_states.shape=(B, T, C, H, W)\n        return pred_states\n</code></pre> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code>embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\noutput_transform = OutputTransform(embedding_model)\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230\uff0c\u7a0b\u5e8f\u9996\u5148\u8f7d\u5165\u4e86\u8bad\u7ec3\u597d\u7684 Embedding \u6a21\u578b\uff0c\u7136\u540e\u5728 <code>OutputTransform</code> \u7684 <code>__call__</code> \u51fd\u6570\u5185\u5b9e\u73b0\u4e86\u7f16\u7801\u5411\u91cf\u5230\u7269\u7406\u72b6\u6001\u7a7a\u95f4\u7684\u53d8\u6362\u3002</p> <p>\u5728\u5b9a\u4e49\u597d\u4e86\u4ee5\u4e0a\u4ee3\u7801\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u5b9e\u73b0\u53ef\u89c6\u5316\u5668\u4ee3\u7801\u7684\u6784\u5efa\u4e86\uff1a</p> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code>visualizer = {\n    \"visualize_states\": ppsci.visualize.Visualizer2DPlot(\n        vis_datas,\n        {\n            \"target_ux\": lambda d: d[\"states\"][:, :, 0],\n            \"pred_ux\": lambda d: output_transform(d)[:, :, 0],\n            \"target_uy\": lambda d: d[\"states\"][:, :, 1],\n            \"pred_uy\": lambda d: output_transform(d)[:, :, 1],\n            \"target_p\": lambda d: d[\"states\"][:, :, 2],\n            \"preds_p\": lambda d: output_transform(d)[:, :, 2],\n        },\n        batch_size=1,\n        num_timestamps=10,\n        stride=20,\n        xticks=np.linspace(-2, 14, 9),\n        yticks=np.linspace(-4, 4, 5),\n        prefix=\"result_states\",\n    )\n}\n</code></pre> <p>\u9996\u5148\u4f7f\u7528\u4e0a\u6587\u4e2d\u7684 <code>mse_validator</code> \u4e2d\u7684\u6570\u636e\u96c6\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u53e6\u5916\u8fd8\u5f15\u5165\u4e86 <code>vis_data_nums</code> \u53d8\u91cf\u7528\u4e8e\u63a7\u5236\u9700\u8981\u53ef\u89c6\u5316\u6837\u672c\u7684\u6570\u91cf\u3002\u6700\u540e\u901a\u8fc7 <code>VisualizerScatter3D</code> \u6784\u5efa\u53ef\u89c6\u5316\u5668\u3002</p>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#336","title":"3.3.6 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code>solver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    validator=validator,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Two-stage training\n# 1. Train a embedding model by running train_enn.py.\n# 2. Load pretrained embedding model and freeze it, then train a transformer model by running train_transformer.py.\n\n# This file is for step2: training a transformer model, based on frozen pretrained embedding model.\n# This file is based on PaddleScience/ppsci API.\nfrom os import path as osp\nfrom typing import Dict\n\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.arch import base\nfrom ppsci.utils import logger\nfrom ppsci.utils import save_load\n\n\ndef build_embedding_model(embedding_model_path: str) -&gt; ppsci.arch.CylinderEmbedding:\n    input_keys = (\"states\", \"visc\")\n    output_keys = (\"pred_states\", \"recover_states\")\n    regularization_key = \"k_matrix\"\n    model = ppsci.arch.CylinderEmbedding(\n        input_keys, output_keys + (regularization_key,)\n    )\n    save_load.load_pretrain(model, embedding_model_path)\n    return model\n\n\nclass OutputTransform(object):\n    def __init__(self, model: base.Arch):\n        self.model = model\n        self.model.eval()\n\n    def __call__(self, x: Dict[str, paddle.Tensor]) -&gt; Dict[str, paddle.Tensor]:\n        pred_embeds = x[\"pred_embeds\"]\n        pred_states = self.model.decoder(pred_embeds)\n        # pred_states.shape=(B, T, C, H, W)\n        return pred_states\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    output_transform = OutputTransform(embedding_model)\n\n    # manually build constraint(s)\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"CylinderDataset\",\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n            \"stride\": 4,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 4,\n    }\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(constraint[\"Sup\"].data_loader)\n\n    # manually init model\n    model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n\n    # init optimizer and lr scheduler\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\n    lr_scheduler = ppsci.optimizer.lr_scheduler.CosineWarmRestarts(\n        iters_per_epoch=ITERS_PER_EPOCH, **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(\n        lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n    )(model)\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"CylinderDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 1024,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    # set visualizer(optional)\n    states = mse_validator.data_loader.dataset.data\n    embedding_data = mse_validator.data_loader.dataset.embedding_data\n\n    vis_datas = {\n        \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1],\n        \"states\": states[: cfg.VIS_DATA_NUMS, 1:],\n    }\n\n    visualizer = {\n        \"visualize_states\": ppsci.visualize.Visualizer2DPlot(\n            vis_datas,\n            {\n                \"target_ux\": lambda d: d[\"states\"][:, :, 0],\n                \"pred_ux\": lambda d: output_transform(d)[:, :, 0],\n                \"target_uy\": lambda d: d[\"states\"][:, :, 1],\n                \"pred_uy\": lambda d: output_transform(d)[:, :, 1],\n                \"target_p\": lambda d: d[\"states\"][:, :, 2],\n                \"preds_p\": lambda d: output_transform(d)[:, :, 2],\n            },\n            batch_size=1,\n            num_timestamps=10,\n            stride=20,\n            xticks=np.linspace(-2, 14, 9),\n            yticks=np.linspace(-4, 4, 5),\n            prefix=\"result_states\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        validator=validator,\n        visualizer=visualizer,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # directly evaluate pretrained model(optional)\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    output_transform = OutputTransform(embedding_model)\n\n    # manually init model\n    model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"CylinderDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 1024,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    # set visualizer(optional)\n    states = mse_validator.data_loader.dataset.data\n    embedding_data = mse_validator.data_loader.dataset.embedding_data\n    vis_datas = {\n        \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1],\n        \"states\": states[: cfg.VIS_DATA_NUMS, 1:],\n    }\n\n    visualizer = {\n        \"visulzie_states\": ppsci.visualize.Visualizer2DPlot(\n            vis_datas,\n            {\n                \"target_ux\": lambda d: d[\"states\"][:, :, 0],\n                \"pred_ux\": lambda d: output_transform(d)[:, :, 0],\n                \"target_uy\": lambda d: d[\"states\"][:, :, 1],\n                \"pred_uy\": lambda d: output_transform(d)[:, :, 1],\n                \"target_p\": lambda d: d[\"states\"][:, :, 2],\n                \"preds_p\": lambda d: output_transform(d)[:, :, 2],\n            },\n            batch_size=1,\n            num_timestamps=10,\n            stride=20,\n            xticks=np.linspace(-2, 14, 9),\n            yticks=np.linspace(-4, 4, 5),\n            prefix=\"result_states\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n    # visualize prediction for pretrained model(optional)\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    model_cfg = {\n        **cfg.MODEL,\n        \"embedding_model\": embedding_model,\n        \"input_keys\": [\"states\"],\n        \"output_keys\": [\"pred_states\"],\n    }\n    model = ppsci.arch.PhysformerGPT2(**model_cfg)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {\n            \"states\": InputSpec([1, 255, 3, 64, 128], \"float32\", name=\"states\"),\n            \"visc\": InputSpec([1, 1], \"float32\", name=\"visc\"),\n        },\n    ]\n\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy import python_infer\n\n    predictor = python_infer.GeneralPredictor(cfg)\n\n    dataset_cfg = {\n        \"name\": \"CylinderDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 1024,\n    }\n\n    dataset = ppsci.data.dataset.build_dataset(dataset_cfg)\n\n    input_dict = {\n        \"states\": dataset.data[: cfg.VIS_DATA_NUMS, :-1],\n        \"visc\": dataset.visc[: cfg.VIS_DATA_NUMS],\n    }\n\n    output_dict = predictor.predict(input_dict)\n\n    # mapping data to cfg.INFER.output_keys\n    output_keys = [\"pred_states\"]\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(output_keys, output_dict.keys())\n    }\n    for i in range(cfg.VIS_DATA_NUMS):\n        ppsci.visualize.plot.save_plot_from_2d_dict(\n            f\"./cylinder_transformer_pred_{i}\",\n            {\n                \"pred_ux\": output_dict[\"pred_states\"][i][:, 0],\n                \"pred_uy\": output_dict[\"pred_states\"][i][:, 1],\n                \"pred_p\": output_dict[\"pred_states\"][i][:, 2],\n            },\n            (\"pred_ux\", \"pred_uy\", \"pred_p\"),\n            10,\n            20,\n            np.linspace(-2, 14, 9),\n            np.linspace(-4, 4, 5),\n        )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"transformer.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> examples/cylinder/2d_unsteady/transformer_physx/train_transformer.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Two-stage training\n# 1. Train a embedding model by running train_enn.py.\n# 2. Load pretrained embedding model and freeze it, then train a transformer model by running train_transformer.py.\n\n# This file is for step2: training a transformer model, based on frozen pretrained embedding model.\n# This file is based on PaddleScience/ppsci API.\nfrom os import path as osp\nfrom typing import Dict\n\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.arch import base\nfrom ppsci.utils import logger\nfrom ppsci.utils import save_load\n\n\ndef build_embedding_model(embedding_model_path: str) -&gt; ppsci.arch.CylinderEmbedding:\n    input_keys = (\"states\", \"visc\")\n    output_keys = (\"pred_states\", \"recover_states\")\n    regularization_key = \"k_matrix\"\n    model = ppsci.arch.CylinderEmbedding(\n        input_keys, output_keys + (regularization_key,)\n    )\n    save_load.load_pretrain(model, embedding_model_path)\n    return model\n\n\nclass OutputTransform(object):\n    def __init__(self, model: base.Arch):\n        self.model = model\n        self.model.eval()\n\n    def __call__(self, x: Dict[str, paddle.Tensor]) -&gt; Dict[str, paddle.Tensor]:\n        pred_embeds = x[\"pred_embeds\"]\n        pred_states = self.model.decoder(pred_embeds)\n        # pred_states.shape=(B, T, C, H, W)\n        return pred_states\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    output_transform = OutputTransform(embedding_model)\n\n    # manually build constraint(s)\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"CylinderDataset\",\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n            \"stride\": 4,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 4,\n    }\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(constraint[\"Sup\"].data_loader)\n\n    # manually init model\n    model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n\n    # init optimizer and lr scheduler\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\n    lr_scheduler = ppsci.optimizer.lr_scheduler.CosineWarmRestarts(\n        iters_per_epoch=ITERS_PER_EPOCH, **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(\n        lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n    )(model)\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"CylinderDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 1024,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    # set visualizer(optional)\n    states = mse_validator.data_loader.dataset.data\n    embedding_data = mse_validator.data_loader.dataset.embedding_data\n\n    vis_datas = {\n        \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1],\n        \"states\": states[: cfg.VIS_DATA_NUMS, 1:],\n    }\n\n    visualizer = {\n        \"visualize_states\": ppsci.visualize.Visualizer2DPlot(\n            vis_datas,\n            {\n                \"target_ux\": lambda d: d[\"states\"][:, :, 0],\n                \"pred_ux\": lambda d: output_transform(d)[:, :, 0],\n                \"target_uy\": lambda d: d[\"states\"][:, :, 1],\n                \"pred_uy\": lambda d: output_transform(d)[:, :, 1],\n                \"target_p\": lambda d: d[\"states\"][:, :, 2],\n                \"preds_p\": lambda d: output_transform(d)[:, :, 2],\n            },\n            batch_size=1,\n            num_timestamps=10,\n            stride=20,\n            xticks=np.linspace(-2, 14, 9),\n            yticks=np.linspace(-4, 4, 5),\n            prefix=\"result_states\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        validator=validator,\n        visualizer=visualizer,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # directly evaluate pretrained model(optional)\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    output_transform = OutputTransform(embedding_model)\n\n    # manually init model\n    model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"CylinderDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 1024,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    # set visualizer(optional)\n    states = mse_validator.data_loader.dataset.data\n    embedding_data = mse_validator.data_loader.dataset.embedding_data\n    vis_datas = {\n        \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1],\n        \"states\": states[: cfg.VIS_DATA_NUMS, 1:],\n    }\n\n    visualizer = {\n        \"visulzie_states\": ppsci.visualize.Visualizer2DPlot(\n            vis_datas,\n            {\n                \"target_ux\": lambda d: d[\"states\"][:, :, 0],\n                \"pred_ux\": lambda d: output_transform(d)[:, :, 0],\n                \"target_uy\": lambda d: d[\"states\"][:, :, 1],\n                \"pred_uy\": lambda d: output_transform(d)[:, :, 1],\n                \"target_p\": lambda d: d[\"states\"][:, :, 2],\n                \"preds_p\": lambda d: output_transform(d)[:, :, 2],\n            },\n            batch_size=1,\n            num_timestamps=10,\n            stride=20,\n            xticks=np.linspace(-2, 14, 9),\n            yticks=np.linspace(-4, 4, 5),\n            prefix=\"result_states\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n    # visualize prediction for pretrained model(optional)\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    model_cfg = {\n        **cfg.MODEL,\n        \"embedding_model\": embedding_model,\n        \"input_keys\": [\"states\"],\n        \"output_keys\": [\"pred_states\"],\n    }\n    model = ppsci.arch.PhysformerGPT2(**model_cfg)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {\n            \"states\": InputSpec([1, 255, 3, 64, 128], \"float32\", name=\"states\"),\n            \"visc\": InputSpec([1, 1], \"float32\", name=\"visc\"),\n        },\n    ]\n\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy import python_infer\n\n    predictor = python_infer.GeneralPredictor(cfg)\n\n    dataset_cfg = {\n        \"name\": \"CylinderDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 1024,\n    }\n\n    dataset = ppsci.data.dataset.build_dataset(dataset_cfg)\n\n    input_dict = {\n        \"states\": dataset.data[: cfg.VIS_DATA_NUMS, :-1],\n        \"visc\": dataset.visc[: cfg.VIS_DATA_NUMS],\n    }\n\n    output_dict = predictor.predict(input_dict)\n\n    # mapping data to cfg.INFER.output_keys\n    output_keys = [\"pred_states\"]\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(output_keys, output_dict.keys())\n    }\n    for i in range(cfg.VIS_DATA_NUMS):\n        ppsci.visualize.plot.save_plot_from_2d_dict(\n            f\"./cylinder_transformer_pred_{i}\",\n            {\n                \"pred_ux\": output_dict[\"pred_states\"][i][:, 0],\n                \"pred_uy\": output_dict[\"pred_states\"][i][:, 1],\n                \"pred_p\": output_dict[\"pred_states\"][i][:, 2],\n            },\n            (\"pred_ux\", \"pred_uy\", \"pred_p\"),\n            10,\n            20,\n            np.linspace(-2, 14, 9),\n            np.linspace(-4, 4, 5),\n        )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"transformer.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/cylinder2d_unsteady_transformer_physx/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u9488\u5bf9\u672c\u6848\u4f8b\u4e2d\u7684\u95ee\u9898\uff0c\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u548c\u4f20\u7edf\u6570\u503c\u5fae\u5206\u7684\u7ed3\u679c\u5982\u4e0b\u6240\u793a\uff0c\u5176\u4e2d ux\u3001uy \u5206\u522b\u4ee3\u8868 x\u3001y\u65b9\u5411\u4e0a\u7684\u901f\u5ea6\uff0cp \u4ee3\u8868\u538b\u529b\u3002</p> <p> </p> \u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff08\"pred\"\uff09\u4e0e\u4f20\u7edf\u6570\u503c\u5fae\u5206\u7ed3\u679c\uff08\"target\"\uff09"},{"location":"zh/examples/darcy2d/","title":"Darcy2D","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#2d-darcy","title":"2D-Darcy","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code>python darcy2d.py\n</code></pre> <pre><code>python darcy2d.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/darcy2d/darcy2d_pretrained.pdparams\n</code></pre> <pre><code>python darcy2d.py mode=export\n</code></pre> <pre><code>python darcy2d.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 darcy2d_pretrained.pdparams loss(Residual): 0.36500MSE.poisson(Residual): 0.00006","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>Darcy Flow\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fbe\u897f\u5b9a\u5f8b\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8ba1\u7b97\u6db2\u4f53\u7684\u6d41\u52a8\u3002\u5728\u5730\u4e0b\u6c34\u6a21\u62df\u3001\u6c34\u6587\u5b66\u3001\u6c34\u6587\u5730\u8d28\u5b66\u548c\u77f3\u6cb9\u5de5\u7a0b\u7b49\u9886\u57df\u4e2d\uff0cDarcy Flow\u88ab\u5e7f\u6cdb\u5e94\u7528\u3002</p> <p>\u4f8b\u5982\uff0c\u5728\u77f3\u6cb9\u5de5\u7a0b\u4e2d\uff0cDarcy Flow\u88ab\u7528\u6765\u9884\u6d4b\u548c\u6a21\u62df\u77f3\u6cb9\u5728\u591a\u5b54\u4ecb\u8d28\u4e2d\u7684\u6d41\u52a8\u3002\u591a\u5b54\u4ecb\u8d28\u662f\u4e00\u79cd\u7531\u5c0f\u9897\u7c92\u7ec4\u6210\u7684\u7269\u8d28\uff0c\u9897\u7c92\u4e4b\u95f4\u5b58\u5728\u7a7a\u9699\u3002\u77f3\u6cb9\u4f1a\u586b\u5145\u8fd9\u4e9b\u7a7a\u9699\u5e76\u5728\u5176\u4e2d\u6d41\u52a8\u3002\u901a\u8fc7Darcy Flow\uff0c\u5de5\u7a0b\u5e08\u53ef\u4ee5\u9884\u6d4b\u548c\u63a7\u5236\u77f3\u6cb9\u7684\u6d41\u52a8\uff0c\u4ece\u800c\u4f18\u5316\u77f3\u6cb9\u5f00\u91c7\u548c\u751f\u4ea7\u8fc7\u7a0b\u3002</p> <p>\u6b64\u5916\uff0cDarcy Flow\u4e5f\u88ab\u7528\u4e8e\u7814\u7a76\u548c\u9884\u6d4b\u5730\u4e0b\u6c34\u7684\u6d41\u52a8\u3002\u4f8b\u5982\uff0c\u5728\u519c\u4e1a\u9886\u57df\uff0c\u901a\u8fc7\u6a21\u62df\u5730\u4e0b\u6c34\u6d41\u52a8\u53ef\u4ee5\u9884\u6d4b\u704c\u6e89\u5bf9\u571f\u58e4\u6c34\u5206\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u4f18\u5316\u4f5c\u7269\u704c\u6e89\u8ba1\u5212\u3002\u5728\u57ce\u5e02\u89c4\u5212\u548c\u73af\u5883\u4fdd\u62a4\u4e2d\uff0cDarcy Flow\u4e5f\u88ab\u7528\u6765\u9884\u6d4b\u548c\u9632\u6b62\u5730\u4e0b\u6c34\u6c61\u67d3\u3002</p> <p>2D-Darcy \u662f\u8fbe\u897f\u6e17\u6d41\uff08Darcy flow\uff09\u7684\u4e00\u79cd\uff0c\u6d41\u4f53\u5728\u591a\u5b54\u4ecb\u8d28\u4e2d\u6d41\u52a8\u65f6\uff0c\u6e17\u6d41\u901f\u5ea6\u5c0f\uff0c\u6d41\u52a8\u670d\u4ece\u8fbe\u897f\u5b9a\u5f8b\uff0c\u6e17\u6d41\u901f\u5ea6\u548c\u538b\u529b\u68af\u5ea6\u4e4b\u95f4\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u8fd9\u79cd\u6d41\u52a8\u79f0\u4e3a\u7ebf\u6027\u6e17\u6d41\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u5047\u8bbe\u8fbe\u897f\u6d41\u6a21\u578b\u4e2d\uff0c\u6bcf\u4e2a\u4f4d\u7f6e \\((x,y)\\) \u4e0a\u7684\u6d41\u901f \\(\\mathbf{u}\\) \u548c\u538b\u529b \\(p\\) \u4e4b\u95f4\u6ee1\u8db3\u4ee5\u4e0b\u5173\u7cfb\u5f0f\uff1a</p> \\[ \\begin{cases} \\begin{aligned}   \\mathbf{u}+\\nabla p =&amp; 0,(x,y) \\in \\Omega \\\\   \\nabla \\cdot \\mathbf{u} =&amp; f,(x,y) \\in \\Omega \\\\   p(x,y) =&amp; \\sin(2 \\pi x )\\cos(2 \\pi y), (x,y) \\in \\partial \\Omega \\end{aligned} \\end{cases} \\]","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 darcy-2d \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y)\\) \u90fd\u6709\u5bf9\u5e94\u7684\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf \\(p\\) \uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((x, y)\\) \u5230 \\(p\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^1\\) \uff0c\u5373\uff1a</p> \\[ p = f(x, y) \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"x\", \"y\")</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>\"p\"</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 5 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 20 \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e 2D-Poisson \u4f7f\u7528\u7684\u662f Poisson \u65b9\u7a0b\u76842\u7ef4\u5f62\u5f0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>Poisson</code>\uff0c\u6307\u5b9a\u8be5\u7c7b\u7684\u53c2\u6570 <code>dim</code> \u4e3a2\u3002</p> <pre><code># set equation\nequation = {\"Poisson\": ppsci.equation.Poisson(2)}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d 2D darcy \u95ee\u9898\u4f5c\u7528\u5728\u4ee5 (0.0, 0.0),  (1.0, 1.0) \u4e3a\u5bf9\u89d2\u7ebf\u7684\u4e8c\u7ef4\u77e9\u5f62\u533a\u57df\uff0c \u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u7a7a\u95f4\u51e0\u4f55 <code>Rectangle</code> \u4f5c\u4e3a\u8ba1\u7b97\u57df\u3002</p> <pre><code># set geometry\ngeom = {\"rect\": ppsci.geometry.Rectangle((0.0, 0.0), (1.0, 1.0))}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e24\u4e2a\u7ea6\u675f\u6761\u4ef6\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u7684\u8bad\u7ec3\u5206\u522b\u662f\u4f5c\u7528\u4e8e\u91c7\u6837\u70b9\u4e0a\u7684 darcy \u65b9\u7a0b\u7ea6\u675f\u548c\u4f5c\u7528\u4e8e\u8fb9\u754c\u70b9\u4e0a\u7684\u7ea6\u675f\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u79cd\u7ea6\u675f\u6307\u5b9a\u91c7\u6837\u70b9\u4e2a\u6570\uff0c\u8868\u793a\u6bcf\u4e00\u79cd\u7ea6\u675f\u5728\u5176\u5bf9\u5e94\u8ba1\u7b97\u57df\u5185\u91c7\u6837\u6570\u636e\u7684\u6570\u91cf\uff0c\u4ee5\u53ca\u901a\u7528\u7684\u91c7\u6837\u914d\u7f6e\u3002</p> <pre><code># set dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": \"IterableNamedArrayDataset\",\n    \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set constraint\ndef poisson_ref_compute_func(_in):\n    return (\n        -8.0\n        * (np.pi**2)\n        * np.sin(2.0 * np.pi * _in[\"x\"])\n        * np.cos(2.0 * np.pi * _in[\"y\"])\n    )\n\npde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"Poisson\"].equations,\n    {\"poisson\": poisson_ref_compute_func},\n    geom[\"rect\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.NPOINT_PDE},\n    ppsci.loss.MSELoss(\"sum\"),\n    evenly=True,\n    name=\"EQ\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"Poisson\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u6211\u4eec\u5e0c\u671b Poisson \u65b9\u7a0b\u4ea7\u751f\u7684\u7ed3\u679c\u88ab\u4f18\u5316\u81f3\u4e0e\u5176\u6807\u51c6\u89e3\u4e00\u81f4\uff0c\u56e0\u6b64\u5c06\u5b83\u7684\u76ee\u6807\u503c\u5168\u8bbe\u4e3a <code>poisson_ref_compute_func</code> \u4ea7\u751f\u7684\u7ed3\u679c\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"rect\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528\u5168\u91cf\u6570\u636e\u70b9\u8bad\u7ec3\uff0c\u56e0\u6b64 <code>dataset</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e3a \"IterableNamedArrayDataset\" \u4e14 <code>iters_per_epoch</code> \u4e5f\u8bbe\u7f6e\u4e3a 1\uff0c\u91c7\u6837\u70b9\u6570 <code>batch_size</code> \u8bbe\u4e3a 9801(\u8868\u793a99x99\u7684\u91c7\u6837\u7f51\u683c)\uff1b</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"sum\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u9009\u62e9\u662f\u5426\u5728\u8ba1\u7b97\u57df\u4e0a\u8fdb\u884c\u7b49\u95f4\u9694\u91c7\u6837\uff0c\u6b64\u5904\u6211\u4eec\u9009\u62e9\u5f00\u542f\u7b49\u95f4\u9694\u91c7\u6837\uff0c\u8fd9\u6837\u80fd\u8ba9\u8bad\u7ec3\u70b9\u5747\u5300\u5206\u5e03\u5728\u8ba1\u7b97\u57df\u4e0a\uff0c\u6709\u5229\u4e8e\u8bad\u7ec3\u6536\u655b\uff1b</p> <p>\u7b2c\u4e03\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"EQ\" \u5373\u53ef\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u540c\u7406\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u6784\u5efa\u77e9\u5f62\u7684\u56db\u4e2a\u8fb9\u754c\u7684\u7ea6\u675f\u3002\u4f46\u4e0e\u6784\u5efa <code>InteriorConstraint</code> \u7ea6\u675f\u4e0d\u540c\u7684\u662f\uff0c\u7531\u4e8e\u4f5c\u7528\u533a\u57df\u662f\u8fb9\u754c\uff0c\u56e0\u6b64\u6211\u4eec\u4f7f\u7528 <code>BoundaryConstraint</code> \u7c7b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>bc = ppsci.constraint.BoundaryConstraint(\n    {\"p\": lambda out: out[\"p\"]},\n    {\n        \"p\": lambda _in: np.sin(2.0 * np.pi * _in[\"x\"])\n        * np.cos(2.0 * np.pi * _in[\"y\"])\n    },\n    geom[\"rect\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.NPOINT_BC},\n    ppsci.loss.MSELoss(\"sum\"),\n    name=\"BC\",\n)\n</code></pre> <p><code>BoundaryConstraint</code> \u7c7b\u7b2c\u4e00\u4e2a\u53c2\u6570\u8868\u793a\u6211\u4eec\u76f4\u63a5\u5bf9\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u51fa\u7ed3\u679c <code>out[\"p\"]</code> \u4f5c\u4e3a\u7a0b\u5e8f\u8fd0\u884c\u65f6\u7684\u7ea6\u675f\u5bf9\u8c61\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u6307\u6211\u4eec\u7ea6\u675f\u5bf9\u8c61\u7684\u771f\u503c\u5982\u4f55\u83b7\u5f97\uff0c\u8fd9\u91cc\u6211\u4eec\u76f4\u63a5\u901a\u8fc7\u5176\u89e3\u6790\u89e3\u8fdb\u884c\u8ba1\u7b97\uff0c\u5b9a\u4e49\u89e3\u6790\u89e3\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>lambda _in: np.sin(2.0 * np.pi * _in[\"x\"]) * np.cos(2.0 * np.pi * _in[\"y\"])\n</code></pre> <p><code>BoundaryConstraint</code> \u7c7b\u5176\u4ed6\u53c2\u6570\u7684\u542b\u4e49\u4e0e <code>InteriorConstraint</code> \u57fa\u672c\u4e00\u81f4\uff0c\u8fd9\u91cc\u4e0d\u518d\u4ecb\u7ecd\u3002</p> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u3001\u8fb9\u754c\u7ea6\u675f\u3001\u521d\u503c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    pde_constraint.name: pde_constraint,\n    bc.name: bc,\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528\u4e00\u4e07\u8f6e\u8bad\u7ec3\u8f6e\u6570\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 10000\n  iters_per_epoch: 1\n  lr_scheduler:\n    epochs: ${TRAIN.epochs}\n    iters_per_epoch: ${TRAIN.iters_per_epoch}\n    max_learning_rate: 1.0e-3\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u914d\u5408\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u5e38\u7528\u7684 OneCycle \u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u3002</p> <pre><code># set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.OneCycleLR(**cfg.TRAIN.lr_scheduler)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.GeometryValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\nresidual_validator = ppsci.validate.GeometryValidator(\n    equation[\"Poisson\"].equations,\n    {\"poisson\": poisson_ref_compute_func},\n    geom[\"rect\"],\n    {\n        \"dataset\": \"NamedArrayDataset\",\n        \"total_size\": cfg.NPOINT_PDE,\n        \"batch_size\": cfg.EVAL.batch_size.residual_validator,\n        \"sampler\": {\"name\": \"BatchSampler\"},\n    },\n    ppsci.loss.MSELoss(\"sum\"),\n    evenly=True,\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"Residual\",\n)\nvalidator = {residual_validator.name: residual_validator}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#38","title":"3.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u672c\u6587\u4e2d\u7684\u8f93\u51fa\u6570\u636e\u662f\u4e00\u4e2a\u533a\u57df\u5185\u7684\u4e8c\u7ef4\u70b9\u96c6\uff0c\u56e0\u6b64\u6211\u4eec\u53ea\u9700\u8981\u5c06\u8bc4\u4f30\u7684\u8f93\u51fa\u6570\u636e\u4fdd\u5b58\u6210 vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u6700\u540e\u7528\u53ef\u89c6\u5316\u8f6f\u4ef6\u6253\u5f00\u67e5\u770b\u5373\u53ef\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set visualizer(optional)\n# manually collate input data for visualization,\nvis_points = geom[\"rect\"].sample_interior(\n    cfg.NPOINT_PDE + cfg.NPOINT_BC, evenly=True\n)\nvisualizer = {\n    \"visualize_p_ux_uy\": ppsci.visualize.VisualizerVtu(\n        vis_points,\n        {\n            \"p\": lambda d: d[\"p\"],\n            \"p_ref\": lambda d: paddle.sin(2 * np.pi * d[\"x\"])\n            * paddle.cos(2 * np.pi * d[\"y\"]),\n            \"p_diff\": lambda d: paddle.sin(2 * np.pi * d[\"x\"])\n            * paddle.cos(2 * np.pi * d[\"y\"])\n            - d[\"p\"],\n            \"ux\": lambda d: jacobian(d[\"p\"], d[\"x\"]),\n            \"ux_ref\": lambda d: 2\n            * np.pi\n            * paddle.cos(2 * np.pi * d[\"x\"])\n            * paddle.cos(2 * np.pi * d[\"y\"]),\n            \"ux_diff\": lambda d: jacobian(d[\"p\"], d[\"x\"])\n            - 2\n            * np.pi\n            * paddle.cos(2 * np.pi * d[\"x\"])\n            * paddle.cos(2 * np.pi * d[\"y\"]),\n            \"uy\": lambda d: jacobian(d[\"p\"], d[\"y\"]),\n            \"uy_ref\": lambda d: -2\n            * np.pi\n            * paddle.sin(2 * np.pi * d[\"x\"])\n            * paddle.sin(2 * np.pi * d[\"y\"]),\n            \"uy_diff\": lambda d: jacobian(d[\"p\"], d[\"y\"])\n            - (\n                -2\n                * np.pi\n                * paddle.sin(2 * np.pi * d[\"x\"])\n                * paddle.sin(2 * np.pi * d[\"y\"])\n            ),\n        },\n        prefix=\"result_p_ux_uy\",\n    )\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#391-adam","title":"3.9.1 \u4f7f\u7528 Adam \u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#392-l-bfgs","title":"3.9.2 \u4f7f\u7528 L-BFGS \u5fae\u8c03[\u53ef\u9009]","text":"<p>\u5728\u4f7f\u7528 <code>Adam</code> \u4f18\u5316\u5668\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u4f18\u5316\u5668\u66f4\u6362\u6210\u4e8c\u9636\u4f18\u5316\u5668 <code>L-BFGS</code> \u7ee7\u7eed\u8bad\u7ec3\u5c11\u91cf\u8f6e\u6570\uff08\u6b64\u5904\u6211\u4eec\u4f7f\u7528 <code>Adam</code> \u4f18\u5316\u8f6e\u6570\u7684 10% \u5373\u53ef\uff09\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002</p> <pre><code>OUTPUT_DIR = cfg.TRAIN.lbfgs.output_dir\nlogger.init_logger(\"ppsci\", osp.join(OUTPUT_DIR, f\"{cfg.mode}.log\"), \"info\")\nEPOCHS = cfg.TRAIN.epochs // 10\noptimizer_lbfgs = ppsci.optimizer.LBFGS(\n    cfg.TRAIN.lbfgs.learning_rate, cfg.TRAIN.lbfgs.max_iter\n)(model)\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    OUTPUT_DIR,\n    optimizer_lbfgs,\n    None,\n    EPOCHS,\n    cfg.TRAIN.lbfgs.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.lbfgs.eval_during_train,\n    eval_freq=cfg.TRAIN.lbfgs.eval_freq,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre> \u63d0\u793a <p>\u5728\u5e38\u89c4\u4f18\u5316\u5668\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4f7f\u7528 <code>L-BFGS</code> \u5fae\u8c03\u5c11\u91cf\u8f6e\u6570\u7684\u65b9\u6cd5\uff0c\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u90fd\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"darcy2d.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"Poisson\": ppsci.equation.Poisson(2)}\n\n    # set geometry\n    geom = {\"rect\": ppsci.geometry.Rectangle((0.0, 0.0), (1.0, 1.0))}\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    }\n\n    # set constraint\n    def poisson_ref_compute_func(_in):\n        return (\n            -8.0\n            * (np.pi**2)\n            * np.sin(2.0 * np.pi * _in[\"x\"])\n            * np.cos(2.0 * np.pi * _in[\"y\"])\n        )\n\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"Poisson\"].equations,\n        {\"poisson\": poisson_ref_compute_func},\n        geom[\"rect\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.NPOINT_PDE},\n        ppsci.loss.MSELoss(\"sum\"),\n        evenly=True,\n        name=\"EQ\",\n    )\n\n    bc = ppsci.constraint.BoundaryConstraint(\n        {\"p\": lambda out: out[\"p\"]},\n        {\n            \"p\": lambda _in: np.sin(2.0 * np.pi * _in[\"x\"])\n            * np.cos(2.0 * np.pi * _in[\"y\"])\n        },\n        geom[\"rect\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.NPOINT_BC},\n        ppsci.loss.MSELoss(\"sum\"),\n        name=\"BC\",\n    )\n    # wrap constraints together\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        bc.name: bc,\n    }\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.OneCycleLR(**cfg.TRAIN.lr_scheduler)()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    # set validator\n    residual_validator = ppsci.validate.GeometryValidator(\n        equation[\"Poisson\"].equations,\n        {\"poisson\": poisson_ref_compute_func},\n        geom[\"rect\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"total_size\": cfg.NPOINT_PDE,\n            \"batch_size\": cfg.EVAL.batch_size.residual_validator,\n            \"sampler\": {\"name\": \"BatchSampler\"},\n        },\n        ppsci.loss.MSELoss(\"sum\"),\n        evenly=True,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"Residual\",\n    )\n    validator = {residual_validator.name: residual_validator}\n\n    # set visualizer(optional)\n    # manually collate input data for visualization,\n    vis_points = geom[\"rect\"].sample_interior(\n        cfg.NPOINT_PDE + cfg.NPOINT_BC, evenly=True\n    )\n    visualizer = {\n        \"visualize_p_ux_uy\": ppsci.visualize.VisualizerVtu(\n            vis_points,\n            {\n                \"p\": lambda d: d[\"p\"],\n                \"p_ref\": lambda d: paddle.sin(2 * np.pi * d[\"x\"])\n                * paddle.cos(2 * np.pi * d[\"y\"]),\n                \"p_diff\": lambda d: paddle.sin(2 * np.pi * d[\"x\"])\n                * paddle.cos(2 * np.pi * d[\"y\"])\n                - d[\"p\"],\n                \"ux\": lambda d: jacobian(d[\"p\"], d[\"x\"]),\n                \"ux_ref\": lambda d: 2\n                * np.pi\n                * paddle.cos(2 * np.pi * d[\"x\"])\n                * paddle.cos(2 * np.pi * d[\"y\"]),\n                \"ux_diff\": lambda d: jacobian(d[\"p\"], d[\"x\"])\n                - 2\n                * np.pi\n                * paddle.cos(2 * np.pi * d[\"x\"])\n                * paddle.cos(2 * np.pi * d[\"y\"]),\n                \"uy\": lambda d: jacobian(d[\"p\"], d[\"y\"]),\n                \"uy_ref\": lambda d: -2\n                * np.pi\n                * paddle.sin(2 * np.pi * d[\"x\"])\n                * paddle.sin(2 * np.pi * d[\"y\"]),\n                \"uy_diff\": lambda d: jacobian(d[\"p\"], d[\"y\"])\n                - (\n                    -2\n                    * np.pi\n                    * paddle.sin(2 * np.pi * d[\"x\"])\n                    * paddle.sin(2 * np.pi * d[\"y\"])\n                ),\n            },\n            prefix=\"result_p_ux_uy\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n    # fine-tuning pretrained model with L-BFGS\n    OUTPUT_DIR = cfg.TRAIN.lbfgs.output_dir\n    logger.init_logger(\"ppsci\", osp.join(OUTPUT_DIR, f\"{cfg.mode}.log\"), \"info\")\n    EPOCHS = cfg.TRAIN.epochs // 10\n    optimizer_lbfgs = ppsci.optimizer.LBFGS(\n        cfg.TRAIN.lbfgs.learning_rate, cfg.TRAIN.lbfgs.max_iter\n    )(model)\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        OUTPUT_DIR,\n        optimizer_lbfgs,\n        None,\n        EPOCHS,\n        cfg.TRAIN.lbfgs.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.lbfgs.eval_during_train,\n        eval_freq=cfg.TRAIN.lbfgs.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"Poisson\": ppsci.equation.Poisson(2)}\n\n    # set geometry\n    geom = {\"rect\": ppsci.geometry.Rectangle((0.0, 0.0), (1.0, 1.0))}\n\n    # set constraint\n    def poisson_ref_compute_func(_in):\n        return (\n            -8.0\n            * (np.pi**2)\n            * np.sin(2.0 * np.pi * _in[\"x\"])\n            * np.cos(2.0 * np.pi * _in[\"y\"])\n        )\n\n    # set validator\n    residual_validator = ppsci.validate.GeometryValidator(\n        equation[\"Poisson\"].equations,\n        {\"poisson\": poisson_ref_compute_func},\n        geom[\"rect\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"total_size\": cfg.NPOINT_PDE,\n            \"batch_size\": cfg.EVAL.batch_size.residual_validator,\n            \"sampler\": {\"name\": \"BatchSampler\"},\n        },\n        ppsci.loss.MSELoss(\"sum\"),\n        evenly=True,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"Residual\",\n    )\n    validator = {residual_validator.name: residual_validator}\n\n    # set visualizer\n    # manually collate input data for visualization,\n    vis_points = geom[\"rect\"].sample_interior(\n        cfg.NPOINT_PDE + cfg.NPOINT_BC, evenly=True\n    )\n    visualizer = {\n        \"visualize_p_ux_uy\": ppsci.visualize.VisualizerVtu(\n            vis_points,\n            {\n                \"p\": lambda d: d[\"p\"],\n                \"p_ref\": lambda d: paddle.sin(2 * np.pi * d[\"x\"])\n                * paddle.cos(2 * np.pi * d[\"y\"]),\n                \"p_diff\": lambda d: paddle.sin(2 * np.pi * d[\"x\"])\n                * paddle.cos(2 * np.pi * d[\"y\"])\n                - d[\"p\"],\n                \"ux\": lambda d: jacobian(d[\"p\"], d[\"x\"]),\n                \"ux_ref\": lambda d: 2\n                * np.pi\n                * paddle.cos(2 * np.pi * d[\"x\"])\n                * paddle.cos(2 * np.pi * d[\"y\"]),\n                \"ux_diff\": lambda d: jacobian(d[\"p\"], d[\"x\"])\n                - 2\n                * np.pi\n                * paddle.cos(2 * np.pi * d[\"x\"])\n                * paddle.cos(2 * np.pi * d[\"y\"]),\n                \"uy\": lambda d: jacobian(d[\"p\"], d[\"y\"]),\n                \"uy_ref\": lambda d: -2\n                * np.pi\n                * paddle.sin(2 * np.pi * d[\"x\"])\n                * paddle.sin(2 * np.pi * d[\"y\"]),\n                \"uy_diff\": lambda d: jacobian(d[\"p\"], d[\"y\"])\n                - (\n                    -2\n                    * np.pi\n                    * paddle.sin(2 * np.pi * d[\"x\"])\n                    * paddle.sin(2 * np.pi * d[\"y\"])\n                ),\n            },\n            prefix=\"result_p_ux_uy\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n    # visualize prediction\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # set geometry\n    geom = {\"rect\": ppsci.geometry.Rectangle((0.0, 0.0), (1.0, 1.0))}\n    # manually collate input data for visualization,\n    input_dict = geom[\"rect\"].sample_interior(\n        cfg.NPOINT_PDE + cfg.NPOINT_BC, evenly=True\n    )\n    output_dict = predictor.predict(\n        {key: input_dict[key] for key in cfg.MODEL.input_keys}, cfg.INFER.batch_size\n    )\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n    ppsci.visualize.save_vtu_from_dict(\n        \"./visual/darcy2d.vtu\",\n        {**input_dict, **output_dict},\n        input_dict.keys(),\n        cfg.MODEL.output_keys,\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"darcy2d.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/darcy2d/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u65b9\u5c55\u793a\u4e86\u6a21\u578b\u5bf9\u6b63\u65b9\u5f62\u8ba1\u7b97\u57df\u4e2d\u6bcf\u4e2a\u70b9\u7684\u538b\u529b\\(p(x,y)\\)\u3001x(\u6c34\u5e73)\u65b9\u5411\u6d41\u901f\\(u(x,y)\\)\u3001y(\u5782\u76f4)\u65b9\u5411\u6d41\u901f\\(v(x,y)\\)\u7684\u9884\u6d4b\u7ed3\u679c\u3001\u53c2\u8003\u7ed3\u679c\u4ee5\u53ca\u4e24\u8005\u4e4b\u5dee\u3002</p> <p> </p> \u5de6\uff1a\u9884\u6d4b\u538b\u529b p\uff0c\u4e2d\uff1a\u53c2\u8003\u538b\u529b p\uff0c\u53f3\uff1a\u538b\u529b\u5dee <p> </p> \u5de6\uff1a\u9884\u6d4bx\u65b9\u5411\u6d41\u901f p\uff0c\u4e2d\uff1a\u53c2\u8003x\u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1ax\u65b9\u5411\u6d41\u901f\u5dee <p> </p> \u5de6\uff1a\u9884\u6d4by\u65b9\u5411\u6d41\u901f p\uff0c\u4e2d\uff1a\u53c2\u8003y\u65b9\u5411\u6d41\u901f p\uff0c\u53f3\uff1ay\u65b9\u5411\u6d41\u901f\u5dee","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Poisson\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/deepcfd/","title":"DeepCFD","text":""},{"location":"zh/examples/deepcfd/#deepcfddeep-computational-fluid-dynamics","title":"DeepCFD(Deep Computational Fluid Dynamics)","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <pre><code># linux\nwget -nc -P ./datasets/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataX.pkl\nwget -nc -P ./datasets/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataY.pkl\n# windows\n# curl --create-dirs -o ./datasets/dataX.pkl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataX.pkl\n# curl --create-dirs -o ./datasets/dataX.pkl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataY.pkl\npython deepcfd.py\n</code></pre> <pre><code># linux\nwget -nc -P ./datasets/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataX.pkl\nwget -nc -P ./datasets/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataY.pkl\n# windows\n# curl --create-dirs -o ./datasets/dataX.pkl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataX.pkl\n# curl --create-dirs -o ./datasets/dataX.pkl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataY.pkl\npython deepcfd.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/deepcfd/deepcfd_pretrained.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 deepcfd_pretrained.pdparams MSE.Total_MSE(mse_validator): 1.92947MSE.Ux_MSE(mse_validator): 0.70684MSE.Uy_MSE(mse_validator): 0.21337MSE.p_MSE(mse_validator): 1.00926"},{"location":"zh/examples/deepcfd/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\uff08Computational fluid dynamics, CFD\uff09\u6a21\u62df\u901a\u8fc7\u6c42\u89e3 Navier-Stokes \u65b9\u7a0b\uff08N-S \u65b9\u7a0b\uff09\uff0c\u53ef\u4ee5\u83b7\u5f97\u6d41\u4f53\u7684\u5404\u79cd\u7269\u7406\u91cf\u7684\u5206\u5e03\uff0c\u5982\u5bc6\u5ea6\u3001\u538b\u529b\u548c\u901f\u5ea6\u7b49\u3002\u5728\u5fae\u7535\u5b50\u7cfb\u7edf\u3001\u571f\u6728\u5de5\u7a0b\u548c\u822a\u7a7a\u822a\u5929\u7b49\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\u3002</p> <p>\u5728\u67d0\u4e9b\u590d\u6742\u7684\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u5982\u673a\u7ffc\u4f18\u5316\u548c\u6d41\u4f53\u4e0e\u7ed3\u6784\u76f8\u4e92\u4f5c\u7528\u65b9\u9762\uff0c\u9700\u8981\u4f7f\u7528\u5343\u4e07\u7ea7\u751a\u81f3\u4e0a\u4ebf\u7684\u7f51\u683c\u5bf9\u95ee\u9898\u8fdb\u884c\u5efa\u6a21\uff08\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u4e0b\u56fe\u5c55\u793a\u4e86 F-18 \u6218\u6597\u673a\u7684\u5168\u673a\u5185\u5916\u6d41\u4e00\u4f53\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u578b\uff09\uff0c\u5bfc\u81f4 CFD \u7684\u8ba1\u7b97\u91cf\u975e\u5e38\u5de8\u5927\u3002\u56e0\u6b64\uff0c\u76ee\u524d\u4e9f\u9700\u53d1\u5c55\u51fa\u4e00\u79cd\u76f8\u6bd4\u4e8e\u4f20\u7edf CFD \u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u4e14\u53ef\u4ee5\u4fdd\u6301\u8ba1\u7b97\u7cbe\u5ea6\u7684\u65b9\u6cd5\u3002</p> <p> </p> F-18 \u6218\u6597\u673a\u7684\u5168\u673a\u5185\u5916\u6d41\u4e00\u4f53\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u578b"},{"location":"zh/examples/deepcfd/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>Navier-Stokes \u65b9\u7a0b\u662f\u7528\u4e8e\u63cf\u8ff0\u6d41\u4f53\u8fd0\u52a8\u7684\u65b9\u7a0b\uff0c\u5b83\u7684\u4e8c\u7ef4\u5f62\u5f0f\u5982\u4e0b\uff0c</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[\\nabla \\cdot    \\bf{u}=0\\] <p>\u52a8\u91cf\u5b88\u6052\uff1a</p> \\[\\rho(\\frac{\\partial}{\\partial t}  + \\bf{u} \\cdot  div ) \\bf{u} = - \\nabla p +  - \\nabla \\tau + \\bf{f}\\] <p>\u5176\u4e2d \\(\\bf{u}\\) \u662f\u901f\u5ea6\u573a\uff08\u5177\u6709 x \u548c y \u4e24\u4e2a\u7ef4\u5ea6\uff09\uff0c\\(\\rho\\) \u662f\u5bc6\u5ea6\uff0c \\(p\\) \u662f\u538b\u5f3a\u573a\uff0c\\(\\bf{f}\\) \u662f\u4f53\u79ef\u529b\uff08\u4f8b\u5982\u91cd\u529b\uff09\u3002</p> <p>\u5047\u8bbe\u6ee1\u8db3\u975e\u5747\u5300\u7a33\u6001\u6d41\u4f53\u6761\u4ef6\uff0c\u65b9\u7a0b\u53ef\u53bb\u6389\u65f6\u95f4\u76f8\u5173\u9879\uff0c\u5e76\u5c06 \\(\\bf{u}\\) \u5206\u89e3\u4e3a\u901f\u5ea6\u5206\u91cf \\(u_x\\) \u548c \\(u_y\\) \uff0c\u52a8\u91cf\u65b9\u7a0b\u53ef\u91cd\u5199\u6210\uff1a</p> \\[u_x\\frac{\\partial u_x}{\\partial x} + u_y\\frac{\\partial u_x}{\\partial y} = - \\frac{1}{\\rho}\\frac{\\partial p}{\\partial x} + \\nu \\nabla^2 u_x + g_x\\] \\[u_x\\frac{\\partial u_y}{\\partial x} + u_y\\frac{\\partial u_y}{\\partial y} = - \\frac{1}{\\rho}\\frac{\\partial p}{\\partial y} + \\nu \\nabla^2 u_y + g_y\\] <p>\u5176\u4e2d \\(g\\) \u4ee3\u8868\u91cd\u529b\u52a0\u901f\u5ea6\uff0c\\(\\nu\\) \u4ee3\u8868\u6d41\u4f53\u7684\u52a8\u529b\u7c98\u5ea6\u3002</p>"},{"location":"zh/examples/deepcfd/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u4e0a\u8ff0\u95ee\u9898\u901a\u5e38\u53ef\u4f7f\u7528 OpenFOAM \u8fdb\u884c\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u7684\u6c42\u89e3\uff0c\u4f46\u8ba1\u7b97\u91cf\u5f88\u5927\uff0c\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u57fa\u4e8e PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002</p> <p>\u672c\u6848\u4f8b\u57fa\u4e8e\u8bba\u6587 Ribeiro M D, Rehman A, Ahmed S, et al. DeepCFD: Efficient steady-state laminar flow approximation with deep convolutional neural networks \u7684\u65b9\u6cd5\u8fdb\u884c\u6c42\u89e3\uff0c\u5173\u4e8e\u8be5\u65b9\u6cd5\u7684\u7406\u8bba\u90e8\u5206\u8bf7\u53c2\u8003\u539f\u8bba\u6587\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/deepcfd/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u8be5\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u4f7f\u7528 OpenFOAM \u6c42\u5f97\u3002\u6570\u636e\u96c6\u6709\u4e24\u4e2a\u6587\u4ef6 dataX \u548c dataY\u3002dataX \u5305\u542b 981 \u4e2a\u901a\u9053\u6d41\u6837\u672c\u51e0\u4f55\u5f62\u72b6\u7684\u8f93\u5165\u4fe1\u606f\uff0cdataY \u5305\u542b\u5bf9\u5e94\u7684 OpenFOAM \u6c42\u89e3\u7ed3\u679c\u3002</p> <p>\u8fd0\u884c\u672c\u95ee\u9898\u4ee3\u7801\u524d\u8bf7\u6309\u7167\u4e0b\u65b9\u547d\u4ee4\u4e0b\u8f7d dataX \u548c dataY\uff1a</p> <pre><code>wget -nc -P ./datasets/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataX.pkl\nwget -nc -P ./datasets/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepCFD/dataY.pkl\n</code></pre> <p>dataX \u548c dataY \u90fd\u5177\u6709\u76f8\u540c\u7684\u7ef4\u5ea6\uff08Ns\uff0cNc\uff0cNx\uff0cNy\uff09\uff0c\u5176\u4e2d\u7b2c\u4e00\u8f74\u662f\u6837\u672c\u6570\uff08Ns\uff09\uff0c\u7b2c\u4e8c\u8f74\u662f\u901a\u9053\u6570\uff08Nc\uff09\uff0c\u7b2c\u4e09\u548c\u7b2c\u56db\u8f74\u5206\u522b\u662f x \u548c y \u4e2d\u7684\u5143\u7d20\u6570\u91cf\uff08Nx \u548c Ny\uff09\u3002\u5728\u8f93\u5165\u6570\u636e dataX \u4e2d\uff0c\u7b2c\u4e00\u901a\u9053\u662f\u8ba1\u7b97\u57df\u4e2d\u969c\u788d\u7269\u7684SDF\uff08Signed distance function\uff09\uff0c\u7b2c\u4e8c\u901a\u9053\u662f\u6d41\u52a8\u533a\u57df\u7684\u6807\u7b7e\uff0c\u7b2c\u4e09\u901a\u9053\u662f\u8ba1\u7b97\u57df\u8fb9\u754c\u7684 SDF\u3002\u5728\u8f93\u51fa\u6570\u636e dataY \u4e2d\uff0c\u7b2c\u4e00\u4e2a\u901a\u9053\u662f\u6c34\u5e73\u901f\u5ea6\u5206\u91cf\uff08Ux\uff09\uff0c\u7b2c\u4e8c\u4e2a\u901a\u9053\u662f\u5782\u76f4\u901f\u5ea6\u5206\u91cf\uff08Uy\uff09\uff0c\u7b2c\u4e09\u4e2a\u901a\u9053\u662f\u6d41\u4f53\u538b\u5f3a\uff08p\uff09\u3002</p> <p>\u6570\u636e\u96c6\u539f\u59cb\u4e0b\u8f7d\u5730\u5740\u4e3a\uff1ahttps://zenodo.org/record/3666056/files/DeepCFD.zip?download=1</p> <p>\u6211\u4eec\u5c06\u6570\u636e\u96c6\u4ee5 7:3 \u7684\u6bd4\u4f8b\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/deepcfd/deepcfd.py<pre><code># set random seed for reproducibility\nppsci.utils.misc.set_random_seed(cfg.seed)\n# initialize logger\nlogger.init_logger(\"ppsci\", os.path.join(cfg.output_dir, \"train.log\"), \"info\")\n\n# initialize datasets\nwith open(cfg.DATAX_PATH, \"rb\") as file:\n    x = pickle.load(file)\nwith open(cfg.DATAY_PATH, \"rb\") as file:\n    y = pickle.load(file)\n\n# split dataset to train dataset and test dataset\ntrain_dataset, test_dataset = split_tensors(x, y, ratio=cfg.SLIPT_RATIO)\ntrain_x, train_y = train_dataset\ntest_x, test_y = test_dataset\n</code></pre>"},{"location":"zh/examples/deepcfd/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u4e0a\u8ff0\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u8f93\u5165\u4e3a input\uff0c\u8f93\u51fa\u4e3a output\uff0c\u6309\u7167\u8bba\u6587\u6240\u8ff0\uff0c\u6211\u4eec\u4f7f\u7528\u542b\u6709 3 \u4e2a encoder \u548c decoder \u7684 UNetEx \u7f51\u7edc\u6765\u521b\u5efa\u6a21\u578b\u3002</p> <p>\u6a21\u578b\u7684\u8f93\u5165\u5305\u542b\u4e86\u969c\u788d\u7269\u7684 SDF\uff08Signed distance function\uff09\u3001\u6d41\u52a8\u533a\u57df\u7684\u6807\u7b7e\u4ee5\u53ca\u8ba1\u7b97\u57df\u8fb9\u754c\u7684 SDF\u3002\u6a21\u578b\u7684\u8f93\u51fa\u5305\u542b\u4e86\u6c34\u5e73\u901f\u5ea6\u5206\u91cf\uff08Ux\uff09\uff0c\u5782\u76f4\u901f\u5ea6\u5206\u91cf\uff08Uy\uff09\u4ee5\u53ca\u6d41\u4f53\u538b\u5f3a\uff08p\uff09\u3002</p> <p> </p> DeepCFD\u7f51\u7edc\u7ed3\u6784 <p>\u6a21\u578b\u521b\u5efa\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/deepcfd/deepcfd.py<pre><code># initialize model\nmodel = ppsci.arch.UNetEx(**cfg.MODEL)\n</code></pre>"},{"location":"zh/examples/deepcfd/#33","title":"3.3 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/deepcfd/deepcfd.py<pre><code># define loss\ndef loss_expr(\n    output_dict: Dict[str, np.ndarray],\n    label_dict: Dict[str, np.ndarray] = None,\n    weight_dict: Dict[str, np.ndarray] = None,\n) -&gt; float:\n    output = output_dict[\"output\"]\n    y = label_dict[\"output\"]\n    loss_u = (output[:, 0:1, :, :] - y[:, 0:1, :, :]) ** 2\n    loss_v = (output[:, 1:2, :, :] - y[:, 1:2, :, :]) ** 2\n    loss_p = (output[:, 2:3, :, :] - y[:, 2:3, :, :]).abs()\n    loss = (loss_u + loss_v + loss_p) / CHANNELS_WEIGHTS\n    return {\"output\": loss.sum()}\n\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"input\": train_x},\n            \"label\": {\"output\": train_y},\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.FunctionalLoss(loss_expr),\n    name=\"sup_constraint\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u6570\u636e\u7684\u52a0\u8f7d\u65b9\u5f0f\uff0c\u8fd9\u91cc\u586b\u5165\u76f8\u5173\u6570\u636e\u7684\u53d8\u91cf\u540d\u3002</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684\u635f\u5931\u51fd\u6570\uff0c\u5206\u522b\u8ba1\u7b97 Ux \u548c Uy \u7684\u5747\u65b9\u8bef\u5dee\uff0c\u4ee5\u53ca p \u7684\u6807\u51c6\u5dee\uff0c\u7136\u540e\u4e09\u8005\u52a0\u6743\u6c42\u548c\u3002</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u6b21\u547d\u540d\u4e3a \"sup_constraint\"\u3002</p> <p>\u5728\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> examples/deepcfd/deepcfd.py<pre><code># manually build constraint\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre>"},{"location":"zh/examples/deepcfd/#34","title":"3.4 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528\u4e00\u5343\u8f6e\u8bad\u7ec3\u8f6e\u6570\u3002</p> examples/deepcfd/conf/deepcfd.yaml<pre><code># training settings\nTRAIN:\n  epochs: 1000\n  learning_rate: 0.001\n  weight_decay: 0.005\n</code></pre>"},{"location":"zh/examples/deepcfd/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a 0.001\uff0c\u6743\u91cd\u8870\u51cf\u8bbe\u7f6e\u4e3a 0.005\u3002</p> examples/deepcfd/deepcfd.py<pre><code># initialize Adam optimizer\noptimizer = ppsci.optimizer.Adam(\n    cfg.TRAIN.learning_rate, weight_decay=cfg.TRAIN.weight_decay\n)(model)\n</code></pre>"},{"location":"zh/examples/deepcfd/#36","title":"3.6 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u6211\u4eec\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> examples/deepcfd/deepcfd.py<pre><code># manually build validator\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"NamedArrayDataset\",\n        \"input\": {\"input\": test_x},\n        \"label\": {\"output\": test_y},\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n}\n\ndef metric_expr(\n    output_dict: Dict[str, np.ndarray],\n    label_dict: Dict[str, np.ndarray] = None,\n    weight_dict: Dict[str, np.ndarray] = None,\n) -&gt; Dict[str, float]:\n    output = output_dict[\"output\"]\n    y = label_dict[\"output\"]\n    total_mse = ((output - y) ** 2).sum() / len(test_x)\n    ux_mse = ((output[:, 0, :, :] - test_y[:, 0, :, :]) ** 2).sum() / len(test_x)\n    uy_mse = ((output[:, 1, :, :] - test_y[:, 1, :, :]) ** 2).sum() / len(test_x)\n    p_mse = ((output[:, 2, :, :] - test_y[:, 2, :, :]) ** 2).sum() / len(test_x)\n    return {\n        \"Total_MSE\": total_mse,\n        \"Ux_MSE\": ux_mse,\n        \"Uy_MSE\": uy_mse,\n        \"p_MSE\": p_mse,\n    }\n\nsup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.FunctionalLoss(loss_expr),\n    {\"output\": lambda out: out[\"output\"]},\n    {\"MSE\": ppsci.metric.FunctionalMetric(metric_expr)},\n    name=\"mse_validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre> <p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u8fd9\u91cc\u81ea\u5b9a\u4e49\u4e86\u56db\u4e2a\u6307\u6807 Total_MSE\u3001Ux_MSE\u3001Uy_MSE \u548c p_MSE\u3002</p> <p>\u5176\u4f59\u914d\u7f6e\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>"},{"location":"zh/examples/deepcfd/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/deepcfd/deepcfd.py<pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    epochs=cfg.TRAIN.epochs,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    seed=cfg.seed,\n    validator=validator,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n\n# train model\nsolver.train()\n\n# evaluate after finished training\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/deepcfd/#38","title":"3.8 \u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u4f7f\u7528 matplotlib \u7ed8\u5236\u76f8\u540c\u8f93\u5165\u53c2\u6570\u65f6\u7684 OpenFOAM \u548c DeepCFD \u7684\u8ba1\u7b97\u7ed3\u679c\uff0c\u8fdb\u884c\u5bf9\u6bd4\u3002\u8fd9\u91cc\u7ed8\u5236\u4e86\u9a8c\u8bc1\u96c6\u7b2c 0 \u4e2a\u6570\u636e\u7684\u8ba1\u7b97\u7ed3\u679c\u3002</p> examples/deepcfd/deepcfd.py<pre><code>PLOT_DIR = os.path.join(cfg.output_dir, \"visual\")\nos.makedirs(PLOT_DIR, exist_ok=True)\n\n# visualize prediction after finished training\npredict_and_save_plot(test_x, test_y, 0, solver, PLOT_DIR)\n</code></pre>"},{"location":"zh/examples/deepcfd/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"examples/deepcfd/deepcfd.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport pickle\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport hydra\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef split_tensors(\n    *tensors: List[np.array], ratio: float\n) -&gt; Tuple[List[np.array], List[np.array]]:\n    \"\"\"Split tensors to two parts.\n\n    Args:\n        tensors (List[np.array]): Non-empty tensor list.\n        ratio (float): Split ratio. For example, tensor list A is split to A1 and A2. len(A1) / len(A) = ratio.\n    Returns:\n        Tuple[List[np.array], List[np.array]]: Split tensors.\n    \"\"\"\n    if len(tensors) == 0:\n        raise ValueError(\"Tensors shouldn't be empty.\")\n\n    split1, split2 = [], []\n    count = len(tensors[0])\n    for tensor in tensors:\n        if len(tensor) != count:\n            raise ValueError(\"The size of tensor should be same.\")\n        x = int(len(tensor) * ratio)\n        split1.append(tensor[:x])\n        split2.append(tensor[x:])\n\n    if len(tensors) == 1:\n        split1, split2 = split1[0], split2[0]\n    return split1, split2\n\n\ndef predict_and_save_plot(\n    x: np.ndarray, y: np.ndarray, index: int, solver: ppsci.solver.Solver, plot_dir: str\n):\n    \"\"\"Make prediction and save visualization of result.\n\n    Args:\n        x (np.ndarray): Input of test dataset.\n        y (np.ndarray): Output of test dataset.\n        index (int): Index of data to visualizer.\n        solver (ppsci.solver.Solver): Trained solver.\n        plot_dir (str): Directory to save plot.\n    \"\"\"\n    min_u = np.min(y[index, 0, :, :])\n    max_u = np.max(y[index, 0, :, :])\n\n    min_v = np.min(y[index, 1, :, :])\n    max_v = np.max(y[index, 1, :, :])\n\n    min_p = np.min(y[index, 2, :, :])\n    max_p = np.max(y[index, 2, :, :])\n\n    output = solver.predict({\"input\": x}, return_numpy=True)\n    pred_y = output[\"output\"]\n    error = np.abs(y - pred_y)\n\n    min_error_u = np.min(error[index, 0, :, :])\n    max_error_u = np.max(error[index, 0, :, :])\n\n    min_error_v = np.min(error[index, 1, :, :])\n    max_error_v = np.max(error[index, 1, :, :])\n\n    min_error_p = np.min(error[index, 2, :, :])\n    max_error_p = np.max(error[index, 2, :, :])\n\n    plt.figure()\n    fig = plt.gcf()\n    fig.set_size_inches(15, 10)\n    plt.subplot(3, 3, 1)\n    plt.title(\"OpenFOAM\", fontsize=18)\n    plt.imshow(\n        np.transpose(y[index, 0, :, :]),\n        cmap=\"jet\",\n        vmin=min_u,\n        vmax=max_u,\n        origin=\"lower\",\n        extent=[0, 260, 0, 120],\n    )\n    plt.colorbar(orientation=\"horizontal\")\n    plt.ylabel(\"Ux\", fontsize=18)\n    plt.subplot(3, 3, 2)\n    plt.title(\"DeepCFD\", fontsize=18)\n    plt.imshow(\n        np.transpose(pred_y[index, 0, :, :]),\n        cmap=\"jet\",\n        vmin=min_u,\n        vmax=max_u,\n        origin=\"lower\",\n        extent=[0, 260, 0, 120],\n    )\n    plt.colorbar(orientation=\"horizontal\")\n    plt.subplot(3, 3, 3)\n    plt.title(\"Error\", fontsize=18)\n    plt.imshow(\n        np.transpose(error[index, 0, :, :]),\n        cmap=\"jet\",\n        vmin=min_error_u,\n        vmax=max_error_u,\n        origin=\"lower\",\n        extent=[0, 260, 0, 120],\n    )\n    plt.colorbar(orientation=\"horizontal\")\n\n    plt.subplot(3, 3, 4)\n    plt.imshow(\n        np.transpose(y[index, 1, :, :]),\n        cmap=\"jet\",\n        vmin=min_v,\n        vmax=max_v,\n        origin=\"lower\",\n        extent=[0, 260, 0, 120],\n    )\n    plt.colorbar(orientation=\"horizontal\")\n    plt.ylabel(\"Uy\", fontsize=18)\n    plt.subplot(3, 3, 5)\n    plt.imshow(\n        np.transpose(pred_y[index, 1, :, :]),\n        cmap=\"jet\",\n        vmin=min_v,\n        vmax=max_v,\n        origin=\"lower\",\n        extent=[0, 260, 0, 120],\n    )\n    plt.colorbar(orientation=\"horizontal\")\n    plt.subplot(3, 3, 6)\n    plt.imshow(\n        np.transpose(error[index, 1, :, :]),\n        cmap=\"jet\",\n        vmin=min_error_v,\n        vmax=max_error_v,\n        origin=\"lower\",\n        extent=[0, 260, 0, 120],\n    )\n    plt.colorbar(orientation=\"horizontal\")\n\n    plt.subplot(3, 3, 7)\n    plt.imshow(\n        np.transpose(y[index, 2, :, :]),\n        cmap=\"jet\",\n        vmin=min_p,\n        vmax=max_p,\n        origin=\"lower\",\n        extent=[0, 260, 0, 120],\n    )\n    plt.colorbar(orientation=\"horizontal\")\n    plt.ylabel(\"p\", fontsize=18)\n    plt.subplot(3, 3, 8)\n    plt.imshow(\n        np.transpose(pred_y[index, 2, :, :]),\n        cmap=\"jet\",\n        vmin=min_p,\n        vmax=max_p,\n        origin=\"lower\",\n        extent=[0, 260, 0, 120],\n    )\n    plt.colorbar(orientation=\"horizontal\")\n    plt.subplot(3, 3, 9)\n    plt.imshow(\n        np.transpose(error[index, 2, :, :]),\n        cmap=\"jet\",\n        vmin=min_error_p,\n        vmax=max_error_p,\n        origin=\"lower\",\n        extent=[0, 260, 0, 120],\n    )\n    plt.colorbar(orientation=\"horizontal\")\n    plt.tight_layout()\n    plt.show()\n    plt.savefig(\n        os.path.join(plot_dir, f\"cfd_{index}.png\"),\n        bbox_inches=\"tight\",\n    )\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", os.path.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # initialize datasets\n    with open(cfg.DATAX_PATH, \"rb\") as file:\n        x = pickle.load(file)\n    with open(cfg.DATAY_PATH, \"rb\") as file:\n        y = pickle.load(file)\n\n    # split dataset to train dataset and test dataset\n    train_dataset, test_dataset = split_tensors(x, y, ratio=cfg.SLIPT_RATIO)\n    train_x, train_y = train_dataset\n    test_x, test_y = test_dataset\n\n    # initialize model\n    model = ppsci.arch.UNetEx(**cfg.MODEL)\n\n    CHANNELS_WEIGHTS = np.reshape(\n        np.sqrt(\n            np.mean(\n                np.transpose(y, (0, 2, 3, 1)).reshape(\n                    (cfg.SAMPLE_SIZE * cfg.X_SIZE * cfg.Y_SIZE, cfg.CHANNEL_SIZE)\n                )\n                ** 2,\n                axis=0,\n            )\n        ),\n        (1, -1, 1, 1),\n    )\n\n    # define loss\n    def loss_expr(\n        output_dict: Dict[str, np.ndarray],\n        label_dict: Dict[str, np.ndarray] = None,\n        weight_dict: Dict[str, np.ndarray] = None,\n    ) -&gt; float:\n        output = output_dict[\"output\"]\n        y = label_dict[\"output\"]\n        loss_u = (output[:, 0:1, :, :] - y[:, 0:1, :, :]) ** 2\n        loss_v = (output[:, 1:2, :, :] - y[:, 1:2, :, :]) ** 2\n        loss_p = (output[:, 2:3, :, :] - y[:, 2:3, :, :]).abs()\n        loss = (loss_u + loss_v + loss_p) / CHANNELS_WEIGHTS\n        return {\"output\": loss.sum()}\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": {\"input\": train_x},\n                \"label\": {\"output\": train_y},\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.FunctionalLoss(loss_expr),\n        name=\"sup_constraint\",\n    )\n\n    # manually build constraint\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # initialize Adam optimizer\n    optimizer = ppsci.optimizer.Adam(\n        cfg.TRAIN.learning_rate, weight_decay=cfg.TRAIN.weight_decay\n    )(model)\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"input\": test_x},\n            \"label\": {\"output\": test_y},\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    def metric_expr(\n        output_dict: Dict[str, np.ndarray],\n        label_dict: Dict[str, np.ndarray] = None,\n        weight_dict: Dict[str, np.ndarray] = None,\n    ) -&gt; Dict[str, float]:\n        output = output_dict[\"output\"]\n        y = label_dict[\"output\"]\n        total_mse = ((output - y) ** 2).sum() / len(test_x)\n        ux_mse = ((output[:, 0, :, :] - test_y[:, 0, :, :]) ** 2).sum() / len(test_x)\n        uy_mse = ((output[:, 1, :, :] - test_y[:, 1, :, :]) ** 2).sum() / len(test_x)\n        p_mse = ((output[:, 2, :, :] - test_y[:, 2, :, :]) ** 2).sum() / len(test_x)\n        return {\n            \"Total_MSE\": total_mse,\n            \"Ux_MSE\": ux_mse,\n            \"Uy_MSE\": uy_mse,\n            \"p_MSE\": p_mse,\n        }\n\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.FunctionalLoss(loss_expr),\n        {\"output\": lambda out: out[\"output\"]},\n        {\"MSE\": ppsci.metric.FunctionalMetric(metric_expr)},\n        name=\"mse_validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        epochs=cfg.TRAIN.epochs,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        seed=cfg.seed,\n        validator=validator,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n\n    # train model\n    solver.train()\n\n    # evaluate after finished training\n    solver.eval()\n\n    PLOT_DIR = os.path.join(cfg.output_dir, \"visual\")\n    os.makedirs(PLOT_DIR, exist_ok=True)\n\n    # visualize prediction after finished training\n    predict_and_save_plot(test_x, test_y, 0, solver, PLOT_DIR)\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", os.path.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # initialize datasets\n    with open(cfg.DATAX_PATH, \"rb\") as file:\n        x = pickle.load(file)\n    with open(cfg.DATAY_PATH, \"rb\") as file:\n        y = pickle.load(file)\n\n    # split dataset to train dataset and test dataset\n    train_dataset, test_dataset = split_tensors(x, y, ratio=cfg.SLIPT_RATIO)\n    train_x, train_y = train_dataset\n    test_x, test_y = test_dataset\n\n    # initialize model\n    model = ppsci.arch.UNetEx(**cfg.MODEL)\n\n    CHANNELS_WEIGHTS = np.reshape(\n        np.sqrt(\n            np.mean(\n                np.transpose(y, (0, 2, 3, 1)).reshape(\n                    (cfg.SAMPLE_SIZE * cfg.X_SIZE * cfg.Y_SIZE, cfg.CHANNEL_SIZE)\n                )\n                ** 2,\n                axis=0,\n            )\n        ),\n        (1, -1, 1, 1),\n    )\n\n    # define loss\n    def loss_expr(\n        output_dict: Dict[str, np.ndarray],\n        label_dict: Dict[str, np.ndarray] = None,\n        weight_dict: Dict[str, np.ndarray] = None,\n    ) -&gt; float:\n        output = output_dict[\"output\"]\n        y = label_dict[\"output\"]\n        loss_u = (output[:, 0:1, :, :] - y[:, 0:1, :, :]) ** 2\n        loss_v = (output[:, 1:2, :, :] - y[:, 1:2, :, :]) ** 2\n        loss_p = (output[:, 2:3, :, :] - y[:, 2:3, :, :]).abs()\n        loss = (loss_u + loss_v + loss_p) / CHANNELS_WEIGHTS\n        return loss.sum()\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"input\": test_x},\n            \"label\": {\"output\": test_y},\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    def metric_expr(\n        output_dict: Dict[str, np.ndarray],\n        label_dict: Dict[str, np.ndarray] = None,\n        weight_dict: Dict[str, np.ndarray] = None,\n    ) -&gt; Dict[str, float]:\n        output = output_dict[\"output\"]\n        y = label_dict[\"output\"]\n        total_mse = ((output - y) ** 2).sum() / len(test_x)\n        ux_mse = ((output[:, 0, :, :] - test_y[:, 0, :, :]) ** 2).sum() / len(test_x)\n        uy_mse = ((output[:, 1, :, :] - test_y[:, 1, :, :]) ** 2).sum() / len(test_x)\n        p_mse = ((output[:, 2, :, :] - test_y[:, 2, :, :]) ** 2).sum() / len(test_x)\n        return {\n            \"Total_MSE\": total_mse,\n            \"Ux_MSE\": ux_mse,\n            \"Uy_MSE\": uy_mse,\n            \"p_MSE\": p_mse,\n        }\n\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.FunctionalLoss(loss_expr),\n        {\"output\": lambda out: out[\"output\"]},\n        {\"MSE\": ppsci.metric.FunctionalMetric(metric_expr)},\n        name=\"mse_validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n\n    # evaluate\n    solver.eval()\n\n    PLOT_DIR = os.path.join(cfg.output_dir, \"visual\")\n    os.makedirs(PLOT_DIR, exist_ok=True)\n\n    # visualize prediction\n    predict_and_save_plot(test_x, test_y, 0, solver, PLOT_DIR)\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"deepcfd.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/deepcfd/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"OpenFOAM \u8ba1\u7b97\u7ed3\u679c\u4e0e DeepCFD \u9884\u6d4b\u7ed3\u679c\u5bf9\u6bd4\uff0c\u4ece\u4e0a\u5230\u4e0b\u5206\u522b\u4e3a\uff1a\u6c34\u5e73\u901f\u5ea6\u5206\u91cf\uff08Ux\uff09\uff0c\u5782\u76f4\u901f\u5ea6\u5206\u91cf\uff08Uy\uff09\u4ee5\u53ca\u6d41\u4f53\u538b\u5f3a\uff08p\uff09 <p>\u53ef\u4ee5\u770b\u5230DeepCFD\u65b9\u6cd5\u4e0eOpenFOAM\u7684\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p>"},{"location":"zh/examples/deepcfd/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li>Ribeiro M D, Rehman A, Ahmed S, et al. DeepCFD: Efficient steady-state laminar flow approximation with deep convolutional neural networks</li> <li>\u57fa\u4e8ePaddlePaddle\u7684DeepCFD\u590d\u73b0</li> </ul>"},{"location":"zh/examples/deephpms/","title":"DeepHPMs","text":""},{"location":"zh/examples/deephpms/#deephpmsdeep-hidden-physics-models","title":"DeepHPMs(Deep Hidden Physics Models)","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <pre><code># \u6848\u4f8b 1\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat --create-dirs -o ./datasets/burgers_sine.mat\npython burgers.py DATASET_PATH=./datasets/burgers_sine.mat DATASET_PATH_SOL=./datasets/burgers_sine.mat\n\n# \u6848\u4f8b 2\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat --create-dirs -o ./datasets/burgers_sine.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers.mat --create-dirs -o ./datasets/burgers.mat\npython burgers.py DATASET_PATH=./datasets/burgers_sine.mat DATASET_PATH_SOL=./datasets/burgers.mat\n\n# \u6848\u4f8b 3\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers.mat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers.mat --create-dirs -o ./datasets/burgers.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat --create-dirs -o ./datasets/burgers_sine.mat\npython burgers.py DATASET_PATH=./datasets/burgers.mat DATASET_PATH_SOL=./datasets/burgers_sine.mat\n\n# \u6848\u4f8b 4\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_sine.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_sine.mat --create-dirs -o ./datasets/KdV_sine.mat\npython korteweg_de_vries.py DATASET_PATH=./datasets/KdV_sine.mat DATASET_PATH_SOL=./datasets/KdV_sine.mat\n\n# \u6848\u4f8b 5\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_sine.mat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_cos.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_sine.mat --create-dirs -o ./datasets/KdV_sine.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_cos.mat --create-dirs -o ./datasets/KdV_cos.mat\npython korteweg_de_vries.py DATASET_PATH=./datasets/KdV_sine.mat DATASET_PATH_SOL=./datasets/KdV_cos.mat\n\n# \u6848\u4f8b 6\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KS.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KS.mat --create-dirs -o ./datasets/KS.mat\npython kuramoto_sivashinsky.py DATASET_PATH=./datasets/KS.mat DATASET_PATH_SOL=./datasets/KS.mat\n\n# \u6848\u4f8b 7\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/cylinder.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/cylinder.mat --create-dirs -o ./datasets/cylinder.mat\npython navier_stokes.py DATASET_PATH=./datasets/cylinder.mat DATASET_PATH_SOL=./datasets/cylinder.mat\n\n# \u6848\u4f8b 8\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/NLS.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/NLS.mat --create-dirs -o ./datasets/NLS.mat\npython schrodinger.py DATASET_PATH=./datasets/NLS.mat DATASET_PATH_SOL=./datasets/NLS.mat\n</code></pre> <pre><code># \u6848\u4f8b 1\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat --create-dirs -o ./datasets/burgers_sine.mat\npython burgers.py mode=eval DATASET_PATH=./datasets/burgers_sine.mat DATASET_PATH_SOL=./datasets/burgers_sine.mat EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/DeepHPMs/burgers_same_pretrained.pdparams\n\n# \u6848\u4f8b 2\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat --create-dirs -o ./datasets/burgers_sine.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers.mat --create-dirs -o ./datasets/burgers.mat\npython burgers.py mode=eval DATASET_PATH=./datasets/burgers_sine.mat DATASET_PATH_SOL=./datasets/burgers.mat EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/DeepHPMs/burgers_diff_pretrained.pdparams\n\n# \u6848\u4f8b 3\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers.mat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers.mat --create-dirs -o ./datasets/burgers.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/burgers_sine.mat --create-dirs -o ./datasets/burgers_sine.mat\npython burgers.py mode=eval DATASET_PATH=./datasets/burgers.mat DATASET_PATH_SOL=./datasets/burgers_sine.mat EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/DeepHPMs/burgers_diff_swap_pretrained.pdparams\n\n# \u6848\u4f8b 4\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_sine.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_sine.mat --create-dirs -o ./datasets/KdV_sine.mat\npython korteweg_de_vries.py mode=eval DATASET_PATH=./datasets/KdV_sine.mat DATASET_PATH_SOL=./datasets/KdV_sine.mat EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/DeepHPMs/kdv_same_pretrained.pdparams\n\n# \u6848\u4f8b 5\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_sine.mat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_cos.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_sine.mat --create-dirs -o ./datasets/KdV_sine.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KdV_cos.mat --create-dirs -o ./datasets/KdV_cos.mat\npython korteweg_de_vries.py mode=eval DATASET_PATH=./datasets/KdV_sine.mat DATASET_PATH_SOL=./datasets/KdV_cos.mat EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/DeepHPMs/kdv_diff_pretrained.pdparams\n\n# \u6848\u4f8b 6\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KS.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/KS.mat --create-dirs -o ./datasets/KS.mat\npython kuramoto_sivashinsky.py mode=eval DATASET_PATH=./datasets/KS.mat DATASET_PATH_SOL=./datasets/KS.mat EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/DeepHPMs/ks_pretrained.pdparams\n\n# \u6848\u4f8b 7\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/cylinder.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/cylinder.mat --create-dirs -o ./datasets/cylinder.mat\npython navier_stokes.py mode=eval DATASET_PATH=./datasets/cylinder.mat DATASET_PATH_SOL=./datasets/cylinder.mat EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/DeepHPMs/ns_pretrained.pdparams\n\n# \u6848\u4f8b 8\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/NLS.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepHPMs/NLS.mat --create-dirs -o ./datasets/NLS.mat\npython schrodinger.py mode=eval DATASET_PATH=./datasets/NLS.mat DATASET_PATH_SOL=./datasets/NLS.mat EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/DeepHPMs/schrodinger_pretrained.pdparams\n</code></pre> \u5e8f\u53f7 \u6848\u4f8b\u540d\u79f0 stage1\u30012 \u6570\u636e\u96c6 stage3(eval)\u6570\u636e\u96c6 \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 1 burgers burgers_sine.mat burgers_sine.mat burgers_same_pretrained.pdparams l2 error: 0.0088 2 burgers burgers_sine.mat burgers.mat burgers_diff_pretrained.pdparams l2 error: 0.0379 3 burgers burgers.mat burgers_sine.mat burgers_diff_swap_pretrained.pdparams l2 error: 0.2904 4 korteweg_de_vries KdV_sine.mat KdV_sine.mat kdv_same_pretrained.pdparams l2 error: 0.0567 5 korteweg_de_vries KdV_sine.mat KdV_cos.mat kdv_diff_pretrained.pdparams l2 error: 0.1142 6 kuramoto_sivashinsky KS.mat KS.mat ks_pretrained.pdparams l2 error: 0.1166 7 navier_stokes cylinder.mat cylinder.mat ns_pretrained.pdparams l2 error: 0.0288 8 schrodinger NLS.mat NLS.mat schrodinger_pretrained.pdparams l2 error: 0.0735 <p>\u6ce8\uff1a\u6839\u636e \u53c2\u8003\u6587\u732e, \u5e8f\u53f7 3 \u7684\u6548\u679c\u8f83\u5dee\u3002</p>"},{"location":"zh/examples/deephpms/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b(PDE) \u662f\u4e00\u7c7b\u57fa\u7840\u7684\u7269\u7406\u95ee\u9898\uff0c\u5728\u8fc7\u53bb\u51e0\u5341\u5e74\u91cc\uff0c\u4ee5\u6709\u9650\u5dee\u5206(FDM)\u3001\u6709\u9650\u4f53\u79ef(FVM)\u3001\u6709\u9650\u5143(FEM)\u4e3a\u4ee3\u8868\u7684\u591a\u79cd\u504f\u5fae\u5206\u65b9\u7a0b\u7ec4\u6570\u503c\u89e3\u6cd5\u8d8b\u4e8e\u6210\u719f\u3002\u968f\u7740\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u9ad8\u901f\u53d1\u5c55\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u6210\u4e3a\u65b0\u7684\u7814\u7a76\u8d8b\u52bf\u3002PINNs(Physics-informed neural networks) \u662f\u4e00\u79cd\u52a0\u5165\u7269\u7406\u7ea6\u675f\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u56e0\u6b64\u4e0e\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u76f8\u6bd4\uff0cPINNs \u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u6570\u636e\u6837\u672c\u5b66\u4e60\u5230\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u6a21\u578b\uff0c\u5176\u5e94\u7528\u8303\u56f4\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u6d41\u4f53\u529b\u5b66\u3001\u70ed\u4f20\u5bfc\u3001\u7535\u78c1\u573a\u3001\u91cf\u5b50\u529b\u5b66\u7b49\u9886\u57df\u3002</p> <p>\u4f20\u7edf\u7684 PINNs \u4f1a\u5c06 PDE \u4f5c\u4e3a loss \u7684\u4e00\u9879\u53c2\u4e0e\u5230\u7f51\u7edc\u8bad\u7ec3\u4e2d\u53bb\uff0c\u8fd9\u5c31\u8981\u6c42 PDE \u516c\u5f0f\u4e3a\u5df2\u77e5\u7684\u5148\u9a8c\u6761\u4ef6\uff0c\u5f53 PDE \u516c\u5f0f\u672a\u77e5\u65f6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5c31\u4e0d\u80fd\u5b9e\u73b0\u3002</p> <p>DeepHPMs \u7740\u773c\u4e8e PDE \u516c\u5f0f\u672a\u77e5\u7684\u60c5\u51b5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u4ece\u5b9e\u9a8c\u4ea7\u751f\u7684\u9ad8\u7ef4\u6570\u636e\u4e2d\u53d1\u73b0\u7269\u7406\u89c4\u5f8b\uff0c\u5373\u975e\u7ebf\u6027 PDE \u65b9\u7a0b\uff0c\u5e76\u7528\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u6765\u8868\u5f81\u8fd9\u4e2a PDE \u65b9\u7a0b\uff0c\u518d\u5c06\u8fd9\u4e2a PDE \u7f51\u7edc\u66ff\u4ee3\u4f20\u7edf PINNs \u65b9\u6cd5\u4e2d\u7684 PDE \u516c\u5f0f\uff0c\u5bf9\u65b0\u7684\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u3002</p> <p>\u672c\u95ee\u9898\u5bf9 Burgers, Korteweg- de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schro \u0308dinger \u548c Navier- Stokes equations \u591a\u79cd PDE \u65b9\u7a0b\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u672c\u6587\u6863\u4e3b\u8981\u9488\u5bf9 Burgers \u65b9\u7a0b\u8fdb\u884c\u8bf4\u660e\u3002</p>"},{"location":"zh/examples/deephpms/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u4f2f\u683c\u65af\u65b9\u7a0b(Burgers equation) \u662f\u4e00\u4e2a\u6a21\u62df\u51b2\u51fb\u6ce2\u7684\u4f20\u64ad\u548c\u53cd\u5c04\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u8be5\u65b9\u7a0b\u8ba4\u4e3a\u8f93\u51fa\u7684\u89e3 \\(u\\) \u4e0e\u8f93\u5165\u7684\u4f4d\u7f6e\u3001\u65f6\u95f4\u53c2\u6570 \\((x, t)\\) \u4e4b\u95f4\u7684\u5173\u7cfb\u4e3a\uff1a</p> \\[ u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx} = 0 \\] <p>\u5176\u4e2d \\(u_t\\) \u4e3a \\(u\\) \u5bf9 \\(t\\) \u7684\u504f\u5bfc\u6570\uff0c\\(u_x\\) \u4e3a \\(u\\) \u5bf9 \\(x\\) \u7684\u504f\u5bfc\u6570\uff0c\\(u_{xx}\\) \u4e3a \\(u\\) \u5bf9 \\(x\\) \u7684\u4e8c\u9636\u504f\u5bfc\u6570\u3002</p> <p>\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u8868\u793a PDE\uff0c\u5373 \\(u_t\\) \u662f\u8f93\u5165\u4e3a \\(u, u_x, u_{xx}\\) \u7684\u7f51\u7edc\u7684\u8f93\u51fa\uff1a</p> \\[ u_t = \\mathcal{N}(u, u_x, u_{xx}) \\]"},{"location":"zh/examples/deephpms/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002\u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/deephpms/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u6570\u636e\u96c6\u4e3a\u5904\u7406\u597d\u7684 burgers \u6570\u636e\u96c6\uff0c\u5305\u542b\u4e0d\u540c\u521d\u59cb\u5316\u6761\u4ef6\u4e0b\u6a21\u62df\u6570\u636e\u7684 \\(x, t, u\\) \u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b58\u50a8\u5728 <code>.mat</code> \u6587\u4ef6\u4e2d\u3002</p> <p>\u8fd0\u884c\u672c\u95ee\u9898\u4ee3\u7801\u524d\u8bf7\u4e0b\u8f7d \u6a21\u62df\u6570\u636e\u96c61 \u548c \u6a21\u62df\u6570\u636e\u96c62\uff0c \u4e0b\u8f7d\u540e\u5206\u522b\u5b58\u653e\u5728\u8def\u5f84\uff1a</p> <pre><code>DATASET_PATH: ./datasets/burgers_sine.mat\nDATASET_PATH_SOL: ./datasets/burgers_sine.mat\n</code></pre>"},{"location":"zh/examples/deephpms/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u5171\u5305\u542b 3 \u4e2a\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u5206\u522b\u4e3a\u6570\u636e\u9a71\u52a8\u7684 Net1\uff0c\u8868\u5f81 PDE \u65b9\u7a0b\u7684 Net2 \u4ee5\u53ca\u7528\u4e8e\u63a8\u7406\u65b0\u6570\u636e\u7684 Net3\u3002</p> <p>Net1 \u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\uff0c\u4f7f\u7528\u8f93\u5165\u7684\u67d0\u79cd\u6a21\u62df\u60c5\u51b5 1 \u4e0b\u7684\u5c11\u91cf\u968f\u673a\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b66\u4e60\u6570\u636e\u89c4\u5f8b\uff0c\u4ece\u800c\u5f97\u5230\u8be5\u6a21\u62df\u60c5\u51b5\u4e0b\u5176\u4ed6\u6240\u6709\u6570\u636e\u7684\u6570\u503c \\(u\\)\u3002\u8f93\u5165\u4e3a\u6a21\u62df\u60c5\u51b5 1 \u6570\u636e\u7684 \\(x, t\\)\uff0c\u8f93\u51fa\u4e3a \\(u\\)\uff0c\u662f\u4e00\u4e2a\\((x, t)\\) \u5230 \\(u\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f_1: \\mathbb{R}^2 \\to \\mathbb{R}^1\\)\u3002</p> <p>\u5bf9 Net1 \u524d\u5411\u63a8\u7406\u5f97\u5230\u7684 \\(u\\) \u503c\uff0c\u8ba1\u7b97\u5176\u5bf9 \\(x, t\\) \u7684\u504f\u5bfc\u6570 \\(u_t, u_x, u_{xx}\\)\uff0c\u5e76\u5c06\u8ba1\u7b97\u5f97\u5230\u7684\u503c\u5f53\u4f5c\u771f\u5b9e\u7269\u7406\u503c\uff0c\u4f5c\u4e3a\u8f93\u5165\u548c\u6807\u7b7e\u4f20\u5230 Net2 \u4e2d\uff0c\u901a\u8fc7\u4f18\u5316 loss\uff0c\u8bad\u7ec3 Net2\u3002\u5bf9\u4e8e Net2\uff0c\u8f93\u5165\u4e3a Net1 \u63a8\u7406\u5f97\u5230\u7684 \\(u\\) \u4ee5\u53ca\u5b83\u5bf9x\u7684\u504f\u5bfc\u6570 \\(u_x, u_{xx}\\)\uff0c\u8f93\u51fa\u4e3a PDE \u7684\u8fd0\u7b97\u7ed3\u679c \\(f_{pde}\\)\uff0c\u8fd9\u4e2a\u503c\u5e94\u8be5\u4e0e \\(u_t\\) \u63a5\u8fd1\uff0c\u5373 \\(u_t\\) \u662f \\(f_{pde}\\) \u7684 label\u3002\u6620\u5c04\u51fd\u6570\u4e3a \\(f_2: \\mathbb{R}^3 \\to \\mathbb{R}^1\\)\u3002</p> <p>\u6700\u540e\uff0c\u5c06\u8bad\u7ec3\u597d\u7684 Net2 \u5f53\u4f5c PDE \u516c\u5f0f\uff0c\u5c06\u65b0\u7684\u6a21\u62df\u60c5\u51b5 2 \u4e0b\u7684\u5c11\u91cf\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u4e0e Net3 \u4e00\u8d77\u8fdb\u884c\u7c7b\u4f3c PINNs \u7684\u8bad\u7ec3\uff0c\u6700\u7ec8\u5f97\u5230\u53ef\u4ee5\u5bf9\u6a21\u62df\u60c5\u51b5 2 \u8fdb\u884c\u9884\u6d4b\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc Net3\u3002\u5bf9\u4e8e Net3\uff0c\u8f93\u5165\u4e3a\u6a21\u62df\u60c5\u51b5 2 \u6570\u636e\u7684 \\(x, t\\)\uff0c\u8f93\u51fa\u4e3a \\(u\\)\uff0c\u662f\u4e00\u4e2a\\((x, t)\\) \u5230 \\(u\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f_3: \\mathbb{R}^2 \\to \\mathbb{R}^1\\)\u3002</p> <p>\u56e0\u4e3a\u8bad\u7ec3\u4e2d\u540e\u4e00\u4e2a\u9636\u6bb5\u7f51\u7edc\u9700\u8981\u4f7f\u7528\u524d\u4e00\u4e2a\u9636\u6bb5\u7f51\u7edc\u7684\u524d\u5411\u63a8\u7406\u503c\uff0c\u56e0\u6b64\u672c\u95ee\u9898\u4f7f\u7528 Model List \u6765\u5b9e\u73b0\uff0c\u4e0a\u5f0f\u4e2d \\(f_1,f_2,f_3\\) \u5206\u522b\u4e3a\u4e00\u4e2a MLP \u6a21\u578b\uff0c\u4e09\u8005\u5171\u540c\u6784\u6210\u4e86\u4e00\u4e2a Model List\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># initialize model list\nmodel_list = ppsci.arch.ModelList((model_idn, model_pde, model_sol))\n</code></pre> <p>\u6ce8\u610f\u5230\u90e8\u5206\u7f51\u7edc\u7684\u8f93\u5165\u7531\u4e4b\u524d\u7684\u7f51\u7edc\u8ba1\u7b97\u5f97\u5230\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6570\u636e\u4e2d \\((x, t)\\) \u8fd9\u4e24\u4e2a\u53d8\u91cf\uff0c\u8fd9\u4e5f\u5c31\u610f\u5473\u7740\u6211\u4eec\u9700\u8981\u5bf9\u90e8\u5206\u7f51\u7edc\u8f93\u5165\u8fdb\u884c transform\u3002</p>"},{"location":"zh/examples/deephpms/#33-transform","title":"3.3 transform\u6784\u5efa","text":"<p>\u5bf9\u4e8e Net1\uff0c\u8f93\u5165\u4e3a \\((x, t)\\) \u672c\u6765\u4e0d\u9700\u8981 transform\uff0c\u4f46\u7531\u4e8e\u8bad\u7ec3\u4e2d\u6839\u636e\u6570\u636e\u7684\u5b9a\u4e49\u57df\u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c\u4e86\u6570\u503c\u53d8\u6362\uff0c\u56e0\u6b64\u540c\u6837\u9700\u8981transform\uff0c\u540c\u6837\uff0cNet3 \u4e5f\u9700\u8981\u5bf9\u8f93\u5165\u8fdb\u884c\u6570\u503c\u53d8\u6362\u7684 transform</p> <pre><code>def transform_u(_in):\n    t, x = _in[\"t\"], _in[\"x\"]\n    t = 2.0 * (t - t_lb) * paddle.pow((t_ub - t_lb), -1) - 1.0\n    x = 2.0 * (x - x_lb) * paddle.pow((x_ub - x_lb), -1) - 1.0\n    input_trans = {\"t\": t, \"x\": x}\n    return input_trans\n</code></pre> <p>\u5bf9\u4e8e Net2\uff0c\u56e0\u4e3a\u5b83\u7684\u8f93\u5165\u4e3a \\(u, u_x, u_{xx}\\) \u800c \\(u\\) \u4e3a\u5176\u4ed6\u4e24\u4e2a\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\u53ea\u8981\u8fdb\u884c Net2 \u7684\u524d\u5411\u63a8\u7406\uff0c\u5c31\u9700\u8981 transform\uff0c\u56e0\u6b64\u9700\u8981\u4e24\u79cd transform\u3002\u540c\u65f6\uff0c\u5728\u8bad\u7ec3 Net3 \u4e4b\u524d\uff0c\u9700\u8981\u91cd\u65b0\u6ce8\u518c transform</p> <pre><code>def transform_f(input, model, out_key):\n    in_idn = {\"t\": input[\"t\"], \"x\": input[\"x\"]}\n    x = input[\"x\"]\n    u = model(in_idn)[out_key]\n    du_x = jacobian(u, x)\n    du_xx = hessian(u, x)\n    input_trans = {\"u_x\": u, \"du_x\": du_x, \"du_xx\": du_xx}\n    return input_trans\n\ndef transform_f_idn(_in):\n    return transform_f(_in, model_idn, \"u_idn\")\n</code></pre> <p>\u7136\u540e\u4f9d\u6b21\u6ce8\u518c transform \u540e\uff0c\u5c063 \u4e2a MLP \u6a21\u578b\u7ec4\u6210 Model List</p> <pre><code># register transform\nmodel_idn.register_input_transform(transform_u)\nmodel_pde.register_input_transform(transform_f_idn)\nmodel_sol.register_input_transform(transform_u)\n\n# initialize model list\nmodel_list = ppsci.arch.ModelList((model_idn, model_pde, model_sol))\n</code></pre> <p>\u6ce8\u610f Net3 \u5f00\u59cb\u8bad\u7ec3\u524d\uff0c\u91cd\u65b0\u6ce8\u518c Net2 \u7684transform</p> <pre><code># re-register transform for model 2, fit for loss of stage 3\nmodel_pde.register_input_transform(transform_f_sol)\n</code></pre> <p>\u8fd9\u6837\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 3 \u4e2a MLP \u6a21\u578b\uff0c\u6bcf\u4e2a MLP \u5305\u542b 4 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 50\uff0c\u4f7f\u7528 \"sin\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5305\u542b\u8f93\u5165 transform \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model list</code>\u3002</p>"},{"location":"zh/examples/deephpms/#34","title":"3.4 \u53c2\u6570\u548c\u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u6211\u4eec\u9700\u8981\u6307\u5b9a\u95ee\u9898\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5982\u6570\u636e\u96c6\u8def\u5f84\u3001\u8f93\u51fa\u6587\u4ef6\u8def\u5f84\u3001\u5b9a\u4e49\u57df\u7684\u503c\u7b49</p> <pre><code>DATASET_PATH: ./datasets/burgers_sine.mat\nDATASET_PATH_SOL: ./datasets/burgers_sine.mat\n\n# set working condition\nT_LB: 0.0\nT_UB: 10.0\nX_LB: -8.0\nX_UB: 8.0\n</code></pre> <p>\u540c\u65f6\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\u7b49\u8d85\u53c2\u6570</p> <pre><code>TRAIN:\n  epochs: 50000 # set 1 for LBFGS\n  iters_per_epoch: 1\n  max_iter: 50000  # for LBFGS\n  learning_rate: 1.0e-3\n</code></pre>"},{"location":"zh/examples/deephpms/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u63d0\u4f9b\u4e86\u4e24\u79cd\u4f18\u5316\u5668\uff0c\u5206\u522b\u4e3a Adam \u4f18\u5316\u5668\u548c LBFGS \u4f18\u5316\u5668\uff0c\u8bad\u7ec3\u65f6\u53ea\u9009\u62e9\u5176\u4e2d\u4e00\u79cd\uff0c\u9700\u8981\u5c06\u53e6\u4e00\u79cd\u4f18\u5316\u5668\u6ce8\u91ca\u6389\u3002</p> <pre><code># initialize optimizer\n# Adam\noptimizer_idn = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model_idn)\noptimizer_pde = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model_pde)\noptimizer_sol = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model_sol)\n\n# LBFGS\n# optimizer_idn = ppsci.optimizer.LBFGS(max_iter=cfg.TRAIN.max_iter)(model_idn)\n# optimizer_pde = ppsci.optimizer.LBFGS(max_iter=cfg.TRAIN.max_iter)(model_pde)\n# optimizer_sol = ppsci.optimizer.LBFGS(max_iter=cfg.TRAIN.max_iter)(model_sol)\n</code></pre>"},{"location":"zh/examples/deephpms/#36","title":"3.6 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u5206\u4e3a\u4e09\u4e2a\u8bad\u7ec3\u9636\u6bb5\uff0c\u90e8\u5206\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u5bf9 \\(u\\) \u8fdb\u884c\u7ea6\u675f\uff0c\u90e8\u5206\u91c7\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u7ea6\u675f\u7ed3\u679c\u6ee1\u8db3 PDE \u516c\u5f0f\u3002</p> <p>\u65e0\u76d1\u7763\u4ecd\u7136\u53ef\u4ee5\u91c7\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code>\uff0c\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u76d1\u7763\u7ea6\u675f\u6307\u5b9a\u6587\u4ef6\u8def\u5f84\u7b49\u6570\u636e\u8bfb\u53d6\u914d\u7f6e\uff0c\u56e0\u4e3a\u6570\u636e\u96c6\u4e2d\u6ca1\u6709\u6807\u7b7e\u6570\u636e\uff0c\u56e0\u6b64\u5728\u6570\u636e\u8bfb\u53d6\u65f6\u6211\u4eec\u9700\u8981\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u5145\u5f53\u6807\u7b7e\u6570\u636e\uff0c\u5e76\u6ce8\u610f\u5728\u4e4b\u540e\u4e0d\u8981\u4f7f\u7528\u8fd9\u90e8\u5206\u201c\u5047\u7684\u201d\u6807\u7b7e\u6570\u636e\uff0c\u4f8b\u5982</p> <pre><code>train_dataloader_cfg_idn = {\n    \"dataset\": {\n        \"name\": \"IterableMatDataset\",\n        \"file_path\": cfg.DATASET_PATH,\n        \"input_keys\": (\"t\", \"x\"),\n        \"label_keys\": (\"u_idn\",),\n        \"alias_dict\": {\"t\": \"t_train\", \"x\": \"x_train\", \"u_idn\": \"u_train\"},\n    },\n}\n</code></pre> <p><code>du_t</code> \u503c\u8bfb\u53d6\u4e86 <code>t</code> \u7684\u503c\uff0c\u662f\u201c\u5047\u7684\u201d\u6807\u7b7e\u6570\u636e\u3002</p>"},{"location":"zh/examples/deephpms/#361","title":"3.6.1 \u7b2c\u4e00\u9636\u6bb5\u7ea6\u675f\u6784\u5efa","text":"<p>\u7b2c\u4e00\u9636\u6bb5\u5bf9 Net1 \u7684\u8bad\u7ec3\u662f\u7eaf\u76d1\u7763\u5b66\u4e60\uff0c\u6b64\u5904\u91c7\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code></p> <pre><code>sup_constraint_idn = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg_idn,\n    ppsci.loss.MSELoss(\"sum\"),\n    {\"u_idn\": lambda out: out[\"u_idn\"]},\n    name=\"u_mse_sup\",\n)\nconstraint_idn = {sup_constraint_idn.name: sup_constraint_idn}\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u914d\u7f6e\u4e2d <code>\u201cdataset\u201d</code> \u5b57\u6bb5\u8868\u793a\u4f7f\u7528\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4fe1\u606f\uff0c\u5176\u5404\u4e2a\u5b57\u6bb5\u5206\u522b\u8868\u793a\uff1a</p> <ol> <li><code>name</code>\uff1a \u6570\u636e\u96c6\u7c7b\u578b\uff0c\u6b64\u5904 <code>\"IterableMatDataset\"</code> \u8868\u793a\u4e0d\u5206 batch \u987a\u5e8f\u8bfb\u53d6\u7684 <code>.mat</code> \u7c7b\u578b\u7684\u6570\u636e\u96c6\uff1b</li> <li><code>file_path</code>\uff1a \u6570\u636e\u96c6\u6587\u4ef6\u8def\u5f84\uff1b</li> <li><code>input_keys</code>\uff1a \u8f93\u5165\u53d8\u91cf\u540d\uff1b</li> <li><code>label_keys</code>\uff1a \u6807\u7b7e\u53d8\u91cf\u540d\uff1b</li> <li><code>alias_dict</code>\uff1a \u53d8\u91cf\u522b\u540d\u3002</li> </ol> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u7531\u4e8e\u662f\u7eaf\u6570\u636e\u9a71\u52a8\uff0c\u6b64\u5904\u4f7f\u7528 <code>MSE</code>\u3002</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u8ba1\u7b97\u540e\u7684\u503c\u5c06\u4f1a\u6309\u7167\u6307\u5b9a\u540d\u79f0\u5b58\u5165\u8f93\u51fa\u5217\u8868\u4e2d\uff0c\u4ece\u800c\u4fdd\u8bc1 loss \u8ba1\u7b97\u65f6\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b\u503c\u3002</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002</p> <p>\u5728\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p>"},{"location":"zh/examples/deephpms/#362","title":"3.6.2 \u7b2c\u4e8c\u9636\u6bb5\u7ea6\u675f\u6784\u5efa","text":"<p>\u7b2c\u4e8c\u9636\u6bb5\u5bf9 Net2 \u7684\u8bad\u7ec3\u662f\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u4f46\u4ecd\u53ef\u91c7\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code>\uff0c\u8981\u6ce8\u610f\u4e0a\u8ff0\u63d0\u5230\u7684\u7ed9\u5b9a\u201c\u5047\u7684\u201d\u6807\u7b7e\u6570\u636e</p> <pre><code>sup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg_pde,\n    ppsci.loss.FunctionalLoss(pde_loss_func),\n    {\n        \"du_t\": lambda out: jacobian(out[\"u_idn\"], out[\"t\"]),\n        \"f_pde\": lambda out: out[\"f_pde\"],\n    },\n    name=\"f_mse_sup\",\n)\nconstraint_pde = {sup_constraint_pde.name: sup_constraint_pde}\n</code></pre> <p>\u5404\u4e2a\u53c2\u6570\u542b\u4e49\u4e0e \u7b2c\u4e00\u9636\u6bb5\u7ea6\u675f\u6784\u5efa \u4e00\u81f4\uff0c\u552f\u4e00\u7684\u533a\u522b\u662f\u8fd9\u4e2a\u7ea6\u675f\u4e2d\u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\uff0c\u635f\u5931\u51fd\u6570\uff0c\u91c7\u7528 PaddleScience \u9884\u7559\u7684\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u7c7b <code>FunctionalLoss</code>\uff0c\u8be5\u7c7b\u652f\u6301\u7f16\u5199\u4ee3\u7801\u65f6\u81ea\u5b9a\u4e49 loss \u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u8bf8\u5982 <code>MSE</code> \u7b49\u73b0\u6709\u65b9\u6cd5\u3002\u672c\u7ea6\u675f\u4e2d\u7684\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u4ee3\u7801\u8bf7\u53c2\u8003 \u81ea\u5b9a\u4e49 loss \u548c metric\u3002</p> <p>\u5728\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p>"},{"location":"zh/examples/deephpms/#363","title":"3.6.3 \u7b2c\u4e09\u9636\u6bb5\u7ea6\u675f\u6784\u5efa","text":"<p>\u7b2c\u4e09\u9636\u6bb5 Net3 \u7684\u8bad\u7ec3\u590d\u6742\uff0c\u5305\u542b\u4e86\u5bf9\u90e8\u5206\u521d\u59cb\u70b9\u7684\u76d1\u7763\u5b66\u4e60\u3001\u4e0e PDE \u6709\u5173\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u4ee5\u53ca\u4e0e\u8fb9\u754c\u6761\u4ef6\u6709\u5173\u7684\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u8fd9\u91cc\u4ecd\u91c7\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code>\uff0c\u540c\u6837\u8981\u6ce8\u610f\u7ed9\u5b9a\u201c\u5047\u7684\u201d\u6807\u7b7e\u6570\u636e\uff0c\u5404\u53c2\u6570\u542b\u4e49\u540c\u4e0a</p> <pre><code>sup_constraint_sol_f = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg_sol_f,\n    ppsci.loss.FunctionalLoss(pde_loss_func),\n    {\n        \"f_pde\": lambda out: out[\"f_pde\"],\n        \"du_t\": lambda out: jacobian(out[\"u_sol\"], out[\"t\"]),\n    },\n    name=\"f_mse_sup\",\n)\nsup_constraint_sol_init = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg_sol_init,\n    ppsci.loss.MSELoss(\"sum\"),\n    {\"u_sol\": lambda out: out[\"u_sol\"]},\n    name=\"u0_mse_sup\",\n)\nsup_constraint_sol_bc = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg_sol_bc,\n    ppsci.loss.FunctionalLoss(boundary_loss_func),\n    {\n        \"x\": lambda out: out[\"x\"],\n        \"u_sol\": lambda out: out[\"u_sol\"],\n    },\n    name=\"ub_mse_sup\",\n)\nconstraint_sol = {\n    sup_constraint_sol_f.name: sup_constraint_sol_f,\n    sup_constraint_sol_init.name: sup_constraint_sol_init,\n    sup_constraint_sol_bc.name: sup_constraint_sol_bc,\n}\n</code></pre> <p>\u5728\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p>"},{"location":"zh/examples/deephpms/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u4e0e\u7ea6\u675f\u540c\u7406\uff0c\u867d\u7136\u672c\u95ee\u9898\u90e8\u5206\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u90e8\u5206\u91c7\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u4f46\u4ecd\u53ef\u4ee5\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\uff0c\u53c2\u6570\u542b\u4e49\u4e5f\u4e0e\u7ea6\u675f\u6784\u5efa\uff0c\u552f\u4e00\u7684\u533a\u522b\u662f\u8bc4\u4ef7\u6307\u6807 <code>metric</code></p>"},{"location":"zh/examples/deephpms/#371","title":"3.7.1 \u7b2c\u4e00\u9636\u6bb5\u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u4e3a <code>L2</code> \u6b63\u5219\u5316\u51fd\u6570</p> <pre><code>sup_validator_idn = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg_idn,\n    ppsci.loss.MSELoss(\"sum\"),\n    {\"u_idn\": lambda out: out[\"u_idn\"]},\n    {\"l2\": ppsci.metric.L2Rel()},\n    name=\"u_L2_sup\",\n)\nvalidator_idn = {sup_validator_idn.name: sup_validator_idn}\n</code></pre>"},{"location":"zh/examples/deephpms/#372","title":"3.7.2 \u7b2c\u4e8c\u9636\u6bb5\u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u4e3a <code>FunctionalMetric</code>\uff0c\u8fd9\u662f PaddleScience \u9884\u7559\u7684\u81ea\u5b9a\u4e49 metric \u51fd\u6570\u7c7b\uff0c\u8be5\u7c7b\u652f\u6301\u7f16\u5199\u4ee3\u7801\u65f6\u81ea\u5b9a\u4e49 metric \u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u8bf8\u5982 <code>MSE</code>\u3001 <code>L2</code> \u7b49\u73b0\u6709\u65b9\u6cd5\u3002\u81ea\u5b9a\u4e49 metric \u51fd\u6570\u4ee3\u7801\u8bf7\u53c2\u8003\u4e0b\u4e00\u90e8\u5206 \u81ea\u5b9a\u4e49 loss \u548c metric\u3002</p> <pre><code>sup_validator_pde = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg_pde,\n    ppsci.loss.FunctionalLoss(pde_loss_func),\n    {\n        \"du_t\": lambda out: jacobian(out[\"u_idn\"], out[\"t\"]),\n        \"f_pde\": lambda out: out[\"f_pde\"],\n    },\n    {\"l2\": ppsci.metric.FunctionalMetric(pde_l2_rel_func)},\n    name=\"f_L2_sup\",\n)\nvalidator_pde = {sup_validator_pde.name: sup_validator_pde}\n</code></pre>"},{"location":"zh/examples/deephpms/#373","title":"3.7.3 \u7b2c\u4e09\u9636\u6bb5\u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u56e0\u4e3a\u7b2c\u4e09\u9636\u6bb5\u8bc4\u4ef7\u65f6\u53ea\u9700\u8981\u5bf9\u8bad\u7ec3\u5f97\u5230\u7684\u70b9\u7684\u503c\u8fdb\u884c\u8bc4\u4ef7\uff0c\u800c\u4e0d\u9700\u8981\u5bf9\u8fb9\u754c\u6761\u4ef6\u6ee1\u8db3\u7a0b\u5ea6\u6216 PDE \u6ee1\u8db3\u7a0b\u5ea6\u8fdb\u884c\u8bc4\u4ef7\uff0c\u56e0\u6b64\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u4e3a <code>L2</code> \u6b63\u5219\u5316\u51fd\u6570</p> <pre><code>sup_validator_sol = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg_sol,\n    ppsci.loss.MSELoss(\"sum\"),\n    {\"u_sol\": lambda out: out[\"u_sol\"]},\n    {\"l2\": ppsci.metric.L2Rel()},\n    name=\"u_L2_sup\",\n)\nvalidator_sol = {sup_validator_sol.name: sup_validator_sol}\n</code></pre>"},{"location":"zh/examples/deephpms/#38-loss-metric","title":"3.8 \u81ea\u5b9a\u4e49 loss \u548c metric","text":"<p>\u7531\u4e8e\u672c\u95ee\u9898\u5305\u542b\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u6807\u7b7e\u6570\u636e\uff0closs \u548c metric \u6839\u636e PDE \u8ba1\u7b97\u5f97\u5230\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u5b9a\u4e49 loss \u548c metric\u3002\u65b9\u6cd5\u4e3a\u5148\u5b9a\u4e49\u76f8\u5173\u51fd\u6570\uff0c\u518d\u5c06\u51fd\u6570\u540d\u4f5c\u4e3a\u53c2\u6570\u4f20\u7ed9 <code>FunctionalLoss</code> \u548c <code>FunctionalMetric</code>\u3002</p> <p>\u9700\u8981\u6ce8\u610f\u81ea\u5b9a\u4e49 loss \u548c metric \u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u53c2\u6570\u9700\u8981\u4e0e PaddleScience \u4e2d\u5982 <code>MSE</code> \u7b49\u5176\u4ed6\u51fd\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u5373\u8f93\u5165\u4e3a\u6a21\u578b\u8f93\u51fa <code>output_dict</code> \u7b49\u5b57\u5178\u53d8\u91cf\uff0closs \u51fd\u6570\u8f93\u51fa\u4e3a loss \u503c <code>paddle.Tensor</code>\uff0cmetric \u51fd\u6570\u8f93\u51fa\u4e3a\u5b57\u5178 <code>Dict[str, paddle.Tensor]</code>\u3002</p> <p>\u4e0e PDE \u76f8\u5173\u7684\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u4e3a</p> <pre><code>def pde_loss_func(output_dict, *args):\n    losses = F.mse_loss(output_dict[\"f_pde\"], output_dict[\"du_t\"], \"sum\")\n    return {\"pde\": losses}\n</code></pre> <p>\u4e0e PDE \u76f8\u5173\u7684\u81ea\u5b9a\u4e49 metric \u51fd\u6570\u4e3a</p> <pre><code>def pde_l2_rel_func(output_dict, *args):\n    rel_l2 = paddle.norm(output_dict[\"du_t\"] - output_dict[\"f_pde\"]) / paddle.norm(\n        output_dict[\"du_t\"]\n    )\n    metric_dict = {\"f_pde\": rel_l2}\n    return metric_dict\n</code></pre> <p>\u4e0e\u8fb9\u754c\u6761\u4ef6\u76f8\u5173\u7684\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u4e3a</p> <pre><code>def boundary_loss_func(output_dict, *args):\n    u_b = output_dict[\"u_sol\"]\n    u_lb, u_ub = paddle.split(u_b, 2, axis=0)\n\n    x_b = output_dict[\"x\"]\n    du_x = jacobian(u_b, x_b)\n\n    du_x_lb, du_x_ub = paddle.split(du_x, 2, axis=0)\n\n    losses = F.mse_loss(u_lb, u_ub, \"sum\")\n    losses += F.mse_loss(du_x_lb, du_x_ub, \"sum\")\n    return {\"boundary\": losses}\n</code></pre>"},{"location":"zh/examples/deephpms/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9\u5404\u4e2a\u9636\u6bb5 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> <p>\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u3001\u8bc4\u4f30</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model_list,\n    constraint_idn,\n    cfg.output_dir,\n    optimizer_idn,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    validator=validator_idn,\n)\n\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre> <p>\u7b2c\u4e8c\u9636\u6bb5\u8bad\u7ec3\u3001\u8bc4\u4f30</p> <pre><code># update solver\nsolver = ppsci.solver.Solver(\n    model_list,\n    constraint_pde,\n    cfg.output_dir,\n    optimizer_pde,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    validator=validator_pde,\n)\n\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre> <p>\u7b2c\u4e09\u9636\u6bb5\u8bad\u7ec3\u3001\u8bc4\u4f30</p> <pre><code># update solver\nsolver = ppsci.solver.Solver(\n    model_list,\n    constraint_sol,\n    cfg.output_dir,\n    optimizer_sol,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    validator=validator_sol,\n)\n\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/deephpms/#310","title":"3.10 \u53ef\u89c6\u5316","text":"<p>\u672c\u95ee\u9898\u8bad\u7ec3\u7ed3\u675f\u540e\uff0c\u53ef\u4ee5\u5728 <code>evalution</code> \u4e2d\u4f7f\u7528\u7b2c\u4e09\u9636\u6bb5\u7f51\u7edc Net3 \u5bf9\u6a21\u62df\u60c5\u51b5 2 \u7684\u6570\u636e\u8fdb\u884c\u63a8\u7406\uff0c\u7ed3\u679c\u4e3a \\(u|_{(x,t)}\\) \u503c\uff0c\u540c\u65f6\u8f93\u51fa <code>l2 error</code> \u7684\u503c\u3002\u753b\u56fe\u90e8\u5206\u5728 plotting.py \u6587\u4ef6\u4e2d\u3002</p> <pre><code># stage 3: solution net\n# load pretrained model\nsave_load.load_pretrain(model_list, cfg.EVAL.pretrained_model_path)\n\n# load dataset\ndataset_val = reader.load_mat_file(\n    cfg.DATASET_PATH_SOL,\n    keys=(\"t\", \"x\", \"u_sol\"),\n    alias_dict={\n        \"t\": \"t_ori\",\n        \"x\": \"x_ori\",\n        \"u_sol\": \"Exact_ori\",\n    },\n)\n\nt_sol, x_sol = np.meshgrid(\n    np.squeeze(dataset_val[\"t\"]), np.squeeze(dataset_val[\"x\"])\n)\nt_sol_flatten = paddle.to_tensor(\n    t_sol.flatten()[:, None], dtype=paddle.get_default_dtype(), stop_gradient=False\n)\nx_sol_flatten = paddle.to_tensor(\n    x_sol.flatten()[:, None], dtype=paddle.get_default_dtype(), stop_gradient=False\n)\nu_sol_pred = model_list({\"t\": t_sol_flatten, \"x\": x_sol_flatten})\n\n# eval\nl2_error = np.linalg.norm(\n    dataset_val[\"u_sol\"] - u_sol_pred[\"u_sol\"], 2\n) / np.linalg.norm(dataset_val[\"u_sol\"], 2)\nlogger.info(f\"l2_error: {l2_error}\")\n\n# plotting\nplot_points = paddle.concat([t_sol_flatten, x_sol_flatten], axis=-1).numpy()\nplot_func.draw_and_save(\n    figname=\"burgers_sol\",\n    data_exact=dataset_val[\"u_sol\"],\n    data_learned=u_sol_pred[\"u_sol\"].numpy(),\n    boundary=[cfg.T_LB, cfg.T_UB, cfg.X_LB, cfg.X_UB],\n    griddata_points=plot_points,\n    griddata_xi=(t_sol, x_sol),\n    save_path=cfg.output_dir,\n)\n</code></pre>"},{"location":"zh/examples/deephpms/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"burgers.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle\nimport paddle.nn.functional as F\nimport plotting as plot_func\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.autodiff import hessian\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\nfrom ppsci.utils import reader\nfrom ppsci.utils import save_load\n\n\ndef pde_loss_func(output_dict, *args):\n    losses = F.mse_loss(output_dict[\"f_pde\"], output_dict[\"du_t\"], \"sum\")\n    return {\"pde\": losses}\n\n\ndef pde_l2_rel_func(output_dict, *args):\n    rel_l2 = paddle.norm(output_dict[\"du_t\"] - output_dict[\"f_pde\"]) / paddle.norm(\n        output_dict[\"du_t\"]\n    )\n    metric_dict = {\"f_pde\": rel_l2}\n    return metric_dict\n\n\ndef boundary_loss_func(output_dict, *args):\n    u_b = output_dict[\"u_sol\"]\n    u_lb, u_ub = paddle.split(u_b, 2, axis=0)\n\n    x_b = output_dict[\"x\"]\n    du_x = jacobian(u_b, x_b)\n\n    du_x_lb, du_x_ub = paddle.split(du_x, 2, axis=0)\n\n    losses = F.mse_loss(u_lb, u_ub, \"sum\")\n    losses += F.mse_loss(du_x_lb, du_x_ub, \"sum\")\n    return {\"boundary\": losses}\n\n\ndef train(cfg: DictConfig):\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # initialize burgers boundaries\n    t_lb = paddle.to_tensor(cfg.T_LB)\n    t_ub = paddle.to_tensor(cfg.T_UB)\n    x_lb = paddle.to_tensor(cfg.X_LB)\n    x_ub = paddle.to_tensor(cfg.T_UB)\n\n    # initialize models\n    model_idn = ppsci.arch.MLP(**cfg.MODEL.idn_net)\n    model_pde = ppsci.arch.MLP(**cfg.MODEL.pde_net)\n    model_sol = ppsci.arch.MLP(**cfg.MODEL.sol_net)\n\n    # initialize transform\n    def transform_u(_in):\n        t, x = _in[\"t\"], _in[\"x\"]\n        t = 2.0 * (t - t_lb) * paddle.pow((t_ub - t_lb), -1) - 1.0\n        x = 2.0 * (x - x_lb) * paddle.pow((x_ub - x_lb), -1) - 1.0\n        input_trans = {\"t\": t, \"x\": x}\n        return input_trans\n\n    def transform_f(input, model, out_key):\n        in_idn = {\"t\": input[\"t\"], \"x\": input[\"x\"]}\n        x = input[\"x\"]\n        u = model(in_idn)[out_key]\n        du_x = jacobian(u, x)\n        du_xx = hessian(u, x)\n        input_trans = {\"u_x\": u, \"du_x\": du_x, \"du_xx\": du_xx}\n        return input_trans\n\n    def transform_f_idn(_in):\n        return transform_f(_in, model_idn, \"u_idn\")\n\n    def transform_f_sol(_in):\n        return transform_f(_in, model_sol, \"u_sol\")\n\n    # register transform\n    model_idn.register_input_transform(transform_u)\n    model_pde.register_input_transform(transform_f_idn)\n    model_sol.register_input_transform(transform_u)\n\n    # initialize model list\n    model_list = ppsci.arch.ModelList((model_idn, model_pde, model_sol))\n\n    # initialize optimizer\n    # Adam\n    optimizer_idn = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model_idn)\n    optimizer_pde = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model_pde)\n    optimizer_sol = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model_sol)\n\n    # LBFGS\n    # optimizer_idn = ppsci.optimizer.LBFGS(max_iter=cfg.TRAIN.max_iter)(model_idn)\n    # optimizer_pde = ppsci.optimizer.LBFGS(max_iter=cfg.TRAIN.max_iter)(model_pde)\n    # optimizer_sol = ppsci.optimizer.LBFGS(max_iter=cfg.TRAIN.max_iter)(model_sol)\n\n    # stage 1: training identification net\n    # manually build constraint(s)\n    train_dataloader_cfg_idn = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH,\n            \"input_keys\": (\"t\", \"x\"),\n            \"label_keys\": (\"u_idn\",),\n            \"alias_dict\": {\"t\": \"t_train\", \"x\": \"x_train\", \"u_idn\": \"u_train\"},\n        },\n    }\n\n    sup_constraint_idn = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_idn,\n        ppsci.loss.MSELoss(\"sum\"),\n        {\"u_idn\": lambda out: out[\"u_idn\"]},\n        name=\"u_mse_sup\",\n    )\n    constraint_idn = {sup_constraint_idn.name: sup_constraint_idn}\n\n    # manually build validator\n    eval_dataloader_cfg_idn = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH,\n            \"input_keys\": (\"t\", \"x\"),\n            \"label_keys\": (\"u_idn\",),\n            \"alias_dict\": {\"t\": \"t_star\", \"x\": \"x_star\", \"u_idn\": \"u_star\"},\n        },\n    }\n\n    sup_validator_idn = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg_idn,\n        ppsci.loss.MSELoss(\"sum\"),\n        {\"u_idn\": lambda out: out[\"u_idn\"]},\n        {\"l2\": ppsci.metric.L2Rel()},\n        name=\"u_L2_sup\",\n    )\n    validator_idn = {sup_validator_idn.name: sup_validator_idn}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        constraint_idn,\n        cfg.output_dir,\n        optimizer_idn,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        validator=validator_idn,\n    )\n\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n    # stage 2: training pde net\n    # manually build constraint(s)\n    train_dataloader_cfg_pde = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH,\n            \"input_keys\": (\"t\", \"x\"),\n            \"label_keys\": (\"du_t\",),\n            \"alias_dict\": {\"t\": \"t_train\", \"x\": \"x_train\", \"du_t\": \"t_train\"},\n        },\n    }\n\n    sup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_pde,\n        ppsci.loss.FunctionalLoss(pde_loss_func),\n        {\n            \"du_t\": lambda out: jacobian(out[\"u_idn\"], out[\"t\"]),\n            \"f_pde\": lambda out: out[\"f_pde\"],\n        },\n        name=\"f_mse_sup\",\n    )\n    constraint_pde = {sup_constraint_pde.name: sup_constraint_pde}\n\n    # manually build validator\n    eval_dataloader_cfg_pde = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH,\n            \"input_keys\": (\"t\", \"x\"),\n            \"label_keys\": (\"du_t\",),\n            \"alias_dict\": {\"t\": \"t_star\", \"x\": \"x_star\", \"du_t\": \"t_star\"},\n        },\n    }\n\n    sup_validator_pde = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg_pde,\n        ppsci.loss.FunctionalLoss(pde_loss_func),\n        {\n            \"du_t\": lambda out: jacobian(out[\"u_idn\"], out[\"t\"]),\n            \"f_pde\": lambda out: out[\"f_pde\"],\n        },\n        {\"l2\": ppsci.metric.FunctionalMetric(pde_l2_rel_func)},\n        name=\"f_L2_sup\",\n    )\n    validator_pde = {sup_validator_pde.name: sup_validator_pde}\n\n    # update solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        constraint_pde,\n        cfg.output_dir,\n        optimizer_pde,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        validator=validator_pde,\n    )\n\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n    # stage 3: training solution net\n    # re-register transform for model 2, fit for loss of stage 3\n    model_pde.register_input_transform(transform_f_sol)\n\n    # manually build constraint(s)\n    train_dataloader_cfg_sol_f = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH_SOL,\n            \"input_keys\": (\"t\", \"x\"),\n            \"label_keys\": (\"du_t\",),\n            \"alias_dict\": {\"t\": \"t_f_train\", \"x\": \"x_f_train\", \"du_t\": \"t_f_train\"},\n        },\n    }\n    train_dataloader_cfg_sol_init = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH_SOL,\n            \"input_keys\": (\"t\", \"x\"),\n            \"label_keys\": (\"u_sol\",),\n            \"alias_dict\": {\"t\": \"t0\", \"x\": \"x0\", \"u_sol\": \"u0\"},\n        },\n    }\n    train_dataloader_cfg_sol_bc = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH_SOL,\n            \"input_keys\": (\"t\", \"x\"),\n            \"label_keys\": (\"x\",),\n            \"alias_dict\": {\"t\": \"tb\", \"x\": \"xb\"},\n        },\n    }\n\n    sup_constraint_sol_f = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_sol_f,\n        ppsci.loss.FunctionalLoss(pde_loss_func),\n        {\n            \"f_pde\": lambda out: out[\"f_pde\"],\n            \"du_t\": lambda out: jacobian(out[\"u_sol\"], out[\"t\"]),\n        },\n        name=\"f_mse_sup\",\n    )\n    sup_constraint_sol_init = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_sol_init,\n        ppsci.loss.MSELoss(\"sum\"),\n        {\"u_sol\": lambda out: out[\"u_sol\"]},\n        name=\"u0_mse_sup\",\n    )\n    sup_constraint_sol_bc = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_sol_bc,\n        ppsci.loss.FunctionalLoss(boundary_loss_func),\n        {\n            \"x\": lambda out: out[\"x\"],\n            \"u_sol\": lambda out: out[\"u_sol\"],\n        },\n        name=\"ub_mse_sup\",\n    )\n    constraint_sol = {\n        sup_constraint_sol_f.name: sup_constraint_sol_f,\n        sup_constraint_sol_init.name: sup_constraint_sol_init,\n        sup_constraint_sol_bc.name: sup_constraint_sol_bc,\n    }\n\n    # manually build validator\n    eval_dataloader_cfg_sol = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH_SOL,\n            \"input_keys\": (\"t\", \"x\"),\n            \"label_keys\": (\"u_sol\",),\n            \"alias_dict\": {\"t\": \"t_star\", \"x\": \"x_star\", \"u_sol\": \"u_star\"},\n        },\n    }\n\n    sup_validator_sol = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg_sol,\n        ppsci.loss.MSELoss(\"sum\"),\n        {\"u_sol\": lambda out: out[\"u_sol\"]},\n        {\"l2\": ppsci.metric.L2Rel()},\n        name=\"u_L2_sup\",\n    )\n    validator_sol = {sup_validator_sol.name: sup_validator_sol}\n\n    # update solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        constraint_sol,\n        cfg.output_dir,\n        optimizer_sol,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        validator=validator_sol,\n    )\n\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # initialize burgers boundaries\n    t_lb = paddle.to_tensor(cfg.T_LB)\n    t_ub = paddle.to_tensor(cfg.T_UB)\n    x_lb = paddle.to_tensor(cfg.X_LB)\n    x_ub = paddle.to_tensor(cfg.T_UB)\n\n    # initialize models\n    model_idn = ppsci.arch.MLP(**cfg.MODEL.idn_net)\n    model_pde = ppsci.arch.MLP(**cfg.MODEL.pde_net)\n    model_sol = ppsci.arch.MLP(**cfg.MODEL.sol_net)\n\n    # initialize transform\n    def transform_u(_in):\n        t, x = _in[\"t\"], _in[\"x\"]\n        t = 2.0 * (t - t_lb) * paddle.pow((t_ub - t_lb), -1) - 1.0\n        x = 2.0 * (x - x_lb) * paddle.pow((x_ub - x_lb), -1) - 1.0\n        input_trans = {\"t\": t, \"x\": x}\n        return input_trans\n\n    def transform_f(input, model, out_key):\n        in_idn = {\"t\": input[\"t\"], \"x\": input[\"x\"]}\n        x = input[\"x\"]\n        u = model(in_idn)[out_key]\n        du_x = jacobian(u, x)\n        du_xx = hessian(u, x)\n        input_trans = {\"u_x\": u, \"du_x\": du_x, \"du_xx\": du_xx}\n        return input_trans\n\n    def transform_f_sol(_in):\n        return transform_f(_in, model_sol, \"u_sol\")\n\n    # register transform\n    model_idn.register_input_transform(transform_u)\n    model_pde.register_input_transform(transform_f_sol)\n    model_sol.register_input_transform(transform_u)\n\n    # initialize model list\n    model_list = ppsci.arch.ModelList((model_idn, model_pde, model_sol))\n\n    # stage 3: solution net\n    # load pretrained model\n    save_load.load_pretrain(model_list, cfg.EVAL.pretrained_model_path)\n\n    # load dataset\n    dataset_val = reader.load_mat_file(\n        cfg.DATASET_PATH_SOL,\n        keys=(\"t\", \"x\", \"u_sol\"),\n        alias_dict={\n            \"t\": \"t_ori\",\n            \"x\": \"x_ori\",\n            \"u_sol\": \"Exact_ori\",\n        },\n    )\n\n    t_sol, x_sol = np.meshgrid(\n        np.squeeze(dataset_val[\"t\"]), np.squeeze(dataset_val[\"x\"])\n    )\n    t_sol_flatten = paddle.to_tensor(\n        t_sol.flatten()[:, None], dtype=paddle.get_default_dtype(), stop_gradient=False\n    )\n    x_sol_flatten = paddle.to_tensor(\n        x_sol.flatten()[:, None], dtype=paddle.get_default_dtype(), stop_gradient=False\n    )\n    u_sol_pred = model_list({\"t\": t_sol_flatten, \"x\": x_sol_flatten})\n\n    # eval\n    l2_error = np.linalg.norm(\n        dataset_val[\"u_sol\"] - u_sol_pred[\"u_sol\"], 2\n    ) / np.linalg.norm(dataset_val[\"u_sol\"], 2)\n    logger.info(f\"l2_error: {l2_error}\")\n\n    # plotting\n    plot_points = paddle.concat([t_sol_flatten, x_sol_flatten], axis=-1).numpy()\n    plot_func.draw_and_save(\n        figname=\"burgers_sol\",\n        data_exact=dataset_val[\"u_sol\"],\n        data_learned=u_sol_pred[\"u_sol\"].numpy(),\n        boundary=[cfg.T_LB, cfg.T_UB, cfg.X_LB, cfg.X_UB],\n        griddata_points=plot_points,\n        griddata_xi=(t_sol, x_sol),\n        save_path=cfg.output_dir,\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"burgers.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> plotting.py<pre><code>from os import path as osp\n\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom scipy.interpolate import griddata\n\n\ndef _draw_subplot(subfigname, figdata, fig, gs, cmap, boundary, loc):\n    ax = plt.subplot(gs[:, loc])\n    h = ax.imshow(\n        figdata,\n        interpolation=\"nearest\",\n        cmap=cmap,\n        extent=boundary,  # [cfg.T_LB, cfg.T_UB, cfg.X_LB, cfg.X_UB]\n        origin=\"lower\",\n        aspect=\"auto\",\n    )\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n\n    fig.colorbar(h, cax=cax)\n    ax.set_xlabel(\"$t$\")\n    ax.set_ylabel(\"$x$\")\n    ax.set_aspect(\"auto\", \"box\")\n    ax.set_title(subfigname, fontsize=10)\n\n\ndef draw_and_save(\n    figname, data_exact, data_learned, boundary, griddata_points, griddata_xi, save_path\n):\n    fig = plt.figure(figname, figsize=(10, 6))\n    gs = gridspec.GridSpec(1, 2)\n    gs.update(top=0.8, bottom=0.2, left=0.1, right=0.9, wspace=0.5)\n\n    # Exact p(t,x,y)\n    plot_data_label = griddata(\n        griddata_points, data_exact.flatten(), griddata_xi, method=\"cubic\"\n    )\n    _draw_subplot(\"Exact Dynamics\", plot_data_label, fig, gs, \"jet\", boundary, loc=0)\n    # Predicted p(t,x,y)\n    plot_data_pred = griddata(\n        griddata_points, data_learned.flatten(), griddata_xi, method=\"cubic\"\n    )\n    _draw_subplot(\"Learned Dynamics\", plot_data_pred, fig, gs, \"jet\", boundary, loc=1)\n\n    plt.savefig(osp.join(save_path, figname))\n    plt.close()\n\n\ndef draw_and_save_ns(figname, data_exact, data_learned, grid_data, save_path):\n    snap = 120\n    nn = 200\n    lb_x, lb_y = grid_data[:, 0].min(), grid_data[:, 1].min()\n    ub_x, ub_y = grid_data[:, 0].max(), grid_data[:, 1].max()\n    x_plot = np.linspace(lb_x, ub_x, nn)\n    y_plot = np.linspace(lb_y, ub_y, nn)\n    X_plot, Y_plot = np.meshgrid(x_plot, y_plot)\n\n    fig = plt.figure(figname, figsize=(10, 6))\n    gs = gridspec.GridSpec(1, 2)\n    gs.update(top=0.8, bottom=0.2, left=0.1, right=0.9, wspace=0.5)\n    # Exact p(t,x,y)\n    plot_data_label = griddata(\n        grid_data,\n        data_exact[:, snap].flatten(),\n        (X_plot, Y_plot),\n        method=\"cubic\",\n    )\n    _draw_subplot(\n        \"Exact Dynamics\",\n        plot_data_label,\n        fig,\n        gs,\n        \"seismic\",\n        [lb_x, lb_y, ub_x, ub_y],\n        loc=0,\n    )\n    # Predicted p(t,x,y)\n    plot_data_pred = griddata(\n        grid_data,\n        data_learned[:, snap].flatten(),\n        (X_plot, Y_plot),\n        method=\"cubic\",\n    )\n    _draw_subplot(\n        \"Learned Dynamics\",\n        plot_data_pred,\n        fig,\n        gs,\n        \"seismic\",\n        [lb_x, lb_y, ub_x, ub_y],\n        loc=1,\n    )\n    plt.savefig(osp.join(save_path, figname))\n    plt.close()\n</code></pre>"},{"location":"zh/examples/deephpms/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u53c2\u8003 \u95ee\u9898\u5b9a\u4e49\uff0c\u4e0b\u56fe\u6a2a\u3001\u7eb5\u5750\u6807\u5206\u522b\u4e3a\u65f6\u95f4\u3001\u4f4d\u7f6e\u53c2\u6570\uff0c\u989c\u8272\u8868\u793a burgers \u7684\u89e3 u\uff0c\u5927\u5c0f\u53c2\u7167\u56fe\u7247\u53f3\u4fa7\u7684\u989c\u8272\u5361\u3002\u5c06 burgers \u65b9\u7a0b\u5e94\u7528\u5728\u4e0d\u540c\u7684\u95ee\u9898\u4e0a\uff0cu \u5b58\u5728\u4e0d\u540c\u7684\u542b\u4e49\uff0c\u8fd9\u91cc\u53ef\u4ee5\u7b80\u5355\u7684\u8ba4\u4e3a u \u503c\u4ee3\u8868\u901f\u5ea6\u3002</p> <p>\u4e0b\u56fe\u5c55\u793a\u4e86\u5728\u4e00\u5b9a\u521d\u59cb\u6761\u4ef6(t=0 \u65f6\u523b x \u5bf9\u5e94\u7684 u \u503c)\u4e0b\uff0c\u968f t \u589e\u957f\uff0cu \u968f x \u53d8\u5316 \u7684\u60c5\u51b5\u3002u \u7684\u771f\u5b9e\u503c\u548c\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u5982\u4e0b\uff0c\u4e0e\u4f20\u7edf\u7684\u5149\u8c31\u65b9\u6cd5\u76f8\u6bd4\u57fa\u672c\u4e00\u81f4\u3002</p> <p>\u6a21\u62df\u6570\u636e\u96c6 1 \u662f\u521d\u59cb\u6761\u4ef6\u4e3a sin \u65b9\u7a0b\u65f6\u7684\u6570\u636e\u96c6 burgers_sine.mat\uff0c\u6a21\u62df\u6570\u636e\u96c6 2 \u662f\u521d\u59cb\u6761\u4ef6\u4e3a exp \u65b9\u7a0b\u65f6\u7684\u6570\u636e\u96c6 burgers.mat\uff1a</p> <p> </p>  \u771f\u5b9e u \u503c\u548c\u9884\u6d4b u \u503c\u5bf9\u6bd4 <p>\u6a21\u62df\u6570\u636e\u96c6 1\u30012 \u90fd\u662f\u521d\u59cb\u6761\u4ef6\u4e3a sin \u65b9\u7a0b\u65f6\u7684\u6570\u636e\u96c6 burgers_sine.mat \u65f6\uff1a</p> <p> </p>  \u771f\u5b9e u \u503c\u548c\u9884\u6d4b u \u503c\u5bf9\u6bd4"},{"location":"zh/examples/deephpms/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li> <p>Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations</p> </li> <li> <p>\u53c2\u8003\u4ee3\u7801</p> </li> </ul>"},{"location":"zh/examples/deeponet/","title":"DeepONet","text":"","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#deeponet","title":"DeepONet","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepONet/antiderivative_unaligned_train.npz\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepONet/antiderivative_unaligned_test.npz\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/deeponet/antiderivative_unaligned_train.npz -o antiderivative_unaligned_train.npz\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/deeponet/antiderivative_unaligned_test.npz -o antiderivative_unaligned_test.npz\npython deeponet.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepONet/antiderivative_unaligned_train.npz\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/DeepONet/antiderivative_unaligned_test.npz\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/deeponet/antiderivative_unaligned_train.npz -o antiderivative_unaligned_train.npz\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/deeponet/antiderivative_unaligned_test.npz -o antiderivative_unaligned_test.npz\npython deeponet.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/deeponet/deeponet_pretrained.pdparams\n</code></pre> <pre><code>python deeponet.py mode=export\n</code></pre> <pre><code>python deeponet.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 deeponet_pretrained.pdparams loss(G_eval): 0.00003L2Rel.G(G_eval): 0.01799","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6839\u636e\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e07\u80fd\u8fd1\u4f3c\u5b9a\u7406\uff0c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e0d\u4ec5\u53ef\u4ee5\u62df\u5408\u8f93\u5165\u6570\u636e\u5230\u8f93\u51fa\u6570\u636e\u7684\u51fd\u6570\u6620\u5c04\u5173\u7cfb\uff0c\u4e5f\u53ef\u4ee5\u6269\u5c55\u5230\u5bf9\u51fd\u6570\u4e0e\u51fd\u6570\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u8fdb\u884c\u62df\u5408\uff0c\u79f0\u4e4b\u4e3a\u201c\u7b97\u5b50\u201d\u5b66\u4e60\u3002</p> <p>\u56e0\u6b64 DeepONet \u5728\u5404\u4e2a\u9886\u57df\u7684\u5e94\u7528\u90fd\u6709\u76f8\u5f53\u7684\u6f5c\u529b\u3002\u4ee5\u4e0b\u662f\u4e00\u4e9b\u53ef\u80fd\u7684\u5e94\u7528\u9886\u57df\uff1a</p> <ol> <li>\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u62df\uff1aDeepONet\u53ef\u4ee5\u7528\u4e8e\u5bf9\u6d41\u4f53\u52a8\u529b\u5b66\u65b9\u7a0b\u8fdb\u884c\u6570\u503c\u6c42\u89e3\uff0c\u4f8b\u5982Navier-Stokes\u65b9\u7a0b\u3002\u8fd9\u4f7f\u5f97DeepONet\u5728\u8bf8\u5982\u7a7a\u6c14\u52a8\u529b\u5b66\u3001\u6d41\u4f53\u673a\u68b0\u3001\u6c14\u5019\u6a21\u62df\u7b49\u9886\u57df\u5177\u6709\u76f4\u63a5\u5e94\u7528\u3002</li> <li>\u56fe\u50cf\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff1aDeepONet\u53ef\u4ee5\u5b66\u4e60\u56fe\u50cf\u4e2d\u7684\u7279\u5f81\uff0c\u5e76\u7528\u4e8e\u5206\u7c7b\u3001\u5206\u5272\u3001\u68c0\u6d4b\u7b49\u4efb\u52a1\u3002\u4f8b\u5982\uff0c\u5b83\u53ef\u4ee5\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff0c\u5305\u62ec\u75be\u75c5\u68c0\u6d4b\u548c\u9884\u540e\u9884\u6d4b\u3002</li> <li>\u4fe1\u53f7\u5904\u7406\uff1aDeepONet\u53ef\u4ee5\u7528\u4e8e\u5404\u79cd\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\uff0c\u5982\u964d\u566a\u3001\u538b\u7f29\u3001\u6062\u590d\u7b49\u3002\u5728\u901a\u4fe1\u3001\u96f7\u8fbe\u3001\u58f0\u7eb3\u7b49\u9886\u57df\uff0cDeepONet\u6709\u6f5c\u5728\u7684\u5e94\u7528\u3002</li> <li>\u63a7\u5236\u7cfb\u7edf\uff1aDeepONet\u53ef\u4ee5\u7528\u4e8e\u63a7\u5236\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\u3002\u4f8b\u5982\uff0c\u5b83\u53ef\u4ee5\u5b66\u4e60\u7cfb\u7edf\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u7528\u4e8e\u9884\u6d4b\u548c\u63a7\u5236\u7cfb\u7edf\u7684\u672a\u6765\u884c\u4e3a\u3002</li> <li>\u91d1\u878d\uff1aDeepONet\u53ef\u4ee5\u7528\u4e8e\u91d1\u878d\u9884\u6d4b\u548c\u5206\u6790\uff0c\u5982\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u3001\u98ce\u9669\u8bc4\u4f30\u3001\u4fe1\u8d37\u98ce\u9669\u5206\u6790\u7b49\u3002</li> <li>\u4eba\u673a\u4ea4\u4e92\uff1aDeepONet\u53ef\u4ee5\u7528\u4e8e\u8bed\u97f3\u8bc6\u522b\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u624b\u52bf\u8bc6\u522b\u7b49\u4efb\u52a1\uff0c\u4f7f\u5f97\u4eba\u673a\u4ea4\u4e92\u66f4\u52a0\u667a\u80fd\u5316\u548c\u81ea\u7136\u3002</li> <li>\u73af\u5883\u79d1\u5b66\uff1aDeepONet\u53ef\u4ee5\u7528\u4e8e\u6c14\u5019\u6a21\u578b\u9884\u6d4b\u3001\u751f\u6001\u7cfb\u7edf\u7684\u6a21\u62df\u3001\u73af\u5883\u6c61\u67d3\u68c0\u6d4b\u7b49\u4efb\u52a1\u3002</li> </ol> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u867d\u7136 DeepONet \u5728\u8bb8\u591a\u9886\u57df\u90fd\u6709\u6f5c\u5728\u7684\u5e94\u7528\uff0c\u4f46\u6bcf\u4e2a\u9886\u57df\u90fd\u6709\u5176\u72ec\u7279\u7684\u95ee\u9898\u548c\u6311\u6218\u3002\u5728\u5c06 DeepONet \u5e94\u7528\u5230\u7279\u5b9a\u9886\u57df\u65f6\uff0c\u9700\u8981\u5bf9\u8be5\u9886\u57df\u7684\u95ee\u9898\u6709\u6df1\u5165\u7684\u7406\u89e3\uff0c\u5e76\u53ef\u80fd\u9700\u8981\u9488\u5bf9\u8be5\u9886\u57df\u8fdb\u884c\u6a21\u578b\u7684\u8c03\u6574\u548c\u4f18\u5316\u3002</p>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u5047\u8bbe\u5b58\u5728\u5982\u4e0b ODE \u7cfb\u7edf\uff1a</p> \\[ \\begin{equation} \\left\\{\\begin{array}{l} \\frac{d}{d x} \\mathbf{s}(x)=\\mathbf{g}(\\mathbf{s}(x), u(x), x) \\\\ \\mathbf{s}(a)=s_0 \\end{array}\\right. \\end{equation} \\] <p>\u5176\u4e2d \\(u \\in V\\)\uff08\u4e14 \\(u\\) \u5728 \\([a, b]\\) \u4e0a\u8fde\u7eed\uff09\u4f5c\u4e3a\u8f93\u5165\u4fe1\u53f7\uff0c\\(\\mathbf{s}: [a,b] \\rightarrow \\mathbb{R}^K\\) \u662f\u8be5\u65b9\u7a0b\u7684\u89e3\uff0c\u4f5c\u4e3a\u8f93\u51fa\u4fe1\u53f7\u3002 \u56e0\u6b64\u53ef\u4ee5\u5b9a\u4e49\u4e00\u79cd\u7b97\u5b50 \\(G\\)\uff0c\u5b83\u6ee1\u8db3\uff1a</p> \\[ \\begin{equation} (G u)(x)=s_0+\\int_a^x \\mathbf{g}((G u)(t), u(t), t) d t \\end{equation} \\] <p>\u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4ee5 \\(u\\)\u3001\\(x\\) \u4e3a\u8f93\u5165\uff0c\\(G(u)(x)\\) \u4e3a\u8f93\u51fa\uff0c\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u6765\u62df\u5408 \\(G\\) \u7b97\u5b50\u672c\u8eab\u3002</p> <p>\u6ce8\uff1a\u6839\u636e\u4e0a\u8ff0\u516c\u5f0f\uff0c\u53ef\u4ee5\u53d1\u73b0\u7b97\u5b50 \\(G\\) \u662f\u4e00\u79cd\u79ef\u5206\u7b97\u5b50 \"\\(\\int\\)\"\uff0c\u5176\u4f5c\u7528\u5728\u7ed9\u5b9a\u51fd\u6570 \\(u\\) \u4e0a\u80fd\u6c42\u5f97\u5176\u7b26\u5408\u67d0\u79cd\u521d\u503c\u6761\u4ef6\uff08\u672c\u95ee\u9898\u4e2d\u521d\u503c\u6761\u4ef6\u4e3a \\(G(u)(0)=0\\)\uff09\u4e0b\u7684\u539f\u51fd\u6570 \\(G(u)\\)\u3002</p>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u672c\u6848\u4f8b\u6570\u636e\u96c6\u4f7f\u7528 DeepXDE \u5b98\u65b9\u6587\u6863\u63d0\u4f9b\u7684\u6570\u636e\u96c6\uff0c\u4e00\u4e2a npz \u6587\u4ef6\u5185\u5df2\u5305\u542b\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\uff0c\u4e0b\u8f7d\u5730\u5740</p> <p>\u6570\u636e\u6587\u4ef6\u8bf4\u660e\u5982\u4e0b\uff1a</p> <p><code>antiderivative_unaligned_train.npz</code></p> \u5b57\u6bb5\u540d \u8bf4\u660e X_train0 \\(u\\) \u5bf9\u5e94\u7684\u8bad\u7ec3\u8f93\u5165\u6570\u636e\uff0c\u5f62\u72b6\u4e3a(10000, 100) X_train1 \\(y\\) \u5bf9\u5e94\u7684\u8bad\u7ec3\u8f93\u5165\u6570\u636e\u6570\u636e\uff0c\u5f62\u72b6\u4e3a(10000, 1) y_train \\(G(u)\\) \u5bf9\u5e94\u7684\u8bad\u7ec3\u6807\u7b7e\u6570\u636e\uff0c\u5f62\u72b6\u4e3a(10000,1) <p><code>antiderivative_unaligned_test.npz</code></p> \u5b57\u6bb5\u540d \u8bf4\u660e X_test0 \\(u\\) \u5bf9\u5e94\u7684\u6d4b\u8bd5\u8f93\u5165\u6570\u636e\uff0c\u5f62\u72b6\u4e3a(100000, 100) X_test1 \\(y\\) \u5bf9\u5e94\u7684\u6d4b\u8bd5\u8f93\u5165\u6570\u636e\u6570\u636e\uff0c\u5f62\u72b6\u4e3a(100000, 1) y_test \\(G(u)\\) \u5bf9\u5e94\u7684\u6d4b\u8bd5\u6807\u7b7e\u6570\u636e\uff0c\u5f62\u72b6\u4e3a(100000,1)","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u4e0a\u8ff0\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u8f93\u5165\u4e3a \\(u\\) \u548c \\(y\\)\uff0c\u8f93\u51fa\u4e3a \\(G(u)\\)\uff0c\u6309\u7167 DeepONet \u8bba\u6587\u6240\u8ff0\uff0c\u6211\u4eec\u4f7f\u7528\u542b\u6709 branch \u548c trunk \u4e24\u4e2a\u5b50\u5206\u652f\u7f51\u7edc\u7684 <code>DeepONet</code> \u6765\u521b\u5efa\u7f51\u7edc\u6a21\u578b\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> <pre><code>model = ppsci.arch.DeepONet(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>u</code> \u548c <code>y</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>G</code>\uff0c\u63a5\u7740\u901a\u8fc7\u6307\u5b9a <code>DeepONet</code> \u7684 SENSORS \u4e2a\u6570\uff0c\u7279\u5f81\u901a\u9053\u6570\u3001\u9690\u85cf\u5c42\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\u4ee5\u53ca\u5b50\u7f51\u7edc\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86 <code>DeepONet</code> \u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#33","title":"3.3 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6587\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u5bf9\u6a21\u578b\u8f93\u51fa \\(G(u)\\) \u8fdb\u884c\u7ea6\u675f\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u76d1\u7763\u7ea6\u675f\u6307\u5b9a\u6587\u4ef6\u8def\u5f84\u7b49\u6570\u636e\u8bfb\u53d6\u914d\u7f6e\uff0c\u5305\u62ec\u6587\u4ef6\u8def\u5f84\u3001\u8f93\u5165\u6570\u636e\u5b57\u6bb5\u540d\u3001\u6807\u7b7e\u6570\u636e\u5b57\u6bb5\u540d\u3001\u6570\u636e\u8f6c\u6362\u524d\u540e\u7684\u522b\u540d\u5b57\u5178\u3002</p> <pre><code>train_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"IterableNPZDataset\",\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"input_keys\": (\"u\", \"y\"),\n        \"label_keys\": (\"G\",),\n        \"alias_dict\": {\"u\": \"X_train0\", \"y\": \"X_train1\", \"G\": \"y_train\"},\n    },\n}\n</code></pre>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#331","title":"3.3.1 \u76d1\u7763\u7ea6\u675f","text":"<p>\u7531\u4e8e\u6211\u4eec\u4ee5\u76d1\u7763\u5b66\u4e60\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u6b64\u5904\u91c7\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code>\uff1a</p> <pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    {\"G\": lambda out: out[\"G\"]},\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u6b64\u5904\u586b\u5165\u5728 3.4 \u7ea6\u675f\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>train_dataloader_cfg</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u4e3a\u9ed8\u8ba4\u503c <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\u53d6\u5e73\u5747\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u6211\u4eec\u53ea\u9700\u8981\u4ece\u8f93\u51fa\u5b57\u5178\u4e2d\uff0c\u83b7\u53d6\u8f93\u51fa <code>G</code> \u8fd9\u4e2a\u5b57\u6bb5\u5bf9\u5e94\u7684\u8f93\u51fa\u5373\u53ef\uff1b</p> <p>\u5728\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#34","title":"3.4 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528\u4e00\u4e07\u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u5e76\u6bcf\u9694 500 \u4e2a epochs \u8bc4\u4f30\u4e00\u6b21\u6a21\u578b\u7cbe\u5ea6\u3002</p> <pre><code>TRAIN:\n  epochs: 10000\n  iters_per_epoch: 1\n  learning_rate: 1.0e-3\n  save_freq: 500\n  eval_freq: 500\n  eval_during_train: true\n</code></pre>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a <code>0.001</code>\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#36","title":"3.6 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"IterableNPZDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": (\"u\", \"y\"),\n        \"label_keys\": (\"G\",),\n        \"alias_dict\": {\"u\": \"X_test0\", \"y\": \"X_test1\", \"G\": \"y_test\"},\n    },\n}\n</code></pre> <p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u9009\u62e9 <code>ppsci.metric.L2Rel</code> \u5373\u53ef\u3002</p> <p>\u5176\u4f59\u914d\u7f6e\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> <pre><code>solver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    eval_freq=cfg.TRAIN.eval_freq,\n    log_freq=cfg.log_freq,\n    seed=cfg.seed,\n    validator=validator,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#38","title":"3.8 \u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u5728\u6a21\u578b\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u6784\u9020 \\(u\\)\u3001\\(y\\) \u5e76\u5728\u9002\u5f53\u8303\u56f4\u5185\u8fdb\u884c\u79bb\u6563\u5316\uff0c\u5f97\u5230\u5bf9\u5e94\u8f93\u5165\u6570\u636e\uff0c\u7ee7\u800c\u9884\u6d4b\u51fa \\(G(u)(y)\\)\uff0c\u5e76\u548c \\(G(u)\\) \u7684\u6807\u51c6\u89e3\u5171\u540c\u7ed8\u5236\u56fe\u50cf\uff0c\u8fdb\u884c\u5bf9\u6bd4\u3002\uff08\u6b64\u5904\u6211\u4eec\u6784\u9020\u4e86 9 \u7ec4 \\(u-G(u)\\) \u51fd\u6570\u5bf9\uff09\u8fdb\u884c\u6d4b\u8bd5</p> <pre><code>    def predict_func(input_dict):\n        return solver.predict(input_dict, return_numpy=True)[cfg.MODEL.G_key]\n\n    plot(cfg, predict_func)\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.DeepONet(**cfg.MODEL)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"IterableNPZDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": (\"u\", \"y\"),\n            \"label_keys\": (\"G\",),\n            \"alias_dict\": {\"u\": \"X_test0\", \"y\": \"X_test1\", \"G\": \"y_test\"},\n        },\n    }\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        {\"G\": lambda out: out[\"G\"]},\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"G_eval\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        None,\n        cfg.output_dir,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    solver.eval()\n\n    def predict_func(input_dict):\n        return solver.predict(input_dict, return_numpy=True)[cfg.MODEL.G_key]\n\n    plot(cfg, predict_func)\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.DeepONet(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n</code></pre>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"deeponet.py<pre><code>\"\"\"\nReference: https://deepxde.readthedocs.io/en/latest/demos/operator/antiderivative_unaligned.html\n\"\"\"\n\nimport os\nfrom os import path as osp\nfrom typing import Callable\nfrom typing import Tuple\n\nimport hydra\nimport numpy as np\nimport paddle\nfrom matplotlib import pyplot as plt\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.DeepONet(**cfg.MODEL)\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"IterableNPZDataset\",\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"input_keys\": (\"u\", \"y\"),\n            \"label_keys\": (\"G\",),\n            \"alias_dict\": {\"u\": \"X_train0\", \"y\": \"X_train1\", \"G\": \"y_train\"},\n        },\n    }\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        {\"G\": lambda out: out[\"G\"]},\n    )\n    # wrap constraints together\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"IterableNPZDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": (\"u\", \"y\"),\n            \"label_keys\": (\"G\",),\n            \"alias_dict\": {\"u\": \"X_test0\", \"y\": \"X_test1\", \"G\": \"y_test\"},\n        },\n    }\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        {\"G\": lambda out: out[\"G\"]},\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"G_eval\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        eval_freq=cfg.TRAIN.eval_freq,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n    def predict_func(input_dict):\n        return solver.predict(input_dict, return_numpy=True)[cfg.MODEL.G_key]\n\n    plot(cfg, predict_func)\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.DeepONet(**cfg.MODEL)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"IterableNPZDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": (\"u\", \"y\"),\n            \"label_keys\": (\"G\",),\n            \"alias_dict\": {\"u\": \"X_test0\", \"y\": \"X_test1\", \"G\": \"y_test\"},\n        },\n    }\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        {\"G\": lambda out: out[\"G\"]},\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"G_eval\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        None,\n        cfg.output_dir,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    solver.eval()\n\n    def predict_func(input_dict):\n        return solver.predict(input_dict, return_numpy=True)[cfg.MODEL.G_key]\n\n    plot(cfg, predict_func)\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.DeepONet(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {\n            model.input_keys[0]: InputSpec(\n                [None, 1000], \"float32\", name=model.input_keys[0]\n            ),\n            model.input_keys[1]: InputSpec(\n                [None, 1], \"float32\", name=model.input_keys[1]\n            ),\n        }\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy import python_infer\n\n    predictor = python_infer.GeneralPredictor(cfg)\n\n    def predict_func(input_dict):\n        return next(iter(predictor.predict(input_dict).values()))\n\n    plot(cfg, predict_func)\n\n\ndef plot(cfg: DictConfig, predict_func: Callable):\n    # visualize prediction for different functions u and corresponding G(u)\n    dtype = paddle.get_default_dtype()\n\n    def generate_y_u_G_ref(\n        u_func: Callable, G_u_func: Callable\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Generate discretized data of given function u and corresponding G(u).\n\n        Args:\n            u_func (Callable): Function u.\n            G_u_func (Callable): Function G(u).\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]: Discretized data of u, y and G(u).\n        \"\"\"\n        x = np.linspace(0, 1, cfg.MODEL.num_loc, dtype=dtype).reshape(\n            [1, cfg.MODEL.num_loc]\n        )\n        u = u_func(x)\n        u = np.tile(u, [cfg.NUM_Y, 1])\n\n        y = np.linspace(0, 1, cfg.NUM_Y, dtype=dtype).reshape([cfg.NUM_Y, 1])\n        G_ref = G_u_func(y)\n        return u, y, G_ref\n\n    func_u_G_pair = [\n        # (title_string, func_u, func_G(u)), s.t. dG/dx == u and G(u)(0) = 0\n        (r\"$u=\\cos(x), G(u)=sin(x$)\", lambda x: np.cos(x), lambda y: np.sin(y)),  # 1\n        (\n            r\"$u=sec^2(x), G(u)=tan(x$)\",\n            lambda x: (1 / np.cos(x)) ** 2,\n            lambda y: np.tan(y),\n        ),  # 2\n        (\n            r\"$u=sec(x)tan(x), G(u)=sec(x) - 1$\",\n            lambda x: (1 / np.cos(x) * np.tan(x)),\n            lambda y: 1 / np.cos(y) - 1,\n        ),  # 3\n        (\n            r\"$u=1.5^x\\ln{1.5}, G(u)=1.5^x-1$\",\n            lambda x: 1.5**x * np.log(1.5),\n            lambda y: 1.5**y - 1,\n        ),  # 4\n        (r\"$u=3x^2, G(u)=x^3$\", lambda x: 3 * x**2, lambda y: y**3),  # 5\n        (r\"$u=4x^3, G(u)=x^4$\", lambda x: 4 * x**3, lambda y: y**4),  # 6\n        (r\"$u=5x^4, G(u)=x^5$\", lambda x: 5 * x**4, lambda y: y**5),  # 7\n        (r\"$u=6x^5, G(u)=x^6$\", lambda x: 5 * x**4, lambda y: y**5),  # 8\n        (r\"$u=e^x, G(u)=e^x-1$\", lambda x: np.exp(x), lambda y: np.exp(y) - 1),  # 9\n    ]\n\n    os.makedirs(os.path.join(cfg.output_dir, \"visual\"), exist_ok=True)\n    for i, (title, u_func, G_func) in enumerate(func_u_G_pair):\n        u, y, G_ref = generate_y_u_G_ref(u_func, G_func)\n        G_pred = predict_func({\"u\": u, \"y\": y})\n        plt.plot(y, G_pred, label=r\"$G(u)(y)_{ref}$\")\n        plt.plot(y, G_ref, label=r\"$G(u)(y)_{pred}$\")\n        plt.legend()\n        plt.title(title)\n        plt.savefig(os.path.join(cfg.output_dir, \"visual\", f\"func_{i}_result.png\"))\n        logger.message(\n            f\"Saved result of function {i} to {cfg.output_dir}/visual/func_{i}_result.png\"\n        )\n        plt.clf()\n    plt.close()\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"deeponet.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/deeponet/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li>DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators</li> <li>DeepXDE - Antiderivative operator from an unaligned dataset</li> </ul>","tags":["DeepONet\u6c42\u89e3\u65b9\u6cd5"]},{"location":"zh/examples/dgmr/","title":"DGMR","text":""},{"location":"zh/examples/dgmr/#dgmrdeep-generative-models-of-radar","title":"DGMR(deep generative models of radar)","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <p>\u6682\u65e0</p> <pre><code># Download data from Huggingface\nmkdir openclimatefix/nimrod-uk-1km/20200718/valid/subsampled_tiles_256_20min_stride\ncd openclimatefix/nimrod-uk-1km/20200718/valid/subsampled_tiles_256_20min_stride\ngit lfs install\ngit lfs pull --include=\"seq-24-*-of-00033.tfrecord.gz\"\n\npython dgmr.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/dgmr/dgmr_pretrained.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 dgmr_pretrained.pdparams d_loss: 127.041g_loss: 59.2409grid_loss: 1.7699"},{"location":"zh/examples/dgmr/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u77ed\u4e34\u964d\u6c34\u9884\u62a5\u662f\u5bf9\u672a\u6765\u4e24\u5c0f\u65f6\u5185\u7684\u964d\u6c34\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u9884\u6d4b\uff0c\u652f\u6301\u4e86\u8bb8\u591a\u4f9d\u8d56\u4e8e\u5929\u6c14\u51b3\u7b56\u7684\u5b9e\u9645\u793e\u4f1a\u7ecf\u6d4e\u9700\u6c42\u3002\u6700\u5148\u8fdb\u7684\u8fd0\u884c\u5373\u65f6\u9884\u62a5\u65b9\u6cd5\u901a\u5e38\u5229\u7528\u57fa\u4e8e\u96f7\u8fbe\u7684\u98ce\u4f30\u8ba1\u5bf9\u964d\u6c34\u573a\u8fdb\u884c\u5e73\u6d41\uff0c\u4f46\u5f80\u5f80\u96be\u4ee5\u6355\u6349\u91cd\u8981\u7684\u975e\u7ebf\u6027\u4e8b\u4ef6\uff0c\u5982\u5bf9\u6d41\u7684\u53d1\u751f\u3002\u6700\u8fd1\u5f15\u5165\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5229\u7528\u96f7\u8fbe\u76f4\u63a5\u9884\u6d4b\u672a\u6765\u7684\u964d\u96e8\u7387\uff0c\u6446\u8131\u4e86\u7269\u7406\u7ea6\u675f\u3002\u867d\u7136\u5b83\u4eec\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u4f4e\u5f3a\u5ea6\u964d\u96e8\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u7ea6\u675f\uff0c\u5728\u66f4\u957f\u7684\u524d\u77bb\u65f6\u95f4\u5185\u4ea7\u751f\u6a21\u7cca\u7684\u5373\u65f6\u9884\u62a5\uff0c\u5bfc\u81f4\u5bf9\u4e2d\u5230\u5927\u96e8\u4e8b\u4ef6\u7684\u6027\u80fd\u8f83\u5dee\uff0c\u5176\u8fd0\u884c\u5b9e\u7528\u6027\u53d7\u5230\u9650\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u57fa\u4e8e\u96f7\u8fbe\u7684\u964d\u6c34\u6982\u7387\u5373\u65f6\u9884\u62a5\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728\u8303\u56f4\u4e3a1536 km \u00d7 1280 km\u7684\u533a\u57df\u5185\uff0c\u80fd\u591f\u57285-90\u5206\u949f\u7684\u524d\u77bb\u65f6\u95f4\u5185\u751f\u6210\u903c\u771f\u4e14\u65f6\u7a7a\u4e00\u81f4\u7684\u9884\u6d4b\u3002\u901a\u8fc7\u82f1\u56fd\u6c14\u8c61\u5c40\u7684\u4e94\u5341\u591a\u4f4d\u4e13\u5bb6\u9884\u62a5\u5458\u8fdb\u884c\u7684\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\uff0c\u6211\u4eec\u7684\u751f\u6210\u6a21\u578b\u572888%\u7684\u60c5\u51b5\u4e0b\u5728\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u6392\u540d\u7b2c\u4e00\uff0c\u8d85\u8fc7\u4e86\u4e24\u79cd\u7ade\u4e89\u65b9\u6cd5\uff0c\u8868\u660e\u4e86\u5176\u5728\u51b3\u7b56\u4ef7\u503c\u548c\u5411\u73b0\u5b9e\u4e16\u754c\u4e13\u5bb6\u63d0\u4f9b\u7269\u7406\u6d1e\u5bdf\u529b\u65b9\u9762\u7684\u80fd\u529b\u3002\u5728\u5b9a\u91cf\u9a8c\u8bc1\u65b9\u9762\uff0c\u8fd9\u4e9b\u5373\u65f6\u9884\u62a5\u5177\u6709\u826f\u597d\u7684\u6280\u80fd\u800c\u65e0\u9700\u8fdb\u884c\u6a21\u7cca\u5904\u7406\u3002\u6211\u4eec\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u5373\u65f6\u9884\u62a5\u53ef\u4ee5\u63d0\u4f9b\u6539\u8fdb\u9884\u62a5\u4ef7\u503c\u5e76\u652f\u6301\u8fd0\u884c\u5b9e\u7528\u6027\u7684\u6982\u7387\u6027\u9884\u6d4b\uff0c\u5728\u5206\u8fa8\u7387\u548c\u524d\u77bb\u65f6\u95f4\u65b9\u9762\uff0c\u66ff\u4ee3\u65b9\u6cd5\u5b58\u5728\u56f0\u96be\u7684\u60c5\u51b5\u4e0b\u5c24\u5176\u5982\u6b64\u3002</p> <p>\u77ed\u4e34\u964d\u6c34\u9884\u62a5\u5728\u5de5\u7a0b\u548c\u79d1\u5b66\u9886\u57df\u5177\u6709\u591a\u65b9\u9762\u7684\u91cd\u8981\u6027\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a</p> <ul> <li>\u793e\u4f1a\u5f71\u54cd\uff1a \u77ed\u4e34\u964d\u6c34\u9884\u62a5\u5bf9\u5404\u884c\u5404\u4e1a\u7684\u5b9e\u9645\u51b3\u7b56\u90fd\u6709\u7740\u76f4\u63a5\u7684\u793e\u4f1a\u5f71\u54cd\u3002\u4f8b\u5982\uff0c\u519c\u4e1a\u3001\u6c34\u8d44\u6e90\u7ba1\u7406\u3001\u57ce\u5e02\u9632\u6c5b\u548c\u4ea4\u901a\u8fd0\u8f93\u7b49\u9886\u57df\u90fd\u9700\u8981\u51c6\u786e\u7684\u964d\u6c34\u9884\u62a5\u6765\u505a\u51fa\u76f8\u5e94\u7684\u5e94\u5bf9\u63aa\u65bd\uff0c\u4ee5\u51cf\u5c11\u53ef\u80fd\u7684\u635f\u5931\u548c\u98ce\u9669\u3002</li> <li>\u5b89\u5168\u4fdd\u969c\uff1a \u77ed\u4e34\u964d\u6c34\u9884\u62a5\u5bf9\u4e8e\u4fdd\u969c\u516c\u4f17\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u4f8b\u5982\uff0c\u9884\u8b66\u7cfb\u7edf\u53ef\u4ee5\u6839\u636e\u77ed\u4e34\u964d\u6c34\u9884\u62a5\u63d0\u524d\u901a\u77e5\u4eba\u4eec\u53ef\u80fd\u53d1\u751f\u7684\u66b4\u96e8\u3001\u6d2a\u6d9d\u3001\u6ce5\u77f3\u6d41\u7b49\u707e\u5bb3\uff0c\u4ece\u800c\u53ca\u65f6\u91c7\u53d6\u907f\u9669\u63aa\u65bd\uff0c\u51cf\u5c11\u4eba\u5458\u4f24\u4ea1\u548c\u8d22\u4ea7\u635f\u5931\u3002</li> <li>\u751f\u6001\u73af\u5883\u4fdd\u62a4\uff1a \u5bf9\u964d\u6c34\u7684\u51c6\u786e\u9884\u62a5\u6709\u52a9\u4e8e\u751f\u6001\u73af\u5883\u7684\u4fdd\u62a4\u548c\u7ba1\u7406\u3002\u4f8b\u5982\uff0c\u9884\u6d4b\u964d\u6c34\u91cf\u53ef\u4ee5\u5e2e\u52a9\u51b3\u7b56\u8005\u53ca\u65f6\u8c03\u6574\u6c34\u5229\u5de5\u7a0b\u7684\u8fd0\u884c\uff0c\u4fdd\u969c\u751f\u6001\u7cfb\u7edf\u7684\u5065\u5eb7\u8fd0\u884c\uff0c\u5e76\u4e3a\u690d\u88ab\u751f\u957f\u63d0\u4f9b\u5fc5\u8981\u7684\u6c34\u8d44\u6e90\u3002</li> <li>\u79d1\u5b66\u7814\u7a76\uff1a \u77ed\u4e34\u964d\u6c34\u9884\u62a5\u4e5f\u662f\u6c14\u8c61\u79d1\u5b66\u7814\u7a76\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u901a\u8fc7\u5bf9\u964d\u6c34\u8fc7\u7a0b\u7684\u7814\u7a76\u548c\u9884\u6d4b\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5927\u6c14\u73af\u5883\u4e2d\u7684\u6c34\u5faa\u73af\u8fc7\u7a0b\uff0c\u4e3a\u6c14\u8c61\u5b66\u3001\u6c14\u5019\u5b66\u7b49\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u6570\u636e\u548c\u652f\u6491\u3002</li> <li>\u5de5\u7a0b\u89c4\u5212\u548c\u8bbe\u8ba1\uff1a \u5728\u57ce\u5e02\u89c4\u5212\u3001\u571f\u6728\u5de5\u7a0b\u3001\u519c\u4e1a\u704c\u6e89\u7b49\u9886\u57df\uff0c\u51c6\u786e\u7684\u77ed\u4e34\u964d\u6c34\u9884\u62a5\u5bf9\u5de5\u7a0b\u7684\u89c4\u5212\u548c\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u4f8b\u5982\uff0c\u5728\u57ce\u5e02\u6392\u6c34\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\uff0c\u9700\u8981\u8003\u8651\u672a\u6765\u77ed\u65f6\u95f4\u5185\u53ef\u80fd\u53d1\u751f\u7684\u964d\u6c34\u60c5\u51b5\uff0c\u4ee5\u4fdd\u8bc1\u6392\u6c34\u7cfb\u7edf\u7684\u6b63\u5e38\u8fd0\u884c\u548c\u57ce\u5e02\u7684\u9632\u6d2a\u80fd\u529b\u3002</li> </ul> <p>\u603b\u7684\u6765\u8bf4\uff0c\u77ed\u4e34\u964d\u6c34\u9884\u62a5\u5728\u5de5\u7a0b\u548c\u79d1\u5b66\u9886\u57df\u7684\u91cd\u8981\u6027\u4f53\u73b0\u5728\u4fdd\u969c\u793e\u4f1a\u5b89\u5168\u3001\u4fc3\u8fdb\u79d1\u5b66\u7814\u7a76\u3001\u652f\u6301\u751f\u6001\u73af\u5883\u4fdd\u62a4\u548c\u63a8\u52a8\u5de5\u7a0b\u53d1\u5c55\u7b49\u591a\u4e2a\u65b9\u9762\uff0c\u5bf9\u4e8e\u5404\u884c\u4e1a\u7684\u53d1\u5c55\u548c\u793e\u4f1a\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u90fd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002</p>"},{"location":"zh/examples/dgmr/#2","title":"2. \u6a21\u578b\u539f\u7406","text":""},{"location":"zh/examples/dgmr/#21","title":"2.1 \u6a21\u578b\u7ed3\u6784","text":"<p>DGMR\u662f\u5728\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u7b97\u6cd5\u6846\u67b6\u4e2d\u6784\u5efa\u7684\u3002\u4ee5\u8fc7\u53bb\u7684\u96f7\u8fbe\u6570\u636e\u4e3a\u57fa\u7840\uff0c\u5bf9\u672a\u6765\u7684\u96f7\u8fbe\u505a\u51fa\u8be6\u7ec6\u548c\u53ef\u4fe1\u7684\u9884\u6d4b\u3002\u4e5f\u5c31\u662f\u8bf4\u5728\u7ed9\u5b9a\u7684\u65f6\u95f4\u70b9 \\(T\\)\uff0c\u4f7f\u7528\u57fa\u4e8e\u96f7\u8fbe\u7684\u5730\u8868\u964d\u6c34\u4f30\u8ba1\u503c \\(X_T\\)\uff0c\u57fa\u4e8e\u8fc7\u53bb \\(M\\) \u4e2a\u96f7\u8fbe\u573a\u9884\u6d4b\u672a\u6765 \\(N\\) \u4e2a\u96f7\u8fbe\u573a\u3002\u5373\uff1a</p> \\[ P\\left(X_{M+1: M+N} \\mid X_{1: M}\\right)=\\int P\\left(X_{M+1: M+N} \\mid \\mathrm{Z}, X_{1: M}, \\boldsymbol{\\theta}\\right) P\\left(\\mathrm{Z} \\mid X_{1: M}\\right) d \\mathrm{Z}. \\] <p>\u5176\u4e2d \\(Z\\) \u4e3a\u968f\u673a\u5411\u91cf\uff0c\\(\\theta\\) \u4e3a\u751f\u6210\u6a21\u578b\u7684\u53c2\u6570\u3002</p> <ul> <li>\u7b49\u5f0f\u5de6\u8fb9\u662f\u6761\u4ef6\u6982\u7387\uff0c\u7ed9\u5b9a\u8fc7\u53bb\\(M\\)\u4e2a\u65f6\u523b\u7684\u96f7\u8fbe\u964d\u6c34\uff0c\u9884\u62a5\u4e4b\u540e\\(N\\)\u4e2a\u65f6\u523b\u7684\u96f7\u8fbe\u964d\u6c34\u3002</li> <li>\u53f3\u8fb9\u5219\u5c06\u6982\u7387\u5199\u4e3a\u96c6\u5408\u9884\u62a5\u7684\u79ef\u5206\u5f62\u5f0f\uff1a</li> <li>\u7ed9\u5b9a\u968f\u673a\u62bd\u6837 \\(Z\\) \u548c\u751f\u6210\u7f51\u7edc\u53c2\u6570 \\(\\theta\\)\uff0c\u5728\u8fc7\u53bb \\(M\\) \u4e2a\u65f6\u523b\u7684\u96f7\u8fbe\u964d\u6c34\u7ea6\u675f\u4e0b\u9884\u62a5\u4e4b\u540e \\(N\\) \u4e2a\u65f6\u523b\u7684\u96f7\u8fbe\u964d\u6c34\uff1b</li> <li>\u8ba1\u7b97\u968f\u673a\u62bd\u6837 \\(Z\\) \u5728\u8fc7\u53bb \\(M\\) \u4e2a\u65f6\u523b\u7684\u96f7\u8fbe\u964d\u6c34\u7ea6\u675f\u4e0b\u7684\u6761\u4ef6\u6982\u7387\uff1b</li> <li>\u4e24\u8005\u76f8\u4e58\uff0c\u4e3a\u8be5\u7ed3\u679c\u7684\u51fa\u73b0\u6982\u7387\uff0c\u79ef\u5206\u540e\u5f97\u5230\u591a\u6b21\u62bd\u6837\u4e0b\u7684\u96c6\u5408\u9884\u62a5\u3002</li> </ul> <p>\u5bf9\u968f\u673a\u5411\u91cf \\(Z\\) \u7684\u79ef\u5206\u786e\u4fdd\u4e86\u6a21\u578b\u4ea7\u751f\u7684\u9884\u6d4b\u5177\u6709\u7a7a\u95f4\u76f8\u5173\u6027\u3002DGMR \u4e13\u95e8\u7528\u4e8e\u964d\u6c34\u9884\u6d4b\u95ee\u9898\u3002\u56db\u4e2a\u8fde\u7eed\u7684\u96f7\u8fbe\u89c2\u6d4b\u6570\u636e\uff08\u524d20\u5206\u949f\uff09\u88ab\u7528\u4f5c\u751f\u6210\u5668\u7684\u8f93\u5165\uff0c\u8be5\u751f\u6210\u5668\u5141\u8bb8\u5bf9\u672a\u6765\u964d\u6c34\u7684\u591a\u4e2a\u5b9e\u73b0\u8fdb\u884c\u62bd\u6837\uff0c\u6bcf\u4e2a\u5b9e\u73b0\u5305\u542b18\u5e27\uff0890\u5206\u949f\uff09\u3002\u6a21\u578b\u67b6\u6784\u793a\u610f\u56fe\u5982\u56fe\u6240\u793a\u3002</p> <p> </p>  \u6a21\u578b\u67b6\u6784\u793a\u610f\u56fe\u3002 <p>DGMR \u662f\u4e00\u4e2a\u4f7f\u7528\u4e24\u4e2a\u5224\u522b\u5668\u548c\u4e00\u4e2a\u9644\u52a0\u6b63\u5219\u5316\u9879\u8fdb\u884c\u8bad\u7ec3\u7684\u751f\u6210\u5668\u3002\u4e0b\u56fe\u663e\u793a\u4e86\u751f\u6210\u6a21\u578b\u548c\u5224\u522b\u5668\u7684\u8be6\u7ec6\u793a\u610f\u56fe\uff1a</p> <p> </p>  a\u3001\u751f\u6210\u5668\u67b6\u6784\u3002b\uff0c\u751f\u6210\u5668\u7684\u65f6\u95f4\u9274\u522b\u5668\u67b6\u6784\uff08\u5de6\u4e0a\uff09\u3001\u7a7a\u95f4\u9274\u522b\u5668\uff08\u5de6\u4e2d\uff09\u548c\u6f5c\u5728\u6761\u4ef6\u5806\u6808\uff08\u5de6\u4e0b\uff09\u3002\u53f3\u4fa7\u662f G \u5757\uff08\u4e0a\uff09\u3001D \u548c 3D \u5757\uff08\u4e2d\uff09\u4ee5\u53ca L \u5757\uff08\u53f3\uff09\u7684\u67b6\u6784\u3002"},{"location":"zh/examples/dgmr/#22","title":"2.2 \u76ee\u6807\u51fd\u6570","text":"<p>\u751f\u6210\u5668\u901a\u8fc7\u4e24\u4e2a\u9274\u522b\u5668\u7684\u635f\u5931\u548c\u4e00\u4e2a\u7f51\u683c\u5355\u5143\u6b63\u5219\u5316\u9879\uff08\u8bb0\u4e3a \\(\\mathcal{L}_R(\\theta)\\) \uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u7a7a\u95f4\u9274\u522b\u5668 \\(D\\phi\\) \u5177\u6709\u53c2\u6570 \\(\\phi\\)\uff0c\u65f6\u95f4\u9274\u522b\u5668 \\(T_\\psi\\) \u5177\u6709\u53c2\u6570 \\(\\psi\\)\uff0c\u751f\u6210\u5668 \\(G_\\theta\\) \u5177\u6709\u53c2\u6570 \\(\\theta\\)\u3002\u6211\u4eec\u4f7f\u7528\u7b26\u53f7 \\(\\{X ; G\\}\\) \u8868\u793a\u4e24\u4e2a\u5b57\u6bb5\u7684\u4e32\u8054\u3002\u6700\u5927\u5316\u7684\u751f\u6210\u5668\u635f\u5931\u5982\u4e0b\uff1a</p> \\[ \\begin{gathered} \\mathcal{L}_G(\\theta)=\\mathbb{E}_{X_{1: M+N}}\\left[\\mathbb{E}_Z\\left[D\\left(G_\\theta\\left(Z ; X_{1: M}\\right)\\right)+T\\left(\\left\\{X_{1: M} ; G_\\theta\\left(Z ; X_{1: M}\\right)\\right\\}\\right)\\right]-\\lambda \\mathcal{L}_R(\\theta)\\right] ; \\\\ \\mathcal{L}_R(\\theta)=\\frac{1}{H W N}\\left\\|\\left(\\mathbb{E}_Z\\left[G_\\theta\\left(Z ; X_{1: M}\\right)\\right]-X_{M+1: M+N}\\right) \\odot w\\left(X_{M+1: M+N}\\right)\\right\\|_1 . \\end{gathered} \\] <p>\u6211\u4eec\u5728\u4e0a\u9762\u516c\u5f0f\u4e2d\u5bf9\u6f5c\u53d8\u91cf \\(\\mathrm{Z}\\) \u7684\u671f\u671b\u4f7f\u7528 Carlo \u4f30\u8ba1\u3002\u8fd9\u4e9b\u4f30\u8ba1\u662f\u4f7f\u7528\u6bcf\u4e2a\u8f93\u5165 \\(X_{1: M}\\) \u7684\u516d\u4e2a\u6837\u672c\u8ba1\u7b97\u7684\uff0c\u5176\u4e2d\u5305\u62ec \\(M=4\\) \u4e2a\u96f7\u8fbe\u89c2\u6d4b\u6570\u636e\u3002\u7f51\u683c\u5355\u5143\u6b63\u5219\u5316\u9879\u786e\u4fdd\u5e73\u5747\u9884\u6d4b\u4fdd\u6301\u63a5\u8fd1\u771f\u5b9e\u503c\uff0c\u5e76\u5728\u9ad8\u5ea6 \\(H\\)\u3001\u5bbd\u5ea6 \\(W\\) \u548c\u63d0\u524d\u65f6\u95f4 \\(N\\) \u8f74\u4e0a\u5bf9\u6240\u6709\u7f51\u683c\u5355\u5143\u8fdb\u884c\u5e73\u5747\u3002\u5b83\u901a\u8fc7\u51fd\u6570 \\(w(y)=\\max (y+1,24)\\) \u52a0\u6743\u81f3\u66f4\u9ad8\u7684\u964d\u96e8\u76ee\u6807\uff0c\u8be5\u51fd\u6570\u5bf9\u8f93\u5165\u5411\u91cf\u8fdb\u884c\u9010\u5143\u7d20\u64cd\u4f5c\uff0c\u5e76\u5728 24 \u5904\u622a\u65ad\u4ee5\u63d0\u9ad8\u5bf9\u96f7\u8fbe\u4e2d\u5f02\u5e38\u5927\u503c\u7684\u9c81\u68d2\u6027\u3002GAN \u7a7a\u95f4\u9274\u522b\u5668\u635f\u5931 \\(\\mathcal{L}_D(\\phi)\\) \u548c\u65f6\u95f4\u9274\u522b\u5668\u635f\u5931 \\(\\mathcal{L}_T(\\psi)\\) \u5206\u522b\u76f8\u5bf9\u4e8e\u53c2\u6570 \\(\\phi\\) \u548c \\(\\psi\\) \u6700\u5c0f\u5316\u3002\u9274\u522b\u5668\u635f\u5931\u91c7\u7528\u94f0\u94fe\u635f\u5931\u516c\u5f0f\uff1a</p> \\[ \\begin{aligned} &amp; \\mathcal{L}_D(\\phi)=\\mathbb{E}_{X_{1: M+N}, Z}\\left[\\operatorname{ReLU}\\left(1-D_\\phi\\left(X_{M+1: M+N}\\right)\\right)+\\operatorname{ReLU}\\left(1+D_\\phi\\left(G\\left(Z ; X_{1: M}\\right)\\right)\\right)\\right], \\\\ &amp; \\mathcal{L}_T(\\psi)=\\mathbb{E}_{X_{1: M+N}, Z}\\left[\\operatorname{ReLU}\\left(1-T_\\psi\\left(X_{1: M+N}\\right)\\right)+\\operatorname{ReLU}\\left(1+T_\\psi\\left(\\left\\{X_{1: M} ; G\\left(Z ; X_{1: M}\\right)\\right\\}\\right)\\right)\\right], \\end{aligned} \\] <p>\u5176\u4e2d \\(\\operatorname{ReLU} = \\max(0,x)\\). \u66f4\u591a\u8be6\u7ec6\u7684\u7406\u8bba\u63a8\u5bfc\u8bf7\u53c2\u8003 Skillful Precipitation Nowcasting using Deep Generative Models of Radar\u3002</p>"},{"location":"zh/examples/dgmr/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u8be5\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528 DGMR \u6765\u9884\u6d4b\u77ed\u4e34\u964d\u6c34\u3002\u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u7ea6\u675f\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003API\u6587\u6863\u3002</p>"},{"location":"zh/examples/dgmr/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u4e3a\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u82f1\u56fd\u7684\u4e34\u8fd1\u9884\u62a5\u6a21\u578b\uff0cDGMR \u4f7f\u7528\u4e86\u82f1\u56fd\u6c14\u8c61\u5c40 RadarNet4 \u7f51\u7edc\u4e2d\u7684\u96f7\u8fbe\u590d\u5408\u6570\u636e\u3002\u4f7f\u7528 2016 \u5e74 1 \u6708 1 \u65e5\u81f3 2019 \u5e74 12 \u6708 31 \u65e5\u671f\u95f4\u6bcf\u4e94\u5206\u949f\u6536\u96c6\u4e00\u6b21\u7684\u96f7\u8fbe\u6570\u636e\u3002\u6211\u4eec\u4f7f\u7528\u4ee5\u4e0b\u6570\u636e\u5206\u5272\u8fdb\u884c\u6a21\u578b\u5f00\u53d1\u3002\u5c06 2016 \u5e74\u81f3 2018 \u5e74\u6bcf\u6708\u7b2c\u4e00\u5929\u7684\u5b57\u6bb5\u5206\u914d\u7ed9\u9a8c\u8bc1\u96c6\u30022016 \u5e74\u81f3 2018 \u5e74\u7684\u6240\u6709\u5176\u4ed6\u65e5\u671f\u90fd\u5206\u914d\u7ed9\u8bad\u7ec3\u96c6\u3002\u6700\u540e\uff0c\u4f7f\u75282019\u5e74\u7684\u6570\u636e\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u9632\u6b62\u6570\u636e\u6cc4\u9732\u548c\u5206\u5e03\u6cdb\u5316\u6d4b\u8bd5\u3002\u5f00\u6e90\u7684\u82f1\u56fd\u8bad\u7ec3\u6570\u636e\u96c6\u5df2\u955c\u50cf\u5230 HuggingFace \u6570\u636e\u96c6\uff0c\u7528\u6237\u53ef\u4ee5\u81ea\u884c\u4e0b\u8f7d\u4f7f\u7528\u3002</p> <p>\u5728\u8be5\u6a21\u578b\u7684PaddleScience \u4ee3\u7801\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u8c03\u7528<code>ppsci.data.dataset.DGMRDataset</code> \u6765\u52a0\u8f7d\u6570\u636e\u96c6\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># load evaluate data\ndataset = ppsci.data.dataset.DGMRDataset(**cfg.DATASET)\nval_loader = paddle.io.DataLoader(dataset, batch_size=4)\n</code></pre>"},{"location":"zh/examples/dgmr/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 DGMR \u6a21\u578b\u4e2d\uff0c\u8f93\u5165\u8fc7\u53bb\u56db\u4e2a\u96f7\u8fbe\u573a\u6570\u636e\uff0c\u5bf9 18 \u4e2a\u672a\u6765\u96f7\u8fbe\u573a\uff08\u63a5\u4e0b\u6765\u7684 90 \u5206\u949f\uff09\u8fdb\u884c\u9884\u6d4b\u3002DGMR \u7f51\u7edc\u53ef\u4ee5\u8868\u793a\u4e3a \\(X_{1:4}\\) \u5230\u8f93\u51fa \\(X_{5:22}\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f\\)\uff0c\u5373\uff1a</p> \\[ X_{5:22} = f(X_{1:4}),\\\\ \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u4ee3\u8868 DGMR \u6a21\u578b\u3002\u6211\u4eec\u5b9a\u4e49 PaddleScience \u5185\u7f6e\u7684 DGMR \u6a21\u578b\u7c7b\uff0c\u5e76\u8c03\u7528\uff0cPaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.DGMR(**cfg.MODEL)\n</code></pre> <p>\u8fd9\u6837\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a DGMR \u6a21\u578b\uff0c\u6a21\u578b\u53c2\u6570\u8bbe\u7f6e\u76f8\u5173\u5185\u5bb9\u8bf7\u53c2\u8003\u6587\u732e Skillful Precipitation Nowcasting using Deep Generative Models of Radar\u3002</p>"},{"location":"zh/examples/dgmr/#33","title":"3.3 \u6a21\u578b\u8bc4\u4f30\u3001\u53ef\u89c6\u5316","text":"<p>\u6a21\u578b\u6784\u5efa\u548c\u52a0\u8f7d\u6570\u636e\u540e\uff0c\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u3002\u9996\u5148\u6211\u4eec\u521d\u59cb\u5316\u6a21\u578b\uff1a</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    pretrained_model_path=cfg.EVAL.pretrained_model_path,\n)\nsolver.model.eval()\n</code></pre> <p>\u7136\u540e\u81ea\u5b9a\u4e49\u8bc4\u4f30\u65b9\u5f0f\u548c\u635f\u5931\u51fd\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>def validation(\n    cfg: DictConfig,\n    solver: ppsci.solver.Solver,\n    batch: Tuple[Dict[str, paddle.Tensor], ...],\n):\n    \"\"\"\n    validation step.\n\n    Args:\n        cfg (DictConfig): Configuration object.\n        solver (ppsci.solver.Solver): Solver object containing the model and related components.\n        batch (Tuple[Dict[str, paddle.Tensor], ...]): Input batch consisting of images and corresponding future images.\n\n    Returns:\n        discriminator_loss: Loss incurred by the discriminator.\n        generator_loss: Loss incurred by the generator.\n        grid_cell_reg: Regularization term to encourage smooth transitions.\n    \"\"\"\n    images, future_images = batch\n    images_value = images[cfg.DATASET.input_keys[0]]\n    future_images_value = future_images[cfg.DATASET.label_keys[0]]\n    # Two discriminator steps per generator step\n    for _ in range(2):\n        predictions = solver.predict(images)\n        predictions_value = predictions[cfg.MODEL.output_keys[0]]\n        generated_sequence = paddle.concat(x=[images_value, predictions_value], axis=1)\n        real_sequence = paddle.concat(x=[images_value, future_images_value], axis=1)\n        concatenated_inputs = paddle.concat(\n            x=[real_sequence, generated_sequence], axis=0\n        )\n        concatenated_outputs = solver.model.discriminator(concatenated_inputs)\n        score_real, score_generated = paddle.split(\n            x=concatenated_outputs,\n            num_or_sections=[real_sequence.shape[0], generated_sequence.shape[0]],\n            axis=0,\n        )\n        score_real_spatial, score_real_temporal = paddle.split(\n            x=score_real, num_or_sections=score_real.shape[1], axis=1\n        )\n        score_generated_spatial, score_generated_temporal = paddle.split(\n            x=score_generated, num_or_sections=score_generated.shape[1], axis=1\n        )\n        discriminator_loss = _loss_hinge_disc(\n            score_generated_spatial, score_real_spatial\n        ) + _loss_hinge_disc(score_generated_temporal, score_real_temporal)\n\n    predictions_value = [\n        solver.predict(images)[cfg.MODEL.output_keys[0]] for _ in range(6)\n    ]\n    grid_cell_reg = _grid_cell_regularizer(\n        paddle.stack(x=predictions_value, axis=0), future_images_value\n    )\n    generated_sequence = [\n        paddle.concat(x=[images_value, x], axis=1) for x in predictions_value\n    ]\n    real_sequence = paddle.concat(x=[images_value, future_images_value], axis=1)\n    generated_scores = []\n    for g_seq in generated_sequence:\n        concatenated_inputs = paddle.concat(x=[real_sequence, g_seq], axis=0)\n        concatenated_outputs = solver.model.discriminator(concatenated_inputs)\n        score_real, score_generated = paddle.split(\n            x=concatenated_outputs,\n            num_or_sections=[real_sequence.shape[0], g_seq.shape[0]],\n            axis=0,\n        )\n        generated_scores.append(score_generated)\n    generator_disc_loss = _loss_hinge_gen(paddle.concat(x=generated_scores, axis=0))\n    generator_loss = generator_disc_loss + 20 * grid_cell_reg\n\n    return discriminator_loss, generator_loss, grid_cell_reg\n\n\ndef _loss_hinge_disc(score_generated, score_real):\n    \"\"\"Discriminator hinge loss.\"\"\"\n    l1 = nn.functional.relu(x=1.0 - score_real)\n    loss = paddle.mean(x=l1)\n    l2 = nn.functional.relu(x=1.0 + score_generated)\n    loss += paddle.mean(x=l2)\n    return loss\n\n\ndef _loss_hinge_gen(score_generated):\n    \"\"\"Generator hinge loss.\"\"\"\n    loss = -paddle.mean(x=score_generated)\n    return loss\n\n\ndef _grid_cell_regularizer(generated_samples, batch_targets):\n    \"\"\"Grid cell regularizer.\n\n    Args:\n      generated_samples: Tensor of size [n_samples, batch_size, 18, 256, 256, 1].\n      batch_targets: Tensor of size [batch_size, 18, 256, 256, 1].\n\n    Returns:\n      loss: A tensor of shape [batch_size].\n    \"\"\"\n    gen_mean = paddle.mean(x=generated_samples, axis=0)\n    weights = paddle.clip(x=batch_targets, min=0.0, max=24.0)\n    loss = paddle.mean(x=paddle.abs(x=gen_mean - batch_targets) * weights)\n    return loss\n</code></pre> <p>\u6700\u540e\u5bf9\u6570\u636e\u4e2d\u7684\u6bcf\u4e2a batch \u8fdb\u884c\u904d\u5386\u8bc4\u4f30\uff0c\u540c\u65f6\u5bf9\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <pre><code># evaluate pretrained model\nd_loss = []\ng_loss = []\ngrid_loss = []\nfor batch_idx, batch in enumerate(val_loader):\n    with paddle.no_grad():\n        out_dict = validation(cfg, solver, batch)\n\n        # visualize\n        images = batch[0][cfg.DATASET.input_keys[0]]\n        future_images = batch[1][cfg.DATASET.label_keys[0]]\n        generated_images = solver.predict(batch[0])[cfg.MODEL.output_keys[0]]\n        if batch_idx % 50 == 0:\n            logger.message(f\"Saving plot of image frame to {cfg.output_dir}\")\n            visualize(\n                cfg.output_dir, images, future_images, generated_images, batch_idx\n            )\n\n    d_loss.append(out_dict[0])\n    g_loss.append(out_dict[1])\n    grid_loss.append(out_dict[2])\nlogger.message(f\"d_loss: {np.array(d_loss).mean()}\")\nlogger.message(f\"g_loss: {np.array(g_loss).mean()}\")\nlogger.message(f\"grid_loss: {np.array(grid_loss).mean()}\")\n</code></pre>"},{"location":"zh/examples/dgmr/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"dgmr.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nReference: https://github.com/openclimatefix/skillful_nowcasting\n\"\"\"\nfrom os import path as osp\nfrom typing import Dict\nfrom typing import Tuple\n\nimport hydra\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef visualize(\n    output_dir: str,\n    x: paddle.Tensor,\n    y: paddle.Tensor,\n    y_hat: paddle.Tensor,\n    batch_idx: int,\n) -&gt; None:\n    \"\"\"\n    Visualizes input, target, and generated images and saves them to the output directory.\n\n    Args:\n        output_dir (str): Directory to save the visualization images.\n        x (paddle.Tensor): Input images tensor.\n        y (paddle.Tensor): Target images tensor.\n        y_hat (paddle.Tensor): Generated images tensor.\n        batch_idx (int): Batch index.\n\n    Returns:\n        None\n    \"\"\"\n    images = x[0]\n    future_images = y[0]\n    generated_images = y_hat[0]\n    fig, axes = plt.subplots(2, 2)\n    for i, ax in enumerate(axes.flat):\n        alpha = images[i][0].numpy()\n        alpha[alpha &lt; 1] = 0\n        alpha[alpha &gt; 1] = 1\n        ax.imshow(images[i].transpose([1, 2, 0]).numpy(), alpha=alpha, cmap=\"viridis\")\n    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n    plt.savefig(osp.join(output_dir, f\"Input_Image_Stack_Frame_{batch_idx}.png\"))\n    fig, axes = plt.subplots(3, 3)\n    for i, ax in enumerate(axes.flat):\n        alpha = future_images[i][0].numpy()\n        alpha[alpha &lt; 1] = 0\n        alpha[alpha &gt; 1] = 1\n        ax.imshow(\n            future_images[i].transpose([1, 2, 0]).numpy(), alpha=alpha, cmap=\"viridis\"\n        )\n    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n    plt.savefig(osp.join(output_dir, f\"Target_Image_Frame_{batch_idx}.png\"))\n    fig, axes = plt.subplots(3, 3)\n    for i, ax in enumerate(axes.flat):\n        alpha = generated_images[i][0].numpy()\n        alpha[alpha &lt; 1] = 0\n        alpha[alpha &gt; 1] = 1\n        ax.imshow(\n            generated_images[i].transpose([1, 2, 0]).numpy(),\n            alpha=alpha,\n            cmap=\"viridis\",\n        )\n    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n    plt.savefig(osp.join(output_dir, f\"Generated_Image_Frame_{batch_idx}.png\"))\n    plt.close()\n\n\ndef validation(\n    cfg: DictConfig,\n    solver: ppsci.solver.Solver,\n    batch: Tuple[Dict[str, paddle.Tensor], ...],\n):\n    \"\"\"\n    validation step.\n\n    Args:\n        cfg (DictConfig): Configuration object.\n        solver (ppsci.solver.Solver): Solver object containing the model and related components.\n        batch (Tuple[Dict[str, paddle.Tensor], ...]): Input batch consisting of images and corresponding future images.\n\n    Returns:\n        discriminator_loss: Loss incurred by the discriminator.\n        generator_loss: Loss incurred by the generator.\n        grid_cell_reg: Regularization term to encourage smooth transitions.\n    \"\"\"\n    images, future_images = batch\n    images_value = images[cfg.DATASET.input_keys[0]]\n    future_images_value = future_images[cfg.DATASET.label_keys[0]]\n    # Two discriminator steps per generator step\n    for _ in range(2):\n        predictions = solver.predict(images)\n        predictions_value = predictions[cfg.MODEL.output_keys[0]]\n        generated_sequence = paddle.concat(x=[images_value, predictions_value], axis=1)\n        real_sequence = paddle.concat(x=[images_value, future_images_value], axis=1)\n        concatenated_inputs = paddle.concat(\n            x=[real_sequence, generated_sequence], axis=0\n        )\n        concatenated_outputs = solver.model.discriminator(concatenated_inputs)\n        score_real, score_generated = paddle.split(\n            x=concatenated_outputs,\n            num_or_sections=[real_sequence.shape[0], generated_sequence.shape[0]],\n            axis=0,\n        )\n        score_real_spatial, score_real_temporal = paddle.split(\n            x=score_real, num_or_sections=score_real.shape[1], axis=1\n        )\n        score_generated_spatial, score_generated_temporal = paddle.split(\n            x=score_generated, num_or_sections=score_generated.shape[1], axis=1\n        )\n        discriminator_loss = _loss_hinge_disc(\n            score_generated_spatial, score_real_spatial\n        ) + _loss_hinge_disc(score_generated_temporal, score_real_temporal)\n\n    predictions_value = [\n        solver.predict(images)[cfg.MODEL.output_keys[0]] for _ in range(6)\n    ]\n    grid_cell_reg = _grid_cell_regularizer(\n        paddle.stack(x=predictions_value, axis=0), future_images_value\n    )\n    generated_sequence = [\n        paddle.concat(x=[images_value, x], axis=1) for x in predictions_value\n    ]\n    real_sequence = paddle.concat(x=[images_value, future_images_value], axis=1)\n    generated_scores = []\n    for g_seq in generated_sequence:\n        concatenated_inputs = paddle.concat(x=[real_sequence, g_seq], axis=0)\n        concatenated_outputs = solver.model.discriminator(concatenated_inputs)\n        score_real, score_generated = paddle.split(\n            x=concatenated_outputs,\n            num_or_sections=[real_sequence.shape[0], g_seq.shape[0]],\n            axis=0,\n        )\n        generated_scores.append(score_generated)\n    generator_disc_loss = _loss_hinge_gen(paddle.concat(x=generated_scores, axis=0))\n    generator_loss = generator_disc_loss + 20 * grid_cell_reg\n\n    return discriminator_loss, generator_loss, grid_cell_reg\n\n\ndef _loss_hinge_disc(score_generated, score_real):\n    \"\"\"Discriminator hinge loss.\"\"\"\n    l1 = nn.functional.relu(x=1.0 - score_real)\n    loss = paddle.mean(x=l1)\n    l2 = nn.functional.relu(x=1.0 + score_generated)\n    loss += paddle.mean(x=l2)\n    return loss\n\n\ndef _loss_hinge_gen(score_generated):\n    \"\"\"Generator hinge loss.\"\"\"\n    loss = -paddle.mean(x=score_generated)\n    return loss\n\n\ndef _grid_cell_regularizer(generated_samples, batch_targets):\n    \"\"\"Grid cell regularizer.\n\n    Args:\n      generated_samples: Tensor of size [n_samples, batch_size, 18, 256, 256, 1].\n      batch_targets: Tensor of size [batch_size, 18, 256, 256, 1].\n\n    Returns:\n      loss: A tensor of shape [batch_size].\n    \"\"\"\n    gen_mean = paddle.mean(x=generated_samples, axis=0)\n    weights = paddle.clip(x=batch_targets, min=0.0, max=24.0)\n    loss = paddle.mean(x=paddle.abs(x=gen_mean - batch_targets) * weights)\n    return loss\n\n\ndef train(cfg: DictConfig):\n    raise NotImplementedError(\"Training of DGMR is not supported now.\")\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.DGMR(**cfg.MODEL)\n    # load evaluate data\n    dataset = ppsci.data.dataset.DGMRDataset(**cfg.DATASET)\n    val_loader = paddle.io.DataLoader(dataset, batch_size=4)\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.model.eval()\n\n    # evaluate pretrained model\n    d_loss = []\n    g_loss = []\n    grid_loss = []\n    for batch_idx, batch in enumerate(val_loader):\n        with paddle.no_grad():\n            out_dict = validation(cfg, solver, batch)\n\n            # visualize\n            images = batch[0][cfg.DATASET.input_keys[0]]\n            future_images = batch[1][cfg.DATASET.label_keys[0]]\n            generated_images = solver.predict(batch[0])[cfg.MODEL.output_keys[0]]\n            if batch_idx % 50 == 0:\n                logger.message(f\"Saving plot of image frame to {cfg.output_dir}\")\n                visualize(\n                    cfg.output_dir, images, future_images, generated_images, batch_idx\n                )\n\n        d_loss.append(out_dict[0])\n        g_loss.append(out_dict[1])\n        grid_loss.append(out_dict[2])\n    logger.message(f\"d_loss: {np.array(d_loss).mean()}\")\n    logger.message(f\"g_loss: {np.array(g_loss).mean()}\")\n    logger.message(f\"grid_loss: {np.array(grid_loss).mean()}\")\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"dgmr.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/dgmr/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u5982\u56fe\u6240\u793a\u4e3a\u5206\u522b\u5728 \\(T+5, T+10, \\cdots, T+45\\) \u5206\u949f\u7684\u6c14\u8c61\u964d\u6c34\u9884\u6d4b\uff0c\u4e0e\u771f\u5b9e\u7684\u964d\u6c34\u60c5\u51b5\u76f8\u6bd4\u53ef\u4ee5\u770b\u51fa\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u7ed9\u51fa\u6bd4\u8f83\u597d\u7684\u77ed\u4e34\u964d\u6c34\u9884\u6d4b\u3002</p> <p> </p>  \u6a21\u578b\u9884\u6d4b\u7684\u964d\u6c34\u60c5\u51b5 <p> </p>  \u771f\u5b9e\u964d\u6c34\u60c5\u51b5"},{"location":"zh/examples/dgmr/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<p>\u53c2\u8003\u6587\u732e\uff1a Skillful Precipitation Nowcasting using Deep Generative Models of Radar</p> <p>\u53c2\u8003\u4ee3\u7801\uff1a DGMR</p>"},{"location":"zh/examples/earthformer/","title":"EarthFormer","text":""},{"location":"zh/examples/earthformer/#earthformer","title":"EarthFormer","text":"<p>\u5f00\u59cb\u8bad\u7ec3\u3001\u8bc4\u4f30\u524d\uff0c\u8bf7\u5148\u4e0b\u8f7d</p> <p>ICAR-ENSO\u6570\u636e\u96c6</p> <p>SEVIR\u6570\u636e\u96c6</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># ICAR-ENSO \u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\npython examples/earthformer/earthformer_enso_train.py\n# SEVIR \u6570\u636e\u96c6\u9884\u8bad\u7ec3\u6a21\u578b\npython examples/earthformer/earthformer_sevir_train.py\n</code></pre> <pre><code># ICAR-ENSO \u6a21\u578b\u8bc4\u4f30\npython examples/earthformer/earthformer_enso_train.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/earthformer/earthformer_enso.pdparams\n# SEVIR \u6a21\u578b\u8bc4\u4f30\npython examples/earthformer/earthformer_sevir_train.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/earthformer/earthformer_sevir.pdparams\n</code></pre> <pre><code># ICAR-ENSO \u6a21\u578b\u63a8\u7406\npython examples/earthformer/earthformer_enso_train.py mode=export\n# SEVIR \u6a21\u578b\u63a8\u7406\npython examples/earthformer/earthformer_sevir_train.py mode=export\n</code></pre> <pre><code># ICAR-ENSO \u6a21\u578b\u63a8\u7406\npython examples/earthformer/earthformer_enso_train.py mode=infer\n# SEVIR \u6a21\u578b\u63a8\u7406\npython examples/earthformer/earthformer_sevir_train.py mode=infer\n</code></pre> \u6a21\u578b \u53d8\u91cf\u540d\u79f0 C-Nino3.4-M C-Nino3.4-WM MSE(1E-4) ENSO \u6a21\u578b sst 0.74130 2.28990 2.5000 \u6a21\u578b \u53d8\u91cf\u540d\u79f0 CSI-M CSI-219 CSI-181 CSI-160 CSI-133 CSI-74 CSI-16 MSE(1E-4) SEVIR \u6a21\u578b vil 0.4419 0.1791 0.2848 0.3232 0.4271 0.6860 0.7513 3.6957"},{"location":"zh/examples/earthformer/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u5730\u7403\u662f\u4e00\u4e2a\u590d\u6742\u7684\u7cfb\u7edf\u3002\u5730\u7403\u7cfb\u7edf\u7684\u53d8\u5316\uff0c\u4ece\u6e29\u5ea6\u6ce2\u52a8\u7b49\u5e38\u89c4\u4e8b\u4ef6\u5230\u5e72\u65f1\u3001\u51b0\u96f9\u548c\u5384\u5c14\u5c3c\u8bfa/\u5357\u65b9\u6d9b\u52a8 (ENSO) \u7b49\u6781\u7aef\u4e8b\u4ef6\uff0c\u5f71\u54cd\u7740\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u3002\u5728\u6240\u6709\u540e\u679c\u4e2d\uff0c\u5730\u7403\u7cfb\u7edf\u7684\u53d8\u5316\u4f1a\u5f71\u54cd\u519c\u4f5c\u7269\u4ea7\u91cf\u3001\u822a\u73ed\u5ef6\u8bef\u3001\u5f15\u53d1\u6d2a\u6c34\u548c\u68ee\u6797\u706b\u707e\u3002\u5bf9\u8fd9\u4e9b\u53d8\u5316\u8fdb\u884c\u51c6\u786e\u53ca\u65f6\u7684\u9884\u6d4b\u53ef\u4ee5\u5e2e\u52a9\u4eba\u4eec\u91c7\u53d6\u5fc5\u8981\u7684\u9884\u9632\u63aa\u65bd\u4ee5\u907f\u514d\u5371\u673a\uff0c\u6216\u8005\u66f4\u597d\u5730\u5229\u7528\u98ce\u80fd\u548c\u592a\u9633\u80fd\u7b49\u81ea\u7136\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u6539\u8fdb\u5730\u7403\u53d8\u5316\uff08\u4f8b\u5982\u5929\u6c14\u548c\u6c14\u5019\uff09\u7684\u9884\u6d4b\u6a21\u578b\u5177\u6709\u5de8\u5927\u7684\u793e\u4f1a\u7ecf\u6d4e\u5f71\u54cd\u3002</p> <p>Earthformer\uff0c\u4e00\u79cd\u7528\u4e8e\u5730\u7403\u7cfb\u7edf\u9884\u6d4b\u7684\u65f6\u7a7a\u8f6c\u6362\u5668\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u63a2\u7d22\u65f6\u7a7a\u6ce8\u610f\u529b\u7684\u8bbe\u8ba1\uff0c\u8bba\u6587\u63d0\u51fa\u4e86 Cuboid Attention \uff0c\u5b83\u662f\u9ad8\u6548\u65f6\u7a7a\u6ce8\u610f\u529b\u7684\u901a\u7528\u6784\u5efa\u5757\u3002\u8fd9\u4e2a\u60f3\u6cd5\u662f\u5c06\u8f93\u5165\u5f20\u91cf\u5206\u89e3\u4e3a\u4e0d\u91cd\u53e0\u7684\u957f\u65b9\u4f53\uff0c\u5e76\u884c\u5e94\u7528\u957f\u65b9\u4f53\u7ea7\u81ea\u6ce8\u610f\u529b\u3002\u7531\u4e8e\u6211\u4eec\u5c06 O(N<sup>2</sup>) \u81ea\u6ce8\u610f\u529b\u9650\u5236\u5728\u5c40\u90e8\u957f\u65b9\u4f53\u5185\uff0c\u56e0\u6b64\u6574\u4f53\u590d\u6742\u5ea6\u5927\u5927\u964d\u4f4e\u3002\u4e0d\u540c\u7c7b\u578b\u7684\u76f8\u5173\u6027\u53ef\u4ee5\u901a\u8fc7\u4e0d\u540c\u7684\u957f\u65b9\u4f53\u5206\u89e3\u6765\u6355\u83b7\u3002\u540c\u65f6\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u7ec4\u5173\u6ce8\u6240\u6709\u5c40\u90e8\u957f\u65b9\u4f53\u7684\u5168\u5c40\u5411\u91cf\uff0c\u4ece\u800c\u6536\u96c6\u7cfb\u7edf\u7684\u6574\u4f53\u72b6\u6001\u3002\u901a\u8fc7\u5173\u6ce8\u5168\u5c40\u5411\u91cf\uff0c\u5c40\u90e8\u957f\u65b9\u4f53\u53ef\u4ee5\u638c\u63e1\u7cfb\u7edf\u7684\u603b\u4f53\u52a8\u6001\u5e76\u76f8\u4e92\u5171\u4eab\u4fe1\u606f\u3002</p>"},{"location":"zh/examples/earthformer/#2","title":"2. \u6a21\u578b\u539f\u7406","text":"<p>\u672c\u7ae0\u8282\u4ec5\u5bf9 EarthFormer \u7684\u6a21\u578b\u539f\u7406\u8fdb\u884c\u7b80\u5355\u5730\u4ecb\u7ecd\uff0c\u8be6\u7ec6\u7684\u7406\u8bba\u63a8\u5bfc\u8bf7\u9605\u8bfb Earthformer: Exploring Space-Time Transformers for Earth System Forecasting\u3002</p> <p>Earthformer \u7684\u7f51\u7edc\u6a21\u578b\u4f7f\u7528\u4e86\u57fa\u4e8e Cuboid Attention \u7684\u5206\u5c42 Transformer incoder-decoder \u3002\u8fd9\u4e2a\u60f3\u6cd5\u662f\u5c06\u6570\u636e\u5206\u89e3\u4e3a\u957f\u65b9\u4f53\u5e76\u5e76\u884c\u5e94\u7528\u957f\u65b9\u4f53\u7ea7\u81ea\u6ce8\u610f\u529b\u3002\u8fd9\u4e9b\u957f\u65b9\u4f53\u8fdb\u4e00\u6b65\u4e0e\u5168\u5c40\u5411\u91cf\u7684\u96c6\u5408\u8fde\u63a5\u3002</p> <p>\u6a21\u578b\u7684\u603b\u4f53\u7ed3\u6784\u5982\u56fe\u6240\u793a\uff1a</p> <p> </p> EarthFormer \u7f51\u7edc\u6a21\u578b <p>EarthFormer \u539f\u4ee3\u7801\u4e2d\u8bad\u7ec3\u4e86 ICAR-ENSO \u6570\u636e\u96c6\u4e2d\u6d77\u9762\u6e29\u5ea6 (sst) \u548c SEVIR \u6570\u636e\u96c6\u4e2d\u5bf9\u4e91\u603b\u964d\u6c34\u91cf (vil) \u7684\u4f30\u8ba1\u6a21\u578b\uff0c\u63a5\u4e0b\u6765\u5c06\u4ecb\u7ecd\u8fd9\u4e24\u4e2a\u6a21\u578b\u7684\u8bad\u7ec3\u3001\u63a8\u7406\u8fc7\u7a0b\u3002</p>"},{"location":"zh/examples/earthformer/#21-icar-enso-sevir","title":"2.1 ICAR-ENSO \u548c SEVIR \u6a21\u578b\u7684\u8bad\u7ec3\u3001\u63a8\u7406\u8fc7\u7a0b","text":"<p>\u6a21\u578b\u9884\u8bad\u7ec3\u9636\u6bb5\u662f\u57fa\u4e8e\u968f\u673a\u521d\u59cb\u5316\u7684\u7f51\u7edc\u6743\u91cd\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5176\u4e2d \\([x_{i}]_{i=1}^{T}\\) \u8868\u793a\u957f\u5ea6\u4e3a \\(T\\) \u65f6\u7a7a\u5e8f\u5217\u7684\u8f93\u5165\u6c14\u8c61\u6570\u636e\uff0c\\([y_{i}]_{i=1}^{K}\\) \u8868\u793a\u9884\u6d4b\u672a\u6765 \\(K\\) \u6b65\u7684\u6c14\u8c61\u6570\u636e\uff0c\\([y_{i_True}]_{i=1}^{K}\\) \u8868\u793a\u672a\u6765 \\(K\\) \u6b65\u7684\u771f\u5b9e\u6570\u636e\uff0c\u5982\u6d77\u9762\u6e29\u5ea6\u6570\u636e\u548c\u4e91\u603b\u964d\u6c34\u91cf\u6570\u636e\u3002\u6700\u540e\u7f51\u7edc\u6a21\u578b\u9884\u6d4b\u7684\u8f93\u51fa\u548c\u771f\u503c\u8ba1\u7b97 mse \u635f\u5931\u51fd\u6570\u3002</p> <p> </p> earthformer \u6a21\u578b\u9884\u8bad\u7ec3 <p>\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u7ed9\u5b9a\u957f\u5ea6\u5e8f\u5217\u4e3a \\(T\\) \u7684\u6570\u636e\uff0c\u5f97\u5230\u957f\u5ea6\u5e8f\u5217\u4e3a \\(K\\) \u7684\u9884\u6d4b\u7ed3\u679c\u3002</p> <p> </p> earthformer \u6a21\u578b\u63a8\u7406"},{"location":"zh/examples/earthformer/#3","title":"3. \u6d77\u9762\u6e29\u5ea6\u6a21\u578b\u5b9e\u73b0","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u57fa\u4e8e PaddleScience \u4ee3\u7801\uff0c\u5b9e\u73b0 EarthFormer \u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\u3002\u5173\u4e8e\u8be5\u6848\u4f8b\u4e2d\u7684\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/earthformer/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u6570\u636e\u96c6\u91c7\u7528\u4e86 EarthFormer \u5904\u7406\u597d\u7684 ICAR-ENSO \u6570\u636e\u96c6\u3002</p> <p>\u672c\u6570\u636e\u96c6\u7531\u6c14\u5019\u4e0e\u5e94\u7528\u524d\u6cbf\u7814\u7a76\u9662 ICAR \u63d0\u4f9b\u3002\u6570\u636e\u5305\u62ec CMIP5/6 \u6a21\u5f0f\u7684\u5386\u53f2\u6a21\u62df\u6570\u636e\u548c\u7f8e\u56fd SODA \u6a21\u5f0f\u91cd\u5efa\u7684\u8fd1100\u591a\u5e74\u5386\u53f2\u89c2\u6d4b\u540c\u5316\u6570\u636e\u3002\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u4ee5\u4e0b\u6c14\u8c61\u53ca\u65f6\u7a7a\u53d8\u91cf\uff1a\u6d77\u8868\u6e29\u5ea6\u5f02\u5e38 (SST) \uff0c\u70ed\u542b\u91cf\u5f02\u5e38 (T300)\uff0c\u7eac\u5411\u98ce\u5f02\u5e38 (Ua)\uff0c\u7ecf\u5411\u98ce\u5f02\u5e38 (Va)\uff0c\u6570\u636e\u7ef4\u5ea6\u4e3a (year,month,lat,lon)\u3002\u8bad\u7ec3\u6570\u636e\u63d0\u4f9b\u5bf9\u5e94\u6708\u4efd\u7684 Nino3.4 index \u6807\u7b7e\u6570\u636e\u3002\u6d4b\u8bd5\u7528\u7684\u521d\u59cb\u573a\u6570\u636e\u4e3a\u56fd\u9645\u591a\u4e2a\u6d77\u6d0b\u8d44\u6599\u540c\u5316\u7ed3\u679c\u63d0\u4f9b\u7684\u968f\u673a\u62bd\u53d6\u7684 n \u6bb5 12 \u4e2a\u65f6\u95f4\u5e8f\u5217\uff0c\u6570\u636e\u683c\u5f0f\u91c7\u7528 NPY \u683c\u5f0f\u4fdd\u5b58\u3002</p> <p>\u8bad\u7ec3\u6570\u636e\uff1a</p> <p>\u6bcf\u4e2a\u6570\u636e\u6837\u672c\u7b2c\u4e00\u7ef4\u5ea6 (year) \u8868\u5f81\u6570\u636e\u6240\u5bf9\u5e94\u8d77\u59cb\u5e74\u4efd\uff0c\u5bf9\u4e8e CMIP \u6570\u636e\u5171 291 \u5e74\uff0c\u5176\u4e2d 1-2265 \u4e3a CMIP6 \u4e2d 15 \u4e2a\u6a21\u5f0f\u63d0\u4f9b\u7684 151 \u5e74\u7684\u5386\u53f2\u6a21\u62df\u6570\u636e (\u603b\u5171\uff1a151\u5e74 15 \u4e2a\u6a21\u5f0f=2265) \uff1b2266-4645 \u4e3a CMIP5 \u4e2d 17 \u4e2a\u6a21\u5f0f\u63d0\u4f9b\u7684 140 \u5e74\u7684\u5386\u53f2\u6a21\u62df\u6570\u636e (\u603b\u5171\uff1a140 \u5e7417 \u4e2a\u6a21\u5f0f=2380)\u3002\u5bf9\u4e8e\u5386\u53f2\u89c2\u6d4b\u540c\u5316\u6570\u636e\u4e3a\u7f8e\u56fd\u63d0\u4f9b\u7684 SODA \u6570\u636e\u3002</p> <p>\u8bad\u7ec3\u6570\u636e\u6807\u7b7e</p> <p>\u6807\u7b7e\u6570\u636e\u4e3a Nino3.4 SST \u5f02\u5e38\u6307\u6570\uff0c\u6570\u636e\u7ef4\u5ea6\u4e3a (year,month)\u3002</p> <p>CMIP(SODA)_train.nc \u5bf9\u5e94\u7684\u6807\u7b7e\u6570\u636e\u5f53\u524d\u65f6\u523b Nino3.4 SST \u5f02\u5e38\u6307\u6570\u7684\u4e09\u4e2a\u6708\u6ed1\u52a8\u5e73\u5747\u503c\uff0c\u56e0\u6b64\u6570\u636e\u7ef4\u5ea6\u4e0e\u7ef4\u5ea6\u4ecb\u7ecd\u540c\u8bad\u7ec3\u6570\u636e\u4e00\u81f4\u3002</p> <p>\u6ce8\uff1a\u4e09\u4e2a\u6708\u6ed1\u52a8\u5e73\u5747\u503c\u4e3a\u5f53\u524d\u6708\u4e0e\u672a\u6765\u4e24\u4e2a\u6708\u7684\u5e73\u5747\u503c\u3002</p> <p>\u6d4b\u8bd5\u6570\u636e</p> <p>\u6d4b\u8bd5\u7528\u7684\u521d\u59cb\u573a (\u8f93\u5165) \u6570\u636e\u4e3a\u56fd\u9645\u591a\u4e2a\u6d77\u6d0b\u8d44\u6599\u540c\u5316\u7ed3\u679c\u63d0\u4f9b\u7684\u968f\u673a\u62bd\u53d6\u7684 n \u6bb5 12 \u4e2a\u65f6\u95f4\u5e8f\u5217\uff0c\u6570\u636e\u683c\u5f0f\u91c7\u7528NPY\u683c\u5f0f\u4fdd\u5b58\uff0c\u7ef4\u5ea6\u4e3a (12\uff0clat\uff0clon, 4), 12 \u4e3a t \u65f6\u523b\u53ca\u8fc7\u53bb 11 \u4e2a\u65f6\u523b\uff0c4 \u4e3a\u9884\u6d4b\u56e0\u5b50\uff0c\u5e76\u6309\u7167 SST,T300,Ua,Va \u7684\u987a\u5e8f\u5b58\u653e\u3002</p> <p>EarthFFormer \u6a21\u578b\u5bf9\u4e8e ICAR-ENSO \u6570\u636e\u96c6\u7684\u8bad\u7ec3\u4e2d\uff0c\u53ea\u5bf9\u5176\u4e2d\u6d77\u9762\u6e29\u5ea6 (SST) \u8fdb\u884c\u8bad\u7ec3\u548c\u9884\u6d4b\u3002\u8bad\u7ec3\u6d77\u6e29\u5f02\u5e38\u89c2\u6d4b\u7684 12 \u6b65 (\u4e00\u5e74) \uff0c\u9884\u6d4b\u6d77\u6e29\u5f02\u5e38\u6700\u591a 14 \u6b65\u3002</p>"},{"location":"zh/examples/earthformer/#32","title":"3.2 \u6a21\u578b\u9884\u8bad\u7ec3","text":""},{"location":"zh/examples/earthformer/#321","title":"3.2.1 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\u3002</p> <p>\u6570\u636e\u52a0\u8f7d\u7684\u4ee3\u7801\u5982\u4e0b:</p> examples/earthformer/earthformer_enso_train.py<pre><code>train_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ENSODataset\",\n        \"data_dir\": cfg.FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.DATASET.label_keys,\n        \"in_len\": cfg.DATASET.in_len,\n        \"out_len\": cfg.DATASET.out_len,\n        \"in_stride\": cfg.DATASET.in_stride,\n        \"out_stride\": cfg.DATASET.out_stride,\n        \"train_samples_gap\": cfg.DATASET.train_samples_gap,\n        \"eval_samples_gap\": cfg.DATASET.eval_samples_gap,\n        \"normalize_sst\": cfg.DATASET.normalize_sst,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 8,\n}\n</code></pre> <p>\u5176\u4e2d\uff0c\"dataset\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Dataset</code> \u7c7b\u540d\u4e3a <code>ENSODataset</code>\uff0c\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u8bbe\u7f6e\u7684 <code>batch_size</code> \u4e3a 16\uff0c<code>num_works</code> \u4e3a 8\u3002</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code># set constraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    loss=ppsci.loss.FunctionalLoss(enso_metric.train_mse_func),\n    name=\"Sup\",\n)\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u6570\u636e\u7684\u52a0\u8f7d\u65b9\u5f0f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e0a\u6587\u4e2d\u5b9a\u4e49\u7684 <code>train_dataloader_cfg</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684\u635f\u5931\u51fd\u6570 <code>mse_loss</code>\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a <code>Sup</code>\u3002</p>"},{"location":"zh/examples/earthformer/#322","title":"3.2.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0c\u6d77\u9762\u6e29\u5ea6\u6a21\u578b\u57fa\u4e8e CuboidTransformer \u7f51\u7edc\u6a21\u578b\u5b9e\u73b0\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code>model = ppsci.arch.CuboidTransformer(\n    **cfg.MODEL,\n)\n</code></pre> <p>\u7f51\u7edc\u6a21\u578b\u7684\u53c2\u6570\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u8bbe\u7f6e\u5982\u4e0b\uff1a</p> examples/earthformer/conf/earthformer_enso_pretrain.yaml<pre><code># model settings\nMODEL:\n  input_keys: [\"sst_data\"]\n  output_keys: [\"sst_target\",\"nino_target\"]\n  input_shape: [12, 24, 48, 1]\n  target_shape: [14, 24, 48, 1]\n  base_units: 64\n  scale_alpha: 1.0\n\n  enc_depth: [1, 1]\n  dec_depth: [1, 1]\n  enc_use_inter_ffn: true\n  dec_use_inter_ffn: true\n  dec_hierarchical_pos_embed: false\n\n  downsample: 2\n  downsample_type: \"patch_merge\"\n  upsample_type: \"upsample\"\n\n  num_global_vectors: 0\n  use_dec_self_global: false\n  dec_self_update_global: true\n  use_dec_cross_global: false\n  use_global_vector_ffn: false\n  use_global_self_attn: false\n  separate_global_qkv: false\n  global_dim_ratio: 1\n\n  self_pattern: \"axial\"\n  cross_self_pattern: \"axial\"\n  cross_pattern: \"cross_1x1\"\n  dec_cross_last_n_frames: null\n\n  attn_drop: 0.1\n  proj_drop: 0.1\n  ffn_drop: 0.1\n  num_heads: 4\n\n  ffn_activation: \"gelu\"\n  gated_ffn: false\n  norm_layer: \"layer_norm\"\n  padding_type: \"zeros\"\n  pos_embed_type: \"t+h+w\"\n  use_relative_pos: true\n  self_attn_use_final_proj: true\n  dec_use_first_self_attn: false\n\n  z_init_method: \"zeros\"\n  initial_downsample_type: \"conv\"\n  initial_downsample_activation: \"leaky_relu\"\n  initial_downsample_scale: [1, 1, 2]\n  initial_downsample_conv_layers: 2\n  final_upsample_conv_layers: 1\n  checkpoint_level: 2\n\n  attn_linear_init_mode: \"0\"\n  ffn_linear_init_mode: \"0\"\n  conv_init_mode: \"0\"\n  down_up_linear_init_mode: \"0\"\n  norm_init_mode: \"0\"\n</code></pre> <p>\u5176\u4e2d\uff0c<code>input_keys</code> \u548c <code>output_keys</code> \u5206\u522b\u4ee3\u8868\u7f51\u7edc\u6a21\u578b\u8f93\u5165\u3001\u8f93\u51fa\u53d8\u91cf\u7684\u540d\u79f0\u3002</p>"},{"location":"zh/examples/earthformer/#323","title":"3.2.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>Cosine</code>\uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a <code>2e-4</code>\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>AdamW</code>\uff0c\u5e76\u5c06\u53c2\u6570\u8fdb\u884c\u5206\u7ec4\uff0c\u4f7f\u7528\u4e0d\u540c\u7684 <code>weight_decay</code>,\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code>decay_parameters = get_parameter_names(model, [nn.LayerNorm])\ndecay_parameters = [name for name in decay_parameters if \"bias\" not in name]\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n        \"weight_decay\": cfg.TRAIN.wd,\n    },\n    {\n        \"params\": [\n            p for n, p in model.named_parameters() if n not in decay_parameters\n        ],\n        \"weight_decay\": 0.0,\n    },\n]\n\n# # init optimizer and lr scheduler\nlr_scheduler_cfg = dict(cfg.TRAIN.lr_scheduler)\nlr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(\n    **lr_scheduler_cfg,\n    iters_per_epoch=ITERS_PER_EPOCH,\n    eta_min=cfg.TRAIN.min_lr_ratio * cfg.TRAIN.lr_scheduler.learning_rate,\n    warmup_epoch=int(0.2 * cfg.TRAIN.epochs),\n)()\noptimizer = paddle.optimizer.AdamW(\n    lr_scheduler, parameters=optimizer_grouped_parameters\n)\n</code></pre>"},{"location":"zh/examples/earthformer/#324","title":"3.2.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code># set eval dataloader config\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ENSODataset\",\n        \"data_dir\": cfg.FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.DATASET.label_keys,\n        \"in_len\": cfg.DATASET.in_len,\n        \"out_len\": cfg.DATASET.out_len,\n        \"in_stride\": cfg.DATASET.in_stride,\n        \"out_stride\": cfg.DATASET.out_stride,\n        \"train_samples_gap\": cfg.DATASET.train_samples_gap,\n        \"eval_samples_gap\": cfg.DATASET.eval_samples_gap,\n        \"normalize_sst\": cfg.DATASET.normalize_sst,\n        \"training\": \"eval\",\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n}\n\nsup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    loss=ppsci.loss.FunctionalLoss(enso_metric.train_mse_func),\n    metric={\n        \"rmse\": ppsci.metric.FunctionalMetric(enso_metric.eval_rmse_func),\n    },\n    name=\"Sup_Validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre> <p><code>SupervisedValidator</code> \u8bc4\u4f30\u5668\u4e0e <code>SupervisedConstraint</code> \u6bd4\u8f83\u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u8bc4\u4f30\u5668\u9700\u8981\u8bbe\u7f6e\u8bc4\u4ef7\u6307\u6807 <code>metric</code>\uff0c\u5728\u8fd9\u91cc\u4f7f\u7528\u4e86\u81ea\u5b9a\u4e49\u7684\u8bc4\u4ef7\u6307\u6807\u5206\u522b\u662f <code>MAE</code>\u3001<code>MSE</code>\u3001<code>RMSE</code>\u3001<code>corr_nino3.4_epoch</code> \u548c <code>corr_nino3.4_weighted_epoch</code>\u3002</p>"},{"location":"zh/examples/earthformer/#325","title":"3.2.5 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/earthformer/earthformer_enso_train.py<pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    seed=cfg.seed,\n    validator=validator,\n    compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/earthformer/#33","title":"3.3 \u6a21\u578b\u8bc4\u4f30\u53ef\u89c6\u5316","text":""},{"location":"zh/examples/earthformer/#331","title":"3.3.1 \u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b","text":"<p>\u6784\u5efa\u6a21\u578b\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code>model = ppsci.arch.CuboidTransformer(\n    **cfg.MODEL,\n)\n</code></pre> <p>\u6784\u5efa\u8bc4\u4f30\u5668\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code># set eval dataloader config\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ENSODataset\",\n        \"data_dir\": cfg.FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.DATASET.label_keys,\n        \"in_len\": cfg.DATASET.in_len,\n        \"out_len\": cfg.DATASET.out_len,\n        \"in_stride\": cfg.DATASET.in_stride,\n        \"out_stride\": cfg.DATASET.out_stride,\n        \"train_samples_gap\": cfg.DATASET.train_samples_gap,\n        \"eval_samples_gap\": cfg.DATASET.eval_samples_gap,\n        \"normalize_sst\": cfg.DATASET.normalize_sst,\n        \"training\": \"test\",\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n}\n\nsup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    loss=ppsci.loss.FunctionalLoss(enso_metric.train_mse_func),\n    metric={\n        \"rmse\": ppsci.metric.FunctionalMetric(enso_metric.eval_rmse_func),\n    },\n    name=\"Sup_Validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre>"},{"location":"zh/examples/earthformer/#332","title":"3.3.2 \u6a21\u578b\u5bfc\u51fa","text":"<p>\u6784\u5efa\u6a21\u578b\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code># set model\nmodel = ppsci.arch.CuboidTransformer(\n    **cfg.MODEL,\n)\n</code></pre> <p>\u5b9e\u4f8b\u5316 <code>ppsci.solver.Solver</code>\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    pretrained_model_path=cfg.INFER.pretrained_model_path,\n)\n</code></pre> <p>\u6784\u5efa\u6a21\u578b\u8f93\u5165\u683c\u5f0f\u5e76\u5bfc\u51fa\u9759\u6001\u6a21\u578b\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code>input_spec = [\n    {\n        key: InputSpec([1, 12, 24, 48, 1], \"float32\", name=key)\n        for key in model.input_keys\n    },\n]\nsolver.export(input_spec, cfg.INFER.export_path)\n</code></pre> <p><code>InputSpec</code> \u51fd\u6570\u4e2d\u7b2c\u4e00\u4e2a\u8bbe\u7f6e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\uff0c\u7b2c\u4e8c\u4e2a\u53c2\u6570\u8bbe\u7f6e\u8f93\u5165\u6570\u636e\u7c7b\u578b\uff0c\u7b2c\u4e09\u4e2a\u8bbe\u7f6e\u8f93\u5165\u6570\u636e\u7684 <code>Key</code>.</p>"},{"location":"zh/examples/earthformer/#333","title":"3.3.3 \u6a21\u578b\u63a8\u7406","text":"<p>\u521b\u5efa\u9884\u6d4b\u5668:</p> examples/earthformer/earthformer_enso_train.py<pre><code>import predictor\n\npredictor = predictor.EarthformerPredictor(cfg)\n</code></pre> <p>\u51c6\u5907\u9884\u6d4b\u6570\u636e\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code>train_cmip = xr.open_dataset(cfg.INFER.data_path).transpose(\n    \"year\", \"month\", \"lat\", \"lon\"\n)\n# select longitudes\nlon = train_cmip.lon.values\nlon = lon[np.logical_and(lon &gt;= 95, lon &lt;= 330)]\ntrain_cmip = train_cmip.sel(lon=lon)\ndata = train_cmip.sst.values\ndata = enso_dataset.fold(data)\n\nidx_sst = enso_dataset.prepare_inputs_targets(\n    len_time=data.shape[0],\n    input_length=cfg.INFER.in_len,\n    input_gap=cfg.INFER.in_stride,\n    pred_shift=cfg.INFER.out_len * cfg.INFER.out_stride,\n    pred_length=cfg.INFER.out_len,\n    samples_gap=cfg.INFER.samples_gap,\n)\ndata = data[idx_sst].astype(\"float32\")\n\nsst_data = data[..., np.newaxis]\nidx = np.random.choice(len(data), None, False)\nin_seq = sst_data[idx, : cfg.INFER.in_len, ...]  # ( in_len, lat, lon, 1)\nin_seq = in_seq[np.newaxis, ...]\n</code></pre> <p>\u8fdb\u884c\u6a21\u578b\u9884\u6d4b\u4e0e\u9884\u6d4b\u503c\u4fdd\u5b58:</p> examples/earthformer/earthformer_enso_train.py<pre><code>pred_data = predictor.predict(in_seq, cfg.INFER.batch_size)\n\n# save predict data\nsave_path = osp.join(cfg.output_dir, \"result_enso_pred.npy\")\nnp.save(save_path, pred_data)\nlogger.info(f\"Save output to {save_path}\")\n</code></pre>"},{"location":"zh/examples/earthformer/#4-vil","title":"4. \u4e91\u603b\u964d\u6c34\u91cf vil \u6a21\u578b\u5b9e\u73b0","text":""},{"location":"zh/examples/earthformer/#41","title":"4.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u6570\u636e\u96c6\u91c7\u7528\u4e86 EarthFormer \u5904\u7406\u597d\u7684 SEVIR \u6570\u636e\u96c6\u3002</p> <p>The Storm Event ImagRy(SEVIR) \u6570\u636e\u96c6\u662f\u7531\u9ebb\u7701\u7406\u5de5\u6797\u80af\u5b9e\u9a8c\u5ba4\u548c\u4e9a\u9a6c\u900a\u6536\u96c6\u5e76\u63d0\u4f9b\u7684\u3002SEVIR \u662f\u4e00\u4e2a\u7ecf\u8fc7\u6ce8\u91ca\u3001\u6574\u7406\u548c\u65f6\u7a7a\u5bf9\u9f50\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b 10,000 \u591a\u4e2a\u5929\u6c14\u4e8b\u4ef6\uff0c\u6bcf\u4e2a\u4e8b\u4ef6\u7531 384 \u5343\u7c73 x 384 \u5343\u7c73\u7684\u56fe\u50cf\u5e8f\u5217\u7ec4\u6210\uff0c\u65f6\u95f4\u8de8\u5ea6\u4e3a 4 \u5c0f\u65f6\u3002SEVIR \u4e2d\u7684\u56fe\u50cf\u901a\u8fc7\u4e94\u79cd\u4e0d\u540c\u7684\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u91c7\u6837\u548c\u5bf9\u9f50\uff1aGOES-16 \u9ad8\u7ea7\u57fa\u7ebf\u6210\u50cf\u4eea\u7684\u4e09\u4e2a\u901a\u9053 (C02\u3001C09\u3001C13)\u3001NEXRAD \u5782\u76f4\u6db2\u6001\u6c34\u542b\u91cf (vil) \u548c GOES-16 \u5730\u7403\u9759\u6b62\u95ea\u7535\u6210\u50cf (GLM) \u95ea\u70c1\u56fe\u3002</p> <p>SEVIR\u6570\u636e\u96c6\u7684\u7ed3\u6784\u5305\u62ec\u4e24\u90e8\u5206\uff1a\u76ee\u5f55 (Catalog) \u548c\u6570\u636e\u6587\u4ef6 (Data File)\u3002\u76ee\u5f55\u662f\u4e00\u4e2a CSV \u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u542b\u63cf\u8ff0\u4e8b\u4ef6\u5143\u6570\u636e\u7684\u884c\u3002\u6570\u636e\u6587\u4ef6\u662f\u4e00\u7ec4 HDF5 \u6587\u4ef6\uff0c\u5305\u542b\u7279\u5b9a\u4f20\u611f\u5668\u7c7b\u578b\u7684\u4e8b\u4ef6\u3002\u8fd9\u4e9b\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u4ee5 4D \u5f20\u91cf\u5f62\u5f0f\u5b58\u50a8\uff0c\u5f62\u72b6\u4e3a N x L x W x T\uff0c\u5176\u4e2d N \u662f\u6587\u4ef6\u4e2d\u7684\u4e8b\u4ef6\u6570\uff0cLxW \u662f\u56fe\u50cf\u5927\u5c0f\uff0cT \u662f\u56fe\u50cf\u5e8f\u5217\u4e2d\u7684\u65f6\u95f4\u6b65\u6570\u3002</p> <p> </p> SEVIR \u4f20\u611f\u5668\u7c7b\u578b\u8bf4\u660e <p>EarthFormer \u91c7\u7528 SEVIR \u4e2d\u7684 NEXRAD \u5782\u76f4\u6db2\u6001\u6c34\u542b\u91cf (VIL) \u4f5c\u4e3a\u964d\u6c34\u9884\u62a5\u7684\u57fa\u51c6\uff0c\u5373\u5728 65 \u5206\u949f\u7684\u5782\u76f4\u7efc\u5408\u6db2\u4f53\u80cc\u666f\u4e0b\uff0c\u9884\u6d4b\u672a\u6765 60 \u5206\u949f\u7684\u5782\u76f4\u7efc\u5408\u6db2\u4f53\u3002\u56e0\u6b64\uff0c\u5206\u8fa8\u7387\u4e3a 13x384x384\u219212x384x384\u3002</p>"},{"location":"zh/examples/earthformer/#42","title":"4.2 \u6a21\u578b\u9884\u8bad\u7ec3","text":""},{"location":"zh/examples/earthformer/#421","title":"4.2.1 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\u3002</p> <p>\u6570\u636e\u52a0\u8f7d\u7684\u4ee3\u7801\u5982\u4e0b:</p> examples/earthformer/earthformer_sevir_train.py<pre><code># set train dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"SEVIRDataset\",\n        \"data_dir\": cfg.FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.DATASET.label_keys,\n        \"data_types\": cfg.DATASET.data_types,\n        \"seq_len\": cfg.DATASET.seq_len,\n        \"raw_seq_len\": cfg.DATASET.raw_seq_len,\n        \"sample_mode\": cfg.DATASET.sample_mode,\n        \"stride\": cfg.DATASET.stride,\n        \"batch_size\": cfg.DATASET.batch_size,\n        \"layout\": cfg.DATASET.layout,\n        \"in_len\": cfg.DATASET.in_len,\n        \"out_len\": cfg.DATASET.out_len,\n        \"split_mode\": cfg.DATASET.split_mode,\n        \"start_date\": cfg.TRAIN.start_date,\n        \"end_date\": cfg.TRAIN.end_date,\n        \"preprocess\": cfg.DATASET.preprocess,\n        \"rescale_method\": cfg.DATASET.rescale_method,\n        \"shuffle\": True,\n        \"verbose\": False,\n        \"training\": True,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 8,\n}\n</code></pre> <p>\u5176\u4e2d\uff0c\"dataset\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Dataset</code> \u7c7b\u540d\u4e3a <code>ENSODataset</code>\uff0c\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u8bbe\u7f6e\u7684 <code>batch_size</code> \u4e3a 1\uff0c<code>num_works</code> \u4e3a 8\u3002</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code># set constraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    loss=ppsci.loss.FunctionalLoss(sevir_metric.train_mse_func),\n    name=\"Sup\",\n)\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u6570\u636e\u7684\u52a0\u8f7d\u65b9\u5f0f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e0a\u6587\u4e2d\u5b9a\u4e49\u7684 <code>train_dataloader_cfg</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684\u635f\u5931\u51fd\u6570 <code>mse_loss</code>\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a <code>Sup</code>\u3002</p>"},{"location":"zh/examples/earthformer/#422","title":"4.2.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0c\u4e91\u603b\u964d\u6c34\u91cf\u6a21\u578b\u57fa\u4e8e CuboidTransformer \u7f51\u7edc\u6a21\u578b\u5b9e\u73b0\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code>model = ppsci.arch.CuboidTransformer(\n    **cfg.MODEL,\n)\n</code></pre> <p>\u5b9a\u4e49\u6a21\u578b\u7684\u53c2\u6570\u901a\u8fc7\u914d\u7f6e\u8fdb\u884c\u8bbe\u7f6e\uff0c\u5982\u4e0b\uff1a</p> examples/earthformer/conf/earthformer_sevir_pretrain.yaml<pre><code>MODEL:\n  input_keys: [\"input\"]\n  output_keys: [\"vil\"]\n  input_shape: [13, 384, 384, 1]\n  target_shape: [12, 384, 384, 1]\n  base_units: 128\n  scale_alpha: 1.0\n\n  enc_depth: [1, 1]\n  dec_depth: [1, 1]\n  enc_use_inter_ffn: true\n  dec_use_inter_ffn: true\n  dec_hierarchical_pos_embed: false\n\n  downsample: 2\n  downsample_type: \"patch_merge\"\n  upsample_type: \"upsample\"\n\n  num_global_vectors: 8\n  use_dec_self_global: false\n  dec_self_update_global: true\n  use_dec_cross_global: false\n  use_global_vector_ffn: false\n  use_global_self_attn: true\n  separate_global_qkv: true\n  global_dim_ratio: 1\n\n  self_pattern: \"axial\"\n  cross_self_pattern: \"axial\"\n  cross_pattern: \"cross_1x1\"\n  dec_cross_last_n_frames: null\n\n  attn_drop: 0.1\n  proj_drop: 0.1\n  ffn_drop: 0.1\n  num_heads: 4\n\n  ffn_activation: \"gelu\"\n  gated_ffn: false\n  norm_layer: \"layer_norm\"\n  padding_type: \"zeros\"\n  pos_embed_type: \"t+h+w\"\n  use_relative_pos: true\n  self_attn_use_final_proj: true\n  dec_use_first_self_attn: false\n\n  z_init_method: \"zeros\"\n  initial_downsample_type: \"stack_conv\"\n  initial_downsample_activation: \"leaky_relu\"\n  initial_downsample_stack_conv_num_layers: 3\n  initial_downsample_stack_conv_dim_list: [16, 64, 128]\n  initial_downsample_stack_conv_downscale_list: [3, 2, 2]\n  initial_downsample_stack_conv_num_conv_list: [2, 2, 2]\n  checkpoint_level: 2\n\n  attn_linear_init_mode: \"0\"\n  ffn_linear_init_mode: \"0\"\n  conv_init_mode: \"0\"\n  down_up_linear_init_mode: \"0\"\n  norm_init_mode: \"0\"\n</code></pre> <p>\u5176\u4e2d\uff0c<code>input_keys</code> \u548c <code>output_keys</code> \u5206\u522b\u4ee3\u8868\u7f51\u7edc\u6a21\u578b\u8f93\u5165\u3001\u8f93\u51fa\u53d8\u91cf\u7684\u540d\u79f0\u3002</p>"},{"location":"zh/examples/earthformer/#423","title":"4.2.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>Cosine</code>\uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a <code>1e-3</code>\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>AdamW</code>\uff0c\u5e76\u5c06\u53c2\u6570\u8fdb\u884c\u5206\u7ec4\uff0c\u4f7f\u7528\u4e0d\u540c\u7684 <code>weight_decay</code>,\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code>decay_parameters = get_parameter_names(model, [nn.LayerNorm])\ndecay_parameters = [name for name in decay_parameters if \"bias\" not in name]\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n        \"weight_decay\": cfg.TRAIN.wd,\n    },\n    {\n        \"params\": [\n            p for n, p in model.named_parameters() if n not in decay_parameters\n        ],\n        \"weight_decay\": 0.0,\n    },\n]\n\n# init optimizer and lr scheduler\nlr_scheduler_cfg = dict(cfg.TRAIN.lr_scheduler)\nlr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(\n    **lr_scheduler_cfg,\n    iters_per_epoch=ITERS_PER_EPOCH,\n    eta_min=cfg.TRAIN.min_lr_ratio * cfg.TRAIN.lr_scheduler.learning_rate,\n    warmup_epoch=int(0.2 * cfg.TRAIN.epochs),\n)()\noptimizer = paddle.optimizer.AdamW(\n    lr_scheduler, parameters=optimizer_grouped_parameters\n)\n</code></pre>"},{"location":"zh/examples/earthformer/#424","title":"4.2.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code># set eval dataloader config\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"SEVIRDataset\",\n        \"data_dir\": cfg.FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.DATASET.label_keys,\n        \"data_types\": cfg.DATASET.data_types,\n        \"seq_len\": cfg.DATASET.seq_len,\n        \"raw_seq_len\": cfg.DATASET.raw_seq_len,\n        \"sample_mode\": cfg.DATASET.sample_mode,\n        \"stride\": cfg.DATASET.stride,\n        \"batch_size\": cfg.DATASET.batch_size,\n        \"layout\": cfg.DATASET.layout,\n        \"in_len\": cfg.DATASET.in_len,\n        \"out_len\": cfg.DATASET.out_len,\n        \"split_mode\": cfg.DATASET.split_mode,\n        \"start_date\": cfg.TRAIN.end_date,\n        \"end_date\": cfg.EVAL.end_date,\n        \"preprocess\": cfg.DATASET.preprocess,\n        \"rescale_method\": cfg.DATASET.rescale_method,\n        \"shuffle\": False,\n        \"verbose\": False,\n        \"training\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n}\n\nsup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    loss=ppsci.loss.MSELoss(),\n    metric={\n        \"rmse\": ppsci.metric.FunctionalMetric(\n            sevir_metric.eval_rmse_func(\n                out_len=cfg.DATASET.seq_len,\n                layout=cfg.DATASET.layout,\n                metrics_mode=cfg.EVAL.metrics_mode,\n                metrics_list=cfg.EVAL.metrics_list,\n                threshold_list=cfg.EVAL.threshold_list,\n            )\n        ),\n    },\n    name=\"Sup_Validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre> <p><code>SupervisedValidator</code> \u8bc4\u4f30\u5668\u4e0e <code>SupervisedConstraint</code> \u6bd4\u8f83\u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u8bc4\u4f30\u5668\u9700\u8981\u8bbe\u7f6e\u8bc4\u4ef7\u6307\u6807 <code>metric</code>\uff0c\u5728\u8fd9\u91cc\u4f7f\u7528\u4e86\u81ea\u5b9a\u4e49\u7684\u8bc4\u4ef7\u6307\u6807\u5206\u522b\u662f <code>MAE</code>\u3001<code>MSE</code>\u3001<code>csi</code>\u3001<code>pod</code>\u3001<code>sucr</code>\u548c <code>bias</code>\uff0c\u4e14\u540e\u56db\u4e2a\u8bc4\u4ef7\u6307\u6807\u5206\u522b\u4f7f\u7528\u4e0d\u540c\u7684\u9608\u503c <code>[16,74,133,160,181,219]</code>\u3002</p>"},{"location":"zh/examples/earthformer/#425","title":"4.2.5 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3002</p> examples/earthformer/earthformer_sevir_train.py<pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    seed=cfg.seed,\n    validator=validator,\n    compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n# train model\nsolver.train()\n</code></pre>"},{"location":"zh/examples/earthformer/#426","title":"4.2.6 \u6a21\u578b\u8bc4\u4f30","text":"<p>\u7531\u4e8e\u76ee\u524d <code>paddlescience</code> \u4e2d\u7684\u9a8c\u8bc1\u7b56\u7565\u5206\u4e3a\u4e24\u7c7b\uff0c\u4e00\u7c7b\u662f\u76f4\u63a5\u5bf9\u9a8c\u8bc1\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u578b\u8f93\u51fa\u62fc\u63a5\uff0c\u7136\u540e\u8ba1\u7b97\u8bc4\u4ef7\u6307\u6807\u3002\u53e6\u4e00\u7c7b\u662f\u6309\u7167\u6bcf\u4e2a batch_size \u8ba1\u7b97\u8bc4\u4ef7\u6307\u6807\uff0c\u7136\u540e\u62fc\u63a5\uff0c\u6700\u540e\u5bf9\u6240\u6709\u7ed3\u679c\u6c42\u5e73\u5747\uff0c\u8be5\u65b9\u6cd5\u9ed8\u8ba4\u6570\u636e\u4e4b\u95f4\u6ca1\u6709\u5173\u8054\u6027\u3002\u4f46\u662f <code>SEVIR</code> \u6570\u636e\u96c6\u6570\u636e\u4e4b\u95f4\u6709\u5173\u8054\u6027\uff0c\u6240\u4ee5\u4e0d\u9002\u7528\u7b2c\u4e8c\u79cd\u65b9\u6cd5\uff1b\u53c8\u7531\u4e8e <code>SEVIR</code> \u6570\u636e\u96c6\u91cf\u5927\uff0c\u4f7f\u7528\u7b2c\u4e00\u79cd\u65b9\u6cd5\u9a8c\u8bc1\u663e\u5b58\u9700\u6c42\u5927\uff0c\u56e0\u6b64\u9a8c\u8bc1 <code>SEVIR</code> \u6570\u636e\u96c6\u4f7f\u7528\u7684\u65b9\u6cd5\u5982\u4e0b\uff1a</p> <ul> <li>1.\u5bf9\u4e00\u4e2a batch size \u8ba1\u7b97 <code>hits</code>\u3001<code>misses</code> \u548c <code>fas</code> \u4e09\u4e2a\u6570\u636e</li> <li>2.\u5bf9\u6570\u636e\u96c6\u6240\u6709\u6570\u636e\u4fdd\u5b58\u6240\u6709 <code>batch</code> \u7684\u4e09\u4e2a\u503c\u7684\u7d2f\u52a0\u548c.</li> <li>3.\u5bf9\u4e09\u4e2a\u503c\u7684\u7d2f\u52a0\u548c\u8ba1\u7b97 <code>csi</code>\u3001<code>pod</code>\u3001<code>sucr</code>\u548c <code>bias</code> \u56db\u4e2a\u6307\u6807\u3002</li> </ul> examples/earthformer/earthformer_sevir_train.py<pre><code># evaluate after finished training\nmetric = sevir_metric.eval_rmse_func(\n    out_len=cfg.DATASET.seq_len,\n    layout=cfg.DATASET.layout,\n    metrics_mode=cfg.EVAL.metrics_mode,\n    metrics_list=cfg.EVAL.metrics_list,\n    threshold_list=cfg.EVAL.threshold_list,\n)\n\nwith solver.no_grad_context_manager(True):\n    for index, (input_, label, _) in enumerate(sup_validator.data_loader):\n        truefield = label[\"vil\"].squeeze(0)\n        prefield = model(input_)[\"vil\"].squeeze(0)\n        metric.sevir_score.update(prefield, truefield)\n\nmetric_dict = metric.sevir_score.compute()\nprint(metric_dict)\n</code></pre>"},{"location":"zh/examples/earthformer/#43","title":"4.3 \u6a21\u578b\u8bc4\u4f30\u53ef\u89c6\u5316","text":""},{"location":"zh/examples/earthformer/#431","title":"4.3.1 \u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b","text":"<p>\u6784\u5efa\u6a21\u578b\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code>model = ppsci.arch.CuboidTransformer(\n    **cfg.MODEL,\n)\n</code></pre> <p>\u6784\u5efa\u8bc4\u4f30\u5668\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code># set eval dataloader config\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"SEVIRDataset\",\n        \"data_dir\": cfg.FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.DATASET.label_keys,\n        \"data_types\": cfg.DATASET.data_types,\n        \"seq_len\": cfg.DATASET.seq_len,\n        \"raw_seq_len\": cfg.DATASET.raw_seq_len,\n        \"sample_mode\": cfg.DATASET.sample_mode,\n        \"stride\": cfg.DATASET.stride,\n        \"batch_size\": cfg.DATASET.batch_size,\n        \"layout\": cfg.DATASET.layout,\n        \"in_len\": cfg.DATASET.in_len,\n        \"out_len\": cfg.DATASET.out_len,\n        \"split_mode\": cfg.DATASET.split_mode,\n        \"start_date\": cfg.TEST.start_date,\n        \"end_date\": cfg.TEST.end_date,\n        \"preprocess\": cfg.DATASET.preprocess,\n        \"rescale_method\": cfg.DATASET.rescale_method,\n        \"shuffle\": False,\n        \"verbose\": False,\n        \"training\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n}\n\nsup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    loss=ppsci.loss.MSELoss(),\n    metric={\n        \"rmse\": ppsci.metric.FunctionalMetric(\n            sevir_metric.eval_rmse_func(\n                out_len=cfg.DATASET.seq_len,\n                layout=cfg.DATASET.layout,\n                metrics_mode=cfg.EVAL.metrics_mode,\n                metrics_list=cfg.EVAL.metrics_list,\n                threshold_list=cfg.EVAL.threshold_list,\n            )\n        ),\n    },\n    name=\"Sup_Validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre> <p>\u6a21\u578b\u8bc4\u4f30\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code># evaluate\nmetric = sevir_metric.eval_rmse_func(\n    out_len=cfg.DATASET.seq_len,\n    layout=cfg.DATASET.layout,\n    metrics_mode=cfg.EVAL.metrics_mode,\n    metrics_list=cfg.EVAL.metrics_list,\n    threshold_list=cfg.EVAL.threshold_list,\n)\n\nwith solver.no_grad_context_manager(True):\n    for index, (input_, label, _) in enumerate(sup_validator.data_loader):\n        truefield = label[\"vil\"].reshape([-1, *label[\"vil\"].shape[2:]])\n        prefield = model(input_)[\"vil\"].reshape([-1, *label[\"vil\"].shape[2:]])\n        metric.sevir_score.update(prefield, truefield)\n\nmetric_dict = metric.sevir_score.compute()\nprint(metric_dict)\n</code></pre>"},{"location":"zh/examples/earthformer/#432","title":"4.3.2 \u6a21\u578b\u5bfc\u51fa","text":"<p>\u6784\u5efa\u6a21\u578b\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code># set model\nmodel = ppsci.arch.CuboidTransformer(\n    **cfg.MODEL,\n)\n</code></pre> <p>\u5b9e\u4f8b\u5316 <code>ppsci.solver.Solver</code>\uff1a</p> examples/earthformer/earthformer_enso_train.py<pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    pretrained_model_path=cfg.INFER.pretrained_model_path,\n)\n</code></pre> <p>\u6784\u5efa\u6a21\u578b\u8f93\u5165\u683c\u5f0f\u5e76\u5bfc\u51fa\u9759\u6001\u6a21\u578b\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code>input_spec = [\n    {\n        key: InputSpec([1, 13, 384, 384, 1], \"float32\", name=key)\n        for key in model.input_keys\n    },\n]\nsolver.export(input_spec, cfg.INFER.export_path)\n</code></pre> <p><code>InputSpec</code> \u51fd\u6570\u4e2d\u7b2c\u4e00\u4e2a\u8bbe\u7f6e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\uff0c\u7b2c\u4e8c\u4e2a\u53c2\u6570\u8bbe\u7f6e\u8f93\u5165\u6570\u636e\u7c7b\u578b\uff0c\u7b2c\u4e09\u4e2a\u8bbe\u7f6e\u8f93\u5165\u6570\u636e\u7684 <code>Key</code>.</p>"},{"location":"zh/examples/earthformer/#433","title":"4.3.3 \u6a21\u578b\u63a8\u7406","text":"<p>\u521b\u5efa\u9884\u6d4b\u5668:</p> examples/earthformer/earthformer_sevir_train.py<pre><code>predictor = predictor.EarthformerPredictor(cfg)\n</code></pre> <p>\u51c6\u5907\u9884\u6d4b\u6570\u636e\u5e76\u8fdb\u884c\u5bf9\u5e94\u6a21\u5f0f\u7684\u6570\u636e\u9884\u5904\u7406\uff1a</p> examples/earthformer/earthformer_sevir_train.py<pre><code>if cfg.INFER.rescale_method == \"sevir\":\n    scale_dict = sevir_dataset.PREPROCESS_SCALE_SEVIR\n    offset_dict = sevir_dataset.PREPROCESS_OFFSET_SEVIR\nelif cfg.INFER.rescale_method == \"01\":\n    scale_dict = sevir_dataset.PREPROCESS_SCALE_01\n    offset_dict = sevir_dataset.PREPROCESS_OFFSET_01\nelse:\n    raise ValueError(f\"Invalid rescale option: {cfg.INFER.rescale_method}.\")\n\n# read h5 data\nh5data = h5py.File(cfg.INFER.data_path, \"r\")\ndata = np.array(h5data[cfg.INFER.data_type]).transpose([0, 3, 1, 2])\n\nidx = np.random.choice(len(data), None, False)\ndata = (\n    scale_dict[cfg.INFER.data_type] * data[idx] + offset_dict[cfg.INFER.data_type]\n)\n\ninput_data = data[: cfg.INFER.in_len, ...]\ninput_data = input_data.reshape(1, *input_data.shape, 1).astype(np.float32)\n</code></pre> <p>\u8fdb\u884c\u6a21\u578b\u9884\u6d4b\u5e76\u53ef\u89c6\u5316:</p> examples/earthformer/earthformer_sevir_train.py<pre><code>pred_data = predictor.predict(input_data, cfg.INFER.batch_size)\n\nsevir_vis_seq.save_example_vis_results(\n    save_dir=cfg.INFER.sevir_vis_save,\n    save_prefix=f\"data_{idx}\",\n    in_seq=input_data,\n    target_seq=target_data,\n    pred_seq=pred_data,\n    layout=cfg.INFER.layout,\n    plot_stride=cfg.INFER.plot_stride,\n    label=cfg.INFER.logging_prefix,\n    interval_real_time=cfg.INFER.interval_real_time,\n)\n</code></pre>"},{"location":"zh/examples/earthformer/#5","title":"5. \u5b8c\u6574\u4ee3\u7801","text":"examples/earthformer/earthformer_enso_train.py<pre><code>from os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\nfrom paddle import nn\n\nimport examples.earthformer.enso_metric as enso_metric\nimport ppsci\nfrom ppsci.data.dataset import enso_dataset\nfrom ppsci.utils import logger\n\ntry:\n    import xarray as xr\nexcept ModuleNotFoundError:\n    raise ModuleNotFoundError(\"Please install xarray with `pip install xarray`.\")\n\n\ndef get_parameter_names(model, forbidden_layer_types):\n    result = []\n    for name, child in model.named_children():\n        result += [\n            f\"{name}.{n}\"\n            for n in get_parameter_names(child, forbidden_layer_types)\n            if not isinstance(child, tuple(forbidden_layer_types))\n        ]\n    # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n    result += list(model._parameters.keys())\n    return result\n\n\ndef train(cfg: DictConfig):\n    # set train dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ENSODataset\",\n            \"data_dir\": cfg.FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.DATASET.label_keys,\n            \"in_len\": cfg.DATASET.in_len,\n            \"out_len\": cfg.DATASET.out_len,\n            \"in_stride\": cfg.DATASET.in_stride,\n            \"out_stride\": cfg.DATASET.out_stride,\n            \"train_samples_gap\": cfg.DATASET.train_samples_gap,\n            \"eval_samples_gap\": cfg.DATASET.eval_samples_gap,\n            \"normalize_sst\": cfg.DATASET.normalize_sst,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 8,\n    }\n\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(enso_metric.train_mse_func),\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(sup_constraint.data_loader)\n    # set eval dataloader config\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ENSODataset\",\n            \"data_dir\": cfg.FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.DATASET.label_keys,\n            \"in_len\": cfg.DATASET.in_len,\n            \"out_len\": cfg.DATASET.out_len,\n            \"in_stride\": cfg.DATASET.in_stride,\n            \"out_stride\": cfg.DATASET.out_stride,\n            \"train_samples_gap\": cfg.DATASET.train_samples_gap,\n            \"eval_samples_gap\": cfg.DATASET.eval_samples_gap,\n            \"normalize_sst\": cfg.DATASET.normalize_sst,\n            \"training\": \"eval\",\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(enso_metric.train_mse_func),\n        metric={\n            \"rmse\": ppsci.metric.FunctionalMetric(enso_metric.eval_rmse_func),\n        },\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    model = ppsci.arch.CuboidTransformer(\n        **cfg.MODEL,\n    )\n\n    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n            \"weight_decay\": cfg.TRAIN.wd,\n        },\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if n not in decay_parameters\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    # # init optimizer and lr scheduler\n    lr_scheduler_cfg = dict(cfg.TRAIN.lr_scheduler)\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(\n        **lr_scheduler_cfg,\n        iters_per_epoch=ITERS_PER_EPOCH,\n        eta_min=cfg.TRAIN.min_lr_ratio * cfg.TRAIN.lr_scheduler.learning_rate,\n        warmup_epoch=int(0.2 * cfg.TRAIN.epochs),\n    )()\n    optimizer = paddle.optimizer.AdamW(\n        lr_scheduler, parameters=optimizer_grouped_parameters\n    )\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        seed=cfg.seed,\n        validator=validator,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    # set eval dataloader config\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ENSODataset\",\n            \"data_dir\": cfg.FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.DATASET.label_keys,\n            \"in_len\": cfg.DATASET.in_len,\n            \"out_len\": cfg.DATASET.out_len,\n            \"in_stride\": cfg.DATASET.in_stride,\n            \"out_stride\": cfg.DATASET.out_stride,\n            \"train_samples_gap\": cfg.DATASET.train_samples_gap,\n            \"eval_samples_gap\": cfg.DATASET.eval_samples_gap,\n            \"normalize_sst\": cfg.DATASET.normalize_sst,\n            \"training\": \"test\",\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(enso_metric.train_mse_func),\n        metric={\n            \"rmse\": ppsci.metric.FunctionalMetric(enso_metric.eval_rmse_func),\n        },\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    model = ppsci.arch.CuboidTransformer(\n        **cfg.MODEL,\n    )\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate\n    solver.eval()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.CuboidTransformer(\n        **cfg.MODEL,\n    )\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {\n            key: InputSpec([1, 12, 24, 48, 1], \"float32\", name=key)\n            for key in model.input_keys\n        },\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    import predictor\n\n    predictor = predictor.EarthformerPredictor(cfg)\n\n    train_cmip = xr.open_dataset(cfg.INFER.data_path).transpose(\n        \"year\", \"month\", \"lat\", \"lon\"\n    )\n    # select longitudes\n    lon = train_cmip.lon.values\n    lon = lon[np.logical_and(lon &gt;= 95, lon &lt;= 330)]\n    train_cmip = train_cmip.sel(lon=lon)\n    data = train_cmip.sst.values\n    data = enso_dataset.fold(data)\n\n    idx_sst = enso_dataset.prepare_inputs_targets(\n        len_time=data.shape[0],\n        input_length=cfg.INFER.in_len,\n        input_gap=cfg.INFER.in_stride,\n        pred_shift=cfg.INFER.out_len * cfg.INFER.out_stride,\n        pred_length=cfg.INFER.out_len,\n        samples_gap=cfg.INFER.samples_gap,\n    )\n    data = data[idx_sst].astype(\"float32\")\n\n    sst_data = data[..., np.newaxis]\n    idx = np.random.choice(len(data), None, False)\n    in_seq = sst_data[idx, : cfg.INFER.in_len, ...]  # ( in_len, lat, lon, 1)\n    in_seq = in_seq[np.newaxis, ...]\n    target_seq = sst_data[idx, cfg.INFER.in_len :, ...]  # ( out_len, lat, lon, 1)\n    target_seq = target_seq[np.newaxis, ...]\n\n    pred_data = predictor.predict(in_seq, cfg.INFER.batch_size)\n\n    # save predict data\n    save_path = osp.join(cfg.output_dir, \"result_enso_pred.npy\")\n    np.save(save_path, pred_data)\n    logger.info(f\"Save output to {save_path}\")\n\n\n@hydra.main(\n    version_base=None,\n    config_path=\"./conf\",\n    config_name=\"earthformer_enso_pretrain.yaml\",\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> examples/earthformer/earthformer_sevir_train.py<pre><code>import h5py\nimport hydra\nimport numpy as np\nimport paddle\nimport sevir_metric\nimport sevir_vis_seq\nfrom omegaconf import DictConfig\nfrom paddle import nn\n\nimport ppsci\n\n\ndef get_parameter_names(model, forbidden_layer_types):\n    result = []\n    for name, child in model.named_children():\n        result += [\n            f\"{name}.{n}\"\n            for n in get_parameter_names(child, forbidden_layer_types)\n            if not isinstance(child, tuple(forbidden_layer_types))\n        ]\n    # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n    result += list(model._parameters.keys())\n    return result\n\n\ndef train(cfg: DictConfig):\n    # set train dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"SEVIRDataset\",\n            \"data_dir\": cfg.FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.DATASET.label_keys,\n            \"data_types\": cfg.DATASET.data_types,\n            \"seq_len\": cfg.DATASET.seq_len,\n            \"raw_seq_len\": cfg.DATASET.raw_seq_len,\n            \"sample_mode\": cfg.DATASET.sample_mode,\n            \"stride\": cfg.DATASET.stride,\n            \"batch_size\": cfg.DATASET.batch_size,\n            \"layout\": cfg.DATASET.layout,\n            \"in_len\": cfg.DATASET.in_len,\n            \"out_len\": cfg.DATASET.out_len,\n            \"split_mode\": cfg.DATASET.split_mode,\n            \"start_date\": cfg.TRAIN.start_date,\n            \"end_date\": cfg.TRAIN.end_date,\n            \"preprocess\": cfg.DATASET.preprocess,\n            \"rescale_method\": cfg.DATASET.rescale_method,\n            \"shuffle\": True,\n            \"verbose\": False,\n            \"training\": True,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 8,\n    }\n\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(sevir_metric.train_mse_func),\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(sup_constraint.data_loader)\n    # set eval dataloader config\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"SEVIRDataset\",\n            \"data_dir\": cfg.FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.DATASET.label_keys,\n            \"data_types\": cfg.DATASET.data_types,\n            \"seq_len\": cfg.DATASET.seq_len,\n            \"raw_seq_len\": cfg.DATASET.raw_seq_len,\n            \"sample_mode\": cfg.DATASET.sample_mode,\n            \"stride\": cfg.DATASET.stride,\n            \"batch_size\": cfg.DATASET.batch_size,\n            \"layout\": cfg.DATASET.layout,\n            \"in_len\": cfg.DATASET.in_len,\n            \"out_len\": cfg.DATASET.out_len,\n            \"split_mode\": cfg.DATASET.split_mode,\n            \"start_date\": cfg.TRAIN.end_date,\n            \"end_date\": cfg.EVAL.end_date,\n            \"preprocess\": cfg.DATASET.preprocess,\n            \"rescale_method\": cfg.DATASET.rescale_method,\n            \"shuffle\": False,\n            \"verbose\": False,\n            \"training\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.MSELoss(),\n        metric={\n            \"rmse\": ppsci.metric.FunctionalMetric(\n                sevir_metric.eval_rmse_func(\n                    out_len=cfg.DATASET.seq_len,\n                    layout=cfg.DATASET.layout,\n                    metrics_mode=cfg.EVAL.metrics_mode,\n                    metrics_list=cfg.EVAL.metrics_list,\n                    threshold_list=cfg.EVAL.threshold_list,\n                )\n            ),\n        },\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    model = ppsci.arch.CuboidTransformer(\n        **cfg.MODEL,\n    )\n\n    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n            \"weight_decay\": cfg.TRAIN.wd,\n        },\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if n not in decay_parameters\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    # init optimizer and lr scheduler\n    lr_scheduler_cfg = dict(cfg.TRAIN.lr_scheduler)\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(\n        **lr_scheduler_cfg,\n        iters_per_epoch=ITERS_PER_EPOCH,\n        eta_min=cfg.TRAIN.min_lr_ratio * cfg.TRAIN.lr_scheduler.learning_rate,\n        warmup_epoch=int(0.2 * cfg.TRAIN.epochs),\n    )()\n    optimizer = paddle.optimizer.AdamW(\n        lr_scheduler, parameters=optimizer_grouped_parameters\n    )\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        seed=cfg.seed,\n        validator=validator,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    metric = sevir_metric.eval_rmse_func(\n        out_len=cfg.DATASET.seq_len,\n        layout=cfg.DATASET.layout,\n        metrics_mode=cfg.EVAL.metrics_mode,\n        metrics_list=cfg.EVAL.metrics_list,\n        threshold_list=cfg.EVAL.threshold_list,\n    )\n\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(sup_validator.data_loader):\n            truefield = label[\"vil\"].squeeze(0)\n            prefield = model(input_)[\"vil\"].squeeze(0)\n            metric.sevir_score.update(prefield, truefield)\n\n    metric_dict = metric.sevir_score.compute()\n    print(metric_dict)\n\n\ndef evaluate(cfg: DictConfig):\n    # set eval dataloader config\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"SEVIRDataset\",\n            \"data_dir\": cfg.FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.DATASET.label_keys,\n            \"data_types\": cfg.DATASET.data_types,\n            \"seq_len\": cfg.DATASET.seq_len,\n            \"raw_seq_len\": cfg.DATASET.raw_seq_len,\n            \"sample_mode\": cfg.DATASET.sample_mode,\n            \"stride\": cfg.DATASET.stride,\n            \"batch_size\": cfg.DATASET.batch_size,\n            \"layout\": cfg.DATASET.layout,\n            \"in_len\": cfg.DATASET.in_len,\n            \"out_len\": cfg.DATASET.out_len,\n            \"split_mode\": cfg.DATASET.split_mode,\n            \"start_date\": cfg.TEST.start_date,\n            \"end_date\": cfg.TEST.end_date,\n            \"preprocess\": cfg.DATASET.preprocess,\n            \"rescale_method\": cfg.DATASET.rescale_method,\n            \"shuffle\": False,\n            \"verbose\": False,\n            \"training\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.MSELoss(),\n        metric={\n            \"rmse\": ppsci.metric.FunctionalMetric(\n                sevir_metric.eval_rmse_func(\n                    out_len=cfg.DATASET.seq_len,\n                    layout=cfg.DATASET.layout,\n                    metrics_mode=cfg.EVAL.metrics_mode,\n                    metrics_list=cfg.EVAL.metrics_list,\n                    threshold_list=cfg.EVAL.threshold_list,\n                )\n            ),\n        },\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    model = ppsci.arch.CuboidTransformer(\n        **cfg.MODEL,\n    )\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate\n    metric = sevir_metric.eval_rmse_func(\n        out_len=cfg.DATASET.seq_len,\n        layout=cfg.DATASET.layout,\n        metrics_mode=cfg.EVAL.metrics_mode,\n        metrics_list=cfg.EVAL.metrics_list,\n        threshold_list=cfg.EVAL.threshold_list,\n    )\n\n    with solver.no_grad_context_manager(True):\n        for index, (input_, label, _) in enumerate(sup_validator.data_loader):\n            truefield = label[\"vil\"].reshape([-1, *label[\"vil\"].shape[2:]])\n            prefield = model(input_)[\"vil\"].reshape([-1, *label[\"vil\"].shape[2:]])\n            metric.sevir_score.update(prefield, truefield)\n\n    metric_dict = metric.sevir_score.compute()\n    print(metric_dict)\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.CuboidTransformer(\n        **cfg.MODEL,\n    )\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {\n            key: InputSpec([1, 13, 384, 384, 1], \"float32\", name=key)\n            for key in model.input_keys\n        },\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    import predictor\n\n    from ppsci.data.dataset import sevir_dataset\n\n    predictor = predictor.EarthformerPredictor(cfg)\n\n    if cfg.INFER.rescale_method == \"sevir\":\n        scale_dict = sevir_dataset.PREPROCESS_SCALE_SEVIR\n        offset_dict = sevir_dataset.PREPROCESS_OFFSET_SEVIR\n    elif cfg.INFER.rescale_method == \"01\":\n        scale_dict = sevir_dataset.PREPROCESS_SCALE_01\n        offset_dict = sevir_dataset.PREPROCESS_OFFSET_01\n    else:\n        raise ValueError(f\"Invalid rescale option: {cfg.INFER.rescale_method}.\")\n\n    # read h5 data\n    h5data = h5py.File(cfg.INFER.data_path, \"r\")\n    data = np.array(h5data[cfg.INFER.data_type]).transpose([0, 3, 1, 2])\n\n    idx = np.random.choice(len(data), None, False)\n    data = (\n        scale_dict[cfg.INFER.data_type] * data[idx] + offset_dict[cfg.INFER.data_type]\n    )\n\n    input_data = data[: cfg.INFER.in_len, ...]\n    input_data = input_data.reshape(1, *input_data.shape, 1).astype(np.float32)\n    target_data = data[cfg.INFER.in_len : cfg.INFER.in_len + cfg.INFER.out_len, ...]\n    target_data = target_data.reshape(1, *target_data.shape, 1).astype(np.float32)\n\n    pred_data = predictor.predict(input_data, cfg.INFER.batch_size)\n\n    sevir_vis_seq.save_example_vis_results(\n        save_dir=cfg.INFER.sevir_vis_save,\n        save_prefix=f\"data_{idx}\",\n        in_seq=input_data,\n        target_seq=target_data,\n        pred_seq=pred_data,\n        layout=cfg.INFER.layout,\n        plot_stride=cfg.INFER.plot_stride,\n        label=cfg.INFER.logging_prefix,\n        interval_real_time=cfg.INFER.interval_real_time,\n    )\n\n\n@hydra.main(\n    version_base=None,\n    config_path=\"./conf\",\n    config_name=\"earthformer_sevir_pretrain.yaml\",\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/earthformer/#6","title":"6. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u56fe\u5c55\u793a\u4e86\u4e91\u603b\u964d\u6c34\u91cf\u6a21\u578b\u6309\u716765\u5206\u949f\u7684\u8f93\u5165\u6570\u636e\uff0c\u5f97\u523060\u5206\u949f\u95f4\u9694\u7684\u9884\u6d4b\u7ed3\u679c\u548c\u771f\u503c\u7ed3\u679c\u3002</p> <p> </p> SEVIR \u4e2d vil \u7684\u9884\u6d4b\u7ed3\u679c\uff08\"prediction\"\uff09\u4e0e\u771f\u503c\u7ed3\u679c\uff08\"target\"\uff09 <p>\u8bf4\u660e\uff1a</p> <p>Hit:TP, Miss:FN, False Alarm\uff1aFP</p> <p>\u7b2c\u4e00\u884c: \u8f93\u5165\u6570\u636e\uff1b</p> <p>\u7b2c\u4e8c\u884c: \u771f\u503c\u7ed3\u679c\uff1b</p> <p>\u7b2c\u4e09\u884c: \u9884\u6d4b\u7ed3\u679c\uff1b</p> <p>\u7b2c\u56db\u884c: \u8bbe\u5b9a\u9608\u503c\u4e3a <code>74</code> \u60c5\u51b5\u4e0b\uff0cTP\u3001FN\u3001FP \u4e09\u79cd\u60c5\u51b5\u6807\u8bb0</p> <p>\u7b2c\u4e94\u884c\uff1a \u5728\u6240\u6709\u9608\u503c\u60c5\u51b5\u4e0b\uff0cTP\u3001FN\u3001FP \u4e09\u79cd\u60c5\u51b5\u6807\u8bb0</p>"},{"location":"zh/examples/epnn/","title":"EPNN","text":""},{"location":"zh/examples/epnn/#epnn","title":"EPNN","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/epnn/dstate-16-plas.dat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/epnn/dstress-16-plas.dat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/epnn/dstate-16-plas.dat --create-dirs -o ./datasets/dstate-16-plas.dat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/epnn/dstress-16-plas.dat --create-dirs -o ./datasets/dstress-16-plas.dat\npython epnn.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/epnn/dstate-16-plas.dat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/epnn/dstress-16-plas.dat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/epnn/dstate-16-plas.dat --create-dirs -o ./datasets/dstate-16-plas.dat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/epnn/dstress-16-plas.dat --create-dirs -o ./datasets/dstress-16-plas.dat\npython epnn.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/epnn/epnn_pretrained.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 epnn_pretrained.pdparams error(total): 3.96903 error(error_elasto): 0.65328 error(error_plastic): 3.04176 error(error_stress): 0.27399"},{"location":"zh/examples/epnn/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u8fd9\u91cc\u4e3b\u8981\u4e3a\u590d\u73b0 Elasto-Plastic Neural Network (EPNN) \u7684 Physics-Informed Neural Network (PINN) \u4ee3\u7406\u6a21\u578b\u3002\u5c06\u8fd9\u4e9b\u7269\u7406\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\u7684\u67b6\u6784\u4e2d\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u8bad\u7ec3\u7f51\u7edc\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u589e\u5f3a\u5bf9\u8bad\u7ec3\u6570\u636e\u5916\u52a0\u8f7d\u5236\u5ea6\u7684\u63a8\u65ad\u80fd\u529b\u3002EPNN \u7684\u67b6\u6784\u662f\u6a21\u578b\u548c\u6750\u6599\u65e0\u5173\u7684\uff0c\u5373\u5b83\u53ef\u4ee5\u9002\u5e94\u5404\u79cd\u5f39\u5851\u6027\u6750\u6599\u7c7b\u578b\uff0c\u5305\u62ec\u5730\u8d28\u6750\u6599\u548c\u91d1\u5c5e\uff1b\u5e76\u4e14\u5b9e\u9a8c\u6570\u636e\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u8bad\u7ec3\u7f51\u7edc\u3002\u4e3a\u4e86\u8bc1\u660e\u6240\u63d0\u51fa\u67b6\u6784\u7684\u7a33\u5065\u6027\uff0c\u6211\u4eec\u5c06\u5176\u4e00\u822c\u6846\u67b6\u5e94\u7528\u4e8e\u7802\u571f\u7684\u5f39\u5851\u6027\u884c\u4e3a\u3002EPNN \u5728\u9884\u6d4b\u4e0d\u540c\u521d\u59cb\u5bc6\u5ea6\u7802\u571f\u7684\u672a\u89c2\u6d4b\u5e94\u53d8\u63a7\u5236\u52a0\u8f7d\u8def\u5f84\u65b9\u9762\u4f18\u4e8e\u5e38\u89c4\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002</p>"},{"location":"zh/examples/epnn/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u4fe1\u606f\u901a\u8fc7\u8fde\u63a5\u7684\u795e\u7ecf\u5143\u6d41\u52a8\u3002\u795e\u7ecf\u7f51\u7edc\u4e2d\u6bcf\u4e2a\u94fe\u63a5\u7684\u201c\u5f3a\u5ea6\u201d\u662f\u7531\u4e00\u4e2a\u53ef\u53d8\u7684\u6743\u91cd\u51b3\u5b9a\u7684\uff1a</p> \\[ z_l^{\\mathrm{i}}=W_{k l}^{\\mathrm{i}-1, \\mathrm{i}} a_k^{\\mathrm{i}-1}+b^{\\mathrm{i}-1}, \\quad k=1: N^{\\mathrm{i}-1} \\quad \\text { or } \\quad \\mathbf{z}^{\\mathrm{i}}=\\mathbf{a}^{\\mathrm{i}-1} \\mathbf{W}^{\\mathrm{i}-1, \\mathrm{i}}+b^{\\mathrm{i}-1} \\mathbf{I} \\] <p>\u5176\u4e2d \\(b\\) \u662f\u504f\u7f6e\u9879\uff1b\\(N\\) \u4e3a\u4e0d\u540c\u5c42\u4e2d\u795e\u7ecf\u5143\u6570\u91cf\uff1b\\(I\\) \u6307\u7684\u662f\u6240\u6709\u5143\u7d20\u90fd\u4e3a 1 \u7684\u5355\u4f4d\u5411\u91cf\u3002</p>"},{"location":"zh/examples/epnn/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/epnn/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 EPNN \u95ee\u9898\u4e2d\uff0c\u5efa\u7acb\u7f51\u7edc\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code>node_sizes_state_plastic.extend(hl_nodes_plastic)\nnode_sizes_stress.extend(hl_nodes_elasto)\nnode_sizes_state_elasto.extend([state_y_output_size - 3])\nnode_sizes_state_plastic.extend([state_y_output_size - 1])\nnode_sizes_stress.extend([1])\n\nactivation_elasto = \"leaky_relu\"\nactivation_plastic = \"leaky_relu\"\nactivations_elasto = [activation_elasto]\nactivations_plastic = [activation_plastic]\nactivations_elasto.extend([activation_elasto for ii in range(nhlayers)])\nactivations_plastic.extend([activation_plastic for ii in range(NHLAYERS_PLASTIC)])\nactivations_elasto.extend([activation_elasto])\nactivations_plastic.extend([activation_plastic])\ndrop_p = 0.0\nn_state_elasto = ppsci.arch.Epnn(\n    (\"state_x\",),\n    (\"out_state_elasto\",),\n    tuple(node_sizes_state_elasto),\n    tuple(activations_elasto),\n    drop_p,\n</code></pre> <p>EPNN \u53c2\u6570 <code>input_keys</code> \u662f\u8f93\u5165\u5b57\u6bb5\u540d\uff0c<code>output_keys</code> \u662f\u8f93\u51fa\u5b57\u6bb5\u540d\uff0c<code>node_sizes</code> \u662f\u8282\u70b9\u5927\u5c0f\u5217\u8868\uff0c<code>activations</code> \u662f\u6fc0\u6d3b\u51fd\u6570\u5b57\u7b26\u4e32\u5217\u8868\uff0c<code>drop_p</code> \u662f\u8282\u70b9\u4e22\u5f03\u6982\u7387\u3002</p>"},{"location":"zh/examples/epnn/#32","title":"3.2 \u6570\u636e\u751f\u6210","text":"<p>\u672c\u6848\u4f8b\u6d89\u53ca\u8bfb\u53d6\u6570\u636e\u751f\u6210\uff0c\u5982\u4e0b\u6240\u793a</p> <pre><code>(\n    input_dict_train,\n    label_dict_train,\n    input_dict_val,\n    label_dict_val,\n) = functions.get_data(cfg.DATASET_STATE, cfg.DATASET_STRESS, cfg.NTRAIN_SIZE)\n</code></pre> <pre><code>        n_train = math.floor(self.train_p * self.x.shape[0])\n        n_cross_valid = math.floor(self.cross_valid_p * self.x.shape[0])\n        n_test = math.floor(self.test_p * self.x.shape[0])\n        self.x_train = self.x[shuffled_indices[0:n_train]]\n        self.y_train = self.y[shuffled_indices[0:n_train]]\n        self.x_valid = self.x[shuffled_indices[n_train : n_train + n_cross_valid]]\n        self.y_valid = self.y[shuffled_indices[n_train : n_train + n_cross_valid]]\n        self.x_test = self.x[\n            shuffled_indices[n_train + n_cross_valid : n_train + n_cross_valid + n_test]\n        ]\n        self.y_test = self.y[\n            shuffled_indices[n_train + n_cross_valid : n_train + n_cross_valid + n_test]\n        ]\n\n\ndef get_data(dataset_state, dataset_stress, ntrain_size):\n</code></pre> <p>\u8fd9\u91cc\u4f7f\u7528 Data \u8bfb\u53d6\u6587\u4ef6\u6784\u9020\u6570\u636e\u7c7b\uff0c\u7136\u540e\u4f7f\u7528 get_shuffled_data \u6df7\u6dc6\u6570\u636e\uff0c\u7136\u540e\u8ba1\u7b97\u9700\u8981\u83b7\u53d6\u7684\u6df7\u6dc6\u6570\u636e\u6570\u91cf itrain\uff0c\u6700\u540e\u4f7f\u7528 get \u83b7\u53d6\u6bcf\u7ec4 itrain \u6570\u91cf\u7684 10 \u7ec4\u6570\u636e\u3002</p>"},{"location":"zh/examples/epnn/#33","title":"3.3 \u7ea6\u675f\u6784\u5efa","text":"<p>\u8bbe\u7f6e\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u635f\u5931\u8ba1\u7b97\u51fd\u6570\uff0c\u8fd4\u56de\u5b57\u6bb5\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>output_keys = [\n    \"state_x\",\n    \"state_y\",\n    \"stress_x\",\n    \"stress_y\",\n    \"out_state_elasto\",\n    \"out_state_plastic\",\n    \"out_stress\",\n]\nsup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict_train,\n            \"label\": label_dict_train,\n        },\n        \"batch_size\": 1,\n        \"num_workers\": 0,\n    },\n    ppsci.loss.FunctionalLoss(functions.train_loss_func),\n    {key: (lambda out, k=key: out[k]) for key in output_keys},\n    name=\"sup_train\",\n)\nconstraint_pde = {sup_constraint_pde.name: sup_constraint_pde}\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u914d\u7f6e\u4e2d <code>\u201cdataset\u201d</code> \u5b57\u6bb5\u8868\u793a\u4f7f\u7528\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4fe1\u606f\uff0c\u5176\u5404\u4e2a\u5b57\u6bb5\u5206\u522b\u8868\u793a\uff1a</p> <ol> <li><code>name</code>\uff1a \u6570\u636e\u96c6\u7c7b\u578b\uff0c\u6b64\u5904 <code>\"NamedArrayDataset\"</code> \u8868\u793a\u987a\u5e8f\u8bfb\u53d6\u7684\u6570\u636e\u96c6\uff1b</li> <li><code>input</code>\uff1a \u8f93\u5165\u6570\u636e\u96c6\uff1b</li> <li><code>label</code>\uff1a \u6807\u7b7e\u6570\u636e\u96c6\uff1b</li> </ol> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u4f7f\u7528\u81ea\u5b9a\u4e49\u51fd\u6570 <code>train_loss_func</code>\u3002</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u8ba1\u7b97\u540e\u7684\u503c\u5c06\u4f1a\u6309\u7167\u6307\u5b9a\u540d\u79f0\u5b58\u5165\u8f93\u51fa\u5217\u8868\u4e2d\uff0c\u4ece\u800c\u4fdd\u8bc1 loss \u8ba1\u7b97\u65f6\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b\u503c\u3002</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002</p> <p>\u5728\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p>"},{"location":"zh/examples/epnn/#34","title":"3.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u4e0e\u7ea6\u675f\u540c\u7406\uff0c\u672c\u95ee\u9898\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\uff0c\u53c2\u6570\u542b\u4e49\u4e5f\u4e0e\u7ea6\u675f\u6784\u5efa\u7c7b\u4f3c\uff0c\u552f\u4e00\u7684\u533a\u522b\u662f\u8bc4\u4ef7\u6307\u6807 <code>metric</code>\u3002\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>sup_validator_pde = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict_val,\n            \"label\": label_dict_val,\n        },\n        \"batch_size\": 1,\n        \"num_workers\": 0,\n    },\n    ppsci.loss.FunctionalLoss(functions.eval_loss_func),\n    {key: (lambda out, k=key: out[k]) for key in output_keys},\n    metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n    name=\"sup_valid\",\n)\nvalidator_pde = {sup_validator_pde.name: sup_validator_pde}\n</code></pre>"},{"location":"zh/examples/epnn/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 10000 \u8f6e\u8bad\u7ec3\u8f6e\u6570\u3002iters_per_epoch \u4e3a 1\u3002</p> <pre><code>epochs: 10000\niters_per_epoch: 1\n</code></pre>"},{"location":"zh/examples/epnn/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u914d\u5408\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u5e38\u7528\u7684 ExponentialDecay \u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u3002</p> <p>\u7531\u4e8e\u4f7f\u7528\u591a\u4e2a\u6a21\u578b\uff0c\u9700\u8981\u8bbe\u7f6e\u591a\u4e2a\u4f18\u5316\u5668\uff0c\u5bf9 EPNN \u7f51\u7edc\u90e8\u5206\uff0c\u9700\u8981\u8bbe\u7f6e <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code>    (\"out_state_plastic\",),\n    tuple(node_sizes_state_plastic),\n    tuple(activations_plastic),\n    drop_p,\n)\nn_stress = ppsci.arch.Epnn(\n    (\"state_x_f\",),\n    (\"out_stress\",),\n    tuple(node_sizes_stress),\n    tuple(activations_elasto),\n</code></pre> <p>\u7136\u540e\u5bf9\u589e\u52a0\u7684 gkratio \u53c2\u6570\uff0c\u9700\u8981\u518d\u8bbe\u7f6e\u4f18\u5316\u5668\u3002</p> <pre><code>    )\n    return (n_state_elasto, n_state_plastic, n_stress)\n\n\ndef get_optimizer_list(model_list, cfg):\n    optimizer_list = []\n    lr_list = [0.001, 0.001, 0.01]\n    for i, model in enumerate(model_list):\n</code></pre> <p>\u4f18\u5316\u5668\u6309\u987a\u5e8f\u4f18\u5316\uff0c\u4ee3\u7801\u6c47\u603b\u4e3a\uff1a</p> <pre><code>        (\"out_state_plastic\",),\n        tuple(node_sizes_state_plastic),\n        tuple(activations_plastic),\n        drop_p,\n    )\n    n_stress = ppsci.arch.Epnn(\n        (\"state_x_f\",),\n        (\"out_stress\",),\n        tuple(node_sizes_stress),\n        tuple(activations_elasto),\n        drop_p,\n    )\n    return (n_state_elasto, n_state_plastic, n_stress)\n\n\ndef get_optimizer_list(model_list, cfg):\n    optimizer_list = []\n    lr_list = [0.001, 0.001, 0.01]\n    for i, model in enumerate(model_list):\n</code></pre>"},{"location":"zh/examples/epnn/#37-loss","title":"3.7 \u81ea\u5b9a\u4e49 loss","text":"<p>\u7531\u4e8e\u672c\u95ee\u9898\u5305\u542b\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u6807\u7b7e\u6570\u636e\uff0closs \u6839\u636e\u6a21\u578b\u8fd4\u56de\u6570\u636e\u8ba1\u7b97\u5f97\u5230\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u5b9a\u4e49 loss\u3002\u65b9\u6cd5\u4e3a\u5148\u5b9a\u4e49\u76f8\u5173\u51fd\u6570\uff0c\u518d\u5c06\u51fd\u6570\u540d\u4f5c\u4e3a\u53c2\u6570\u4f20\u7ed9 <code>FunctionalLoss</code> \u548c <code>FunctionalMetric</code>\u3002</p> <p>\u9700\u8981\u6ce8\u610f\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u53c2\u6570\u9700\u8981\u4e0e PaddleScience \u4e2d\u5982 <code>MSE</code> \u7b49\u5176\u4ed6\u51fd\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u5373\u8f93\u5165\u4e3a\u6a21\u578b\u8f93\u51fa <code>output_dict</code> \u7b49\u5b57\u5178\u53d8\u91cf\uff0closs \u51fd\u6570\u8f93\u51fa\u4e3a loss \u503c <code>paddle.Tensor</code>\u3002</p> <p>\u76f8\u5173\u7684\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u4f7f\u7528 <code>MAELoss</code> \u8ba1\u7b97\uff0c\u4ee3\u7801\u4e3a</p> <pre><code>        )\n    }\n\n\ndef train_loss_func(output_dict, *args) -&gt; paddle.Tensor:\n    \"\"\"For model calculation of loss in model.train().\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): The output dict.\n\n    Returns:\n        paddle.Tensor: Loss value.\n    \"\"\"\n</code></pre>"},{"location":"zh/examples/epnn/#38","title":"3.8 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\u3002</p> <pre><code>solver = ppsci.solver.Solver(\n    model_list_obj,\n    constraint_pde,\n    cfg.output_dir,\n    optimizer_list,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    validator=validator_pde,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n</code></pre> <p>\u6a21\u578b\u8bad\u7ec3\u65f6\u8bbe\u7f6e eval_during_train \u4e3a True\uff0c\u5c06\u5728\u6bcf\u6b21\u8bad\u7ec3\u540e\u8bc4\u4f30\u3002</p> <pre><code>eval_during_train: true\n</code></pre> <p>\u6700\u540e\u542f\u52a8\u8bad\u7ec3\u5373\u53ef\uff1a</p> <pre><code>solver.train()\n</code></pre>"},{"location":"zh/examples/epnn/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"epnn.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nReference: https://github.com/meghbali/ANNElastoplasticity\n\"\"\"\n\nfrom os import path as osp\n\nimport functions\nimport hydra\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    (\n        input_dict_train,\n        label_dict_train,\n        input_dict_val,\n        label_dict_val,\n    ) = functions.get_data(cfg.DATASET_STATE, cfg.DATASET_STRESS, cfg.NTRAIN_SIZE)\n    model_list = functions.get_model_list(\n        cfg.MODEL.ihlayers,\n        cfg.MODEL.ineurons,\n        input_dict_train[\"state_x\"][0].shape[1],\n        input_dict_train[\"state_y\"][0].shape[1],\n        input_dict_train[\"stress_x\"][0].shape[1],\n    )\n    optimizer_list = functions.get_optimizer_list(model_list, cfg)\n    model_state_elasto, model_state_plastic, model_stress = model_list\n    model_list_obj = ppsci.arch.ModelList(model_list)\n\n    def _transform_in_stress(_in):\n        return functions.transform_in_stress(\n            _in, model_state_elasto, \"out_state_elasto\"\n        )\n\n    model_state_elasto.register_input_transform(functions.transform_in)\n    model_state_plastic.register_input_transform(functions.transform_in)\n    model_stress.register_input_transform(_transform_in_stress)\n    model_stress.register_output_transform(functions.transform_out)\n\n    output_keys = [\n        \"state_x\",\n        \"state_y\",\n        \"stress_x\",\n        \"stress_y\",\n        \"out_state_elasto\",\n        \"out_state_plastic\",\n        \"out_stress\",\n    ]\n    sup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_train,\n                \"label\": label_dict_train,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.train_loss_func),\n        {key: (lambda out, k=key: out[k]) for key in output_keys},\n        name=\"sup_train\",\n    )\n    constraint_pde = {sup_constraint_pde.name: sup_constraint_pde}\n\n    sup_validator_pde = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_val,\n                \"label\": label_dict_val,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.eval_loss_func),\n        {key: (lambda out, k=key: out[k]) for key in output_keys},\n        metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n        name=\"sup_valid\",\n    )\n    validator_pde = {sup_validator_pde.name: sup_validator_pde}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list_obj,\n        constraint_pde,\n        cfg.output_dir,\n        optimizer_list,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        validator=validator_pde,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n\n    # train model\n    solver.train()\n    functions.plotting(cfg.output_dir)\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    (\n        input_dict_train,\n        _,\n        input_dict_val,\n        label_dict_val,\n    ) = functions.get_data(cfg.DATASET_STATE, cfg.DATASET_STRESS, cfg.NTRAIN_SIZE)\n    model_list = functions.get_model_list(\n        cfg.MODEL.ihlayers,\n        cfg.MODEL.ineurons,\n        input_dict_train[\"state_x\"][0].shape[1],\n        input_dict_train[\"state_y\"][0].shape[1],\n        input_dict_train[\"stress_x\"][0].shape[1],\n    )\n    model_state_elasto, model_state_plastic, model_stress = model_list\n    model_list_obj = ppsci.arch.ModelList(model_list)\n\n    def _transform_in_stress(_in):\n        return functions.transform_in_stress(\n            _in, model_state_elasto, \"out_state_elasto\"\n        )\n\n    model_state_elasto.register_input_transform(functions.transform_in)\n    model_state_plastic.register_input_transform(functions.transform_in)\n    model_stress.register_input_transform(_transform_in_stress)\n    model_stress.register_output_transform(functions.transform_out)\n\n    output_keys = [\n        \"state_x\",\n        \"state_y\",\n        \"stress_x\",\n        \"stress_y\",\n        \"out_state_elasto\",\n        \"out_state_plastic\",\n        \"out_stress\",\n    ]\n    sup_validator_pde = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_val,\n                \"label\": label_dict_val,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.eval_loss_func),\n        {key: (lambda out, k=key: out[k]) for key in output_keys},\n        metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n        name=\"sup_valid\",\n    )\n    validator_pde = {sup_validator_pde.name: sup_validator_pde}\n    functions.OUTPUT_DIR = cfg.output_dir\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list_obj,\n        output_dir=cfg.output_dir,\n        validator=validator_pde,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate\n    solver.eval()\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"epnn.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/epnn/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>EPNN \u6848\u4f8b\u9488\u5bf9 epoch=10000 \u7684\u53c2\u6570\u914d\u7f6e\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8fd4\u56de Loss \u4e3a 0.00471\u3002</p> <p>\u4e0b\u56fe\u5206\u522b\u4e3a\u4e0d\u540c epoch \u7684 Loss, Training error, Cross validation error \u56fe\u5f62\uff1a</p> <p> </p>  \u8bad\u7ec3 loss \u56fe\u5f62"},{"location":"zh/examples/epnn/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<ul> <li> <p>A physics-informed deep neural network for surrogate modeling in classical elasto-plasticity</p> </li> <li> <p>\u53c2\u8003\u4ee3\u7801</p> </li> </ul>"},{"location":"zh/examples/euler_beam/","title":"Euler_Beam","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#euler-beam","title":"Euler Beam","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code>python euler_beam.py\n</code></pre> <pre><code>python euler_beam.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/euler_beam/euler_beam_pretrained.pdparams\n</code></pre> <pre><code>python euler_beam.py mode=export\n</code></pre> <pre><code>python euler_beam.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 euler_beam_pretrained.pdparams loss(L2Rel_Metric): 0.00000L2Rel.u(L2Rel_Metric): 0.00058","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#1","title":"1. \u95ee\u9898\u5b9a\u4e49","text":"<p>Euler Beam \u516c\u5f0f\uff1a</p> \\[ \\dfrac{\\partial^{4} u}{\\partial x^{4}} + 1 = 0, x \\in [0, 1] \\] <p>\u8fb9\u754c\u6761\u4ef6\uff1a</p> \\[ u''(1)=0, u'''(1)=0 \\] <p>\u72c4\u5229\u514b\u96f7\u6761\u4ef6\uff1a</p> \\[ u(0)=0 \\] <p>\u8bfa\u4f9d\u66fc\u8fb9\u754c\u6761\u4ef6\uff1a</p> \\[ u'(0)=0 \\]","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#2","title":"2. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#21","title":"2.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 Euler Beam \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\(x\\) \u90fd\u6709\u5bf9\u5e94\u7684\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf \\(u\\) \uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\(x\\) \u5230 \\(u\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^1 \\to \\mathbb{R}^1\\) \uff0c\u5373\uff1a</p> \\[ u = f(x) \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u5176\u4e2d\uff0c\u7528\u4e8e\u521d\u59cb\u5316\u6a21\u578b\u7684\u53c2\u6570\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u914d\u7f6e\uff1a</p> <pre><code># model settings\nMODEL:\n  input_keys: [\"x\"]\n  output_keys: [\"u\"]\n  num_layers: 3\n  hidden_size: 20\n</code></pre> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 3 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 20 \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#22","title":"2.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>Euler Beam \u7684\u65b9\u7a0b\u6784\u5efa\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>Biharmonic</code>\uff0c\u6307\u5b9a\u8be5\u7c7b\u7684\u53c2\u6570 <code>dim</code> \u4e3a 1\uff0c<code>q</code> \u4e3a -1\uff0c<code>D</code> \u4e3a1\u3002</p> <pre><code># set equation(s)\nequation = {\"biharmonic\": ppsci.equation.Biharmonic(dim=1, q=cfg.q, D=cfg.D)}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#23","title":"2.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d Euler Beam \u95ee\u9898\u4f5c\u7528\u5728\u4ee5 (0.0, 1.0) \u7684\u4e00\u7ef4\u533a\u57df\u4e0a\uff0c \u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u7a7a\u95f4\u51e0\u4f55 <code>Interval</code> \u4f5c\u4e3a\u8ba1\u7b97\u57df\u3002</p> <pre><code># set geometry\ngeom = {\"interval\": ppsci.geometry.Interval(0, 1)}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#24","title":"2.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e24\u4e2a\u7ea6\u675f\u6761\u4ef6\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u7684\u8bad\u7ec3\u5206\u522b\u662f\u4f5c\u7528\u4e8e\u91c7\u6837\u70b9\u4e0a\u7684\u65b9\u7a0b\u7ea6\u675f\u548c\u4f5c\u7528\u4e8e\u8fb9\u754c\u70b9\u4e0a\u7684\u7ea6\u675f\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u79cd\u7ea6\u675f\u6307\u5b9a\u91c7\u6837\u70b9\u4e2a\u6570\uff0c\u8868\u793a\u6bcf\u4e00\u79cd\u7ea6\u675f\u5728\u5176\u5bf9\u5e94\u8ba1\u7b97\u57df\u5185\u91c7\u6837\u6570\u636e\u7684\u6570\u91cf\uff0c\u4ee5\u53ca\u901a\u7528\u7684\u91c7\u6837\u914d\u7f6e\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 10000\n  iters_per_epoch: 1\n  save_freq: 1000\n  eval_during_train: true\n  eval_freq: 1000\n  learning_rate: 1.0e-3\n  batch_size:\n    pde: 100\n    bc: 4\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#241","title":"2.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set dataloader config\ndataloader_cfg = {\n    \"dataset\": \"IterableNamedArrayDataset\",\n    \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n}\n# set constraint\npde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"biharmonic\"].equations,\n    {\"biharmonic\": 0},\n    geom[\"interval\"],\n    {**dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.pde},\n    ppsci.loss.MSELoss(),\n    random=\"Hammersley\",\n    name=\"EQ\",\n)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#242","title":"2.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u540c\u7406\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u6784\u5efa\u8fb9\u754c\u7684\u7ea6\u675f\u3002\u4f46\u4e0e\u6784\u5efa <code>InteriorConstraint</code> \u7ea6\u675f\u4e0d\u540c\u7684\u662f\uff0c\u7531\u4e8e\u4f5c\u7528\u533a\u57df\u662f\u8fb9\u754c\uff0c\u56e0\u6b64\u6211\u4eec\u4f7f\u7528 <code>BoundaryConstraint</code> \u7c7b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>bc = ppsci.constraint.BoundaryConstraint(\n    {\n        \"u0\": lambda d: d[\"u\"][0:1],\n        \"u__x\": lambda d: jacobian(d[\"u\"], d[\"x\"])[1:2],\n        \"u__x__x\": lambda d: hessian(d[\"u\"], d[\"x\"])[2:3],\n        \"u__x__x__x\": lambda d: jacobian(hessian(d[\"u\"], d[\"x\"]), d[\"x\"])[3:4],\n    },\n    {\"u0\": 0, \"u__x\": 0, \"u__x__x\": 0, \"u__x__x__x\": 0},\n    geom[\"interval\"],\n    {**dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n    ppsci.loss.MSELoss(\"sum\"),\n    evenly=True,\n    name=\"BC\",\n)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#25","title":"2.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528\u4e00\u4e07\u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u8bc4\u4f30\u95f4\u9694\u4e3a\u4e00\u5343\u8f6e\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 10000\n  iters_per_epoch: 1\n  save_freq: 1000\n  eval_during_train: true\n  eval_freq: 1000\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#26","title":"2.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#27","title":"2.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.GeometryValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code>l2_rel_metric = ppsci.validate.GeometryValidator(\n    {\"u\": lambda out: out[\"u\"]},\n    {\"u\": u_solution_func},\n    geom[\"interval\"],\n    {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"total_size\": cfg.EVAL.total_size,\n    },\n    ppsci.loss.MSELoss(),\n    evenly=True,\n    metric={\"L2Rel\": ppsci.metric.L2Rel()},\n    name=\"L2Rel_Metric\",\n)\nvalidator = {l2_rel_metric.name: l2_rel_metric}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#28","title":"2.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u672c\u6587\u4e2d\u7684\u8f93\u51fa\u6570\u636e\u662f\u4e00\u4e2a\u66f2\u7ebf\u56fe\uff0c\u56e0\u6b64\u6211\u4eec\u53ea\u9700\u8981\u5c06\u8bc4\u4f30\u7684\u8f93\u51fa\u6570\u636e\u4fdd\u5b58\u6210 png \u6587\u4ef6\u5373\u53ef\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set visualizer(optional)\nvisu_points = geom[\"interval\"].sample_interior(cfg.EVAL.total_size, evenly=True)\nvisualizer = {\n    \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n        visu_points,\n        (\"x\",),\n        {\n            \"u_label\": lambda d: u_solution_func(d),\n            \"u_pred\": lambda d: d[\"u\"],\n        },\n        num_timestamps=1,\n        prefix=\"result_u\",\n    )\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#29","title":"2.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    epochs=cfg.TRAIN.epochs,\n    iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    seed=cfg.seed,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    visualizer=visualizer,\n    pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    to_static=cfg.to_static,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#3","title":"3. \u5b8c\u6574\u4ee3\u7801","text":"euler_beam.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hydra\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.autodiff import hessian\nfrom ppsci.autodiff import jacobian\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set geometry\n    geom = {\"interval\": ppsci.geometry.Interval(0, 1)}\n\n    # set equation(s)\n    equation = {\"biharmonic\": ppsci.equation.Biharmonic(dim=1, q=cfg.q, D=cfg.D)}\n\n    # set dataloader config\n    dataloader_cfg = {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    }\n    # set constraint\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"biharmonic\"].equations,\n        {\"biharmonic\": 0},\n        geom[\"interval\"],\n        {**dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.pde},\n        ppsci.loss.MSELoss(),\n        random=\"Hammersley\",\n        name=\"EQ\",\n    )\n    bc = ppsci.constraint.BoundaryConstraint(\n        {\n            \"u0\": lambda d: d[\"u\"][0:1],\n            \"u__x\": lambda d: jacobian(d[\"u\"], d[\"x\"])[1:2],\n            \"u__x__x\": lambda d: hessian(d[\"u\"], d[\"x\"])[2:3],\n            \"u__x__x__x\": lambda d: jacobian(hessian(d[\"u\"], d[\"x\"]), d[\"x\"])[3:4],\n        },\n        {\"u0\": 0, \"u__x\": 0, \"u__x__x\": 0, \"u__x__x__x\": 0},\n        geom[\"interval\"],\n        {**dataloader_cfg, \"batch_size\": cfg.TRAIN.batch_size.bc},\n        ppsci.loss.MSELoss(\"sum\"),\n        evenly=True,\n        name=\"BC\",\n    )\n    # wrap constraints together\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        bc.name: bc,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    def u_solution_func(out):\n        \"\"\"compute ground truth for u as label data\"\"\"\n        x = out[\"x\"]\n        return -(x**4) / 24 + x**3 / 6 - x**2 / 4\n\n    l2_rel_metric = ppsci.validate.GeometryValidator(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": u_solution_func},\n        geom[\"interval\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"total_size\": cfg.EVAL.total_size,\n        },\n        ppsci.loss.MSELoss(),\n        evenly=True,\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"L2Rel_Metric\",\n    )\n    validator = {l2_rel_metric.name: l2_rel_metric}\n\n    # set visualizer(optional)\n    visu_points = geom[\"interval\"].sample_interior(cfg.EVAL.total_size, evenly=True)\n    visualizer = {\n        \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n            visu_points,\n            (\"x\",),\n            {\n                \"u_label\": lambda d: u_solution_func(d),\n                \"u_pred\": lambda d: d[\"u\"],\n            },\n            num_timestamps=1,\n            prefix=\"result_u\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        epochs=cfg.TRAIN.epochs,\n        iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n        to_static=cfg.to_static,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set geometry\n    geom = {\"interval\": ppsci.geometry.Interval(0, 1)}\n\n    # set equation(s)\n    equation = {\"biharmonic\": ppsci.equation.Biharmonic(dim=1, q=cfg.q, D=cfg.D)}\n\n    # set validator\n    def u_solution_func(out):\n        \"\"\"compute ground truth for u as label data\"\"\"\n        x = out[\"x\"]\n        return -(x**4) / 24 + x**3 / 6 - x**2 / 4\n\n    l2_rel_metric = ppsci.validate.GeometryValidator(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": u_solution_func},\n        geom[\"interval\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"total_size\": cfg.EVAL.total_size,\n        },\n        ppsci.loss.MSELoss(),\n        evenly=True,\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"L2Rel_Metric\",\n    )\n    validator = {l2_rel_metric.name: l2_rel_metric}\n\n    # set visualizer(optional)\n    visu_points = geom[\"interval\"].sample_interior(cfg.EVAL.total_size, evenly=True)\n    visualizer = {\n        \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n            visu_points,\n            (\"x\",),\n            {\n                \"u_label\": lambda d: u_solution_func(d),\n                \"u_pred\": lambda d: d[\"u\"],\n            },\n            num_timestamps=1,\n            prefix=\"result_u\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        None,\n        cfg.output_dir,\n        None,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n        to_static=cfg.to_static,\n    )\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # set geometry\n    geom = {\"interval\": ppsci.geometry.Interval(0, 1)}\n    input_dict = geom[\"interval\"].sample_interior(cfg.INFER.total_size, evenly=True)\n\n    output_dict = predictor.predict({\"x\": input_dict[\"x\"]}, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n\n    def u_solution_func(out):\n        \"\"\"compute ground truth for u as label data\"\"\"\n        x = out[\"x\"]\n        return -(x**4) / 24 + x**3 / 6 - x**2 / 4\n\n    ppsci.visualize.save_plot_from_1d_dict(\n        \"./euler_beam_pred\",\n        {**input_dict, **output_dict, \"u_label\": u_solution_func(input_dict)},\n        (\"x\",),\n        (\"u\", \"u_label\"),\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"euler_beam.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/euler_beam/#4","title":"4. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4f7f\u7528\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u5bf9\u4e0a\u8ff0\u8ba1\u7b97\u57df\u4e2d\u5747\u5300\u53d6\u7684\u5171 <code>NPOINT_TOTAL</code> \u4e2a\u70b9 \\(x_i\\) \u8fdb\u884c\u9884\u6d4b\uff0c\u9884\u6d4b\u7ed3\u679c\u5982\u4e0b\u6240\u793a\u3002\u56fe\u50cf\u4e2d\u6a2a\u5750\u6807\u4e3a \\(x\\)\uff0c\u7eb5\u5750\u6807\u4e3a\u5bf9\u5e94\u7684\u9884\u6d4b\u7ed3\u679c \\(u\\)\u3002</p> <p> </p> \u6a21\u578b\u9884\u6d4b\u7ed3\u679c","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Euler\u2013Bernoulli\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/fourcastnet/","title":"FourCastNet","text":"","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#fourcastnet","title":"FourCastNet","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> <p>\u5f00\u59cb\u8bad\u7ec3\u3001\u8bc4\u4f30\u524d\uff0c\u8bf7\u5148\u4e0b\u8f7d\u6570\u636e\u96c6\u3002</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <pre><code># \u98ce\u901f\u9884\u8bad\u7ec3\u6a21\u578b\npython train_pretrain.py\n# \u98ce\u901f\u5fae\u8c03\u6a21\u578b\npython train_finetune.py\n# \u964d\u6c34\u6a21\u578b\u8bad\u7ec3\npython train_precip.py\n</code></pre> <pre><code># \u98ce\u901f\u9884\u8bad\u7ec3\u6a21\u578b\u8bc4\u4f30\npython train_pretrain.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/fourcastnet/pretrain.pdparams\n# \u98ce\u901f\u5fae\u8c03\u6a21\u578b\u8bc4\u4f30\npython train_finetune.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/fourcastnet/finetune.pdparams\n# \u964d\u6c34\u91cf\u6a21\u578b\u8bc4\u4f30\npython train_precip.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/fourcastnet/precip.pdparams WIND_MODEL_PATH=https://paddle-org.bj.bcebos.com/paddlescience/models/fourcastnet/finetune.pdparams\n</code></pre> \u6a21\u578b \u53d8\u91cf\u540d\u79f0 ACC/RMSE(6h) ACC/RMSE(30h) ACC/RMSE(60h) ACC/RMSE(120h) ACC/RMSE(192h) \u98ce\u901f\u6a21\u578b U10 0.991/0.567 0.963/1.130 0.891/1.930 0.645/3.438 0.371/4.915 \u6a21\u578b \u53d8\u91cf\u540d\u79f0 ACC/RMSE(6h) ACC/RMSE(12h) ACC/RMSE(24h) ACC/RMSE(36h) \u964d\u6c34\u91cf\u6a21\u578b TP 0.808/1.390 0.760/1.540 0.668/1.690 0.590/1.920","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u5728\u5929\u6c14\u9884\u62a5\u4efb\u52a1\u4e2d\uff0c\u6709\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u4e24\u79cd\u65b9\u6cd5\u5b9e\u73b0\u5929\u6c14\u9884\u62a5\u3002\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5f80\u5f80\u4f9d\u8d56\u7269\u7406\u65b9\u7a0b\uff0c\u901a\u8fc7\u5efa\u6a21\u5927\u6c14\u53d8\u91cf\u4e4b\u95f4\u7684\u7269\u7406\u5173\u7cfb\u5b9e\u73b0\u5929\u6c14\u9884\u62a5\u3002\u4f8b\u5982\u5728 IFS \u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u4e86\u5206\u5e03\u5728 50 \u591a\u4e2a\u5782\u76f4\u9ad8\u5ea6\u4e0a\u5171 150 \u591a\u4e2a\u5927\u6c14\u53d8\u91cf\u5b9e\u73b0\u5929\u6c14\u7684\u9884\u6d4b\u3002\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u7269\u7406\u65b9\u7a0b\uff0c\u4f46\u662f\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4e00\u822c\u5c06\u795e\u7ecf\u7f51\u7edc\u770b\u4f5c\u4e00\u4e2a\u9ed1\u76d2\u7ed3\u6784\uff0c\u8bad\u7ec3\u7f51\u7edc\u5b66\u4e60\u8f93\u5165\u6570\u636e\u4e0e\u8f93\u51fa\u6570\u636e\u4e4b\u95f4\u7684\u51fd\u6570\u5173\u7cfb\uff0c\u5b9e\u73b0\u7ed9\u5b9a\u8f93\u5165\u6761\u4ef6\u4e0b\u5bf9\u4e8e\u8f93\u51fa\u6570\u636e\u7684\u9884\u6d4b\u3002FourCastNet\u662f\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u6c14\u8c61\u9884\u62a5\u7b97\u6cd5\uff0c\u5b83\u4f7f\u7528\u81ea\u9002\u5e94\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08AFNO\uff09\u8fdb\u884c\u8bad\u7ec3\u548c\u9884\u6d4b\u3002\u8be5\u7b97\u6cd5\u4e13\u6ce8\u4e8e\u9884\u6d4b\u4e24\u5927\u6c14\u8c61\u53d8\u91cf\uff1a\u8ddd\u79bb\u5730\u7403\u8868\u976210\u7c73\u5904\u7684\u98ce\u901f\u548c6\u5c0f\u65f6\u603b\u964d\u6c34\u91cf\uff0c\u4ee5\u5bf9\u6781\u7aef\u5929\u6c14\u3001\u81ea\u7136\u707e\u5bb3\u7b49\u8fdb\u884c\u9884\u8b66\u3002\u76f8\u6bd4\u4e8e IFS \u6a21\u578b\uff0c\u5b83\u4ec5\u4ec5\u4f7f\u7528\u4e86 5 \u4e2a\u5782\u76f4\u9ad8\u5ea6\u4e0a\u5171 20 \u4e2a\u5927\u6c14\u53d8\u91cf\uff0c\u5177\u6709\u5927\u6c14\u53d8\u91cf\u8f93\u5165\u4e2a\u6570\u5c11\uff0c\u63a8\u7406\u7406\u901f\u5ea6\u5feb\u7684\u7279\u70b9\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#2","title":"2. \u6a21\u578b\u539f\u7406","text":"<p>\u672c\u7ae0\u8282\u4ec5\u5bf9 FourCastNet \u7684\u6a21\u578b\u539f\u7406\u8fdb\u884c\u7b80\u5355\u5730\u4ecb\u7ecd\uff0c\u8be6\u7ec6\u7684\u7406\u8bba\u63a8\u5bfc\u8bf7\u9605\u8bfb FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators\u3002</p> <p>FourCastNet \u7684\u7f51\u7edc\u6a21\u578b\u4f7f\u7528\u4e86 AFNO \u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u6b64\u524d\u5e38\u7528\u4e8e\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u3002\u8fd9\u4e2a\u7f51\u7edc\u901a\u8fc7 FNO \u5f25\u8865\u4e86 ViT \u7f51\u7edc\u7684\u7f3a\u70b9\uff0c\u4f7f\u7528\u5085\u7acb\u53f6\u53d8\u6362\u5b8c\u6210\u4e0d\u540c token \u4fe1\u606f\u4ea4\u4e92\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u5206\u8fa8\u7387\u4e0b ViT \u4e2d self-attention \u7684\u8ba1\u7b97\u91cf\u3002\u5173\u4e8e AFNO\u3001FNO\u3001VIT \u7684\u76f8\u5173\u539f\u7406\u4e5f\u8bf7\u9605\u8bfb\u5bf9\u5e94\u8bba\u6587\u3002</p> <p>\u6a21\u578b\u7684\u603b\u4f53\u7ed3\u6784\u5982\u56fe\u6240\u793a\uff1a</p> <p> </p> FourCastNet \u7f51\u7edc\u6a21\u578b <p>FourCastNet\u8bba\u6587\u4e2d\u8bad\u7ec3\u4e86\u98ce\u901f\u6a21\u578b\u548c\u964d\u6c34\u91cf\u6a21\u578b\uff0c\u63a5\u4e0b\u6765\u5c06\u4ecb\u7ecd\u8fd9\u4e24\u4e2a\u6a21\u578b\u7684\u8bad\u7ec3\u3001\u63a8\u7406\u8fc7\u7a0b\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#21","title":"2.1 \u98ce\u901f\u6a21\u578b\u7684\u8bad\u7ec3\u3001\u63a8\u7406\u8fc7\u7a0b","text":"<p>\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e3b\u8981\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a\u6a21\u578b\u9884\u8bad\u7ec3\u3001\u6a21\u578b\u5fae\u8c03\u3002</p> <p>\u6a21\u578b\u9884\u8bad\u7ec3\u9636\u6bb5\u662f\u57fa\u4e8e\u968f\u673a\u521d\u59cb\u5316\u7684\u7f51\u7edc\u6743\u91cd\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5176\u4e2d \\(X(k)\\) \u8868\u793a\u7b2c \\(k\\) \u65f6\u523b\u7684\u5927\u6c14\u6570\u636e\uff0c\\(X(k+1)\\) \u8868\u793a\u7b2c \\(k+1\\) \u65f6\u523b\u6a21\u578b\u9884\u6d4b\u7684\u5927\u6c14\u6570\u636e\uff0c\\(X_{true}(k+1)\\) \u8868\u793a\u7b2c \\(k+1\\) \u65f6\u523b\u7684\u771f\u5b9e\u5927\u6c14\u6570\u636e\u3002\u6700\u540e\u7f51\u7edc\u6a21\u578b\u9884\u6d4b\u7684\u8f93\u51fa\u548c\u771f\u503c\u8ba1\u7b97 L2 \u635f\u5931\u51fd\u6570\u3002</p> <p> </p> \u98ce\u901f\u6a21\u578b\u9884\u8bad\u7ec3 <p>\u6a21\u578b\u8bad\u7ec3\u7684\u7b2c\u4e8c\u4e2a\u9636\u6bb5\u662f\u6a21\u578b\u5fae\u8c03\uff0c\u8fd9\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\u4e3b\u8981\u662f\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u5728\u4e2d\u957f\u671f\u5929\u6c14\u9884\u62a5\u7684\u7cbe\u5ea6\u3002\u5177\u4f53\u5730\uff0c\u5f53\u6a21\u578b\u8f93\u5165 \\(k\\) \u65f6\u523b\u7684\u6570\u636e\uff0c\u9884\u6d4b\u4e86 \\(k+1\\) \u65f6\u523b\u7684\u6570\u636e\u540e\uff0c\u518d\u5c06\u5176\u91cd\u65b0\u4f5c\u4e3a\u8f93\u5165\u9884\u6d4b \\(k+2\\) \u65f6\u523b\u7684\u6570\u636e\uff0c\u4ee5\u8fde\u7eed\u9884\u6d4b\u4e24\u4e2a\u65f6\u523b\u7684\u8bad\u7ec3\u65b9\u5f0f\uff0c\u63d0\u9ad8\u6a21\u578b\u957f\u65f6\u9884\u6d4b\u80fd\u529b\u3002</p> <p> </p> \u98ce\u901f\u6a21\u578b\u5fae\u8c03 <p>\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u7ed9\u5b9a \\(k\\) \u65f6\u523b\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e0d\u65ad\u8fed\u4ee3\uff0c\u5f97\u5230 \\(k+1\\)\u3001\\(k+2\\)\u3001\\(k+3\\) \u7b49\u65f6\u523b\u7684\u9884\u6d4b\u7ed3\u679c\u3002</p> <p> </p> \u98ce\u901f\u6a21\u578b\u63a8\u7406","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#22","title":"2.2 \u964d\u6c34\u91cf\u6a21\u578b\u7684\u8bad\u7ec3\u3001\u63a8\u7406\u8fc7\u7a0b","text":"<p>\u964d\u6c34\u91cf\u6a21\u578b\u7684\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u98ce\u901f\u6a21\u578b\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u4f7f\u7528 \\(k\\) \u65f6\u523b\u7684\u5927\u6c14\u53d8\u91cf\u6570\u636e \\(X(k)\\) \u8f93\u5165\u8bad\u7ec3\u597d\u7684\u98ce\u901f\u6a21\u578b\uff0c\u5f97\u5230\u9884\u6d4b\u7684 \\(k+1\\) \u65f6\u523b\u7684\u5927\u6c14\u53d8\u91cf\u6570\u636e \\(X(k+1)\\)\u3002\u964d\u6c34\u91cf\u6a21\u578b\u4ee5 \\(X(k+1)\\) \u4e3a\u8f93\u5165\uff0c\u8f93\u51fa\u4e3a \\(k+1\\) \u65f6\u523b\u7684\u964d\u6c34\u91cf\u9884\u6d4b\u7ed3\u679c \\(p(k+1)\\)\u3002\u6a21\u578b\u8bad\u7ec3\u65f6 \\(p(k+1)\\) \u4e0e\u771f\u503c\u6570\u636e \\(p_{true}(k+1)\\) \u8ba1\u7b97 L2 \u635f\u5931\u51fd\u6570\u7ea6\u675f\u7f51\u7edc\u8bad\u7ec3\u3002</p> <p> </p> \u964d\u6c34\u91cf\u6a21\u578b\u8bad\u7ec3 <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\u5728\u964d\u6c34\u91cf\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u98ce\u901f\u6a21\u578b\u7684\u53c2\u6570\u5904\u4e8e\u51bb\u7ed3\u72b6\u6001\uff0c\u4e0d\u53c2\u4e0e\u4f18\u5316\u5668\u53c2\u6570\u66f4\u65b0\u8fc7\u7a0b\u3002</p> <p>\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u7ed9\u5b9a \\(k\\) \u65f6\u523b\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e0d\u65ad\u8fed\u4ee3\uff0c\u5229\u7528\u98ce\u901f\u6a21\u578b\u5f97\u5230 \\(k+1\\)\u3001\\(k+2\\)\u3001\\(k+3\\) \u7b49\u65f6\u523b\u7684\u5927\u6c14\u53d8\u91cf\u9884\u6d4b\u7ed3\u679c\uff0c\u4f5c\u4e3a\u964d\u6c34\u91cf\u6a21\u578b\u7684\u8f93\u5165\uff0c\u9884\u6d4b\u5bf9\u5e94\u65f6\u523b\u7684\u964d\u6c34\u91cf\u3002</p> <p> </p> \u964d\u6c34\u91cf\u6a21\u578b\u63a8\u7406","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#3","title":"3. \u98ce\u901f\u6a21\u578b\u5b9e\u73b0","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u57fa\u4e8e PaddleScience \u4ee3\u7801\uff0c\u5b9e\u73b0 FourCastNet \u98ce\u901f\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\u3002\u5173\u4e8e\u8be5\u6848\u4f8b\u4e2d\u7684\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p> Info <p>\u7531\u4e8e\u5b8c\u6574\u590d\u73b0\u9700\u8981 5+TB \u7684\u5b58\u50a8\u7a7a\u95f4\u548c 64 \u5361\u7684\u8bad\u7ec3\u8d44\u6e90\uff0c\u56e0\u6b64\u5982\u679c\u4ec5\u4ec5\u662f\u4e3a\u4e86\u5b66\u4e60 FourCastNet \u7684\u7b97\u6cd5\u539f\u7406\uff0c\u5efa\u8bae\u5bf9\u4e00\u5c0f\u90e8\u5206\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u51cf\u5c0f\u5b66\u4e60\u6210\u672c\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u6570\u636e\u96c6\u91c7\u7528\u4e86 FourCastNet \u4e2d\u5904\u7406\u597d\u7684 ERA5 \u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u7684\u5206\u8fa8\u7387\u5927\u5c0f\u4e3a 0.25 \u5ea6\uff0c\u6bcf\u4e2a\u53d8\u91cf\u7684\u6570\u636e\u5c3a\u5bf8\u4e3a \\(720 \\times 1440\\)\uff0c\u5176\u4e2d\u5355\u4e2a\u6570\u636e\u70b9\u4ee3\u8868\u7684\u5b9e\u9645\u8ddd\u79bb\u4e3a 30km \u5de6\u53f3\u3002FourCastNet \u4f7f\u7528\u4e86 1979-2018 \u5e74\u7684\u6570\u636e\uff0c\u6839\u636e\u5e74\u4efd\u5212\u5206\u4e3a\u4e86\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u3001\u6d4b\u8bd5\u96c6\uff0c\u5212\u5206\u7ed3\u679c\u5982\u4e0b\uff1a</p> \u6570\u636e\u96c6 \u5e74\u4efd \u8bad\u7ec3\u96c6 1979-2015 \u9a8c\u8bc1\u96c6 2016-2017 \u6d4b\u8bd5\u96c6 2018 <p>\u8be5\u6570\u636e\u96c6\u53ef\u4ee5\u4ece\u6b64\u5904\u4e0b\u8f7d\u3002</p> <p>\u6a21\u578b\u8bad\u7ec3\u4f7f\u7528\u4e86\u5206\u5e03\u5728 5 \u4e2a\u538b\u529b\u5c42\u4e0a\u7684 20 \u4e2a\u5927\u6c14\u53d8\u91cf\uff0c\u5982\u4e0b\u8868\u6240\u793a\uff0c</p> <p> </p> 20 \u4e2a\u5927\u6c14\u53d8\u91cf <p>\u5176\u4e2d \\(T\\)\u3001\\(U\\)\u3001\\(V\\) \u3001\\(Z\\)\u3001\\(RH\\) \u5206\u522b\u4ee3\u8868\u6307\u5b9a\u5782\u76f4\u9ad8\u5ea6\u4e0a\u7684\u6e29\u5ea6\u3001\u7eac\u5411\u98ce\u901f\u3001\u7ecf\u5411\u98ce\u901f\u3001\u4f4d\u52bf\u548c\u76f8\u5bf9\u6e7f\u5ea6\uff1b\\(U_{10}\\)\u3001\\(V_{10}\\)\u3001\\(T_{2m}\\) \u5219\u4ee3\u8868\u8ddd\u79bb\u5730\u9762 10 \u7c73\u7684\u7eac\u5411\u98ce\u901f\u3001\u7ecf\u5411\u98ce\u901f\u548c\u8ddd\u79bb\u5730\u9762 2 \u7c73\u7684\u6e29\u5ea6\u3002\\(sp\\) \u4ee3\u8868\u5730\u9762\u6c14\u538b\uff0c\\(mslp\\) \u4ee3\u8868\u5e73\u5747\u6d77\u5e73\u9762\u6c14\u538b\u3002\\(TCWV\\) \u4ee3\u8868\u6574\u5c42\u6c14\u67f1\u6c34\u6c7d\u603b\u91cf\u3002</p> <p>\u5bf9\u6bcf\u5929 24 \u4e2a\u5c0f\u65f6\u7684\u6570\u636e\u95f4\u9694 6 \u5c0f\u65f6\u91c7\u6837\uff0c\u5f97\u5230 0.00h/6.00h/12.00h/18.00h \u65f6\u523b\u5168\u7403 20 \u4e2a\u5927\u6c14\u53d8\u91cf\u7684\u6570\u636e\uff0c\u4f7f\u7528\u8fd9\u6837\u7684\u6570\u636e\u8fdb\u884c\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\u3002\u5373\u8f93\u51650.00h \u65f6\u523b\u7684 20 \u4e2a\u5927\u6c14\u53d8\u91cf\u7684\u6570\u636e\uff0c\u6a21\u578b\u8f93\u51fa\u9884\u6d4b\u5f97\u5230\u7684 6.00h \u65f6\u523b\u7684 20 \u4e2a\u5927\u6c14\u53d8\u91cf\u7684\u6570\u636e\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#32","title":"3.2 \u6a21\u578b\u9884\u8bad\u7ec3","text":"<p>\u9996\u5148\u5c55\u793a\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u5404\u4e2a\u53c2\u6570\u53d8\u91cf\uff0c\u6bcf\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u4f1a\u5728\u4e0b\u9762\u4f7f\u7528\u5230\u65f6\u8fdb\u884c\u89e3\u91ca\u3002</p> examples/fourcastnet/conf/fourcastnet_pretrain.yaml<pre><code># set training hyper-parameters\nIMG_H: 720\nIMG_W: 1440\n# FourCastNet use 20 atmospheric variable\uff0ctheir index in the dataset is from 0 to 19.\n# The variable name is 'u10', 'v10', 't2m', 'sp', 'msl', 't850', 'u1000', 'v1000', 'z000',\n# 'u850', 'v850', 'z850',  'u500', 'v500', 'z500', 't500', 'z50', 'r500', 'r850', 'tcwv'.\n# You can obtain detailed information about each variable from\n# https://cds.climate.copernicus.eu/cdsapp#!/search?text=era5&amp;type=dataset\nVARS_CHANNEL: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nUSE_SAMPLED_DATA: false\n\n# set train data path\nTRAIN_FILE_PATH: ./datasets/era5/train\nDATA_MEAN_PATH: ./datasets/era5/stat/global_means.npy\nDATA_STD_PATH: ./datasets/era5/stat/global_stds.npy\nDATA_TIME_MEAN_PATH: ./datasets/era5/stat/time_means.npy\n\n# set evaluate data path\nVALID_FILE_PATH: ./datasets/era5/test\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#321","title":"3.2.1 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\uff0c\u9996\u5148\u4ecb\u7ecd\u6570\u636e\u9884\u5904\u7406\u90e8\u5206\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_pretrain.py<pre><code>data_mean, data_std = fourcast_utils.get_mean_std(\n    cfg.DATA_MEAN_PATH, cfg.DATA_STD_PATH, cfg.VARS_CHANNEL\n)\ndata_time_mean = fourcast_utils.get_time_mean(\n    cfg.DATA_TIME_MEAN_PATH, cfg.IMG_H, cfg.IMG_W, cfg.VARS_CHANNEL\n)\ndata_time_mean_normalize = np.expand_dims(\n    (data_time_mean[0] - data_mean) / data_std, 0\n)\n# set train transforms\ntransforms = [\n    {\"SqueezeData\": {}},\n    {\"CropData\": {\"xmin\": (0, 0), \"xmax\": (cfg.IMG_H, cfg.IMG_W)}},\n    {\"Normalize\": {\"mean\": data_mean, \"std\": data_std}},\n]\n</code></pre> <p>\u6570\u636e\u9884\u5904\u7406\u90e8\u5206\u603b\u5171\u5305\u542b 3 \u4e2a\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5206\u522b\u662f:</p> <ol> <li><code>SqueezeData</code>: \u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u7ef4\u5ea6\u8fdb\u884c\u538b\u7f29\uff0c\u5982\u679c\u8f93\u5165\u6570\u636e\u7684\u7ef4\u5ea6\u4e3a 4\uff0c\u5219\u5c06\u7b2c 0 \u7ef4\u548c\u7b2c 1 \u7ef4\u7684\u6570\u636e\u538b\u7f29\u5230\u4e00\u8d77\uff0c\u6700\u7ec8\u5c06\u8f93\u5165\u6570\u636e\u7684\u7ef4\u5ea6\u53d8\u6362\u4e3a 3\u3002</li> <li><code>CropData</code>: \u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u88c1\u526a\u6307\u5b9a\u4f4d\u7f6e\u7684\u6570\u636e\u3002\u56e0\u4e3a ERA5 \u6570\u636e\u96c6\u4e2d\u7684\u539f\u59cb\u6570\u636e\u5c3a\u5bf8\u4e3a \\(721 \\times 1440\\)\uff0c\u672c\u6848\u4f8b\u6839\u636e\u539f\u59cb\u8bba\u6587\u8bbe\u7f6e\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u88c1\u526a\u4e3a \\(720 \\times 1440\\)\u3002</li> <li><code>Normalize</code>: \u6839\u636e\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u7684\u5747\u503c\u3001\u65b9\u5dee\u5bf9\u6570\u636e\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\u3002</li> </ol> <p>\u7531\u4e8e\u5b8c\u6574\u590d\u73b0 FourCastNet \u9700\u8981 5TB+ \u7684\u5b58\u50a8\u7a7a\u95f4\u548c 64 \u5361\u7684 GPU \u8d44\u6e90\uff0c\u9700\u8981\u7684\u5b58\u50a8\u8d44\u6e90\u6bd4\u8f83\u591a\uff0c\u56e0\u6b64\u6709\u4ee5\u4e0b\u4e24\u79cd\u8bad\u7ec3\u65b9\u5f0f\uff08\u5b9e\u9a8c\u8bc1\u660e\u4e24\u79cd\u8bad\u7ec3\u65b9\u5f0f\u7684\u635f\u5931\u51fd\u6570\u6536\u655b\u66f2\u7ebf\u57fa\u672c\u4e00\u81f4\uff0c\u5f53\u5b58\u50a8\u8d44\u6e90\u6bd4\u8f83\u6709\u9650\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u65b9\u5f0f b\uff09\u3002</p> <p>\u65b9\u5f0f a\uff1a \u5f53\u5b58\u50a8\u8d44\u6e90\u5145\u8db3\u65f6\uff0c\u53ef\u4ee5\u4e0d\u5bf9\u6570\u636e\u8fdb\u884c\u5212\u5206\uff0c\u6bcf\u4e2a\u8282\u70b9\u90fd\u6709\u4e00\u4efd\u5b8c\u65745TB+\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u7136\u540e\u76f4\u63a5\u542f\u52a8\u8bad\u7ec3\u7a0b\u5e8f\u8fdb\u884c\u8bad\u7ec3\uff0c\u6b64\u65f6\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7684\u6570\u636e\u968f\u673a\u62bd\u53d6\u81ea\u5b8c\u6574\u8bad\u7ec3\u6570\u636e\u3002\u672c\u65b9\u5f0f\u7684\u8bad\u7ec3\u6570\u636e\u7684\u52a0\u8f7d\u662f\u4f7f\u7528\u5168\u5c40 shuffle \u7684\u65b9\u5f0f\u8fdb\u884c\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002</p> <p> </p> \u5168\u5c40 shuffle <p>\u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u6570\u636e\u52a0\u8f7d\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_pretrain.py<pre><code>train_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ERA5Dataset\",\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"input_keys\": cfg.MODEL.afno.input_keys,\n        \"label_keys\": cfg.MODEL.afno.output_keys,\n        \"vars_channel\": cfg.VARS_CHANNEL,\n        \"transforms\": transforms,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 8,\n}\n</code></pre> <p>\u5176\u4e2d\uff0c\"dataset\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Dataset</code> \u7c7b\u540d\u4e3a <code>ERA5Dataset</code>\uff0c\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u8bbe\u7f6e\u7684 <code>batch_size</code> \u4e3a 1\uff0c<code>num_works</code> \u4e3a 8\u3002</p> <p>\u65b9\u5f0f b\uff1a\u5728\u5b58\u50a8\u8d44\u6e90\u6709\u9650\u65f6\uff0c\u9700\u8981\u5c06\u6570\u636e\u96c6\u5747\u5300\u5207\u5206\u81f3\u6bcf\u4e2a\u8282\u70b9\u4e0a\uff0c\u672c\u6848\u4f8b\u63d0\u4f9b\u4e86\u968f\u673a\u91c7\u6837\u6570\u636e\u7684\u7a0b\u5e8f\uff0c\u53ef\u4ee5\u6267\u884c <code>ppsci/fourcastnet/sample_data.py</code>\uff0c\u53ef\u4ee5\u6839\u636e\u9700\u8981\u8fdb\u884c\u4fee\u6539\u3002\u672c\u6848\u4f8b\u9ed8\u8ba4\u4f7f\u7528\u65b9\u5f0f a, \u56e0\u6b64\u4f7f\u7528\u65b9\u5f0f b \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u65f6\u9700\u8981\u624b\u52a8\u5c06 <code>USE_SAMPLED_DATA</code> \u8bbe\u7f6e\u4e3a <code>True</code>\u3002\u672c\u65b9\u5f0f\u7684\u8bad\u7ec3\u6570\u636e\u7684\u52a0\u8f7d\u662f\u4f7f\u7528\u5c40\u90e8 shuffle \u7684\u65b9\u5f0f\u8fdb\u884c\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u9996\u5148\u5c06\u8bad\u7ec3\u6570\u636e\u5e73\u5747\u5207\u5206\u81f3 8 \u4e2a\u8282\u70b9\u4e0a\uff0c\u8bad\u7ec3\u65f6\u6bcf\u4e2a\u8282\u70b9\u7684\u6570\u636e\u968f\u673a\u62bd\u53d6\u81ea\u88ab\u5207\u5206\u5230\u7684\u6570\u636e\u4e0a\uff0c\u5728\u8fd9\u4e00\u60c5\u51b5\u4e0b\uff0c\u6bcf\u4e2a\u8282\u70b9\u9700\u8981\u7ea6 1.2TB \u7684\u5b58\u50a8\u7a7a\u95f4\uff0c\u76f8\u6bd4\u4e8e\u65b9\u5f0f a\uff0c\u65b9\u5f0f b \u5927\u5927\u51cf\u5c0f\u4e86\u5bf9\u5b58\u50a8\u7a7a\u95f4\u7684\u4f9d\u8d56\u3002</p> <p> </p> \u5c40\u90e8 shuffle <p>\u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u6570\u636e\u52a0\u8f7d\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_pretrain.py<pre><code>NUM_GPUS_PER_NODE = 8\ntrain_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ERA5SampledDataset\",\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"input_keys\": cfg.MODEL.afno.input_keys,\n        \"label_keys\": cfg.MODEL.afno.output_keys,\n    },\n    \"sampler\": {\n        \"name\": \"DistributedBatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n        \"num_replicas\": NUM_GPUS_PER_NODE,\n        \"rank\": dist.get_rank() % NUM_GPUS_PER_NODE,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 8,\n}\n</code></pre> <p>\u5176\u4e2d\uff0c\"dataset\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Dataset</code> \u7c7b\u540d\u4e3a <code>ERA5SampledDataset</code>\uff0c\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>DistributedBatchSampler</code>\uff0c\u8bbe\u7f6e\u7684 <code>batch_size</code> \u4e3a 1\uff0c<code>num_works</code> \u4e3a 8\u3002</p> <p>\u5f53\u4e0d\u9700\u8981\u5b8c\u6574\u590d\u73b0 FourCastNet \u65f6\uff0c\u76f4\u63a5\u4f7f\u7528\u672c\u6848\u4f8b\u7684\u9ed8\u8ba4\u8bbe\u7f6e\uff08\u65b9\u5f0f a\uff09\u5373\u53ef\uff0c</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_pretrain.py<pre><code># set constraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.L2RelLoss(),\n    name=\"Sup\",\n)\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u6570\u636e\u7684\u52a0\u8f7d\u65b9\u5f0f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e0a\u6587\u4e2d\u5b9a\u4e49\u7684 <code>train_dataloader_cfg</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u4f7f\u7528 <code>L2RelLoss</code>\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"Sup\"\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#322","title":"3.2.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0c\u98ce\u901f\u6a21\u578b\u57fa\u4e8e AFNONet \u7f51\u7edc\u6a21\u578b\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_pretrain.py<pre><code># set model\nmodel = ppsci.arch.AFNONet(**cfg.MODEL.afno)\n</code></pre> <p>\u7f51\u7edc\u6a21\u578b\u7684\u53c2\u6570\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u8bbe\u7f6e\u5982\u4e0b\uff1a</p> examples/fourcastnet/conf/fourcastnet_pretrain.yaml<pre><code># model settings\nMODEL:\n  afno:\n    input_keys: [\"input\"]\n    output_keys: [\"output\"]\n</code></pre> <p>\u5176\u4e2d\uff0c<code>input_keys</code> \u548c <code>output_keys</code> \u5206\u522b\u4ee3\u8868\u7f51\u7edc\u6a21\u578b\u8f93\u5165\u3001\u8f93\u51fa\u53d8\u91cf\u7684\u540d\u79f0\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#323","title":"3.2.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>Cosine</code>\uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a 5e-4\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>Adam</code>\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_pretrain.py<pre><code># init optimizer and lr scheduler\nlr_scheduler_cfg = dict(cfg.TRAIN.lr_scheduler)\nlr_scheduler_cfg.update({\"iters_per_epoch\": ITERS_PER_EPOCH})\nlr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(**lr_scheduler_cfg)()\n\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#324","title":"3.2.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_pretrain.py<pre><code># set eval dataloader config\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ERA5Dataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.afno.input_keys,\n        \"label_keys\": cfg.MODEL.afno.output_keys,\n        \"vars_channel\": cfg.VARS_CHANNEL,\n        \"transforms\": transforms,\n        \"training\": False,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n}\n\n# set validator\nsup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.L2RelLoss(),\n    metric={\n        \"MAE\": ppsci.metric.MAE(keep_batch=True),\n        \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n            num_lat=cfg.IMG_H,\n            std=data_std,\n            keep_batch=True,\n            variable_dict={\"u10\": 0, \"v10\": 1},\n        ),\n        \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n            num_lat=cfg.IMG_H,\n            mean=data_time_mean_normalize,\n            keep_batch=True,\n            variable_dict={\"u10\": 0, \"v10\": 1},\n        ),\n    },\n    name=\"Sup_Validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre> <p><code>SupervisedValidator</code> \u8bc4\u4f30\u5668\u4e0e <code>SupervisedConstraint</code> \u6bd4\u8f83\u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u8bc4\u4f30\u5668\u9700\u8981\u8bbe\u7f6e\u8bc4\u4ef7\u6307\u6807 <code>metric</code>\uff0c\u5728\u8fd9\u91cc\u4f7f\u7528\u4e86 3 \u4e2a\u8bc4\u4ef7\u6307\u6807\u5206\u522b\u662f <code>MAE</code>\u3001<code>LatitudeWeightedRMSE</code> \u548c <code>LatitudeWeightedACC</code>\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#325","title":"3.2.5 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/fourcastnet/train_pretrain.py<pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=True,\n    seed=cfg.seed,\n    validator=validator,\n    compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#33","title":"3.3 \u6a21\u578b\u5fae\u8c03","text":"<p>\u4e0a\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u5bf9\u98ce\u901f\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5728\u672c\u8282\u4e2d\u5c06\u4ecb\u7ecd\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u56e0\u4e3a\u98ce\u901f\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u6b65\u9aa4\u4e0e\u5fae\u8c03\u7684\u6b65\u9aa4\u57fa\u672c\u76f8\u4f3c\uff0c\u56e0\u6b64\u672c\u8282\u5728\u4e24\u8005\u7684\u91cd\u590d\u90e8\u5206\u4e0d\u518d\u4ecb\u7ecd\uff0c\u800c\u4ec5\u4ec5\u4ecb\u7ecd\u6a21\u578b\u5fae\u8c03\u7279\u6709\u7684\u90e8\u5206\u3002\u9996\u5148\u5c06\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u5404\u4e2a\u53c2\u6570\u53d8\u91cf\u5c55\u793a\u5982\u4e0b\uff0c\u6bcf\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u4f1a\u5728\u4e0b\u9762\u4f7f\u7528\u5230\u65f6\u8fdb\u884c\u89e3\u91ca\u3002</p> examples/fourcastnet/conf/fourcastnet_finetune.yaml<pre><code># set training hyper-parameters\nIMG_H: 720\nIMG_W: 1440\n# FourCastNet use 20 atmospheric variable\uff0ctheir index in the dataset is from 0 to 19.\n# The variable name is 'u10', 'v10', 't2m', 'sp', 'msl', 't850', 'u1000', 'v1000', 'z000',\n# 'u850', 'v850', 'z850',  'u500', 'v500', 'z500', 't500', 'z50', 'r500', 'r850', 'tcwv'.\n# You can obtain detailed information about each variable from\n# https://cds.climate.copernicus.eu/cdsapp#!/search?text=era5&amp;type=dataset\nVARS_CHANNEL: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n# set train data path\nTRAIN_FILE_PATH: ./datasets/era5/train\nDATA_MEAN_PATH: ./datasets/era5/stat/global_means.npy\nDATA_STD_PATH: ./datasets/era5/stat/global_stds.npy\nDATA_TIME_MEAN_PATH: ./datasets/era5/stat/time_means.npy\n\n# set evaluate data path\nVALID_FILE_PATH: ./datasets/era5/test\n\n# set test data path\nTEST_FILE_PATH: ./datasets/era5/out_of_sample/2018.h5\n</code></pre> <p>\u5fae\u8c03\u6a21\u578b\u7684\u7a0b\u5e8f\u65b0\u589e\u4e86 <code>num_timestamps</code> \u53c2\u6570\uff0c\u7528\u4e8e\u63a7\u5236\u6a21\u578b\u5fae\u8c03\u8bad\u7ec3\u65f6\u8fed\u4ee3\u7684\u65f6\u95f4\u6b65\u7684\u4e2a\u6570\u3002\u8fd9\u4e2a\u53c2\u6570\u9996\u5148\u4f1a\u5728\u6570\u636e\u52a0\u8f7d\u7684\u8bbe\u7f6e\u4e2d\u7528\u5230\uff0c\u7528\u4e8e\u8bbe\u7f6e\u6570\u636e\u96c6\u4ea7\u751f\u7684\u771f\u503c\u7684\u65f6\u95f4\u6b65\u5927\u5c0f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_finetune.py<pre><code># set train dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ERA5Dataset\",\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"input_keys\": cfg.MODEL.afno.input_keys,\n        \"label_keys\": output_keys,\n        \"vars_channel\": cfg.VARS_CHANNEL,\n        \"num_label_timestamps\": cfg.TRAIN.num_timestamps,\n        \"transforms\": transforms,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 8,\n}\n</code></pre> <p><code>num_timestamps</code> \u53c2\u6570\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u8bbe\u7f6e\uff0c\u5982\u4e0b\uff1a</p> examples/fourcastnet/conf/fourcastnet_finetune.yaml<pre><code>num_timestamps: 2\n</code></pre> <p>\u53e6\u5916\uff0c\u4e0e\u9884\u8bad\u7ec3\u4e0d\u540c\u7684\u662f\uff0c\u5fae\u8c03\u7684\u6a21\u578b\u6784\u5efa\u4e5f\u9700\u8981\u8bbe\u7f6e <code>num_timestamps</code> \u53c2\u6570\uff0c\u7528\u4e8e\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u7684\u9884\u6d4b\u7ed3\u679c\u7684\u65f6\u95f4\u6b65\u5927\u5c0f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_finetune.py<pre><code># set model\nmodel_cfg = dict(cfg.MODEL.afno)\nmodel_cfg.update(\n    {\"output_keys\": output_keys, \"num_timestamps\": cfg.TRAIN.num_timestamps}\n)\n</code></pre> <p>\u8bad\u7ec3\u5fae\u8c03\u6a21\u578b\u7684\u7a0b\u5e8f\u4e2d\u589e\u52a0\u4e86\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u7684\u4ee3\u7801\u548c\u53ef\u89c6\u5316\u4ee3\u7801\uff0c\u63a5\u4e0b\u6765\u5c06\u5bf9\u8fd9\u4e24\u90e8\u5206\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#331","title":"3.3.1 \u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b","text":"<p>\u6839\u636e\u8bba\u6587\u4e2d\u7684\u8bbe\u7f6e\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c<code>num_timestamps</code> \u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u8bbe\u7f6e\u7684\u4e3a 32\uff0c\u76f8\u90bb\u7684\u4e24\u4e2a\u6d4b\u8bd5\u6837\u672c\u7684\u95f4\u9694\u4e3a 8\u3002</p> examples/fourcastnet/conf/fourcastnet_finetune.yaml<pre><code># evaluation settings\nEVAL:\n  num_timestamps: 32\n</code></pre> <p>\u6784\u5efa\u6a21\u578b\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/fourcastnet/train_finetune.py<pre><code># set model\nmodel_cfg = dict(cfg.MODEL.afno)\nmodel_cfg.update(\n    {\"output_keys\": output_keys, \"num_timestamps\": cfg.EVAL.num_timestamps}\n)\nmodel = ppsci.arch.AFNONet(**model_cfg)\n</code></pre> <p>\u6784\u5efa\u8bc4\u4f30\u5668\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/fourcastnet/train_finetune.py<pre><code># set eval dataloader config\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ERA5Dataset\",\n        \"file_path\": cfg.TEST_FILE_PATH,\n        \"input_keys\": cfg.MODEL.afno.input_keys,\n        \"label_keys\": output_keys,\n        \"vars_channel\": cfg.VARS_CHANNEL,\n        \"transforms\": transforms,\n        \"num_label_timestamps\": cfg.EVAL.num_timestamps,\n        \"training\": False,\n        \"stride\": 8,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n}\n\n# set metirc\nmetric = {\n    \"MAE\": ppsci.metric.MAE(keep_batch=True),\n    \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n        num_lat=cfg.IMG_H,\n        std=data_std,\n        keep_batch=True,\n        variable_dict={\"u10\": 0, \"v10\": 1},\n    ),\n    \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n        num_lat=cfg.IMG_H,\n        mean=data_time_mean_normalize,\n        keep_batch=True,\n        variable_dict={\"u10\": 0, \"v10\": 1},\n    ),\n}\n\n# set validator for testing\nsup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.L2RelLoss(),\n    metric=metric,\n    name=\"Sup_Validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#332","title":"3.3.2 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u98ce\u901f\u6a21\u578b\u4f7f\u7528\u81ea\u56de\u5f52\u7684\u65b9\u5f0f\u8fdb\u884c\u63a8\u7406\uff0c\u9700\u8981\u9996\u5148\u8bbe\u7f6e\u6a21\u578b\u63a8\u7406\u7684\u8f93\u5165\u6570\u636e\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_finetune.py<pre><code># set visualizer data\nDATE_STRINGS = (\"2018-09-08 00:00:00\",)\nvis_data = get_vis_data(\n    cfg.TEST_FILE_PATH,\n    DATE_STRINGS,\n    cfg.EVAL.num_timestamps,\n    cfg.VARS_CHANNEL,\n    cfg.IMG_H,\n    data_mean,\n    data_std,\n)\n</code></pre> examples/fourcastnet/train_finetune.py<pre><code>def get_vis_data(\n    file_path: str,\n    date_strings: Tuple[str, ...],\n    num_timestamps: int,\n    vars_channel: Tuple[int, ...],\n    img_h: int,\n    data_mean: np.ndarray,\n    data_std: np.ndarray,\n):\n    _file = h5py.File(file_path, \"r\")[\"fields\"]\n    data = []\n    for date_str in date_strings:\n        hours_since_jan_01_epoch = fourcast_utils.date_to_hours(date_str)\n        ic = int(hours_since_jan_01_epoch / 6)\n        data.append(_file[ic : ic + num_timestamps + 1, vars_channel, 0:img_h])\n    data = np.asarray(data)\n\n    vis_data = {\"input\": (data[:, 0] - data_mean) / data_std}\n    for t in range(num_timestamps):\n        hour = (t + 1) * 6\n        data_t = data[:, t + 1]\n        wind_data = []\n        for i in range(data_t.shape[0]):\n            wind_data.append((data_t[i][0] ** 2 + data_t[i][1] ** 2) ** 0.5)\n        vis_data[f\"target_{hour}h\"] = np.asarray(wind_data)\n    return vis_data\n</code></pre> <p>\u4ee5\u4e0a\u7684\u4ee3\u7801\u4e2d\u4f1a\u6839\u636e\u8bbe\u7f6e\u7684\u65f6\u95f4\u53c2\u6570 <code>DATE_STRINGS</code> \u8bfb\u53d6\u5bf9\u5e94\u7684\u6570\u636e\u7528\u4e8e\u6a21\u578b\u7684\u8f93\u5165\uff0c\u53e6\u5916 <code>get_vis_datas</code> \u51fd\u6570\u5185\u8fd8\u8bfb\u53d6\u4e86\u5bf9\u5e94\u65f6\u523b\u7684\u771f\u503c\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u4e5f\u5c06\u53ef\u89c6\u5316\u51fa\u6765\uff0c\u65b9\u4fbf\u4e0e\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\u3002</p> <p>\u7531\u4e8e\u6a21\u578b\u5bf9\u98ce\u901f\u7684\u7eac\u5411\u548c\u7ecf\u5411\u5206\u5f00\u9884\u6d4b\uff0c\u56e0\u6b64\u9700\u8981\u628a\u8fd9\u4e24\u4e2a\u65b9\u5411\u4e0a\u7684\u98ce\u901f\u5408\u6210\u4e3a\u771f\u6b63\u7684\u98ce\u901f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_finetune.py<pre><code>def output_wind_func(d, var_name, data_mean, data_std):\n    output = (d[var_name] * data_std) + data_mean\n    wind_data = []\n    for i in range(output.shape[0]):\n        wind_data.append((output[i][0] ** 2 + output[i][1] ** 2) ** 0.5)\n    return paddle.to_tensor(wind_data, paddle.get_default_dtype())\n\nvis_output_expr = {}\nfor i in range(cfg.EVAL.num_timestamps):\n    hour = (i + 1) * 6\n    vis_output_expr[f\"output_{hour}h\"] = functools.partial(\n        output_wind_func,\n        var_name=f\"output_{i}\",\n        data_mean=paddle.to_tensor(data_mean, paddle.get_default_dtype()),\n        data_std=paddle.to_tensor(data_std, paddle.get_default_dtype()),\n    )\n    vis_output_expr[f\"target_{hour}h\"] = lambda d, hour=hour: d[f\"target_{hour}h\"]\n</code></pre> <p>\u6700\u540e\uff0c\u6784\u5efa\u53ef\u89c6\u5316\u5668\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_finetune.py<pre><code># set visualizer\nvisualizer = {\n    \"visualize_wind\": ppsci.visualize.VisualizerWeather(\n        vis_data,\n        vis_output_expr,\n        xticks=np.linspace(0, 1439, 13),\n        xticklabels=[str(i) for i in range(360, -1, -30)],\n        yticks=np.linspace(0, 719, 7),\n        yticklabels=[str(i) for i in range(90, -91, -30)],\n        vmin=0,\n        vmax=25,\n        colorbar_label=\"m\\s\",\n        batch_size=cfg.EVAL.batch_size,\n        num_timestamps=cfg.EVAL.num_timestamps,\n        prefix=\"wind\",\n    )\n}\n</code></pre> <p>\u4ee5\u4e0a\u6784\u5efa\u597d\u7684\u6a21\u578b\u3001\u8bc4\u4f30\u5668\u3001\u53ef\u89c6\u5316\u5668\u5c06\u4f1a\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code> \u7528\u4e8e\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u548c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> examples/fourcastnet/train_finetune.py<pre><code>solver = ppsci.solver.Solver(\n    model,\n    output_dir=cfg.output_dir,\n    validator=validator,\n    visualizer=visualizer,\n    pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\nsolver.eval()\n# visualize prediction from pretrained_model_path\nsolver.visualize()\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#4","title":"4. \u964d\u6c34\u91cf\u6a21\u578b\u5b9e\u73b0","text":"<p>\u9996\u5148\u5c55\u793a\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u5404\u4e2a\u53c2\u6570\u53d8\u91cf\uff0c\u6bcf\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u4f1a\u5728\u4e0b\u9762\u4f7f\u7528\u5230\u65f6\u8fdb\u884c\u89e3\u91ca\u3002</p> examples/fourcastnet/conf/fourcastnet_precip.yaml<pre><code># set training hyper-parameters\nIMG_H: 720\nIMG_W: 1440\n# FourCastNet use 20 atmospheric variable\uff0ctheir index in the dataset is from 0 to 19.\n# The variable name is 'u10', 'v10', 't2m', 'sp', 'msl', 't850', 'u1000', 'v1000', 'z000',\n# 'u850', 'v850', 'z850',  'u500', 'v500', 'z500', 't500', 'z50', 'r500', 'r850', 'tcwv'.\n# You can obtain detailed information about each variable from\n# https://cds.climate.copernicus.eu/cdsapp#!/search?text=era5&amp;type=dataset\nVARS_CHANNEL: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n# set train data path\nWIND_TRAIN_FILE_PATH: ./datasets/era5/train\nWIND_MEAN_PATH: ./datasets/era5/stat/global_means.npy\nWIND_STD_PATH: ./datasets/era5/stat/global_stds.npy\nWIND_TIME_MEAN_PATH: ./datasets/era5/stat/time_means.npy\n\nTRAIN_FILE_PATH: ./datasets/era5/precip/train\nTIME_MEAN_PATH: ./datasets/era5/stat/precip/time_means.npy\n\n# set evaluate data path\nWIND_VALID_FILE_PATH: ./datasets/era5/test\nVALID_FILE_PATH: ./datasets/era5/precip/test\n\n# set test data path\nWIND_TEST_FILE_PATH: ./datasets/era5/out_of_sample/2018.h5\nTEST_FILE_PATH: ./datasets/era5/precip/out_of_sample/2018.h5\n\n# set wind model path\nWIND_MODEL_PATH: outputs_fourcastnet_finetune/checkpoints/latest\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#41","title":"4.1 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\uff0c\u9996\u5148\u4ecb\u7ecd\u6570\u636e\u9884\u5904\u7406\u90e8\u5206\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_precip.py<pre><code>wind_data_mean, wind_data_std = fourcast_utils.get_mean_std(\n    cfg.WIND_MEAN_PATH, cfg.WIND_STD_PATH, cfg.VARS_CHANNEL\n)\ndata_time_mean = fourcast_utils.get_time_mean(\n    cfg.TIME_MEAN_PATH, cfg.IMG_H, cfg.IMG_W\n)\n\n# set train transforms\ntransforms = [\n    {\"SqueezeData\": {}},\n    {\"CropData\": {\"xmin\": (0, 0), \"xmax\": (cfg.IMG_H, cfg.IMG_W)}},\n    {\n        \"Normalize\": {\n            \"mean\": wind_data_mean,\n            \"std\": wind_data_std,\n            \"apply_keys\": (\"input\",),\n        }\n    },\n    {\"Log1p\": {\"scale\": 1e-5, \"apply_keys\": (\"label\",)}},\n]\n</code></pre> <p>\u6570\u636e\u9884\u5904\u7406\u90e8\u5206\u603b\u5171\u5305\u542b 4 \u4e2a\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5206\u522b\u662f:</p> <ol> <li><code>SqueezeData</code>: \u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u7ef4\u5ea6\u8fdb\u884c\u538b\u7f29\uff0c\u5982\u679c\u8f93\u5165\u6570\u636e\u7684\u7ef4\u5ea6\u4e3a 4\uff0c\u5219\u5c06\u7b2c 0 \u7ef4\u548c\u7b2c 1 \u7ef4\u7684\u6570\u636e\u538b\u7f29\u5230\u4e00\u8d77\uff0c\u6700\u7ec8\u5c06\u8f93\u5165\u6570\u636e\u7684\u7ef4\u5ea6\u53d8\u6362\u4e3a 3\u3002</li> <li><code>CropData</code>: \u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u88c1\u526a\u6307\u5b9a\u4f4d\u7f6e\u7684\u6570\u636e\u3002\u56e0\u4e3a ERA5 \u6570\u636e\u96c6\u4e2d\u7684\u539f\u59cb\u6570\u636e\u5c3a\u5bf8\u4e3a \\(721 \\times 1440\\)\uff0c\u672c\u6848\u4f8b\u6839\u636e\u539f\u59cb\u8bba\u6587\u8bbe\u7f6e\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u5c3a\u5bf8\u88c1\u526a\u4e3a \\(720 \\times 1440\\)\u3002</li> <li><code>Normalize</code>: \u6839\u636e\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u7684\u5747\u503c\u3001\u65b9\u5dee\u5bf9\u6570\u636e\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\uff0c\u8fd9\u91cc\u901a\u8fc7 <code>apply_keys</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e86\u8be5\u9884\u5904\u7406\u65b9\u6cd5\u4ec5\u4ec5\u5e94\u7528\u5230\u8f93\u5165\u6570\u636e\u4e0a\u3002</li> <li><code>Log1p</code>: \u5c06\u6570\u636e\u6620\u5c04\u5230\u5bf9\u6570\u7a7a\u95f4\uff0c\u8fd9\u91cc\u901a\u8fc7 <code>apply_keys</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e86\u8be5\u9884\u5904\u7406\u65b9\u6cd5\u4ec5\u4ec5\u5e94\u7528\u5230\u771f\u503c\u6570\u636e\u4e0a\u3002</li> </ol> <p>\u6570\u636e\u52a0\u8f7d\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_precip.py<pre><code># set train dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ERA5Dataset\",\n        \"file_path\": cfg.WIND_TRAIN_FILE_PATH,\n        \"input_keys\": cfg.MODEL.precip.input_keys,\n        \"label_keys\": cfg.MODEL.precip.output_keys,\n        \"vars_channel\": cfg.VARS_CHANNEL,\n        \"precip_file_path\": cfg.TRAIN_FILE_PATH,\n        \"transforms\": transforms,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 8,\n}\n</code></pre> <p>\u5176\u4e2d\uff0c\"dataset\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Dataset</code> \u7c7b\u540d\u4e3a <code>ERA5Dataset</code>\uff0c\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u8bbe\u7f6e\u7684 <code>batch_size</code> \u4e3a 1\uff0c<code>num_works</code> \u4e3a 8\u3002</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_precip.py<pre><code># set constraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.L2RelLoss(),\n    name=\"Sup\",\n)\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u6570\u636e\u7684\u52a0\u8f7d\u65b9\u5f0f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e0a\u6587\u4e2d\u5b9a\u4e49\u7684 <code>train_dataloader_cfg</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u4f7f\u7528 <code>L2RelLoss</code>\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"Sup\"\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#42","title":"4.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0c\u9700\u8981\u9996\u5148\u5b9a\u4e49\u98ce\u901f\u6a21\u578b\u7684\u7f51\u7edc\u7ed3\u6784\u5e76\u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u53c2\u6570\uff0c\u7136\u540e\u5b9a\u4e49\u964d\u6c34\u91cf\u6a21\u578b\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_precip.py<pre><code># set model\nwind_model = ppsci.arch.AFNONet(**cfg.MODEL.afno)\nppsci.utils.save_load.load_pretrain(wind_model, path=cfg.WIND_MODEL_PATH)\nmodel_cfg = dict(cfg.MODEL.precip)\nmodel_cfg.update({\"wind_model\": wind_model})\nmodel = ppsci.arch.PrecipNet(**model_cfg)\n</code></pre> <p>\u5b9a\u4e49\u6a21\u578b\u7684\u53c2\u6570\u901a\u8fc7\u914d\u7f6e\u8fdb\u884c\u8bbe\u7f6e\uff0c\u5982\u4e0b\uff1a</p> examples/fourcastnet/conf/fourcastnet_precip.yaml<pre><code># model settings\nMODEL:\n  afno:\n    input_keys: [\"input\"]\n    output_keys: [\"output\"]\n  precip:\n    input_keys: [\"input\"]\n    output_keys: [\"output\"]\n</code></pre> <p>\u5176\u4e2d\uff0c<code>input_keys</code> \u548c <code>output_keys</code> \u5206\u522b\u4ee3\u8868\u7f51\u7edc\u6a21\u578b\u8f93\u5165\u3001\u8f93\u51fa\u53d8\u91cf\u7684\u540d\u79f0\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#43","title":"4.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>Cosine</code>\uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a 2.5e-4\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>Adam</code>\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_precip.py<pre><code># init optimizer and lr scheduler\nlr_scheduler_cfg = dict(cfg.TRAIN.lr_scheduler)\nlr_scheduler_cfg.update({\"iters_per_epoch\": ITERS_PER_EPOCH})\nlr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(**lr_scheduler_cfg)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#44","title":"4.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_precip.py<pre><code># set eval dataloader config\neval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ERA5Dataset\",\n        \"file_path\": cfg.WIND_VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.precip.input_keys,\n        \"label_keys\": cfg.MODEL.precip.output_keys,\n        \"vars_channel\": cfg.VARS_CHANNEL,\n        \"precip_file_path\": cfg.VALID_FILE_PATH,\n        \"transforms\": transforms,\n        \"training\": False,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n}\n\n# set metric\nmetric = {\n    \"MAE\": ppsci.metric.MAE(keep_batch=True),\n    \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n        num_lat=cfg.IMG_H, keep_batch=True, unlog=True\n    ),\n    \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n        num_lat=cfg.IMG_H, mean=data_time_mean, keep_batch=True, unlog=True\n    ),\n}\n\n# set validator\nsup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.L2RelLoss(),\n    metric=metric,\n    name=\"Sup_Validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre> <p><code>SupervisedValidator</code> \u8bc4\u4f30\u5668\u4e0e <code>SupervisedConstraint</code> \u6bd4\u8f83\u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u8bc4\u4f30\u5668\u9700\u8981\u8bbe\u7f6e\u8bc4\u4ef7\u6307\u6807 <code>metric</code>\uff0c\u5728\u8fd9\u91cc\u4f7f\u7528\u4e86 3 \u4e2a\u8bc4\u4ef7\u6307\u6807\u5206\u522b\u662f <code>MAE</code>\u3001<code>LatitudeWeightedRMSE</code> \u548c <code>LatitudeWeightedACC</code>\u3002</p>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#45","title":"4.5 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/fourcastnet/train_precip.py<pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=True,\n    validator=validator,\n    compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#46","title":"4.6 \u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b","text":"<p>\u6839\u636e\u8bba\u6587\u4e2d\u7684\u8bbe\u7f6e\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c<code>num_timestamps</code> \u8bbe\u7f6e\u4e3a 6\uff0c\u76f8\u90bb\u7684\u4e24\u4e2a\u6d4b\u8bd5\u6837\u672c\u7684\u95f4\u9694\u4e3a 8\u3002</p> <p>\u6784\u5efa\u6a21\u578b\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/fourcastnet/train_precip.py<pre><code># set model for testing\nwind_model = ppsci.arch.AFNONet(**cfg.MODEL.afno)\nppsci.utils.save_load.load_pretrain(wind_model, path=cfg.WIND_MODEL_PATH)\nmodel_cfg = dict(cfg.MODEL.precip)\nmodel_cfg.update(\n    {\n        \"output_keys\": output_keys,\n        \"num_timestamps\": cfg.EVAL.num_timestamps,\n        \"wind_model\": wind_model,\n    }\n)\nmodel = ppsci.arch.PrecipNet(**model_cfg)\n</code></pre> <p>\u6784\u5efa\u8bc4\u4f30\u5668\u7684\u4ee3\u7801\u4e3a\uff1a</p> examples/fourcastnet/train_precip.py<pre><code>eval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"ERA5Dataset\",\n        \"file_path\": cfg.WIND_TEST_FILE_PATH,\n        \"input_keys\": cfg.MODEL.precip.input_keys,\n        \"label_keys\": output_keys,\n        \"vars_channel\": cfg.VARS_CHANNEL,\n        \"precip_file_path\": cfg.TEST_FILE_PATH,\n        \"num_label_timestamps\": cfg.EVAL.num_timestamps,\n        \"stride\": 8,\n        \"transforms\": transforms,\n        \"training\": False,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n}\n# set metirc\nmetric = {\n    \"MAE\": ppsci.metric.MAE(keep_batch=True),\n    \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n        num_lat=cfg.IMG_H, keep_batch=True, unlog=True\n    ),\n    \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n        num_lat=cfg.IMG_H, mean=data_time_mean, keep_batch=True, unlog=True\n    ),\n}\n\n# set validator for testing\nsup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.L2RelLoss(),\n    metric=metric,\n    name=\"Sup_Validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#47","title":"4.7 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u964d\u6c34\u91cf\u6a21\u578b\u4f7f\u7528\u81ea\u56de\u5f52\u7684\u65b9\u5f0f\u8fdb\u884c\u63a8\u7406\uff0c\u9700\u8981\u9996\u5148\u8bbe\u7f6e\u6a21\u578b\u63a8\u7406\u7684\u8f93\u5165\u6570\u636e\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_precip.py<pre><code># set set visualizer data\nDATE_STRINGS = (\"2018-04-04 00:00:00\",)\nvis_data = get_vis_data(\n    cfg.WIND_TEST_FILE_PATH,\n    cfg.TEST_FILE_PATH,\n    DATE_STRINGS,\n    cfg.EVAL.num_timestamps,\n    cfg.VARS_CHANNEL,\n    cfg.IMG_H,\n    wind_data_mean,\n    wind_data_std,\n)\n</code></pre> examples/fourcastnet/train_precip.py<pre><code>def get_vis_data(\n    wind_file_path: str,\n    file_path: str,\n    date_strings: Tuple[str, ...],\n    num_timestamps: int,\n    vars_channel: Tuple[int, ...],\n    img_h: int,\n    data_mean: np.ndarray,\n    data_std: np.ndarray,\n):\n    __wind_file = h5py.File(wind_file_path, \"r\")[\"fields\"]\n    _file = h5py.File(file_path, \"r\")[\"tp\"]\n    wind_data = []\n    data = []\n    for date_str in date_strings:\n        hours_since_jan_01_epoch = fourcast_utils.date_to_hours(date_str)\n        ic = int(hours_since_jan_01_epoch / 6)\n        wind_data.append(__wind_file[ic, vars_channel, 0:img_h])\n        data.append(_file[ic + 1 : ic + num_timestamps + 1, 0:img_h])\n    wind_data = np.asarray(wind_data)\n    data = np.asarray(data)\n\n    vis_data = {\"input\": (wind_data - data_mean) / data_std}\n    for t in range(num_timestamps):\n        hour = (t + 1) * 6\n        data_t = data[:, t]\n        vis_data[f\"target_{hour}h\"] = np.asarray(data_t)\n    return vis_data\n</code></pre> <p>\u4ee5\u4e0a\u7684\u4ee3\u7801\u4e2d\u4f1a\u6839\u636e\u8bbe\u7f6e\u7684\u65f6\u95f4\u53c2\u6570 <code>DATE_STRINGS</code> \u8bfb\u53d6\u5bf9\u5e94\u7684\u6570\u636e\u7528\u4e8e\u6a21\u578b\u7684\u8f93\u5165\uff0c\u53e6\u5916 <code>get_vis_datas</code> \u51fd\u6570\u5185\u8fd8\u8bfb\u53d6\u4e86\u5bf9\u5e94\u65f6\u523b\u7684\u771f\u503c\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u4e5f\u5c06\u53ef\u89c6\u5316\u51fa\u6765\uff0c\u65b9\u4fbf\u4e0e\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\u3002</p> <p>\u7531\u4e8e\u6a21\u578b\u5bf9\u964d\u6c34\u91cf\u8fdb\u884c\u4e86\u5bf9\u6570\u5904\u7406\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u6a21\u578b\u7ed3\u679c\u91cd\u65b0\u6620\u5c04\u56de\u7ebf\u6027\u7a7a\u95f4\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_precip.py<pre><code>def output_precip_func(d, var_name):\n    output = 1e-2 * paddle.expm1(d[var_name][0])\n    return output\n\nvisu_output_expr = {}\nfor i in range(cfg.EVAL.num_timestamps):\n    hour = (i + 1) * 6\n    visu_output_expr[f\"output_{hour}h\"] = functools.partial(\n        output_precip_func,\n        var_name=f\"output_{i}\",\n    )\n    visu_output_expr[f\"target_{hour}h\"] = (\n        lambda d, hour=hour: d[f\"target_{hour}h\"] * 1000\n    )\n</code></pre> <p>\u6700\u540e\uff0c\u6784\u5efa\u53ef\u89c6\u5316\u5668\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/fourcastnet/train_precip.py<pre><code># set visualizer\nvisualizer = {\n    \"visualize_precip\": ppsci.visualize.VisualizerWeather(\n        vis_data,\n        visu_output_expr,\n        xticks=np.linspace(0, 1439, 13),\n        xticklabels=[str(i) for i in range(360, -1, -30)],\n        yticks=np.linspace(0, 719, 7),\n        yticklabels=[str(i) for i in range(90, -91, -30)],\n        vmin=0.001,\n        vmax=130,\n        colorbar_label=\"mm\",\n        log_norm=True,\n        batch_size=cfg.EVAL.batch_size,\n        num_timestamps=cfg.EVAL.num_timestamps,\n        prefix=\"precip\",\n    )\n}\n</code></pre> <p>\u4ee5\u4e0a\u6784\u5efa\u597d\u7684\u6a21\u578b\u3001\u8bc4\u4f30\u5668\u3001\u53ef\u89c6\u5316\u5668\u5c06\u4f1a\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code> \u7528\u4e8e\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u548c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> examples/fourcastnet/train_precip.py<pre><code>solver = ppsci.solver.Solver(\n    model,\n    output_dir=cfg.output_dir,\n    validator=validator,\n    visualizer=visualizer,\n    pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\nsolver.eval()\n# visualize prediction\nsolver.visualize()\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#5","title":"5. \u5b8c\u6574\u4ee3\u7801","text":"examples/fourcastnet/train_pretrain.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle.distributed as dist\nfrom omegaconf import DictConfig\n\nimport examples.fourcastnet.utils as fourcast_utils\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef get_data_stat(cfg: DictConfig):\n    data_mean, data_std = fourcast_utils.get_mean_std(\n        cfg.DATA_MEAN_PATH, cfg.DATA_STD_PATH, cfg.VARS_CHANNEL\n    )\n    data_time_mean = fourcast_utils.get_time_mean(\n        cfg.DATA_TIME_MEAN_PATH, cfg.IMG_H, cfg.IMG_W, cfg.VARS_CHANNEL\n    )\n    data_time_mean_normalize = np.expand_dims(\n        (data_time_mean[0] - data_mean) / data_std, 0\n    )\n    return data_mean, data_std, data_time_mean_normalize\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    data_mean, data_std = fourcast_utils.get_mean_std(\n        cfg.DATA_MEAN_PATH, cfg.DATA_STD_PATH, cfg.VARS_CHANNEL\n    )\n    data_time_mean = fourcast_utils.get_time_mean(\n        cfg.DATA_TIME_MEAN_PATH, cfg.IMG_H, cfg.IMG_W, cfg.VARS_CHANNEL\n    )\n    data_time_mean_normalize = np.expand_dims(\n        (data_time_mean[0] - data_mean) / data_std, 0\n    )\n    # set train transforms\n    transforms = [\n        {\"SqueezeData\": {}},\n        {\"CropData\": {\"xmin\": (0, 0), \"xmax\": (cfg.IMG_H, cfg.IMG_W)}},\n        {\"Normalize\": {\"mean\": data_mean, \"std\": data_std}},\n    ]\n\n    # set train dataloader config\n    if not cfg.USE_SAMPLED_DATA:\n        train_dataloader_cfg = {\n            \"dataset\": {\n                \"name\": \"ERA5Dataset\",\n                \"file_path\": cfg.TRAIN_FILE_PATH,\n                \"input_keys\": cfg.MODEL.afno.input_keys,\n                \"label_keys\": cfg.MODEL.afno.output_keys,\n                \"vars_channel\": cfg.VARS_CHANNEL,\n                \"transforms\": transforms,\n            },\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": True,\n                \"shuffle\": True,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"num_workers\": 8,\n        }\n    else:\n        NUM_GPUS_PER_NODE = 8\n        train_dataloader_cfg = {\n            \"dataset\": {\n                \"name\": \"ERA5SampledDataset\",\n                \"file_path\": cfg.TRAIN_FILE_PATH,\n                \"input_keys\": cfg.MODEL.afno.input_keys,\n                \"label_keys\": cfg.MODEL.afno.output_keys,\n            },\n            \"sampler\": {\n                \"name\": \"DistributedBatchSampler\",\n                \"drop_last\": True,\n                \"shuffle\": True,\n                \"num_replicas\": NUM_GPUS_PER_NODE,\n                \"rank\": dist.get_rank() % NUM_GPUS_PER_NODE,\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"num_workers\": 8,\n        }\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(sup_constraint.data_loader)\n\n    # set eval dataloader config\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ERA5Dataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.afno.input_keys,\n            \"label_keys\": cfg.MODEL.afno.output_keys,\n            \"vars_channel\": cfg.VARS_CHANNEL,\n            \"transforms\": transforms,\n            \"training\": False,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n\n    # set validator\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        metric={\n            \"MAE\": ppsci.metric.MAE(keep_batch=True),\n            \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n                num_lat=cfg.IMG_H,\n                std=data_std,\n                keep_batch=True,\n                variable_dict={\"u10\": 0, \"v10\": 1},\n            ),\n            \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n                num_lat=cfg.IMG_H,\n                mean=data_time_mean_normalize,\n                keep_batch=True,\n                variable_dict={\"u10\": 0, \"v10\": 1},\n            ),\n        },\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set model\n    model = ppsci.arch.AFNONet(**cfg.MODEL.afno)\n\n    # init optimizer and lr scheduler\n    lr_scheduler_cfg = dict(cfg.TRAIN.lr_scheduler)\n    lr_scheduler_cfg.update({\"iters_per_epoch\": ITERS_PER_EPOCH})\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(**lr_scheduler_cfg)()\n\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=True,\n        seed=cfg.seed,\n        validator=validator,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    data_mean, data_std = fourcast_utils.get_mean_std(\n        cfg.DATA_MEAN_PATH, cfg.DATA_STD_PATH, cfg.VARS_CHANNEL\n    )\n    data_time_mean = fourcast_utils.get_time_mean(\n        cfg.DATA_TIME_MEAN_PATH, cfg.IMG_H, cfg.IMG_W, cfg.VARS_CHANNEL\n    )\n    data_time_mean_normalize = np.expand_dims(\n        (data_time_mean[0] - data_mean) / data_std, 0\n    )\n    # set train transforms\n    transforms = [\n        {\"SqueezeData\": {}},\n        {\"CropData\": {\"xmin\": (0, 0), \"xmax\": (cfg.IMG_H, cfg.IMG_W)}},\n        {\"Normalize\": {\"mean\": data_mean, \"std\": data_std}},\n    ]\n\n    # set eval dataloader config\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ERA5Dataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.afno.input_keys,\n            \"label_keys\": cfg.MODEL.afno.output_keys,\n            \"vars_channel\": cfg.VARS_CHANNEL,\n            \"transforms\": transforms,\n            \"training\": False,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n\n    # set validator\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        metric={\n            \"MAE\": ppsci.metric.MAE(keep_batch=True),\n            \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n                num_lat=cfg.IMG_H,\n                std=data_std,\n                keep_batch=True,\n                variable_dict={\"u10\": 0, \"v10\": 1},\n            ),\n            \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n                num_lat=cfg.IMG_H,\n                mean=data_time_mean_normalize,\n                keep_batch=True,\n                variable_dict={\"u10\": 0, \"v10\": 1},\n            ),\n        },\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set model\n    model = ppsci.arch.AFNONet(**cfg.MODEL.afno)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate\n    solver.eval()\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"fourcastnet_pretrain.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> examples/fourcastnet/train_finetune.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nfrom os import path as osp\nfrom typing import Tuple\n\nimport h5py\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport examples.fourcastnet.utils as fourcast_utils\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef get_vis_data(\n    file_path: str,\n    date_strings: Tuple[str, ...],\n    num_timestamps: int,\n    vars_channel: Tuple[int, ...],\n    img_h: int,\n    data_mean: np.ndarray,\n    data_std: np.ndarray,\n):\n    _file = h5py.File(file_path, \"r\")[\"fields\"]\n    data = []\n    for date_str in date_strings:\n        hours_since_jan_01_epoch = fourcast_utils.date_to_hours(date_str)\n        ic = int(hours_since_jan_01_epoch / 6)\n        data.append(_file[ic : ic + num_timestamps + 1, vars_channel, 0:img_h])\n    data = np.asarray(data)\n\n    vis_data = {\"input\": (data[:, 0] - data_mean) / data_std}\n    for t in range(num_timestamps):\n        hour = (t + 1) * 6\n        data_t = data[:, t + 1]\n        wind_data = []\n        for i in range(data_t.shape[0]):\n            wind_data.append((data_t[i][0] ** 2 + data_t[i][1] ** 2) ** 0.5)\n        vis_data[f\"target_{hour}h\"] = np.asarray(wind_data)\n    return vis_data\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.set_random_seed(cfg.seed)\n\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # set training hyper-parameters\n    output_keys = tuple(f\"output_{i}\" for i in range(cfg.TRAIN.num_timestamps))\n\n    data_mean, data_std = fourcast_utils.get_mean_std(\n        cfg.DATA_MEAN_PATH, cfg.DATA_STD_PATH, cfg.VARS_CHANNEL\n    )\n    data_time_mean = fourcast_utils.get_time_mean(\n        cfg.DATA_TIME_MEAN_PATH, cfg.IMG_H, cfg.IMG_W, cfg.VARS_CHANNEL\n    )\n    data_time_mean_normalize = np.expand_dims(\n        (data_time_mean[0] - data_mean) / data_std, 0\n    )\n\n    # set transforms\n    transforms = [\n        {\"SqueezeData\": {}},\n        {\"CropData\": {\"xmin\": (0, 0), \"xmax\": (cfg.IMG_H, cfg.IMG_W)}},\n        {\"Normalize\": {\"mean\": data_mean, \"std\": data_std}},\n    ]\n    # set train dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ERA5Dataset\",\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"input_keys\": cfg.MODEL.afno.input_keys,\n            \"label_keys\": output_keys,\n            \"vars_channel\": cfg.VARS_CHANNEL,\n            \"num_label_timestamps\": cfg.TRAIN.num_timestamps,\n            \"transforms\": transforms,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 8,\n    }\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(sup_constraint.data_loader)\n\n    # set eval dataloader config\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ERA5Dataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.afno.input_keys,\n            \"label_keys\": output_keys,\n            \"vars_channel\": cfg.VARS_CHANNEL,\n            \"transforms\": transforms,\n            \"num_label_timestamps\": cfg.TRAIN.num_timestamps,\n            \"training\": False,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n\n    # set metric\n    metric = {\n        \"MAE\": ppsci.metric.MAE(keep_batch=True),\n        \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n            num_lat=cfg.IMG_H,\n            std=data_std,\n            keep_batch=True,\n            variable_dict={\"u10\": 0, \"v10\": 1},\n        ),\n        \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n            num_lat=cfg.IMG_H,\n            mean=data_time_mean_normalize,\n            keep_batch=True,\n            variable_dict={\"u10\": 0, \"v10\": 1},\n        ),\n    }\n\n    # set validator\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        metric=metric,\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set model\n    model_cfg = dict(cfg.MODEL.afno)\n    model_cfg.update(\n        {\"output_keys\": output_keys, \"num_timestamps\": cfg.TRAIN.num_timestamps}\n    )\n\n    model = ppsci.arch.AFNONet(**model_cfg)\n\n    # init optimizer and lr scheduler\n    lr_scheduler_cfg = dict(cfg.TRAIN.lr_scheduler)\n    lr_scheduler_cfg.update({\"iters_per_epoch\": ITERS_PER_EPOCH})\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(**lr_scheduler_cfg)()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=True,\n        validator=validator,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set testing hyper-parameters\n    output_keys = tuple(f\"output_{i}\" for i in range(cfg.EVAL.num_timestamps))\n\n    data_mean, data_std = fourcast_utils.get_mean_std(\n        cfg.DATA_MEAN_PATH, cfg.DATA_STD_PATH, cfg.VARS_CHANNEL\n    )\n    data_time_mean = fourcast_utils.get_time_mean(\n        cfg.DATA_TIME_MEAN_PATH, cfg.IMG_H, cfg.IMG_W, cfg.VARS_CHANNEL\n    )\n    data_time_mean_normalize = np.expand_dims(\n        (data_time_mean[0] - data_mean) / data_std, 0\n    )\n\n    # set transforms\n    transforms = [\n        {\"SqueezeData\": {}},\n        {\"CropData\": {\"xmin\": (0, 0), \"xmax\": (cfg.IMG_H, cfg.IMG_W)}},\n        {\"Normalize\": {\"mean\": data_mean, \"std\": data_std}},\n    ]\n\n    # set model\n    model_cfg = dict(cfg.MODEL.afno)\n    model_cfg.update(\n        {\"output_keys\": output_keys, \"num_timestamps\": cfg.EVAL.num_timestamps}\n    )\n    model = ppsci.arch.AFNONet(**model_cfg)\n\n    # set eval dataloader config\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ERA5Dataset\",\n            \"file_path\": cfg.TEST_FILE_PATH,\n            \"input_keys\": cfg.MODEL.afno.input_keys,\n            \"label_keys\": output_keys,\n            \"vars_channel\": cfg.VARS_CHANNEL,\n            \"transforms\": transforms,\n            \"num_label_timestamps\": cfg.EVAL.num_timestamps,\n            \"training\": False,\n            \"stride\": 8,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n\n    # set metirc\n    metric = {\n        \"MAE\": ppsci.metric.MAE(keep_batch=True),\n        \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n            num_lat=cfg.IMG_H,\n            std=data_std,\n            keep_batch=True,\n            variable_dict={\"u10\": 0, \"v10\": 1},\n        ),\n        \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n            num_lat=cfg.IMG_H,\n            mean=data_time_mean_normalize,\n            keep_batch=True,\n            variable_dict={\"u10\": 0, \"v10\": 1},\n        ),\n    }\n\n    # set validator for testing\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        metric=metric,\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set visualizer data\n    DATE_STRINGS = (\"2018-09-08 00:00:00\",)\n    vis_data = get_vis_data(\n        cfg.TEST_FILE_PATH,\n        DATE_STRINGS,\n        cfg.EVAL.num_timestamps,\n        cfg.VARS_CHANNEL,\n        cfg.IMG_H,\n        data_mean,\n        data_std,\n    )\n\n    def output_wind_func(d, var_name, data_mean, data_std):\n        output = (d[var_name] * data_std) + data_mean\n        wind_data = []\n        for i in range(output.shape[0]):\n            wind_data.append((output[i][0] ** 2 + output[i][1] ** 2) ** 0.5)\n        return paddle.to_tensor(wind_data, paddle.get_default_dtype())\n\n    vis_output_expr = {}\n    for i in range(cfg.EVAL.num_timestamps):\n        hour = (i + 1) * 6\n        vis_output_expr[f\"output_{hour}h\"] = functools.partial(\n            output_wind_func,\n            var_name=f\"output_{i}\",\n            data_mean=paddle.to_tensor(data_mean, paddle.get_default_dtype()),\n            data_std=paddle.to_tensor(data_std, paddle.get_default_dtype()),\n        )\n        vis_output_expr[f\"target_{hour}h\"] = lambda d, hour=hour: d[f\"target_{hour}h\"]\n    # set visualizer\n    visualizer = {\n        \"visualize_wind\": ppsci.visualize.VisualizerWeather(\n            vis_data,\n            vis_output_expr,\n            xticks=np.linspace(0, 1439, 13),\n            xticklabels=[str(i) for i in range(360, -1, -30)],\n            yticks=np.linspace(0, 719, 7),\n            yticklabels=[str(i) for i in range(90, -91, -30)],\n            vmin=0,\n            vmax=25,\n            colorbar_label=\"m\\s\",\n            batch_size=cfg.EVAL.batch_size,\n            num_timestamps=cfg.EVAL.num_timestamps,\n            prefix=\"wind\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    solver.eval()\n    # visualize prediction from pretrained_model_path\n    solver.visualize()\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"fourcastnet_finetune.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> examples/fourcastnet/train_precip.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nimport os.path as osp\nfrom typing import Tuple\n\nimport h5py\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport examples.fourcastnet.utils as fourcast_utils\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef get_vis_data(\n    wind_file_path: str,\n    file_path: str,\n    date_strings: Tuple[str, ...],\n    num_timestamps: int,\n    vars_channel: Tuple[int, ...],\n    img_h: int,\n    data_mean: np.ndarray,\n    data_std: np.ndarray,\n):\n    __wind_file = h5py.File(wind_file_path, \"r\")[\"fields\"]\n    _file = h5py.File(file_path, \"r\")[\"tp\"]\n    wind_data = []\n    data = []\n    for date_str in date_strings:\n        hours_since_jan_01_epoch = fourcast_utils.date_to_hours(date_str)\n        ic = int(hours_since_jan_01_epoch / 6)\n        wind_data.append(__wind_file[ic, vars_channel, 0:img_h])\n        data.append(_file[ic + 1 : ic + num_timestamps + 1, 0:img_h])\n    wind_data = np.asarray(wind_data)\n    data = np.asarray(data)\n\n    vis_data = {\"input\": (wind_data - data_mean) / data_std}\n    for t in range(num_timestamps):\n        hour = (t + 1) * 6\n        data_t = data[:, t]\n        vis_data[f\"target_{hour}h\"] = np.asarray(data_t)\n    return vis_data\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", f\"{cfg.output_dir}/train.log\", \"info\")\n\n    wind_data_mean, wind_data_std = fourcast_utils.get_mean_std(\n        cfg.WIND_MEAN_PATH, cfg.WIND_STD_PATH, cfg.VARS_CHANNEL\n    )\n    data_time_mean = fourcast_utils.get_time_mean(\n        cfg.TIME_MEAN_PATH, cfg.IMG_H, cfg.IMG_W\n    )\n\n    # set train transforms\n    transforms = [\n        {\"SqueezeData\": {}},\n        {\"CropData\": {\"xmin\": (0, 0), \"xmax\": (cfg.IMG_H, cfg.IMG_W)}},\n        {\n            \"Normalize\": {\n                \"mean\": wind_data_mean,\n                \"std\": wind_data_std,\n                \"apply_keys\": (\"input\",),\n            }\n        },\n        {\"Log1p\": {\"scale\": 1e-5, \"apply_keys\": (\"label\",)}},\n    ]\n\n    # set train dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ERA5Dataset\",\n            \"file_path\": cfg.WIND_TRAIN_FILE_PATH,\n            \"input_keys\": cfg.MODEL.precip.input_keys,\n            \"label_keys\": cfg.MODEL.precip.output_keys,\n            \"vars_channel\": cfg.VARS_CHANNEL,\n            \"precip_file_path\": cfg.TRAIN_FILE_PATH,\n            \"transforms\": transforms,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 8,\n    }\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(sup_constraint.data_loader)\n\n    # set eval dataloader config\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ERA5Dataset\",\n            \"file_path\": cfg.WIND_VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.precip.input_keys,\n            \"label_keys\": cfg.MODEL.precip.output_keys,\n            \"vars_channel\": cfg.VARS_CHANNEL,\n            \"precip_file_path\": cfg.VALID_FILE_PATH,\n            \"transforms\": transforms,\n            \"training\": False,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n\n    # set metric\n    metric = {\n        \"MAE\": ppsci.metric.MAE(keep_batch=True),\n        \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n            num_lat=cfg.IMG_H, keep_batch=True, unlog=True\n        ),\n        \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n            num_lat=cfg.IMG_H, mean=data_time_mean, keep_batch=True, unlog=True\n        ),\n    }\n\n    # set validator\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        metric=metric,\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set model\n    wind_model = ppsci.arch.AFNONet(**cfg.MODEL.afno)\n    ppsci.utils.save_load.load_pretrain(wind_model, path=cfg.WIND_MODEL_PATH)\n    model_cfg = dict(cfg.MODEL.precip)\n    model_cfg.update({\"wind_model\": wind_model})\n    model = ppsci.arch.PrecipNet(**model_cfg)\n\n    # init optimizer and lr scheduler\n    lr_scheduler_cfg = dict(cfg.TRAIN.lr_scheduler)\n    lr_scheduler_cfg.update({\"iters_per_epoch\": ITERS_PER_EPOCH})\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(**lr_scheduler_cfg)()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=True,\n        validator=validator,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set testing hyper-parameters\n    output_keys = tuple(f\"output_{i}\" for i in range(cfg.EVAL.num_timestamps))\n\n    # set model for testing\n    wind_model = ppsci.arch.AFNONet(**cfg.MODEL.afno)\n    ppsci.utils.save_load.load_pretrain(wind_model, path=cfg.WIND_MODEL_PATH)\n    model_cfg = dict(cfg.MODEL.precip)\n    model_cfg.update(\n        {\n            \"output_keys\": output_keys,\n            \"num_timestamps\": cfg.EVAL.num_timestamps,\n            \"wind_model\": wind_model,\n        }\n    )\n    model = ppsci.arch.PrecipNet(**model_cfg)\n\n    wind_data_mean, wind_data_std = fourcast_utils.get_mean_std(\n        cfg.WIND_MEAN_PATH, cfg.WIND_STD_PATH, cfg.VARS_CHANNEL\n    )\n    data_time_mean = fourcast_utils.get_time_mean(\n        cfg.TIME_MEAN_PATH, cfg.IMG_H, cfg.IMG_W\n    )\n\n    # set train transforms\n    transforms = [\n        {\"SqueezeData\": {}},\n        {\"CropData\": {\"xmin\": (0, 0), \"xmax\": (cfg.IMG_H, cfg.IMG_W)}},\n        {\n            \"Normalize\": {\n                \"mean\": wind_data_mean,\n                \"std\": wind_data_std,\n                \"apply_keys\": (\"input\",),\n            }\n        },\n        {\"Log1p\": {\"scale\": 1e-5, \"apply_keys\": (\"label\",)}},\n    ]\n\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"ERA5Dataset\",\n            \"file_path\": cfg.WIND_TEST_FILE_PATH,\n            \"input_keys\": cfg.MODEL.precip.input_keys,\n            \"label_keys\": output_keys,\n            \"vars_channel\": cfg.VARS_CHANNEL,\n            \"precip_file_path\": cfg.TEST_FILE_PATH,\n            \"num_label_timestamps\": cfg.EVAL.num_timestamps,\n            \"stride\": 8,\n            \"transforms\": transforms,\n            \"training\": False,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    }\n    # set metirc\n    metric = {\n        \"MAE\": ppsci.metric.MAE(keep_batch=True),\n        \"LatitudeWeightedRMSE\": ppsci.metric.LatitudeWeightedRMSE(\n            num_lat=cfg.IMG_H, keep_batch=True, unlog=True\n        ),\n        \"LatitudeWeightedACC\": ppsci.metric.LatitudeWeightedACC(\n            num_lat=cfg.IMG_H, mean=data_time_mean, keep_batch=True, unlog=True\n        ),\n    }\n\n    # set validator for testing\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        metric=metric,\n        name=\"Sup_Validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set set visualizer data\n    DATE_STRINGS = (\"2018-04-04 00:00:00\",)\n    vis_data = get_vis_data(\n        cfg.WIND_TEST_FILE_PATH,\n        cfg.TEST_FILE_PATH,\n        DATE_STRINGS,\n        cfg.EVAL.num_timestamps,\n        cfg.VARS_CHANNEL,\n        cfg.IMG_H,\n        wind_data_mean,\n        wind_data_std,\n    )\n\n    def output_precip_func(d, var_name):\n        output = 1e-2 * paddle.expm1(d[var_name][0])\n        return output\n\n    visu_output_expr = {}\n    for i in range(cfg.EVAL.num_timestamps):\n        hour = (i + 1) * 6\n        visu_output_expr[f\"output_{hour}h\"] = functools.partial(\n            output_precip_func,\n            var_name=f\"output_{i}\",\n        )\n        visu_output_expr[f\"target_{hour}h\"] = (\n            lambda d, hour=hour: d[f\"target_{hour}h\"] * 1000\n        )\n    # set visualizer\n    visualizer = {\n        \"visualize_precip\": ppsci.visualize.VisualizerWeather(\n            vis_data,\n            visu_output_expr,\n            xticks=np.linspace(0, 1439, 13),\n            xticklabels=[str(i) for i in range(360, -1, -30)],\n            yticks=np.linspace(0, 719, 7),\n            yticklabels=[str(i) for i in range(90, -91, -30)],\n            vmin=0.001,\n            vmax=130,\n            colorbar_label=\"mm\",\n            log_norm=True,\n            batch_size=cfg.EVAL.batch_size,\n            num_timestamps=cfg.EVAL.num_timestamps,\n            prefix=\"precip\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        compute_metric_by_batch=cfg.EVAL.compute_metric_by_batch,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    solver.eval()\n    # visualize prediction\n    solver.visualize()\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"fourcastnet_precip.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/fourcastnet/#6","title":"6. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u56fe\u5c55\u793a\u4e86\u98ce\u901f\u6a21\u578b\u6309\u71676\u5c0f\u65f6\u95f4\u9694\u7684\u9884\u6d4b\u7ed3\u679c\u548c\u771f\u503c\u7ed3\u679c\u3002</p> <p> </p> \u98ce\u901f\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff08\"output\"\uff09\u4e0e\u771f\u503c\u7ed3\u679c\uff08\"target\"\uff09 <p>\u4e0b\u56fe\u5c55\u793a\u4e86\u964d\u6c34\u91cf\u6a21\u578b\u6309\u71676\u5c0f\u65f6\u95f4\u9694\u7684\u9884\u6d4b\u7ed3\u679c\u548c\u771f\u503c\u7ed3\u679c\u3002</p> <p> </p> \u964d\u6c34\u91cf\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff08\"output\"\uff09\u4e0e\u771f\u503c\u7ed3\u679c\uff08\"target\"\uff09","tags":["\u6570\u636e\u5904\u7406","FNO\u6c42\u89e3\u65b9\u6cd5","\u590d\u6570","\u5085\u91cc\u53f6\u53d8\u6362","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/heat_exchanger/","title":"Heat_Exchanger","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#heat_exchanger","title":"Heat_Exchanger","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code>python heat_exchanger.py\n</code></pre> <pre><code>python heat_exchanger.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/HEDeepONet/HEDeepONet_pretrained.pdparams\n</code></pre> <pre><code>python heat_exchanger.py mode=export\n</code></pre> <pre><code>python heat_exchanger.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 heat_exchanger_pretrained.pdparams The L2 norm error between the actual heat exchanger efficiency and the predicted heat exchanger efficiency: 0.02087MSE.heat_boundary(interior_mse): 0.52005MSE.cold_boundary(interior_mse): 0.16590MSE.wall(interior_mse): 0.01203","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#11","title":"1.1 \u6362\u70ed\u5668","text":"<p>\u6362\u70ed\u5668\uff08\u4ea6\u79f0\u4e3a\u70ed\u4ea4\u6362\u5668\u6216\u70ed\u4ea4\u6362\u8bbe\u5907\uff09\u662f\u7528\u6765\u4f7f\u70ed\u91cf\u4ece\u70ed\u6d41\u4f53\u4f20\u9012\u5230\u51b7\u6d41\u4f53\uff0c\u4ee5\u6ee1\u8db3\u89c4\u5b9a\u7684\u5de5\u827a\u8981\u6c42\u7684\u88c5\u7f6e\uff0c\u662f\u5bf9\u6d41\u4f20\u70ed\u53ca\u70ed\u4f20\u5bfc\u7684\u4e00\u79cd\u5de5\u4e1a\u5e94\u7528\u3002</p> <p>\u5728\u4e00\u822c\u7a7a\u8c03\u8bbe\u5907\u4e2d\u90fd\u6709\u6362\u70ed\u5668\uff0c\u5373\u7a7a\u8c03\u5ba4\u5185\u673a\u548c\u5ba4\u5916\u673a\u7684\u51b7\u70ed\u6392\uff1b\u6362\u70ed\u5668\u4f5c\u653e\u70ed\u7528\u65f6\u79f0\u4e3a\u201c\u51b7\u51dd\u5668\u201d\uff0c\u4f5c\u5438\u70ed\u7528\u65f6\u79f0\u4e3a\u201c\u84b8\u53d1\u5668\u201d\uff0c\u51b7\u5a92\u5728\u6b64\u4e8c\u8005\u7684\u7269\u7406\u53cd\u5e94\u76f8\u53cd\u3002\u6240\u4ee5\u5bb6\u7528\u7a7a\u8c03\u673a\u4f5c\u4e3a\u51b7\u6c14\u673a\u65f6\uff0c\u5ba4\u5185\u673a\u7684\u6362\u70ed\u5668\u79f0\u4f5c\u84b8\u53d1\u5668\uff0c\u5ba4\u5916\u673a\u7684\u5219\u79f0\u4e3a\u51b7\u51dd\u5668\uff1b\u6362\u505a\u6696\u6c14\u673a\u7684\u89d2\u8272\u65f6\uff0c\u5219\u76f8\u53cd\u79f0\u4e4b\uff0c\u5982\u56fe\u6240\u793a\u4e3a\u84b8\u53d1\u5faa\u73af\u5236\u51b7\u7cfb\u7edf\u3002\u7814\u7a76\u6362\u70ed\u5668\u70ed\u4eff\u771f\u53ef\u4ee5\u4e3a\u4f18\u5316\u8bbe\u8ba1\u3001\u63d0\u9ad8\u6027\u80fd\u548c\u53ef\u9760\u6027\u3001\u8282\u80fd\u51cf\u6392\u4ee5\u53ca\u65b0\u6280\u672f\u7814\u53d1\u63d0\u4f9b\u91cd\u8981\u7684\u53c2\u8003\u548c\u6307\u5bfc\u3002</p> <p> </p>  \u84b8\u53d1\u5faa\u73af\u5236\u51b7\u7cfb\u7edf <p>\u6362\u70ed\u5668\u5728\u5de5\u7a0b\u548c\u79d1\u5b66\u9886\u57df\u5177\u6709\u591a\u65b9\u9762\u7684\u91cd\u8981\u6027\uff0c\u5176\u4f5c\u7528\u548c\u4ef7\u503c\u4e3b\u8981\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a</p> <ul> <li>\u80fd\u6e90\u8f6c\u6362\u6548\u7387\uff1a\u6362\u70ed\u5668\u5728\u80fd\u6e90\u8f6c\u6362\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\u3002\u901a\u8fc7\u4f18\u5316\u70ed\u80fd\u7684\u4f20\u9012\u548c\u5229\u7528\uff0c\u80fd\u591f\u63d0\u9ad8\u53d1\u7535\u5382\u3001\u5de5\u4e1a\u751f\u4ea7\u548c\u5176\u4ed6\u80fd\u6e90\u8f6c\u6362\u8fc7\u7a0b\u7684\u6548\u7387\u3002\u5b83\u4eec\u6709\u52a9\u4e8e\u5c06\u71c3\u6599\u4e2d\u7684\u70ed\u80fd\u8f6c\u5316\u4e3a\u7535\u80fd\u6216\u673a\u68b0\u80fd\uff0c\u6700\u5927\u9650\u5ea6\u5730\u5229\u7528\u80fd\u6e90\u8d44\u6e90\u3002</li> <li>\u5de5\u4e1a\u751f\u4ea7\u4f18\u5316\uff1a\u5728\u5316\u5de5\u3001\u77f3\u6cb9\u3001\u5236\u836f\u7b49\u884c\u4e1a\u4e2d\uff0c\u6362\u70ed\u5668\u7528\u4e8e\u52a0\u70ed\u3001\u51b7\u5374\u3001\u84b8\u998f\u548c\u84b8\u53d1\u7b49\u5de5\u827a\u3002\u901a\u8fc7\u6709\u6548\u7684\u6362\u70ed\u5668\u8bbe\u8ba1\u548c\u8fd0\u7528\uff0c\u53ef\u4ee5\u6539\u5584\u751f\u4ea7\u6548\u7387\u3001\u63a7\u5236\u6e29\u5ea6\u548c\u538b\u529b\uff0c\u63d0\u9ad8\u4ea7\u54c1\u8d28\u91cf\uff0c\u5e76\u4e14\u51cf\u5c11\u80fd\u6e90\u6d88\u8017\u3002</li> <li>\u6e29\u5ea6\u63a7\u5236\u4e0e\u8c03\u8282\uff1a\u6362\u70ed\u5668\u53ef\u4ee5\u7528\u4e8e\u63a7\u5236\u6e29\u5ea6\u3002\u5728\u5de5\u4e1a\u751f\u4ea7\u4e2d\uff0c\u4fdd\u6301\u9002\u5f53\u7684\u6e29\u5ea6\u5bf9\u4e8e\u53cd\u5e94\u901f\u7387\u3001\u4ea7\u54c1\u8d28\u91cf\u548c\u8bbe\u5907\u5bff\u547d\u81f3\u5173\u91cd\u8981\u3002\u6362\u70ed\u5668\u80fd\u591f\u5e2e\u52a9\u8c03\u8282\u548c\u7ef4\u6301\u7cfb\u7edf\u7684\u6e29\u5ea6\u5728\u7406\u60f3\u7684\u64cd\u4f5c\u8303\u56f4\u5185\u3002</li> <li>\u73af\u5883\u4fdd\u62a4\u4e0e\u53ef\u6301\u7eed\u53d1\u5c55\uff1a\u901a\u8fc7\u63d0\u9ad8\u80fd\u6e90\u8f6c\u6362\u6548\u7387\u548c\u5de5\u4e1a\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u7684\u80fd\u6e90\u5229\u7528\u7387\uff0c\u6362\u70ed\u5668\u6709\u52a9\u4e8e\u51cf\u5c11\u5bf9\u81ea\u7136\u8d44\u6e90\u7684\u4f9d\u8d56\uff0c\u5e76\u964d\u4f4e\u5bf9\u73af\u5883\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u80fd\u6e90\u6548\u7387\u7684\u63d0\u9ad8\u4e5f\u53ef\u4ee5\u51cf\u5c11\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\uff0c\u6709\u5229\u4e8e\u73af\u5883\u4fdd\u62a4\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u3002</li> <li>\u5de5\u7a0b\u8bbe\u8ba1\u4e0e\u521b\u65b0\uff1a\u5728\u5de5\u7a0b\u8bbe\u8ba1\u9886\u57df\uff0c\u6362\u70ed\u5668\u7684\u4f18\u5316\u8bbe\u8ba1\u548c\u521b\u65b0\u63a8\u52a8\u4e86\u5de5\u7a0b\u6280\u672f\u7684\u53d1\u5c55\u3002\u4e0d\u65ad\u6539\u8fdb\u7684\u6362\u70ed\u5668\u8bbe\u8ba1\u80fd\u591f\u63d0\u9ad8\u6027\u80fd\u3001\u51cf\u5c11\u7a7a\u95f4\u5360\u7528\u5e76\u9002\u5e94\u591a\u79cd\u590d\u6742\u5de5\u827a\u9700\u6c42\u3002</li> </ul> <p>\u7efc\u4e0a\u6240\u8ff0\uff0c\u6362\u70ed\u5668\u5728\u5de5\u7a0b\u548c\u79d1\u5b66\u9886\u57df\u4e2d\u7684\u91cd\u8981\u6027\u4f53\u73b0\u5728\u5176\u5bf9\u80fd\u6e90\u5229\u7528\u6548\u7387\u3001\u5de5\u4e1a\u751f\u4ea7\u8fc7\u7a0b\u4f18\u5316\u3001\u6e29\u5ea6\u63a7\u5236\u3001\u73af\u5883\u4fdd\u62a4\u548c\u5de5\u7a0b\u6280\u672f\u521b\u65b0\u7b49\u65b9\u9762\u7684\u91cd\u8981\u8d21\u732e\u3002\u8fd9\u4e9b\u65b9\u9762\u7684\u4e0d\u65ad\u6539\u8fdb\u548c\u521b\u65b0\u63a8\u52a8\u7740\u5de5\u7a0b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u80fd\u6e90\u548c\u73af\u5883\u65b9\u9762\u7684\u91cd\u8981\u6311\u6218\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#21","title":"2.1 \u95ee\u9898\u63cf\u8ff0","text":"<p>\u5047\u8bbe\u6362\u70ed\u5668\u5185\u90e8\u6d41\u4f53\u6d41\u52a8\u662f\u4e00\u7ef4\u7684\uff0c\u5982\u56fe\u6240\u793a\u3002</p> <p></p> <p>\u5ffd\u7565\u58c1\u9762\u7684\u4f20\u70ed\u70ed\u963b\u548c\u8f74\u5411\u70ed\u4f20\u5bfc\uff1b\u4e0e\u5916\u754c\u65e0\u70ed\u91cf\u4ea4\u6362\uff0c\u5982\u56fe\u6240\u793a\u3002\u5219\u51b7\u70ed\u6d41\u4f53\u548c\u4f20\u70ed\u58c1\u9762\u4e09\u4e2a\u8282\u70b9\u7684\u80fd\u91cf\u5b88\u6052\u65b9\u7a0b\u5206\u522b\u4e3a\uff1a</p> \\[ \\begin{aligned} &amp; L\\left(\\frac{q_m c_p}{v}\\right)_{\\mathrm{c}} \\frac{\\partial T_{\\mathrm{c}}}{\\partial \\tau}-L\\left(q_m c_p\\right)_{\\mathrm{c}} \\frac{\\partial T_{\\mathrm{c}}}{\\partial x}=\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{c}}\\left(T_{\\mathrm{w}}-T_{\\mathrm{c}}\\right), \\\\ &amp; L\\left(\\frac{q_m c_p}{v}\\right)_{\\mathrm{h}} \\frac{\\partial T_{\\mathrm{h}}}{\\partial \\tau}+L\\left(q_m c_p\\right)_{\\mathrm{h}} \\frac{\\partial T_{\\mathrm{h}}}{\\partial x}=\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{h}}\\left(T_{\\mathrm{w}}-T_{\\mathrm{h}}\\right), \\\\ &amp; \\left(M c_p\\right)_{\\mathrm{w}} \\frac{\\partial T_{\\mathrm{w}}}{\\partial \\tau}=\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{h}}\\left(T_{\\mathrm{h}}-T_{\\mathrm{w}}\\right)+\\left(\\eta_{\\mathrm{o}} \\alpha A\\right)_{\\mathrm{c}}\\left(T_{\\mathrm{c}}-T_{\\mathrm{w}}\\right). \\end{aligned} \\] <p>\u5176\u4e2d:</p> <ul> <li>\\(T\\) \u4ee3\u8868\u6e29\u5ea6\uff0c</li> <li>\\(q_m\\) \u4ee3\u8868\u8d28\u91cf\u6d41\u91cf\uff0c</li> <li>\\(c_p\\) \u4ee3\u8868\u6bd4\u70ed\u5bb9\uff0c</li> <li>\\(v\\) \u4ee3\u8868\u6d41\u901f\uff0c</li> <li>\\(L\\) \u4ee3\u8868\u6d41\u52a8\u957f\u5ea6\uff0c</li> <li>\\(\\eta_{\\mathrm{o}}\\) \u4ee3\u8868\u7fc5\u7247\u8868\u9762\u6548\u7387\uff0c</li> <li>\\(\\alpha\\) \u4ee3\u8868\u4f20\u70ed\u7cfb\u6570\uff0c</li> <li>\\(A\\) \u4ee3\u8868\u4f20\u70ed\u9762\u79ef\uff0c</li> <li>\\(M\\) \u4ee3\u8868\u4f20\u70ed\u7ed3\u6784\u7684\u8d28\u91cf\uff0c</li> <li>\\(\\tau\\) \u4ee3\u8868\u5bf9\u5e94\u65f6\u95f4\uff0c</li> <li>\\(x\\) \u4ee3\u8868\u6d41\u52a8\u65b9\u5411\uff0c</li> <li>\u4e0b\u6807 \\(\\mathrm{h}\\)\u3001\\(\\mathrm{c}\\) \u548c \\(\\mathrm{w}\\) \u5206\u522b\u8868\u793a\u70ed\u8fb9\u6d41\u4f53\u3001\u51b7\u8fb9\u6d41\u4f53\u548c\u6362\u70ed\u58c1\u9762\u3002</li> </ul> <p>\u6362\u70ed\u5668\u51b7\u3001\u70ed\u6d41\u4f53\u8fdb\u51fa\u53e3\u53c2\u6570\u6ee1\u8db3\u80fd\u91cf\u5b88\u6052, \u5373:</p> \\[ \\left(q_m c_p\\right)_{\\mathrm{h}}\\left(T_{\\mathrm{h}, \\text { in }}-T_{\\mathrm{h}, \\text { out }}\\right)=\\left(q_m c_p\\right)_c\\left(T_{\\mathrm{c}, \\text {out }}-T_{\\mathrm{c}, \\text {in }}\\right). \\] <p>\u6362\u70ed\u5668\u6548\u7387 \\(\\eta\\) \u4e3a\u5b9e\u9645\u4f20\u70ed\u91cf\u4e0e\u7406\u8bba\u6700\u5927\u7684\u4f20\u70ed\u91cf\u4e4b\u6bd4\uff0c\u5373\uff1a</p> \\[ \\eta=\\frac{\\left(q_m c_p\\right)_{\\mathrm{h}}\\left(T_{\\mathrm{h}, \\text { in }}-T_{\\mathrm{h}, \\text { out }}\\right)}{\\left(q_m c_p\\right)_{\\text {min }}\\left(T_{\\mathrm{h}, \\text { in }}-T_{\\mathrm{c}, \\text { in }}\\right)}, \\] <p>\u5f0f\u4e2d\uff0c\u4e0b\u6807 \\(min\\) \u8868\u793a\u51b7\u70ed\u6d41\u4f53\u70ed\u5bb9\u8f83\u5c0f\u503c\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#22-pi-deeponet","title":"2.2 PI-DeepONet\u6a21\u578b","text":"<p>PI-DeepONet\u6a21\u578b\uff0c\u5c06 DeepONet \u548c PINN \u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u7269\u7406\u4fe1\u606f\u548c\u7b97\u5b50\u5b66\u4e60\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u8fd9\u79cd\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u65b9\u7a0b\u7684\u7269\u7406\u4fe1\u606f\u6765\u589e\u5f3a DeepONet \u6a21\u578b\uff0c\u540c\u65f6\u53ef\u4ee5\u5c06\u4e0d\u540c\u7684 PDE \u914d\u7f6e\u5206\u522b\u4f5c\u4e3a\u4e0d\u540c\u7684\u5206\u652f\u7f51\u7edc\u7684\u8f93\u5165\u6570\u636e\uff0c\u4ece\u800c\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u5728\u5404\u79cd\uff08\u53c2\u6570\u548c\u975e\u53c2\u6570\uff09PDE \u914d\u7f6e\u4e0b\u8fdb\u884c\u8d85\u5feb\u901f\u7684\u6a21\u578b\u9884\u6d4b\u3002</p> <p>\u5bf9\u4e8e\u6362\u70ed\u5668\u95ee\u9898\uff0cPI-DeepONet \u6a21\u578b\u53ef\u4ee5\u8868\u793a\u4e3a\u5982\u56fe\u6240\u793a\u7684\u6a21\u578b\u7ed3\u6784\uff1a</p> <p></p> <p>\u5982\u56fe\u6240\u793a\uff0c\u6211\u4eec\u4e00\u5171\u4f7f\u7528\u4e86 2 \u4e2a\u5206\u652f\u7f51\u7edc\u548c\u4e00\u4e2a\u4e3b\u5e72\u7f51\u7edc\uff0c\u5206\u652f\u7f51\u7edc\u5206\u522b\u8f93\u5165\u70ed\u8fb9\u7684\u8d28\u91cf\u6d41\u91cf\u548c\u51b7\u8fb9\u7684\u8d28\u91cf\u6d41\u91cf\uff0c\u4e3b\u5e72\u7f51\u7edc\u8f93\u5165\u4e00\u7ef4\u5750\u6807\u70b9\u5750\u6807\u548c\u65f6\u95f4\u4fe1\u606f\u3002\u6bcf\u4e2a\u5206\u652f\u7f51\u548c\u4e3b\u5e72\u7f51\u5747\u8f93\u51fa \\(q\\) \u7ef4\u7279\u5f81\u5411\u91cf\uff0c\u901a\u8fc7Hadamard\uff08\u9010\u5143\u7d20\uff09\u4e58\u79ef\u7ec4\u5408\u6240\u6709\u8fd9\u4e9b\u8f93\u51fa\u7279\u5f81\uff0c\u7136\u540e\u5c06\u6240\u5f97\u5411\u91cf\u76f8\u52a0\u4e3a\u9884\u6d4b\u6e29\u5ea6\u573a\u7684\u6807\u91cf\u8f93\u51fa\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u8be5\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u6362\u70ed\u5668\u70ed\u4eff\u771f\u95ee\u9898\u3002\u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u7ea6\u675f\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003API\u6587\u6863\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u6362\u70ed\u5668\u70ed\u4eff\u771f\u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((t, x)\\) \u548c\u6bcf\u4e00\u7ec4\u70ed\u8fb9\u7684\u8d28\u91cf\u6d41\u91cf\u548c\u51b7\u8fb9\u7684\u8d28\u91cf\u6d41\u91cf \\((q_{mh}, q_{mc})\\) \u90fd\u5bf9\u5e94\u4e00\u7ec4\u70ed\u8fb9\u6d41\u4f53\u7684\u6e29\u5ea6 \\(T_h\\) \u3001\u51b7\u8fb9\u6d41\u4f53\u7684\u6e29\u5ea6 \\(T_c\\) \u548c\u6362\u70ed\u58c1\u9762\u7684\u6e29\u5ea6 \\(T_h\\) \u4e09\u4e2a\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\u3002\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528 2 \u4e2a\u5206\u652f\u7f51\u7edc\u548c\u4e00\u4e2a\u4e3b\u5e72\u7f51\u7edc\uff0c3 \u4e2a\u7f51\u7edc\u5747\u4e3a MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u3002 2 \u4e2a\u5206\u652f\u7f51\u7edc\u5206\u522b\u8868\u793a \\((q_{mh}, q_{mc})\\) \u5230\u8f93\u51fa\u51fd\u6570 \\((b_1\uff0cb_2)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f_1\uff0cf_2: \\mathbb{R}^2 \\to \\mathbb{R}^{3q}\\)\uff0c\u5373\uff1a</p> \\[ \\begin{aligned} b_1 &amp;= f_1(q_{mh}),\\\\ b_2 &amp;= f_2(q_{mc}). \\end{aligned} \\] <p>\u4e0a\u5f0f\u4e2d \\(f_1,f_2\\) \u5747\u4e3a MLP \u6a21\u578b\uff0c\\((b_1\uff0cb_2)\\) \u5206\u522b\u4e3a\u4e24\u4e2a\u5206\u652f\u7f51\u7edc\u7684\u8f93\u51fa\u51fd\u6570\uff0c\\(3q\\) \u4e3a\u8f93\u51fa\u51fd\u6570\u7684\u7ef4\u6570\u3002\u4e3b\u5e72\u7f51\u7edc\u8868\u793a \\((t, x)\\) \u5230\u8f93\u51fa\u51fd\u6570 \\(t_0\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f_3: \\mathbb{R}^2 \\to \\mathbb{R}^{3q}\\)\uff0c\u5373\uff1a</p> \\[ \\begin{aligned} t_0 &amp;= f_3(t,x). \\end{aligned} \\] <p>\u4e0a\u5f0f\u4e2d \\(f_3\\) \u4e3a MLP \u6a21\u578b\uff0c\\((t_0)\\) \u4e3a\u4e3b\u652f\u7f51\u7edc\u7684\u8f93\u51fa\u51fd\u6570\uff0c\\(3q\\) \u4e3a\u8f93\u51fa\u51fd\u6570\u7684\u7ef4\u6570\u3002\u6211\u4eec\u53ef\u4ee5\u5c06\u4e24\u4e2a\u5206\u652f\u7f51\u7edc\u548c\u4e3b\u5e72\u7f51\u7edc\u7684\u8f93\u51fa\u51fd\u6570 \\((b_1,b_2, t_0)\\) \u5206\u62103\u7ec4\uff0c\u7136\u540e\u5bf9\u6bcf\u4e00\u7ec4\u7684\u8f93\u51fa\u51fd\u6570\u5206\u522b\u8fdb\u884cHadamard\uff08\u9010\u5143\u7d20\uff09\u4e58\u79ef\u518d\u76f8\u52a0\u5f97\u5230\u6807\u91cf\u6e29\u5ea6\u573a\uff0c\u5373\uff1a</p> \\[ \\begin{aligned} T_h &amp;= \\sum_{i=1}^q b_1^ib_2^i t_0^i,\\\\ T_c &amp;= \\sum_{i=q+1}^{2q} b_1^ib_2^i t_0^i,\\\\ T_w &amp;= \\sum_{i=2q+1}^{3q} b_1^ib_2^i t_0^i. \\end{aligned} \\] <p>\u6211\u4eec\u5b9a\u4e49 PaddleScience \u5185\u7f6e\u7684 HEDeepONets \u6a21\u578b\u7c7b\uff0c\u5e76\u8c03\u7528\uff0cPaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.HEDeepONets(**cfg.MODEL)\n</code></pre> <p>\u8fd9\u6837\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 3 \u4e2a MLP \u6a21\u578b\u7684 HEDeepONets \u6a21\u578b\uff0c\u6bcf\u4e2a\u5206\u652f\u7f51\u7edc\u5305\u542b 9 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 256\uff0c\u4e3b\u5e72\u7f51\u7edc\u5305\u542b 6 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 128\uff0c\u4f7f\u7528 \"swish\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5305\u542b\u4e09\u4e2a\u8f93\u51fa\u51fd\u6570 \\(T_h,T_c,T_w\\) \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#32","title":"3.2 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u5bf9\u672c\u6587\u4e2d\u6362\u70ed\u5668\u95ee\u9898\u6784\u9020\u8bad\u7ec3\u533a\u57df\uff0c\u5373\u4ee5 [0, 1] \u7684\u4e00\u7ef4\u533a\u57df\uff0c\u4e14\u65f6\u95f4\u57df\u4e3a 21 \u4e2a\u65f6\u523b [0,1,2,...,21]\uff0c\u8be5\u533a\u57df\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u7a7a\u95f4\u51e0\u4f55 <code>Interval</code> \u548c\u65f6\u95f4\u57df <code>TimeDomain</code>\uff0c\u7ec4\u5408\u6210\u65f6\u95f4-\u7a7a\u95f4\u7684 <code>TimeXGeometry</code> \u8ba1\u7b97\u57df\u3002\u4ee3\u7801\u5982\u4e0b</p> <pre><code># set time-geometry\ntimestamps = np.linspace(0.0, 2, cfg.NTIME + 1, endpoint=True)\ngeom = {\n    \"time_rect\": ppsci.geometry.TimeXGeometry(\n        ppsci.geometry.TimeDomain(0.0, 1, timestamps=timestamps),\n        ppsci.geometry.Interval(0, cfg.DL),\n    )\n}\n</code></pre> \u63d0\u793a <p><code>Rectangle</code> \u548c <code>TimeDomain</code> \u662f\u4e24\u79cd\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\u7684 <code>Geometry</code> \u6d3e\u751f\u7c7b\u3002</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e8e\u4e8c\u7ef4\u77e9\u5f62\u51e0\u4f55\u57df\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.Rectangle(...)</code> \u521b\u5efa\u7a7a\u95f4\u51e0\u4f55\u57df\u5bf9\u8c61\uff1b</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e00\u7ef4\u65f6\u95f4\u57df\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.TimeDomain(...)</code> \u6784\u5efa\u65f6\u95f4\u57df\u5bf9\u8c61\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#33","title":"3.3 \u8f93\u5165\u6570\u636e\u6784\u5efa","text":"<ul> <li>\u901a\u8fc7 <code>TimeXGeometry</code> \u8ba1\u7b97\u57df\u6765\u6784\u5efa\u8f93\u5165\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u5747\u5300\u6570\u636e\uff0c</li> <li>\u901a\u8fc7 <code>np.random.rand</code> \u6765\u751f\u6210 (0,2) \u4e4b\u95f4\u7684\u968f\u673a\u6570\uff0c\u8fd9\u4e9b\u968f\u673a\u6570\u7528\u4e8e\u6784\u5efa\u70ed\u8fb9\u548c\u51b7\u8fb9\u7684\u8d28\u91cf\u6d41\u91cf\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u3002</li> </ul> <p>\u5bf9\u65f6\u95f4\u3001\u7a7a\u95f4\u5747\u5300\u6570\u636e\u548c\u70ed\u8fb9\u3001\u51b7\u8fb9\u7684\u8d28\u91cf\u6d41\u91cf\u6570\u636e\u8fdb\u884c\u7ec4\u5408\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u8f93\u5165\u6570\u636e\u3002\u4ee3\u7801\u5982\u4e0b</p> <pre><code># Generate train data and eval data\nvisu_input = geom[\"time_rect\"].sample_interior(cfg.NPOINT * cfg.NTIME, evenly=True)\ndata_h = np.random.rand(cfg.NQM).reshape([-1, 1]) * 2\ndata_c = np.random.rand(cfg.NQM).reshape([-1, 1]) * 2\ndata_h = data_h.astype(\"float32\")\ndata_c = data_c.astype(\"float32\")\ntest_h = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\ntest_c = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\n# rearrange train data and eval data\npoints = visu_input.copy()\npoints[\"t\"] = np.repeat(points[\"t\"], cfg.NQM, axis=0)\npoints[\"x\"] = np.repeat(points[\"x\"], cfg.NQM, axis=0)\npoints[\"qm_h\"] = np.tile(data_h, (cfg.NPOINT * cfg.NTIME, 1))\npoints[\"t\"] = np.repeat(points[\"t\"], cfg.NQM, axis=0)\npoints[\"x\"] = np.repeat(points[\"x\"], cfg.NQM, axis=0)\npoints[\"qm_h\"] = np.repeat(points[\"qm_h\"], cfg.NQM, axis=0)\npoints[\"qm_c\"] = np.tile(data_c, (cfg.NPOINT * cfg.NTIME * cfg.NQM, 1))\nvisu_input[\"qm_h\"] = np.tile(test_h, (cfg.NPOINT * cfg.NTIME, 1))\nvisu_input[\"qm_c\"] = np.tile(test_c, (cfg.NPOINT * cfg.NTIME, 1))\n</code></pre> <p>\u7136\u540e\u5bf9\u8bad\u7ec3\u6570\u636e\u6309\u7167\u7a7a\u95f4\u5750\u6807\u548c\u65f6\u95f4\u8fdb\u884c\u5206\u7c7b\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u7c7b\u6210\u5de6\u8fb9\u754c\u6570\u636e\u3001\u5185\u90e8\u6570\u636e\u3001\u53f3\u8fb9\u754c\u6570\u636e\u4ee5\u53ca\u521d\u503c\u6570\u636e\u3002\u4ee3\u7801\u5982\u4e0b</p> <pre><code>left_indices = visu_input[\"x\"] == 0\nright_indices = visu_input[\"x\"] == cfg.DL\ninterior_indices = (visu_input[\"x\"] != 0) &amp; (visu_input[\"x\"] != cfg.DL)\nleft_indices = np.where(left_indices)\nright_indices = np.where(right_indices)\ninterior_indices = np.where(interior_indices)\n\nleft_indices1 = points[\"x\"] == 0\nright_indices1 = points[\"x\"] == cfg.DL\ninterior_indices1 = (points[\"x\"] != 0) &amp; (points[\"x\"] != cfg.DL)\ninitial_indices1 = points[\"t\"] == points[\"t\"][0]\nleft_indices1 = np.where(left_indices1)\nright_indices1 = np.where(right_indices1)\ninterior_indices1 = np.where(interior_indices1)\ninitial_indices1 = np.where(initial_indices1)\n\n# Classification train data\nleft_data = {\n    \"x\": points[\"x\"][left_indices1[0]],\n    \"t\": points[\"t\"][left_indices1[0]],\n    \"qm_h\": points[\"qm_h\"][left_indices1[0]],\n    \"qm_c\": points[\"qm_c\"][left_indices1[0]],\n}\nright_data = {\n    \"x\": points[\"x\"][right_indices1[0]],\n    \"t\": points[\"t\"][right_indices1[0]],\n    \"qm_h\": points[\"qm_h\"][right_indices1[0]],\n    \"qm_c\": points[\"qm_c\"][right_indices1[0]],\n}\ninterior_data = {\n    \"x\": points[\"x\"],\n    \"t\": points[\"t\"],\n    \"qm_h\": points[\"qm_h\"],\n    \"qm_c\": points[\"qm_c\"],\n}\ninitial_data = {\n    \"x\": points[\"x\"][initial_indices1[0]],\n    \"t\": points[\"t\"][initial_indices1[0]] * 0,\n    \"qm_h\": points[\"qm_h\"][initial_indices1[0]],\n    \"qm_c\": points[\"qm_c\"][initial_indices1[0]],\n}\n# Classification eval data\ntest_left_data = {\n    \"x\": visu_input[\"x\"][left_indices[0]],\n    \"t\": visu_input[\"t\"][left_indices[0]],\n    \"qm_h\": visu_input[\"qm_h\"][left_indices[0]],\n    \"qm_c\": visu_input[\"qm_c\"][left_indices[0]],\n}\ntest_right_data = {\n    \"x\": visu_input[\"x\"][right_indices[0]],\n    \"t\": visu_input[\"t\"][right_indices[0]],\n    \"qm_h\": visu_input[\"qm_h\"][right_indices[0]],\n    \"qm_c\": visu_input[\"qm_c\"][right_indices[0]],\n}\ntest_interior_data = {\n    \"x\": visu_input[\"x\"],\n    \"t\": visu_input[\"t\"],\n    \"qm_h\": visu_input[\"qm_h\"],\n    \"qm_c\": visu_input[\"qm_c\"],\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#34","title":"3.4 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u6362\u70ed\u5668\u70ed\u4eff\u771f\u95ee\u9898\u7531 2.1 \u95ee\u9898\u63cf\u8ff0 \u4e2d\u63cf\u8ff0\u7684\u65b9\u7a0b\u7ec4\u6210\uff0c\u8fd9\u91cc\u6211\u4eec\u5b9a\u4e49 PaddleScience \u5185\u7f6e\u7684 <code>HeatEquation</code> \u65b9\u7a0b\u7c7b\u6765\u6784\u5efa\u8be5\u65b9\u7a0b\u3002\u6307\u5b9a\u8be5\u7c7b\u7684\u53c2\u6570\u5747\u4e3a1\uff0c\u4ee3\u7801\u5982\u4e0b</p> <pre><code># set equation\nequation = {\n    \"heat_exchanger\": ppsci.equation.HeatExchanger(\n        cfg.alpha_h / (cfg.L * cfg.cp_h),\n        cfg.alpha_c / (cfg.L * cfg.cp_c),\n        cfg.v_h,\n        cfg.v_c,\n        cfg.alpha_h / (cfg.M * cfg.cp_w),\n        cfg.alpha_c / (cfg.M * cfg.cp_w),\n    )\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#35","title":"3.5 \u7ea6\u675f\u6784\u5efa","text":"<p>\u6362\u70ed\u5668\u70ed\u4eff\u771f\u95ee\u9898\u7531 2.1 \u95ee\u9898\u63cf\u8ff0 \u4e2d\u63cf\u8ff0\u7684\u65b9\u7a0b\u7ec4\u6210\uff0c\u6211\u4eec\u8bbe\u7f6e\u4ee5\u4e0b\u8fb9\u754c\u6761\u4ef6\uff1a</p> \\[ \\begin{aligned} T_h(t,0) &amp;= 10,\\\\ T_c(t,1) &amp;= 1. \\end{aligned} \\] <p>\u540c\u65f6\uff0c\u6211\u4eec\u8bbe\u7f6e\u521d\u503c\u6761\u4ef6\uff1a</p> \\[ \\begin{aligned} T_h(0,x) &amp;= 10,\\\\ T_c(0,x) &amp;= 1,\\\\ T_w(0,x) &amp;= 5.5. \\end{aligned} \\] <p>\u6b64\u65f6\u6211\u4eec\u5bf9\u5de6\u8fb9\u754c\u6570\u636e\u3001\u5185\u90e8\u6570\u636e\u3001\u53f3\u8fb9\u754c\u6570\u636e\u4ee5\u53ca\u521d\u503c\u6570\u636e\u8bbe\u7f6e\u56db\u4e2a\u7ea6\u675f\u6761\u4ef6\uff0c\u63a5\u4e0b\u6765\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u4e0a\u8ff0\u56db\u79cd\u7ea6\u675f\u6761\u4ef6\uff0c\u4ee3\u7801\u5982\u4e0b</p> <pre><code># set constraint\nbc_label = {\n    \"T_h\": np.zeros([left_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n}\ninterior_label = {\n    \"heat_boundary\": np.zeros([interior_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    \"cold_boundary\": np.zeros([interior_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    \"wall\": np.zeros([interior_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n}\ninitial_label = {\n    \"T_h\": np.zeros([initial_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    \"T_c\": np.zeros([initial_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    \"T_w\": np.zeros([initial_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n}\n\nleft_sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": left_data,\n            \"label\": bc_label,\n            \"weight\": {\n                \"T_h\": np.full_like(\n                    left_data[\"x\"], cfg.TRAIN.weight.left_sup_constraint.T_h\n                )\n            },\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\"T_h\": lambda out: out[\"T_h\"] - cfg.T_hin},\n    name=\"left_sup\",\n)\nright_sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": right_data,\n            \"label\": bc_label,\n            \"weight\": {\n                \"T_h\": np.full_like(\n                    right_data[\"x\"], cfg.TRAIN.weight.right_sup_constraint.T_h\n                )\n            },\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\"T_h\": lambda out: out[\"T_c\"] - cfg.T_cin},\n    name=\"right_sup\",\n)\ninterior_sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": interior_data,\n            \"label\": interior_label,\n            \"weight\": {\n                \"heat_boundary\": np.full_like(\n                    interior_data[\"x\"],\n                    cfg.TRAIN.weight.interior_sup_constraint.heat_boundary,\n                ),\n                \"cold_boundary\": np.full_like(\n                    interior_data[\"x\"],\n                    cfg.TRAIN.weight.interior_sup_constraint.cold_boundary,\n                ),\n                \"wall\": np.full_like(\n                    interior_data[\"x\"],\n                    cfg.TRAIN.weight.interior_sup_constraint.wall,\n                ),\n            },\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr=equation[\"heat_exchanger\"].equations,\n    name=\"interior_sup\",\n)\ninitial_sup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": initial_data,\n            \"label\": initial_label,\n            \"weight\": {\n                \"T_h\": np.full_like(\n                    initial_data[\"x\"], cfg.TRAIN.weight.initial_sup_constraint.T_h\n                ),\n                \"T_c\": np.full_like(\n                    initial_data[\"x\"], cfg.TRAIN.weight.initial_sup_constraint.T_c\n                ),\n                \"T_w\": np.full_like(\n                    initial_data[\"x\"], cfg.TRAIN.weight.initial_sup_constraint.T_w\n                ),\n            },\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\n        \"T_h\": lambda out: out[\"T_h\"] - cfg.T_hin,\n        \"T_c\": lambda out: out[\"T_c\"] - cfg.T_cin,\n        \"T_w\": lambda out: out[\"T_w\"] - cfg.T_win,\n    },\n    name=\"initial_sup\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u5176\u4e2d <code>\u201cdataset\u201d</code> \u5b57\u6bb5\u8868\u793a\u4f7f\u7528\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4fe1\u606f\uff0c\u5404\u4e2a\u5b57\u6bb5\u5206\u522b\u8868\u793a\uff1a</p> <ol> <li><code>name</code>\uff1a \u6570\u636e\u96c6\u7c7b\u578b\uff0c\u6b64\u5904 <code>\"NamedArrayDataset\"</code> \u8868\u793a\u5206 batch \u987a\u5e8f\u8bfb\u53d6\u6570\u636e\uff1b</li> <li><code>input</code>\uff1a \u8f93\u5165\u53d8\u91cf\u540d\uff1b</li> <li><code>label</code>\uff1a \u6807\u7b7e\u53d8\u91cf\u540d\uff1b</li> <li><code>weight</code>\uff1a \u6743\u91cd\u5927\u5c0f\u3002</li> </ol> <p>\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u53e6\u5916\u8fd8\u6307\u5b9a\u4e86\u8be5\u7c7b\u521d\u59cb\u5316\u65f6\u53c2\u6570 <code>drop_last</code> \u4e3a <code>False</code>\u3001<code>shuffle</code> \u4e3a <code>True</code>\u3002</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684 MSE \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\u53d6\u5e73\u5747\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002</p> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u548c\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    left_sup_constraint.name: left_sup_constraint,\n    right_sup_constraint.name: right_sup_constraint,\n    interior_sup_constraint.name: interior_sup_constraint,\n    initial_sup_constraint.name: initial_sup_constraint,\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u5b66\u4e60\u7387\uff0c\u5b66\u4e60\u7387\u8bbe\u4e3a 0.001\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u6211\u4eec\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\ntest_bc_label = {\n    \"T_h\": np.zeros([test_left_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n}\ntest_interior_label = {\n    \"heat_boundary\": np.zeros(\n        [test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"\n    ),\n    \"cold_boundary\": np.zeros(\n        [test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"\n    ),\n    \"wall\": np.zeros([test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n}\nleft_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": test_left_data,\n            \"label\": test_bc_label,\n        },\n        \"batch_size\": cfg.NTIME,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\"T_h\": lambda out: out[\"T_h\"] - cfg.T_hin},\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"left_mse\",\n)\nright_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": test_right_data,\n            \"label\": test_bc_label,\n        },\n        \"batch_size\": cfg.NTIME,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr={\"T_h\": lambda out: out[\"T_c\"] - cfg.T_cin},\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"right_mse\",\n)\ninterior_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": test_interior_data,\n            \"label\": test_interior_label,\n        },\n        \"batch_size\": cfg.NTIME,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr=equation[\"heat_exchanger\"].equations,\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"interior_mse\",\n)\nvalidator = {\n    left_validator.name: left_validator,\n    right_validator.name: right_validator,\n    interior_validator.name: interior_validator,\n}\n</code></pre> <p>\u914d\u7f6e\u4e0e 3.5 \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#38","title":"3.8 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# plotting iteration/epoch-loss curve.\nsolver.plot_loss_history()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#39","title":"3.9 \u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u6700\u540e\u5728\u7ed9\u5b9a\u7684\u53ef\u89c6\u5316\u533a\u57df\u4e0a\u8fdb\u884c\u9884\u6d4b\u5e76\u53ef\u89c6\u5316\uff0c\u8bbe\u51b7\u8fb9\u548c\u70ed\u8fb9\u7684\u8d28\u91cf\u6d41\u91cf\u5747\u4e3a1\uff0c\u53ef\u89c6\u5316\u6570\u636e\u662f\u533a\u57df\u5185\u7684\u4e00\u7ef4\u70b9\u96c6\uff0c\u6bcf\u4e2a\u65f6\u523b \\(t\\) \u5bf9\u5e94\u7684\u5750\u6807\u662f \\(x^i\\)\uff0c\u5bf9\u5e94\u503c\u662f \\((T_h^{i}, T_c^i, T_w^i)\\)\uff0c\u5728\u6b64\u6211\u4eec\u753b\u51fa \\(T_h,T_c,T_w\\) \u968f\u65f6\u95f4\u7684\u53d8\u5316\u56fe\u50cf\u3002\u540c\u65f6\u6839\u636e\u6362\u70ed\u5668\u6548\u7387\u7684\u516c\u5f0f\u8ba1\u7b97\u51fa\u6362\u70ed\u5668\u6548\u7387 \\(\\eta\\) \uff0c\u753b\u51fa\u6362\u70ed\u5668\u6548\u7387 \\(\\eta\\) \u968f\u65f6\u95f4\u7684\u53d8\u5316\u56fe\u50cf\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>    # visualize prediction after finished training\n    visu_input[\"qm_c\"] = np.full_like(visu_input[\"qm_c\"], cfg.qm_h)\n    visu_input[\"qm_h\"] = np.full_like(visu_input[\"qm_c\"], cfg.qm_c)\n    pred = solver.predict(visu_input, return_numpy=True)\n    plot(visu_input, pred, cfg)\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.HEDeepONets(**cfg.MODEL)\n\n    # set time-geometry\n    timestamps = np.linspace(0.0, 2, cfg.NTIME + 1, endpoint=True)\n    geom = {\n        \"time_rect\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(0.0, 1, timestamps=timestamps),\n            ppsci.geometry.Interval(0, cfg.DL),\n        )\n    }\n\n    # Generate eval data\n    visu_input = geom[\"time_rect\"].sample_interior(cfg.NPOINT * cfg.NTIME, evenly=True)\n    test_h = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\n    test_c = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\n    # rearrange train data and eval data\n    visu_input[\"qm_h\"] = np.tile(test_h, (cfg.NPOINT * cfg.NTIME, 1))\n    visu_input[\"qm_c\"] = np.tile(test_c, (cfg.NPOINT * cfg.NTIME, 1))\n\n    left_indices = visu_input[\"x\"] == 0\n    right_indices = visu_input[\"x\"] == cfg.DL\n    interior_indices = (visu_input[\"x\"] != 0) &amp; (visu_input[\"x\"] != cfg.DL)\n    left_indices = np.where(left_indices)\n    right_indices = np.where(right_indices)\n    interior_indices = np.where(interior_indices)\n\n    # Classification eval data\n    test_left_data = {\n        \"x\": visu_input[\"x\"][left_indices[0]],\n        \"t\": visu_input[\"t\"][left_indices[0]],\n        \"qm_h\": visu_input[\"qm_h\"][left_indices[0]],\n        \"qm_c\": visu_input[\"qm_c\"][left_indices[0]],\n    }\n    test_right_data = {\n        \"x\": visu_input[\"x\"][right_indices[0]],\n        \"t\": visu_input[\"t\"][right_indices[0]],\n        \"qm_h\": visu_input[\"qm_h\"][right_indices[0]],\n        \"qm_c\": visu_input[\"qm_c\"][right_indices[0]],\n    }\n    test_interior_data = {\n        \"x\": visu_input[\"x\"],\n        \"t\": visu_input[\"t\"],\n        \"qm_h\": visu_input[\"qm_h\"],\n        \"qm_c\": visu_input[\"qm_c\"],\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"heat_exchanger.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom os import path as osp\n\nimport hydra\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.HEDeepONets(**cfg.MODEL)\n\n    # set time-geometry\n    timestamps = np.linspace(0.0, 2, cfg.NTIME + 1, endpoint=True)\n    geom = {\n        \"time_rect\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(0.0, 1, timestamps=timestamps),\n            ppsci.geometry.Interval(0, cfg.DL),\n        )\n    }\n\n    # Generate train data and eval data\n    visu_input = geom[\"time_rect\"].sample_interior(cfg.NPOINT * cfg.NTIME, evenly=True)\n    data_h = np.random.rand(cfg.NQM).reshape([-1, 1]) * 2\n    data_c = np.random.rand(cfg.NQM).reshape([-1, 1]) * 2\n    data_h = data_h.astype(\"float32\")\n    data_c = data_c.astype(\"float32\")\n    test_h = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\n    test_c = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\n    # rearrange train data and eval data\n    points = visu_input.copy()\n    points[\"t\"] = np.repeat(points[\"t\"], cfg.NQM, axis=0)\n    points[\"x\"] = np.repeat(points[\"x\"], cfg.NQM, axis=0)\n    points[\"qm_h\"] = np.tile(data_h, (cfg.NPOINT * cfg.NTIME, 1))\n    points[\"t\"] = np.repeat(points[\"t\"], cfg.NQM, axis=0)\n    points[\"x\"] = np.repeat(points[\"x\"], cfg.NQM, axis=0)\n    points[\"qm_h\"] = np.repeat(points[\"qm_h\"], cfg.NQM, axis=0)\n    points[\"qm_c\"] = np.tile(data_c, (cfg.NPOINT * cfg.NTIME * cfg.NQM, 1))\n    visu_input[\"qm_h\"] = np.tile(test_h, (cfg.NPOINT * cfg.NTIME, 1))\n    visu_input[\"qm_c\"] = np.tile(test_c, (cfg.NPOINT * cfg.NTIME, 1))\n\n    left_indices = visu_input[\"x\"] == 0\n    right_indices = visu_input[\"x\"] == cfg.DL\n    interior_indices = (visu_input[\"x\"] != 0) &amp; (visu_input[\"x\"] != cfg.DL)\n    left_indices = np.where(left_indices)\n    right_indices = np.where(right_indices)\n    interior_indices = np.where(interior_indices)\n\n    left_indices1 = points[\"x\"] == 0\n    right_indices1 = points[\"x\"] == cfg.DL\n    interior_indices1 = (points[\"x\"] != 0) &amp; (points[\"x\"] != cfg.DL)\n    initial_indices1 = points[\"t\"] == points[\"t\"][0]\n    left_indices1 = np.where(left_indices1)\n    right_indices1 = np.where(right_indices1)\n    interior_indices1 = np.where(interior_indices1)\n    initial_indices1 = np.where(initial_indices1)\n\n    # Classification train data\n    left_data = {\n        \"x\": points[\"x\"][left_indices1[0]],\n        \"t\": points[\"t\"][left_indices1[0]],\n        \"qm_h\": points[\"qm_h\"][left_indices1[0]],\n        \"qm_c\": points[\"qm_c\"][left_indices1[0]],\n    }\n    right_data = {\n        \"x\": points[\"x\"][right_indices1[0]],\n        \"t\": points[\"t\"][right_indices1[0]],\n        \"qm_h\": points[\"qm_h\"][right_indices1[0]],\n        \"qm_c\": points[\"qm_c\"][right_indices1[0]],\n    }\n    interior_data = {\n        \"x\": points[\"x\"],\n        \"t\": points[\"t\"],\n        \"qm_h\": points[\"qm_h\"],\n        \"qm_c\": points[\"qm_c\"],\n    }\n    initial_data = {\n        \"x\": points[\"x\"][initial_indices1[0]],\n        \"t\": points[\"t\"][initial_indices1[0]] * 0,\n        \"qm_h\": points[\"qm_h\"][initial_indices1[0]],\n        \"qm_c\": points[\"qm_c\"][initial_indices1[0]],\n    }\n    # Classification eval data\n    test_left_data = {\n        \"x\": visu_input[\"x\"][left_indices[0]],\n        \"t\": visu_input[\"t\"][left_indices[0]],\n        \"qm_h\": visu_input[\"qm_h\"][left_indices[0]],\n        \"qm_c\": visu_input[\"qm_c\"][left_indices[0]],\n    }\n    test_right_data = {\n        \"x\": visu_input[\"x\"][right_indices[0]],\n        \"t\": visu_input[\"t\"][right_indices[0]],\n        \"qm_h\": visu_input[\"qm_h\"][right_indices[0]],\n        \"qm_c\": visu_input[\"qm_c\"][right_indices[0]],\n    }\n    test_interior_data = {\n        \"x\": visu_input[\"x\"],\n        \"t\": visu_input[\"t\"],\n        \"qm_h\": visu_input[\"qm_h\"],\n        \"qm_c\": visu_input[\"qm_c\"],\n    }\n\n    # set equation\n    equation = {\n        \"heat_exchanger\": ppsci.equation.HeatExchanger(\n            cfg.alpha_h / (cfg.L * cfg.cp_h),\n            cfg.alpha_c / (cfg.L * cfg.cp_c),\n            cfg.v_h,\n            cfg.v_c,\n            cfg.alpha_h / (cfg.M * cfg.cp_w),\n            cfg.alpha_c / (cfg.M * cfg.cp_w),\n        )\n    }\n\n    # set constraint\n    bc_label = {\n        \"T_h\": np.zeros([left_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    }\n    interior_label = {\n        \"heat_boundary\": np.zeros([interior_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n        \"cold_boundary\": np.zeros([interior_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n        \"wall\": np.zeros([interior_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    }\n    initial_label = {\n        \"T_h\": np.zeros([initial_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n        \"T_c\": np.zeros([initial_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n        \"T_w\": np.zeros([initial_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    }\n\n    left_sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": left_data,\n                \"label\": bc_label,\n                \"weight\": {\n                    \"T_h\": np.full_like(\n                        left_data[\"x\"], cfg.TRAIN.weight.left_sup_constraint.T_h\n                    )\n                },\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"T_h\": lambda out: out[\"T_h\"] - cfg.T_hin},\n        name=\"left_sup\",\n    )\n    right_sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": right_data,\n                \"label\": bc_label,\n                \"weight\": {\n                    \"T_h\": np.full_like(\n                        right_data[\"x\"], cfg.TRAIN.weight.right_sup_constraint.T_h\n                    )\n                },\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"T_h\": lambda out: out[\"T_c\"] - cfg.T_cin},\n        name=\"right_sup\",\n    )\n    interior_sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": interior_data,\n                \"label\": interior_label,\n                \"weight\": {\n                    \"heat_boundary\": np.full_like(\n                        interior_data[\"x\"],\n                        cfg.TRAIN.weight.interior_sup_constraint.heat_boundary,\n                    ),\n                    \"cold_boundary\": np.full_like(\n                        interior_data[\"x\"],\n                        cfg.TRAIN.weight.interior_sup_constraint.cold_boundary,\n                    ),\n                    \"wall\": np.full_like(\n                        interior_data[\"x\"],\n                        cfg.TRAIN.weight.interior_sup_constraint.wall,\n                    ),\n                },\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr=equation[\"heat_exchanger\"].equations,\n        name=\"interior_sup\",\n    )\n    initial_sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": initial_data,\n                \"label\": initial_label,\n                \"weight\": {\n                    \"T_h\": np.full_like(\n                        initial_data[\"x\"], cfg.TRAIN.weight.initial_sup_constraint.T_h\n                    ),\n                    \"T_c\": np.full_like(\n                        initial_data[\"x\"], cfg.TRAIN.weight.initial_sup_constraint.T_c\n                    ),\n                    \"T_w\": np.full_like(\n                        initial_data[\"x\"], cfg.TRAIN.weight.initial_sup_constraint.T_w\n                    ),\n                },\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"T_h\": lambda out: out[\"T_h\"] - cfg.T_hin,\n            \"T_c\": lambda out: out[\"T_c\"] - cfg.T_cin,\n            \"T_w\": lambda out: out[\"T_w\"] - cfg.T_win,\n        },\n        name=\"initial_sup\",\n    )\n    # wrap constraints together\n    constraint = {\n        left_sup_constraint.name: left_sup_constraint,\n        right_sup_constraint.name: right_sup_constraint,\n        interior_sup_constraint.name: interior_sup_constraint,\n        initial_sup_constraint.name: initial_sup_constraint,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    test_bc_label = {\n        \"T_h\": np.zeros([test_left_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    }\n    test_interior_label = {\n        \"heat_boundary\": np.zeros(\n            [test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"\n        ),\n        \"cold_boundary\": np.zeros(\n            [test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"\n        ),\n        \"wall\": np.zeros([test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    }\n    left_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_left_data,\n                \"label\": test_bc_label,\n            },\n            \"batch_size\": cfg.NTIME,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"T_h\": lambda out: out[\"T_h\"] - cfg.T_hin},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"left_mse\",\n    )\n    right_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_right_data,\n                \"label\": test_bc_label,\n            },\n            \"batch_size\": cfg.NTIME,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\"T_h\": lambda out: out[\"T_c\"] - cfg.T_cin},\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"right_mse\",\n    )\n    interior_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_interior_data,\n                \"label\": test_interior_label,\n            },\n            \"batch_size\": cfg.NTIME,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr=equation[\"heat_exchanger\"].equations,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"interior_mse\",\n    )\n    validator = {\n        left_validator.name: left_validator,\n        right_validator.name: right_validator,\n        interior_validator.name: interior_validator,\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # plotting iteration/epoch-loss curve.\n    solver.plot_loss_history()\n\n    # visualize prediction after finished training\n    visu_input[\"qm_c\"] = np.full_like(visu_input[\"qm_c\"], cfg.qm_h)\n    visu_input[\"qm_h\"] = np.full_like(visu_input[\"qm_c\"], cfg.qm_c)\n    pred = solver.predict(visu_input, return_numpy=True)\n    plot(visu_input, pred, cfg)\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.HEDeepONets(**cfg.MODEL)\n\n    # set time-geometry\n    timestamps = np.linspace(0.0, 2, cfg.NTIME + 1, endpoint=True)\n    geom = {\n        \"time_rect\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(0.0, 1, timestamps=timestamps),\n            ppsci.geometry.Interval(0, cfg.DL),\n        )\n    }\n\n    # Generate eval data\n    visu_input = geom[\"time_rect\"].sample_interior(cfg.NPOINT * cfg.NTIME, evenly=True)\n    test_h = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\n    test_c = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\n    # rearrange train data and eval data\n    visu_input[\"qm_h\"] = np.tile(test_h, (cfg.NPOINT * cfg.NTIME, 1))\n    visu_input[\"qm_c\"] = np.tile(test_c, (cfg.NPOINT * cfg.NTIME, 1))\n\n    left_indices = visu_input[\"x\"] == 0\n    right_indices = visu_input[\"x\"] == cfg.DL\n    interior_indices = (visu_input[\"x\"] != 0) &amp; (visu_input[\"x\"] != cfg.DL)\n    left_indices = np.where(left_indices)\n    right_indices = np.where(right_indices)\n    interior_indices = np.where(interior_indices)\n\n    # Classification eval data\n    test_left_data = {\n        \"x\": visu_input[\"x\"][left_indices[0]],\n        \"t\": visu_input[\"t\"][left_indices[0]],\n        \"qm_h\": visu_input[\"qm_h\"][left_indices[0]],\n        \"qm_c\": visu_input[\"qm_c\"][left_indices[0]],\n    }\n    test_right_data = {\n        \"x\": visu_input[\"x\"][right_indices[0]],\n        \"t\": visu_input[\"t\"][right_indices[0]],\n        \"qm_h\": visu_input[\"qm_h\"][right_indices[0]],\n        \"qm_c\": visu_input[\"qm_c\"][right_indices[0]],\n    }\n    test_interior_data = {\n        \"x\": visu_input[\"x\"],\n        \"t\": visu_input[\"t\"],\n        \"qm_h\": visu_input[\"qm_h\"],\n        \"qm_c\": visu_input[\"qm_c\"],\n    }\n\n    # set equation\n    equation = {\n        \"heat_exchanger\": ppsci.equation.HeatExchanger(\n            cfg.alpha_h / (cfg.L * cfg.cp_h),\n            cfg.alpha_c / (cfg.L * cfg.cp_c),\n            cfg.v_h,\n            cfg.v_c,\n            cfg.alpha_h / (cfg.M * cfg.cp_w),\n            cfg.alpha_c / (cfg.M * cfg.cp_w),\n        )\n    }\n\n    # set validator\n    test_bc_label = {\n        \"T_h\": np.zeros([test_left_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    }\n    test_interior_label = {\n        \"heat_boundary\": np.zeros(\n            [test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"\n        ),\n        \"cold_boundary\": np.zeros(\n            [test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"\n        ),\n        \"wall\": np.zeros([test_interior_data[\"x\"].shape[0], 1], dtype=\"float32\"),\n    }\n    left_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_left_data,\n                \"label\": test_bc_label,\n            },\n            \"batch_size\": cfg.NTIME,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"T_h\": lambda out: out[\"T_h\"] - cfg.T_hin,\n        },\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"left_mse\",\n    )\n    right_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_right_data,\n                \"label\": test_bc_label,\n            },\n            \"batch_size\": cfg.NTIME,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr={\n            \"T_h\": lambda out: out[\"T_c\"] - cfg.T_cin,\n        },\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"right_mse\",\n    )\n    interior_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": test_interior_data,\n                \"label\": test_interior_label,\n            },\n            \"batch_size\": cfg.NTIME,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr=equation[\"heat_exchanger\"].equations,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"interior_mse\",\n    )\n    validator = {\n        left_validator.name: left_validator,\n        right_validator.name: right_validator,\n        interior_validator.name: interior_validator,\n    }\n\n    # directly evaluate pretrained model(optional)\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n\n    # visualize prediction after finished training\n    visu_input[\"qm_c\"] = np.full_like(visu_input[\"qm_c\"], cfg.qm_h)\n    visu_input[\"qm_h\"] = np.full_like(visu_input[\"qm_c\"], cfg.qm_c)\n    pred = solver.predict(visu_input, return_numpy=True)\n    plot(visu_input, pred, cfg)\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.HEDeepONets(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # set time-geometry\n    timestamps = np.linspace(0.0, 2, cfg.NTIME + 1, endpoint=True)\n    geom = {\n        \"time_rect\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(0.0, 1, timestamps=timestamps),\n            ppsci.geometry.Interval(0, cfg.DL),\n        )\n    }\n    input_dict = geom[\"time_rect\"].sample_interior(cfg.NPOINT * cfg.NTIME, evenly=True)\n    test_h = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\n    test_c = np.random.rand(1).reshape([-1, 1]).astype(\"float32\")\n    # rearrange train data and eval data\n    input_dict[\"qm_h\"] = np.tile(test_h, (cfg.NPOINT * cfg.NTIME, 1))\n    input_dict[\"qm_c\"] = np.tile(test_c, (cfg.NPOINT * cfg.NTIME, 1))\n    input_dict[\"qm_c\"] = np.full_like(input_dict[\"qm_c\"], cfg.qm_h)\n    input_dict[\"qm_h\"] = np.full_like(input_dict[\"qm_c\"], cfg.qm_c)\n    output_dict = predictor.predict(\n        {key: input_dict[key] for key in cfg.INFER.input_keys}, cfg.INFER.batch_size\n    )\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n    plot(input_dict, output_dict, cfg)\n\n\ndef plot(visu_input, pred, cfg: DictConfig):\n    x = visu_input[\"x\"][: cfg.NPOINT]\n    # plot temperature of heat boundary\n    plt.figure()\n    y = np.full_like(pred[\"T_h\"][: cfg.NPOINT], cfg.T_hin)\n    plt.plot(x, y, label=\"t = 0.0 s\")\n    for i in range(10):\n        y = pred[\"T_h\"][cfg.NPOINT * i * 2 : cfg.NPOINT * (i * 2 + 1)]\n        plt.plot(x, y, label=f\"t = {(i+1)*0.1:,.1f} s\")\n    plt.xlabel(\"A\")\n    plt.ylabel(r\"$T_h$\")\n    plt.legend()\n    plt.grid()\n    plt.savefig(\"T_h.png\")\n    # plot temperature of cold boundary\n    plt.figure()\n    y = np.full_like(pred[\"T_c\"][: cfg.NPOINT], cfg.T_cin)\n    plt.plot(x, y, label=\"t = 0.0 s\")\n    for i in range(10):\n        y = pred[\"T_c\"][cfg.NPOINT * i * 2 : cfg.NPOINT * (i * 2 + 1)]\n        plt.plot(x, y, label=f\"t = {(i+1)*0.1:,.1f} s\")\n    plt.xlabel(\"A\")\n    plt.ylabel(r\"$T_c$\")\n    plt.legend()\n    plt.grid()\n    plt.savefig(\"T_c.png\")\n    # plot temperature of wall\n    plt.figure()\n    y = np.full_like(pred[\"T_w\"][: cfg.NPOINT], cfg.T_win)\n    plt.plot(x, y, label=\"t = 0.0 s\")\n    for i in range(10):\n        y = pred[\"T_w\"][cfg.NPOINT * i * 2 : cfg.NPOINT * (i * 2 + 1)]\n        plt.plot(x, y, label=f\"t = {(i+1)*0.1:,.1f} s\")\n    plt.xlabel(\"A\")\n    plt.ylabel(r\"$T_w$\")\n    plt.legend()\n    plt.grid()\n    plt.savefig(\"T_w.png\")\n    # plot the heat exchanger efficiency as a function of time.\n    plt.figure()\n    qm_min = np.min((visu_input[\"qm_h\"][0], visu_input[\"qm_c\"][0]))\n    eta = (\n        visu_input[\"qm_h\"][0]\n        * (pred[\"T_h\"][:: cfg.NPOINT] - pred[\"T_h\"][cfg.NPOINT - 1 :: cfg.NPOINT])\n        / (\n            qm_min\n            * (pred[\"T_h\"][:: cfg.NPOINT] - pred[\"T_c\"][cfg.NPOINT - 1 :: cfg.NPOINT])\n        )\n    )\n    x = list(range(1, cfg.NTIME + 1))\n    plt.plot(x, eta)\n    plt.xlabel(\"time\")\n    plt.ylabel(r\"$\\eta$\")\n    plt.grid()\n    plt.savefig(\"eta.png\")\n    error = np.square(eta[-1] - cfg.eta_true)\n    logger.info(\n        f\"The L2 norm error between the actual heat exchanger efficiency and the predicted heat exchanger efficiency is {error}\"\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"heat_exchanger.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_exchanger/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u5982\u56fe\u6240\u793a\u4e3a\u4e0d\u540c\u65f6\u523b\u70ed\u8fb9\u6e29\u5ea6\u3001\u51b7\u8fb9\u6e29\u5ea6\u3001\u58c1\u9762\u6e29\u5ea6 \\(T_h, T_c, T_w\\) \u968f\u4f20\u70ed\u9762\u79ef \\(A\\) \u7684\u53d8\u5316\u56fe\u50cf\u4ee5\u53ca\u6362\u70ed\u5668\u6548\u7387 \\(\\eta\\) \u968f\u65f6\u95f4\u7684\u53d8\u5316\u56fe\u50cf\u3002</p> \u8bf4\u660e <p>\u672c\u6848\u4f8b\u53ea\u4f5c\u4e3ademo\u5c55\u793a\uff0c\u5c1a\u672a\u8fdb\u884c\u5145\u5206\u8c03\u4f18\uff0c\u4e0b\u65b9\u90e8\u5206\u5c55\u793a\u7ed3\u679c\u53ef\u80fd\u4e0e OpenFOAM \u5b58\u5728\u4e00\u5b9a\u5dee\u522b\u3002</p> <p> </p>  \u4e0d\u540c\u65f6\u523b\u70ed\u8fb9\u6e29\u5ea6 T_h \u968f\u4f20\u70ed\u9762\u79ef A \u7684\u53d8\u5316\u56fe\u50cf <p> </p>  \u4e0d\u540c\u65f6\u523b\u51b7\u8fb9\u6e29\u5ea6 T_c \u968f\u4f20\u70ed\u9762\u79ef A \u7684\u53d8\u5316\u56fe\u50cf <p> </p>  \u4e0d\u540c\u65f6\u523b\u58c1\u9762\u6e29\u5ea6 T_w \u968f\u4f20\u70ed\u9762\u79ef A \u7684\u53d8\u5316\u56fe\u50cf <p> </p>  \u6362\u70ed\u5668\u6548\u7387\u968f\u65f6\u95f4\u7684\u53d8\u5316\u56fe\u50cf <p>\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff1a</p> <ul> <li>\u70ed\u8fb9\u6e29\u5ea6\u5728 \\(A=1\\) \u5904\u968f\u65f6\u95f4\u7684\u53d8\u5316\u9010\u6e10\u9012\u51cf\uff0c\u51b7\u8fb9\u6e29\u5ea6\u5728 \\(A=0\\) \u5904\u968f\u65f6\u95f4\u7684\u53d8\u5316\u9010\u6e10\u9012\u589e\uff1b</li> <li>\u58c1\u9762\u6e29\u5ea6\u5728 \\(A=1\\) \u5904\u968f\u65f6\u95f4\u7684\u53d8\u5316\u9010\u6e10\u9012\u51cf\uff0c\u5728 \\(A=0\\) \u5904\u968f\u65f6\u95f4\u7684\u53d8\u5316\u9010\u6e10\u9012\u589e\uff1b</li> <li>\u6362\u70ed\u5668\u6548\u7387\u968f\u65f6\u95f4\u7684\u53d8\u5316\u9010\u6e10\u9012\u589e\uff0c\u5728 \\(t=21\\) \u65f6\u8fbe\u5230\u6700\u5927\u503c\u3002</li> </ul> <p>\u540c\u65f6\u6211\u4eec\u53ef\u4ee5\u5047\u8bbe\u70ed\u8fb9\u8d28\u91cf\u6d41\u91cf\u548c\u51b7\u8fb9\u8d28\u91cf\u6d41\u91cf\u76f8\u7b49\uff0c\u5373 \\(q_h=q_c\\)\uff0c\u5b9a\u4e49\u4f20\u70ed\u5355\u5143\u6570\uff1a</p> \\[ NTU = \\dfrac{Ak}{(q_mc)_{min}}. \\] <p>\u5bf9\u4e0d\u540c\u7684\u4f20\u70ed\u5355\u5143\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u5206\u522b\u8ba1\u7b97\u5bf9\u5e94\u7684\u6362\u70ed\u5668\u6548\u7387\uff0c\u5e76\u753b\u51fa\u6362\u70ed\u5668\u6548\u7387\u968f\u4f20\u70ed\u5355\u5143\u6570\u7684\u53d8\u5316\u56fe\u50cf\uff0c\u5982\u56fe\u6240\u793a\u3002</p> <p> </p>  \u6362\u70ed\u5668\u6548\u7387\u968f\u4f20\u70ed\u5355\u5143\u6570\u7684\u53d8\u5316\u56fe\u50cf <p>\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff1a\u6362\u70ed\u5668\u6548\u7387\u968f\u4f20\u70ed\u5355\u5143\u6570\u7684\u53d8\u5316\u9010\u6e10\u9012\u589e\uff0c\u8fd9\u4e5f\u7b26\u5408\u5b9e\u9645\u7684\u6362\u70ed\u5668\u6548\u7387\u968f\u4f20\u70ed\u5355\u5143\u6570\u7684\u53d8\u5316\u89c4\u5f8b\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","HeatExchanger\u65b9\u7a0b"]},{"location":"zh/examples/heat_pinn/","title":"Heat_PINN","text":""},{"location":"zh/examples/heat_pinn/#heat_pinn","title":"Heat_PINN","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <pre><code>python heat_pinn.py\n</code></pre> <pre><code>python heat_pinn.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/heat_pinn/heat_pinn_pretrained.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 heat_pinn_pretrained.pdparams norm MSE loss between the FDM and PINN is 1.30174e-03"},{"location":"zh/examples/heat_pinn/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u70ed\u4f20\u5bfc\u662f\u81ea\u7136\u754c\u4e2d\u7684\u5e38\u89c1\u73b0\u8c61\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5de5\u7a0b\u3001\u79d1\u5b66\u548c\u6280\u672f\u9886\u57df\u3002\u70ed\u4f20\u5bfc\u95ee\u9898\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u90fd\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u548c\u91cd\u8981\u6027\uff0c\u5bf9\u4e8e\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u3001\u6539\u8fdb\u6750\u6599\u6027\u80fd\u3001\u4fc3\u8fdb\u79d1\u5b66\u7814\u7a76\u548c\u63a8\u52a8\u6280\u672f\u521b\u65b0\u90fd\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u56e0\u6b64\u4e86\u89e3\u548c\u6a21\u62df\u4f20\u70ed\u8fc7\u7a0b\u5bf9\u4e8e\u8bbe\u8ba1\u548c\u4f18\u5316\u70ed\u4f20\u5bfc\u8bbe\u5907\u3001\u6750\u6599\u548c\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u30022D \u5b9a\u5e38\u70ed\u4f20\u5bfc\u65b9\u7a0b\u63cf\u8ff0\u4e86\u7a33\u6001\u70ed\u4f20\u5bfc\u8fc7\u7a0b\uff0c\u4f20\u7edf\u7684\u6c42\u89e3\u65b9\u6cd5\u6d89\u53ca\u4f7f\u7528\u6570\u503c\u65b9\u6cd5\u5982\u6709\u9650\u5143\u6cd5\u6216\u6709\u9650\u5dee\u5206\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u79bb\u6563\u5316\u9886\u57df\u5e76\u6c42\u89e3\u5927\u89c4\u6a21\u77e9\u9635\u7cfb\u7edf\u3002\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7f51\u7edc\uff08Physics-informed neural networks, PINN\uff09\u9010\u6e10\u6210\u4e3a\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u65b0\u65b9\u6cd5\u3002PINN \u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u7075\u6d3b\u6027\u548c\u5bf9\u7269\u7406\u7ea6\u675f\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u80fd\u591f\u76f4\u63a5\u5728\u8fde\u7eed\u9886\u57df\u4e2d\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\u95ee\u9898\u3002</p>"},{"location":"zh/examples/heat_pinn/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u5047\u8bbe\u4e8c\u7ef4\u70ed\u4f20\u5bfc\u65b9\u7a0b\u4e2d\uff0c\u6bcf\u4e2a\u4f4d\u7f6e \\((x,y)\\) \u4e0a\u7684\u6e29\u5ea6 \\(T\\) \u6ee1\u8db3\u4ee5\u4e0b\u5173\u7cfb\u5f0f\uff1a</p> \\[ \\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2}=0, \\] <p>\u5e76\u4e14\u5728\u4ee5\u4e0b\u533a\u57df\u5185\uff1a</p> \\[ D = \\{(x, y)|-1\\leq{x}\\leq{+1},-1\\leq{y}\\leq{+1}\\}, \\] <p>\u5177\u6709\u4ee5\u4e0b\u8fb9\u754c\u6761\u4ef6\uff1a</p> \\[ \\begin{cases} T(-1, y) = 75.0 ^\\circ{C}, \\\\ T(+1, y) = 0.0 ^\\circ{C}, \\\\ T(x, -1) = 50.0 ^\\circ{C}, \\\\ T(x, +1) = 0.0 ^\\circ{C}. \\end{cases} \\]"},{"location":"zh/examples/heat_pinn/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/heat_pinn/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u4e8c\u7ef4\u70ed\u4f20\u5bfc\u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y)\\) \u90fd\u6709\u5bf9\u5e94\u7684\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf \\(T\\) \uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((x, y)\\) \u5230 \\(u\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^1\\) \uff0c\u5373\uff1a</p> \\[ u = f(x, y), \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"x\", \"y\")</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>\"u\"</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\u548c\u6fc0\u6d3b\u51fd\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 9 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\u3001\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 20 \u4ee5\u53ca\u6fc0\u6d3b\u51fd\u6570\u4e3a <code>tanh</code> \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>"},{"location":"zh/examples/heat_pinn/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e\u4e8c\u7ef4\u70ed\u4f20\u5bfc\u65b9\u7a0b\u4f7f\u7528\u7684\u662f Laplace \u65b9\u7a0b\u7684 2 \u7ef4\u5f62\u5f0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>Laplace</code>\uff0c\u6307\u5b9a\u8be5\u7c7b\u7684\u53c2\u6570 <code>dim</code> \u4e3a 2\u3002</p> <pre><code># set equation\nequation = {\"heat\": ppsci.equation.Laplace(dim=2)}\n</code></pre>"},{"location":"zh/examples/heat_pinn/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d\u4e8c\u7ef4\u70ed\u4f20\u5bfc\u95ee\u9898\u4f5c\u7528\u5728\u4ee5 (-1.0, -1.0),  (1.0, 1.0) \u4e3a\u5bf9\u89d2\u7ebf\u7684\u4e8c\u7ef4\u77e9\u5f62\u533a\u57df\uff0c \u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u7a7a\u95f4\u51e0\u4f55 <code>Rectangle</code> \u4f5c\u4e3a\u8ba1\u7b97\u57df\u3002</p> <pre><code># set geometry\ngeom = {\"rect\": ppsci.geometry.Rectangle((-1.0, -1.0), (1.0, 1.0))}\n</code></pre>"},{"location":"zh/examples/heat_pinn/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e24\u79cd\u7ea6\u675f\u6761\u4ef6\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u7684\u8bad\u7ec3\u5206\u522b\u662f\u4f5c\u7528\u4e8e\u91c7\u6837\u70b9\u4e0a\u7684\u70ed\u4f20\u5bfc\u65b9\u7a0b\u7ea6\u675f\u548c\u4f5c\u7528\u4e8e\u8fb9\u754c\u70b9\u4e0a\u7684\u7ea6\u675f\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u79cd\u7ea6\u675f\u6307\u5b9a\u91c7\u6837\u70b9\u4e2a\u6570\uff0c\u8868\u793a\u6bcf\u4e00\u79cd\u7ea6\u675f\u5728\u5176\u5bf9\u5e94\u8ba1\u7b97\u57df\u5185\u91c7\u6837\u6570\u636e\u7684\u6570\u91cf\uff0c\u4ee5\u53ca\u901a\u7528\u7684\u91c7\u6837\u914d\u7f6e\u3002</p> <pre><code># set constraint\nNPOINT_PDE = 99**2\nNPOINT_TOP = 25\nNPOINT_BOTTOM = 25\nNPOINT_LEFT = 25\nNPOINT_RIGHT = 25\n</code></pre>"},{"location":"zh/examples/heat_pinn/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>pde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"heat\"].equations,\n    {\"laplace\": 0},\n    geom[\"rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_PDE},\n    ppsci.loss.MSELoss(\"mean\"),\n    evenly=True,\n    name=\"EQ\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"Laplace\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u6839\u636e\u70ed\u4f20\u5bfc\u65b9\u7a0b\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u5e0c\u671b Laplace \u65b9\u7a0b\u4ea7\u751f\u7684\u7ed3\u679c\u5168\u4e3a 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"rect\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528\u5168\u91cf\u6570\u636e\u70b9\u8bad\u7ec3\uff0c\u56e0\u6b64 <code>dataset</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e3a \"IterableNamedArrayDataset\" \u4e14 <code>iters_per_epoch</code> \u4e5f\u8bbe\u7f6e\u4e3a 1\uff0c\u91c7\u6837\u70b9\u6570 <code>batch_size</code> \u8bbe\u4e3a <code>NPOINT_PDE</code>(\u8868\u793a99x99\u7684\u91c7\u6837\u7f51\u683c)\uff1b</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u5e73\u5747\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u8ba1\u7b97 loss \u7684\u65f6\u5019\u7684\u8be5\u7ea6\u675f\u7684\u6743\u503c\u5927\u5c0f\uff0c\u53c2\u8003PINN\u8bba\u6587\uff0c\u8fd9\u91cc\u6211\u4eec\u8bbe\u7f6e\u4e3a 1;</p> <p>\u7b2c\u4e03\u4e2a\u53c2\u6570\u662f\u9009\u62e9\u662f\u5426\u5728\u8ba1\u7b97\u57df\u4e0a\u8fdb\u884c\u7b49\u95f4\u9694\u91c7\u6837\uff0c\u6b64\u5904\u6211\u4eec\u9009\u62e9\u5f00\u542f\u7b49\u95f4\u9694\u91c7\u6837\uff0c\u8fd9\u6837\u80fd\u8ba9\u8bad\u7ec3\u70b9\u5747\u5300\u5206\u5e03\u5728\u8ba1\u7b97\u57df\u4e0a\uff0c\u6709\u5229\u4e8e\u8bad\u7ec3\u6536\u655b\uff1b</p> <p>\u7b2c\u516b\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"EQ\" \u5373\u53ef\u3002</p>"},{"location":"zh/examples/heat_pinn/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u540c\u7406\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u6784\u5efa\u77e9\u5f62\u7684\u56db\u4e2a\u8fb9\u754c\u7684\u7ea6\u675f\u3002\u4f46\u4e0e\u6784\u5efa <code>InteriorConstraint</code> \u7ea6\u675f\u4e0d\u540c\u7684\u662f\uff0c\u7531\u4e8e\u4f5c\u7528\u533a\u57df\u662f\u8fb9\u754c\uff0c\u56e0\u6b64\u6211\u4eec\u4f7f\u7528 <code>BoundaryConstraint</code> \u7c7b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>bc_top = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"]},\n    {\"u\": 0},\n    geom[\"rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_TOP},\n    ppsci.loss.MSELoss(\"mean\"),\n    weight_dict={\"u\": cfg.TRAIN.weight.bc_top},\n    criteria=lambda x, y: np.isclose(y, 1),\n    name=\"BC_top\",\n)\nbc_bottom = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"]},\n    {\"u\": 50 / 75},\n    geom[\"rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_BOTTOM},\n    ppsci.loss.MSELoss(\"mean\"),\n    weight_dict={\"u\": cfg.TRAIN.weight.bc_bottom},\n    criteria=lambda x, y: np.isclose(y, -1),\n    name=\"BC_bottom\",\n)\nbc_left = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"]},\n    {\"u\": 1},\n    geom[\"rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_LEFT},\n    ppsci.loss.MSELoss(\"mean\"),\n    weight_dict={\"u\": cfg.TRAIN.weight.bc_left},\n    criteria=lambda x, y: np.isclose(x, -1),\n    name=\"BC_left\",\n)\nbc_right = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"]},\n    {\"u\": 0},\n    geom[\"rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_RIGHT},\n    ppsci.loss.MSELoss(\"mean\"),\n    weight_dict={\"u\": cfg.TRAIN.weight.bc_right},\n    criteria=lambda x, y: np.isclose(x, 1),\n    name=\"BC_right\",\n)\n</code></pre> <p><code>BoundaryConstraint</code> \u7c7b\u7b2c\u4e00\u4e2a\u53c2\u6570\u8868\u793a\u6211\u4eec\u76f4\u63a5\u5bf9\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u51fa\u7ed3\u679c <code>out[\"u\"]</code> \u4f5c\u4e3a\u7a0b\u5e8f\u8fd0\u884c\u65f6\u7684\u7ea6\u675f\u5bf9\u8c61\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u6307\u6211\u4eec\u7ea6\u675f\u5bf9\u8c61\u7684\u771f\u503c\u4e3a\u591a\u5c11\uff0c\u8be5\u95ee\u9898\u4e2d\u8fb9\u754c\u6761\u4ef6\u4e3a Dirichlet \u8fb9\u754c\u6761\u4ef6\uff0c\u4e5f\u5c31\u662f\u8be5\u8fb9\u754c\u6761\u4ef6\u76f4\u63a5\u63cf\u8ff0\u7269\u7406\u7cfb\u7edf\u8fb9\u754c\u4e0a\u7684\u7269\u7406\u91cf\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u56fa\u5b9a\u7684\u8fb9\u754c\u503c\uff0c\u5177\u4f53\u7684\u8fb9\u754c\u6761\u4ef6\u503c\u5df2\u5728 2. \u95ee\u9898\u5b9a\u4e49 \u4e2d\u7ed9\u51fa;</p> <p><code>BoundaryConstraint</code> \u7c7b\u5176\u4ed6\u53c2\u6570\u7684\u542b\u4e49\u4e0e <code>InteriorConstraint</code> \u57fa\u672c\u4e00\u81f4\uff0c\u8fd9\u91cc\u4e0d\u518d\u4ecb\u7ecd\u3002</p> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u548c\u8fb9\u754c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    pde_constraint.name: pde_constraint,\n    bc_top.name: bc_top,\n    bc_bottom.name: bc_bottom,\n    bc_left.name: bc_left,\n    bc_right.name: bc_right,\n}\n</code></pre>"},{"location":"zh/examples/heat_pinn/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\uff0c\u5e76\u8bbe\u7f6e\u5b66\u4e60\u7387\u4e3a 0.0005\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 1000\n  iters_per_epoch: 1\n  save_freq: 20\n  learning_rate: 5.0e-4\n</code></pre> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(learning_rate=cfg.TRAIN.learning_rate)(model)\n</code></pre>"},{"location":"zh/examples/heat_pinn/#36","title":"3.6 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u6240\u6709\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    epochs=cfg.TRAIN.epochs,\n    iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    log_freq=cfg.log_freq,\n    seed=cfg.seed,\n    equation=equation,\n    geom=geom,\n    pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n)\n# train model\nsolver.train()\n</code></pre>"},{"location":"zh/examples/heat_pinn/#37","title":"3.7 \u6a21\u578b\u8bc4\u4f30\u3001\u53ef\u89c6\u5316","text":"<p>\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\u4e4b\u540e\u5c31\u9700\u8981\u8fdb\u884c\u4e0e\u6b63\u5f0f FDM \u65b9\u6cd5\u8ba1\u7b97\u51fa\u6765\u7684\u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u4e86 <code>geom[\"rect\"].sample_interior</code> \u91c7\u6837\u51fa\u6d4b\u8bd5\u6240\u9700\u8981\u7684\u5750\u6807\u6570\u636e\u3002 \u7136\u540e\uff0c\u518d\u5c06\u91c7\u6837\u51fa\u6765\u7684\u5750\u6807\u6570\u636e\u8f93\u5165\u5230\u6a21\u578b\u4e2d\uff0c\u5f97\u5230\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u6700\u540e\u5c06\u9884\u6d4b\u7ed3\u679c\u4e0e FDM \u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5f97\u5230\u6a21\u578b\u7684\u8bef\u5dee\u3002</p> <pre><code># begin eval\nN_EVAL = 100\ninput_data = geom[\"rect\"].sample_interior(N_EVAL**2, evenly=True)\npinn_output = solver.predict(input_data, return_numpy=True)[\"u\"].reshape(\n    N_EVAL, N_EVAL\n)\nfdm_output = fdm.solve(N_EVAL, 1).T\nmse_loss = np.mean(np.square(pinn_output - (fdm_output / 75.0)))\nlogger.info(f\"The norm MSE loss between the FDM and PINN is {mse_loss}\")\n\nx = input_data[\"x\"].reshape(N_EVAL, N_EVAL)\ny = input_data[\"y\"].reshape(N_EVAL, N_EVAL)\n\nplt.subplot(2, 1, 1)\nplt.pcolormesh(x, y, pinn_output * 75.0, cmap=\"magma\")\nplt.colorbar()\nplt.title(\"PINN\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tight_layout()\nplt.axis(\"square\")\n\nplt.subplot(2, 1, 2)\nplt.pcolormesh(x, y, fdm_output, cmap=\"magma\")\nplt.colorbar()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"FDM\")\nplt.tight_layout()\nplt.axis(\"square\")\nplt.savefig(osp.join(cfg.output_dir, \"pinn_fdm_comparison.png\"))\nplt.close()\n\nframes_val = np.array([-0.75, -0.5, -0.25, 0.0, +0.25, +0.5, +0.75])\nframes = [*map(int, (frames_val + 1) / 2 * (N_EVAL - 1))]\nheight = 3\nplt.figure(\"\", figsize=(len(frames) * height, 2 * height))\n\nfor i, var_index in enumerate(frames):\n    plt.subplot(2, len(frames), i + 1)\n    plt.title(f\"y = {frames_val[i]:.2f}\")\n    plt.plot(\n        x[:, var_index],\n        pinn_output[:, var_index] * 75.0,\n        \"r--\",\n        lw=4.0,\n        label=\"pinn\",\n    )\n    plt.plot(x[:, var_index], fdm_output[:, var_index], \"b\", lw=2.0, label=\"FDM\")\n    plt.ylim(0.0, 100.0)\n    plt.xlim(-1.0, +1.0)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"T\")\n    plt.tight_layout()\n    plt.legend()\n\nfor i, var_index in enumerate(frames):\n    plt.subplot(2, len(frames), len(frames) + i + 1)\n    plt.title(f\"x = {frames_val[i]:.2f}\")\n    plt.plot(\n        y[var_index, :],\n        pinn_output[var_index, :] * 75.0,\n        \"r--\",\n        lw=4.0,\n        label=\"pinn\",\n    )\n    plt.plot(y[var_index, :], fdm_output[var_index, :], \"b\", lw=2.0, label=\"FDM\")\n    plt.ylim(0.0, 100.0)\n    plt.xlim(-1.0, +1.0)\n    plt.xlabel(\"y\")\n    plt.ylabel(\"T\")\n    plt.tight_layout()\n    plt.legend()\n\nplt.savefig(osp.join(cfg.output_dir, \"profiles.png\"))\n</code></pre>"},{"location":"zh/examples/heat_pinn/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"heat_pinn.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\n\nimport fdm\nimport hydra\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n\n    # set output directory\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"heat\": ppsci.equation.Laplace(dim=2)}\n\n    # set geometry\n    geom = {\"rect\": ppsci.geometry.Rectangle((-1.0, -1.0), (1.0, 1.0))}\n\n    # set train dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    }\n\n    # set constraint\n    NPOINT_PDE = 99**2\n    NPOINT_TOP = 25\n    NPOINT_BOTTOM = 25\n    NPOINT_LEFT = 25\n    NPOINT_RIGHT = 25\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"heat\"].equations,\n        {\"laplace\": 0},\n        geom[\"rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_PDE},\n        ppsci.loss.MSELoss(\"mean\"),\n        evenly=True,\n        name=\"EQ\",\n    )\n    bc_top = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": 0},\n        geom[\"rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_TOP},\n        ppsci.loss.MSELoss(\"mean\"),\n        weight_dict={\"u\": cfg.TRAIN.weight.bc_top},\n        criteria=lambda x, y: np.isclose(y, 1),\n        name=\"BC_top\",\n    )\n    bc_bottom = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": 50 / 75},\n        geom[\"rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_BOTTOM},\n        ppsci.loss.MSELoss(\"mean\"),\n        weight_dict={\"u\": cfg.TRAIN.weight.bc_bottom},\n        criteria=lambda x, y: np.isclose(y, -1),\n        name=\"BC_bottom\",\n    )\n    bc_left = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": 1},\n        geom[\"rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_LEFT},\n        ppsci.loss.MSELoss(\"mean\"),\n        weight_dict={\"u\": cfg.TRAIN.weight.bc_left},\n        criteria=lambda x, y: np.isclose(x, -1),\n        name=\"BC_left\",\n    )\n    bc_right = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": 0},\n        geom[\"rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_RIGHT},\n        ppsci.loss.MSELoss(\"mean\"),\n        weight_dict={\"u\": cfg.TRAIN.weight.bc_right},\n        criteria=lambda x, y: np.isclose(x, 1),\n        name=\"BC_right\",\n    )\n    # wrap constraints together\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        bc_top.name: bc_top,\n        bc_bottom.name: bc_bottom,\n        bc_left.name: bc_left,\n        bc_right.name: bc_right,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(learning_rate=cfg.TRAIN.learning_rate)(model)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        epochs=cfg.TRAIN.epochs,\n        iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n    )\n    # train model\n    solver.train()\n\n    # begin eval\n    N_EVAL = 100\n    input_data = geom[\"rect\"].sample_interior(N_EVAL**2, evenly=True)\n    pinn_output = solver.predict(input_data, return_numpy=True)[\"u\"].reshape(\n        N_EVAL, N_EVAL\n    )\n    fdm_output = fdm.solve(N_EVAL, 1).T\n    mse_loss = np.mean(np.square(pinn_output - (fdm_output / 75.0)))\n    logger.info(f\"The norm MSE loss between the FDM and PINN is {mse_loss}\")\n\n    x = input_data[\"x\"].reshape(N_EVAL, N_EVAL)\n    y = input_data[\"y\"].reshape(N_EVAL, N_EVAL)\n\n    plt.subplot(2, 1, 1)\n    plt.pcolormesh(x, y, pinn_output * 75.0, cmap=\"magma\")\n    plt.colorbar()\n    plt.title(\"PINN\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.tight_layout()\n    plt.axis(\"square\")\n\n    plt.subplot(2, 1, 2)\n    plt.pcolormesh(x, y, fdm_output, cmap=\"magma\")\n    plt.colorbar()\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"FDM\")\n    plt.tight_layout()\n    plt.axis(\"square\")\n    plt.savefig(osp.join(cfg.output_dir, \"pinn_fdm_comparison.png\"))\n    plt.close()\n\n    frames_val = np.array([-0.75, -0.5, -0.25, 0.0, +0.25, +0.5, +0.75])\n    frames = [*map(int, (frames_val + 1) / 2 * (N_EVAL - 1))]\n    height = 3\n    plt.figure(\"\", figsize=(len(frames) * height, 2 * height))\n\n    for i, var_index in enumerate(frames):\n        plt.subplot(2, len(frames), i + 1)\n        plt.title(f\"y = {frames_val[i]:.2f}\")\n        plt.plot(\n            x[:, var_index],\n            pinn_output[:, var_index] * 75.0,\n            \"r--\",\n            lw=4.0,\n            label=\"pinn\",\n        )\n        plt.plot(x[:, var_index], fdm_output[:, var_index], \"b\", lw=2.0, label=\"FDM\")\n        plt.ylim(0.0, 100.0)\n        plt.xlim(-1.0, +1.0)\n        plt.xlabel(\"x\")\n        plt.ylabel(\"T\")\n        plt.tight_layout()\n        plt.legend()\n\n    for i, var_index in enumerate(frames):\n        plt.subplot(2, len(frames), len(frames) + i + 1)\n        plt.title(f\"x = {frames_val[i]:.2f}\")\n        plt.plot(\n            y[var_index, :],\n            pinn_output[var_index, :] * 75.0,\n            \"r--\",\n            lw=4.0,\n            label=\"pinn\",\n        )\n        plt.plot(y[var_index, :], fdm_output[var_index, :], \"b\", lw=2.0, label=\"FDM\")\n        plt.ylim(0.0, 100.0)\n        plt.xlim(-1.0, +1.0)\n        plt.xlabel(\"y\")\n        plt.ylabel(\"T\")\n        plt.tight_layout()\n        plt.legend()\n\n    plt.savefig(osp.join(cfg.output_dir, \"profiles.png\"))\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n\n    # set output directory\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set geometry\n    geom = {\"rect\": ppsci.geometry.Rectangle((-1.0, -1.0), (1.0, 1.0))}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    # begin eval\n    N_EVAL = 100\n    input_data = geom[\"rect\"].sample_interior(N_EVAL**2, evenly=True)\n    pinn_output = solver.predict(input_data, no_grad=True, return_numpy=True)[\n        \"u\"\n    ].reshape(N_EVAL, N_EVAL)\n    fdm_output = fdm.solve(N_EVAL, 1).T\n    mse_loss = np.mean(np.square(pinn_output - (fdm_output / 75.0)))\n    logger.info(f\"The norm MSE loss between the FDM and PINN is {mse_loss:.5e}\")\n\n    x = input_data[\"x\"].reshape(N_EVAL, N_EVAL)\n    y = input_data[\"y\"].reshape(N_EVAL, N_EVAL)\n\n    plt.subplot(2, 1, 1)\n    plt.pcolormesh(x, y, pinn_output * 75.0, cmap=\"magma\")\n    plt.colorbar()\n    plt.title(\"PINN\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.tight_layout()\n    plt.axis(\"square\")\n\n    plt.subplot(2, 1, 2)\n    plt.pcolormesh(x, y, fdm_output, cmap=\"magma\")\n    plt.colorbar()\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"FDM\")\n    plt.tight_layout()\n    plt.axis(\"square\")\n    plt.savefig(osp.join(cfg.output_dir, \"pinn_fdm_comparison.png\"))\n    plt.close()\n\n    frames_val = np.array([-0.75, -0.5, -0.25, 0.0, +0.25, +0.5, +0.75])\n    frames = [*map(int, (frames_val + 1) / 2 * (N_EVAL - 1))]\n    height = 3\n    plt.figure(\"\", figsize=(len(frames) * height, 2 * height))\n\n    for i, var_index in enumerate(frames):\n        plt.subplot(2, len(frames), i + 1)\n        plt.title(f\"y = {frames_val[i]:.2f}\")\n        plt.plot(\n            x[:, var_index],\n            pinn_output[:, var_index] * 75.0,\n            \"r--\",\n            lw=4.0,\n            label=\"pinn\",\n        )\n        plt.plot(x[:, var_index], fdm_output[:, var_index], \"b\", lw=2.0, label=\"FDM\")\n        plt.ylim(0.0, 100.0)\n        plt.xlim(-1.0, +1.0)\n        plt.xlabel(\"x\")\n        plt.ylabel(\"T\")\n        plt.tight_layout()\n        plt.legend()\n\n    for i, var_index in enumerate(frames):\n        plt.subplot(2, len(frames), len(frames) + i + 1)\n        plt.title(f\"x = {frames_val[i]:.2f}\")\n        plt.plot(\n            y[var_index, :],\n            pinn_output[var_index, :] * 75.0,\n            \"r--\",\n            lw=4.0,\n            label=\"pinn\",\n        )\n        plt.plot(y[var_index, :], fdm_output[var_index, :], \"b\", lw=2.0, label=\"FDM\")\n        plt.ylim(0.0, 100.0)\n        plt.xlim(-1.0, +1.0)\n        plt.xlabel(\"y\")\n        plt.ylabel(\"T\")\n        plt.tight_layout()\n        plt.legend()\n\n    plt.savefig(osp.join(cfg.output_dir, \"profiles.png\"))\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"heat_pinn.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/heat_pinn/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"\u4e0a\uff1aPINN \u8ba1\u7b97\u7ed3\u679c\uff0c\u4e0b\uff1aFDM \u8ba1\u7b97\u7ed3\u679c  <p>\u4e0a\u56fe\u5c55\u793a\u4e86\u4f7f\u7528 PINN \u548c FDM \u65b9\u6cd5\u5206\u522b\u8ba1\u7b97\u51fa\u7684\u6e29\u5ea6\u5206\u5e03\u56fe\uff0c\u4ece\u4e2d\u53ef\u4ee5\u770b\u51fa\u5b83\u4eec\u4e4b\u95f4\u7684\u7ed3\u679c\u975e\u5e38\u63a5\u8fd1\u3002\u6b64\u5916\uff0cPINN \u548c FDM \u4e24\u8005\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE Loss\uff09\u4ec5\u4e3a 0.0013\u3002\u7efc\u5408\u8003\u8651\u56fe\u5f62\u548c\u6570\u503c\u7ed3\u679c\uff0c\u53ef\u4ee5\u5f97\u51fa\u7ed3\u8bba\uff0cPINN \u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u672c\u6848\u4f8b\u7684\u4f20\u70ed\u95ee\u9898\u3002</p> <p> </p> \u4e0a\uff1aPINN \u4e0eFDM \u5728 x \u65b9\u5411 T \u7ed3\u679c\u5bf9\u6bd4\uff0c\u4e0b\uff1aPINN \u4e0e FDM \u5728 y \u65b9\u5411 T \u7ed3\u679c\u5bf9\u6bd4  <p>\u4e0a\u56fe\u5206\u522b\u4e3a\u6e29\u5ea6 \\(T\\) \u7684\u6a2a\u622a\u7ebf\u56fe\uff08 \\(y=\\{-0.75,-0.50,-0.25,0.00,0.25,0.50,0.75\\}\\) \uff09\u548c\u7eb5\u622a\u7ebf\u56fe\uff08 \\(x=\\{-0.75,-0.50,-0.25,0.00,0.25,0.50,0.75\\}\\) \uff09\uff0c\u53ef\u4ee5\u770b\u5230 PINN \u4e0e FDM \u65b9\u6cd5\u7684\u8ba1\u7b97\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p>"},{"location":"zh/examples/heat_pinn/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations</li> <li>Heat-PINN</li> </ul>"},{"location":"zh/examples/hpinns/","title":"hPINNs","text":""},{"location":"zh/examples/hpinns/#hpinnspinn-with-hard-constraints","title":"hPINNs(PINN with hard constraints)","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_train.mat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_valid.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_train.mat --create-dirs -o ./datasets/hpinns_holo_train.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_valid.mat --create-dirs -o ./datasets/hpinns_holo_valid.mat\npython holography.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_train.mat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_valid.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_train.mat --create-dirs -o ./datasets/hpinns_holo_train.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_valid.mat --create-dirs -o ./datasets/hpinns_holo_valid.mat\npython holography.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/hPINNs/hpinns_pretrained.pdparams\n</code></pre> <pre><code>python holography.py mode=export\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_train.mat -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_valid.mat -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_train.mat --create-dirs -o ./datasets/hpinns_holo_train.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_valid.mat --create-dirs -o ./datasets/hpinns_holo_valid.mat\npython holography.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 hpinns_pretrained.pdparams loss(opt_sup): 0.05352MSE.eval_metric(opt_sup): 0.00002loss(val_sup): 0.02205MSE.eval_metric(val_sup): 0.00001"},{"location":"zh/examples/hpinns/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b(PDE) \u662f\u4e00\u7c7b\u57fa\u7840\u7684\u7269\u7406\u95ee\u9898\uff0c\u5728\u8fc7\u53bb\u51e0\u5341\u5e74\u91cc\uff0c\u4ee5\u6709\u9650\u5dee\u5206(FDM)\u3001\u6709\u9650\u4f53\u79ef(FVM)\u3001\u6709\u9650\u5143(FEM)\u4e3a\u4ee3\u8868\u7684\u591a\u79cd\u504f\u5fae\u5206\u65b9\u7a0b\u7ec4\u6570\u503c\u89e3\u6cd5\u8d8b\u4e8e\u6210\u719f\u3002\u968f\u7740\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u9ad8\u901f\u53d1\u5c55\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u6210\u4e3a\u65b0\u7684\u7814\u7a76\u8d8b\u52bf\u3002PINNs(Physics-informed neural networks) \u662f\u4e00\u79cd\u52a0\u5165\u7269\u7406\u7ea6\u675f\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u56e0\u6b64\u4e0e\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u76f8\u6bd4\uff0cPINNs \u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u6570\u636e\u6837\u672c\u5b66\u4e60\u5230\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u6a21\u578b\uff0c\u5176\u5e94\u7528\u8303\u56f4\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u6d41\u4f53\u529b\u5b66\u3001\u70ed\u4f20\u5bfc\u3001\u7535\u78c1\u573a\u3001\u91cf\u5b50\u529b\u5b66\u7b49\u9886\u57df\u3002</p> <p>\u4f20\u7edf\u7684 PINNs \u7f51\u7edc\u4e2d\u7684\u7ea6\u675f\u90fd\u662f\u8f6f\u7ea6\u675f\uff0c\u5373 PDE(\u504f\u5fae\u5206\u65b9\u7a0b) \u4f5c\u4e3a loss \u9879\u53c2\u4e0e\u7f51\u7edc\u8bad\u7ec3\u3002\u800c\u672c\u6848\u4f8b hPINNs \u901a\u8fc7\u4fee\u6539\u7f51\u7edc\u8f93\u51fa\u7684\u65b9\u6cd5\uff0c\u5c06\u7ea6\u675f\u4e25\u683c\u5730\u52a0\u5165\u7f51\u7edc\u7ed3\u6784\u4e2d\uff0c\u5f62\u6210\u4e00\u79cd\u66f4\u6709\u6548\u7684\u786c\u7ea6\u675f\u3002</p> <p>\u540c\u65f6 hPINNs \u8bbe\u8ba1\u4e86\u4e0d\u540c\u7684\u7ea6\u675f\u7ec4\u5408\uff0c\u8fdb\u884c\u4e86\u8f6f\u7ea6\u675f\u3001\u5e26\u6b63\u5219\u5316\u7684\u786c\u7ea6\u675f\u548c\u5e94\u7528\u589e\u5f3a\u7684\u62c9\u683c\u6717\u65e5\u786c\u7ea6\u675f 3 \u79cd\u6761\u4ef6\u4e0b\u7684\u5b9e\u9a8c\u3002\u672c\u6587\u6863\u4e3b\u8981\u9488\u5bf9\u5e94\u7528\u589e\u5f3a\u7684\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u7684\u786c\u7ea6\u675f\u8fdb\u884c\u8bf4\u660e\uff0c\u4f46\u5b8c\u6574\u4ee3\u7801\u4e2d\u53ef\u4ee5\u901a\u8fc7 <code>train_mode</code> \u53c2\u6570\u6765\u5207\u6362\u4e09\u79cd\u8bad\u7ec3\u6a21\u5f0f\u3002</p> <p>\u672c\u95ee\u9898\u53ef\u53c2\u8003 AI Studio\u9898\u76ee.</p>"},{"location":"zh/examples/hpinns/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u95ee\u9898\u4f7f\u7528 hPINNs \u89e3\u51b3\u57fa\u4e8e\u5085\u7acb\u53f6\u5149\u5b66\u7684\u5168\u606f\u9886\u57df (holography) \u7684\u95ee\u9898\uff0c\u65e8\u5728\u8bbe\u8ba1\u6563\u5c04\u677f\u7684\u4ecb\u7535\u5e38\u6570\u56fe\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u4ecb\u7535\u5e38\u6570\u56fe\u6563\u5c04\u5149\u7ebf\u7684\u4f20\u64ad\u5f3a\u5ea6\u5177\u5907\u76ee\u6807\u51fd\u6570\u7684\u5f62\u72b6\u3002</p> <p>objective \u51fd\u6570\uff1a</p> \\[ \\begin{aligned} \\mathcal{J}(E) &amp;= \\dfrac{1}{Area(\\Omega_3)} \\left\\| |E(x,y)|^2-f(x,y)\\right\\|^2_{2,\\Omega_3} \\\\ &amp;= \\dfrac{1}{Area(\\Omega_3)} \\int_{\\Omega_3} (|E(x,y)|^2-f(x,y))^2 {\\rm d}x {\\rm d}y \\end{aligned} \\] <p>\u5176\u4e2dE\u4e3a\u7535\u573a\u5f3a\u5ea6\uff1a\\(\\vert E\\vert^2 = (\\mathfrak{R} [E])^2+(\\mathfrak{I} [E])^2\\)</p> <p>target \u51fd\u6570\uff1a</p> \\[ f(x,y) = \\begin{cases} \\begin{aligned} &amp; 1, \\ (x,y) \\in [-0.5,0.5] \\cap [1,2]\\\\ &amp; 0, \\ otherwise \\end{aligned} \\end{cases} \\] <p>PDE\u516c\u5f0f\uff1a</p> \\[ \\nabla^2 E + \\varepsilon \\omega^2 E = -i \\omega \\mathcal{J} \\]"},{"location":"zh/examples/hpinns/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002\u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u7ea6\u675f\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/hpinns/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u6570\u636e\u96c6\u4e3a\u5904\u7406\u597d\u7684 holography \u6570\u636e\u96c6\uff0c\u5305\u542b\u8bad\u7ec3\u3001\u6d4b\u8bd5\u6570\u636e\u7684 \\(x, y\\) \u4ee5\u53ca\u8868\u5f81 optimizer area \u6570\u636e\u4e0e\u5168\u533a\u57df\u6570\u636e\u5206\u754c\u7684\u503c \\(bound\\)\uff0c\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b58\u50a8\u5728 <code>.mat</code> \u6587\u4ef6\u4e2d\u3002</p> <p>\u8fd0\u884c\u672c\u95ee\u9898\u4ee3\u7801\u524d\u8bf7\u6309\u7167\u4e0b\u65b9\u547d\u4ee4\u4e0b\u8f7d \u8bad\u7ec3\u6570\u636e\u96c6 \u548c \u9a8c\u8bc1\u6570\u636e\u96c6\uff1a</p> <pre><code>wget -nc -P ./datasets/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_train.mat\nwget -nc -P ./datasets/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/hPINNs/hpinns_holo_valid.mat\n</code></pre>"},{"location":"zh/examples/hpinns/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"<p>holograpy \u95ee\u9898\u7684\u6a21\u578b\u7ed3\u6784\u56fe\u4e3a\uff1a</p> <p> </p> holography \u95ee\u9898\u7684 hPINNs \u7f51\u7edc\u6a21\u578b <p>\u5728 holography \u95ee\u9898\u4e2d\uff0c\u5e94\u7528 PMLs(perfectly matched layers) \u65b9\u6cd5\u540e\uff0cPDE\u516c\u5f0f\u53d8\u4e3a\uff1a</p> \\[ \\dfrac{1}{1+i \\dfrac{\\sigma_x\\left(x\\right)}{\\omega}} \\dfrac{\\partial}{\\partial x} \\left(\\dfrac{1}{1+i \\dfrac{\\sigma_x\\left(x\\right)}{\\omega}} \\dfrac{\\partial E}{\\partial x}\\right)+\\dfrac{1}{1+i \\dfrac{\\sigma_y\\left(y\\right)}{\\omega}} \\dfrac{\\partial}{\\partial y} \\left(\\dfrac{1}{1+i \\dfrac{\\sigma_y\\left(y\\right)}{\\omega}} \\dfrac{\\partial E}{\\partial y}\\right) + \\varepsilon \\omega^2 E = -i \\omega \\mathcal{J} \\] <p>PMLs \u65b9\u6cd5\u8bf7\u53c2\u8003 \u76f8\u5173\u8bba\u6587\u3002</p> <p>\u672c\u95ee\u9898\u4e2d\u9891\u7387 \\(\\omega\\) \u4e3a\u5e38\u91cf \\(\\dfrac{2\\pi}{\\mathcal{P}}\\)\uff08\\(\\mathcal{P}\\) \u4e3aPeriod\uff09\uff0c\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf \\(E\\) \u4e0e\u4f4d\u7f6e\u53c2\u6570 \\((x, y)\\) \u76f8\u5173\uff0c\u5728\u672c\u4f8b\u4e2d\uff0c\u4ecb\u7535\u5e38\u6570 \\(\\varepsilon\\) \u540c\u6837\u4e3a\u672a\u77e5\u91cf, \\(\\sigma_x(x)\\) \u548c \\(\\sigma_y(y)\\) \u4e3a\u7531 PMLs \u5f97\u5230\u7684\uff0c\u5206\u522b\u4e0e \\(x, y\\) \u76f8\u5173\u7684\u53d8\u91cf\u3002\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((x, y)\\) \u5230 \\((E, \\varepsilon)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) \uff0c\u4f46\u5982\u4e0a\u56fe\u6240\u793a\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u672c\u95ee\u9898\u4e2d\u5c06 \\(E\\) \u6309\u7167\u5b9e\u90e8\u548c\u865a\u90e8\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206 \\((\\mathfrak{R} [E],\\mathfrak{I} [E])\\)\uff0c\u4e14\u4f7f\u7528 3 \u4e2a\u5e76\u884c\u7684 MLP \u7f51\u7edc\u5206\u522b\u5bf9 \\((\\mathfrak{R} [E], \\mathfrak{I} [E], \\varepsilon)\\) \u8fdb\u884c\u6620\u5c04\uff0c\u6620\u5c04\u51fd\u6570 \\(f_i: \\mathbb{R}^2 \\to \\mathbb{R}^1\\) \uff0c\u5373\uff1a</p> \\[ \\mathfrak{R} [E] = f_1(x,y), \\ \\mathfrak{R} [E] = f_2(x,y), \\ \\varepsilon = f_3(x,y) \\] <p>\u4e0a\u5f0f\u4e2d \\(f_1,f_2,f_3\\) \u5206\u522b\u4e3a\u4e00\u4e2a MLP \u6a21\u578b\uff0c\u4e09\u8005\u5171\u540c\u6784\u6210\u4e86\u4e00\u4e2a Model List\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code>model_re = ppsci.arch.MLP(**cfg.MODEL.re_net)\nmodel_im = ppsci.arch.MLP(**cfg.MODEL.im_net)\nmodel_eps = ppsci.arch.MLP(**cfg.MODEL.eps_net)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"x_cos_1\",\"x_sin_1\",...,\"x_cos_6\",\"x_sin_6\",\"y\",\"y_cos_1\",\"y_sin_1\")</code> \uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u5206\u522b\u662f <code>(\"e_re\",)</code>, <code>(\"e_im\",)</code>, <code>(\"eps\",)</code>\u3002 \u6ce8\u610f\u5230\u8fd9\u91cc\u7684\u8f93\u5165\u53d8\u91cf\u8fdc\u8fdc\u591a\u4e8e \\((x, y)\\) \u8fd9\u4e24\u4e2a\u53d8\u91cf\uff0c\u8fd9\u662f\u56e0\u4e3a\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u6a21\u578b\u7684\u8f93\u5165\u5b9e\u9645\u4e0a\u662f \\((x, y)\\) \u5085\u7acb\u53f6\u5c55\u5f00\u7684\u9879\u800c\u4e0d\u662f\u5b83\u4eec\u672c\u8eab\u3002\u800c\u6570\u636e\u96c6\u4e2d\u63d0\u4f9b\u7684\u8bad\u7ec3\u6570\u636e\u4e3a \\((x, y)\\) \u503c\uff0c\u8fd9\u4e5f\u5c31\u610f\u5473\u7740\u6211\u4eec\u9700\u8981\u5bf9\u8f93\u5165\u8fdb\u884c transform\u3002\u540c\u65f6\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u7531\u4e8e\u786c\u7ea6\u675f\u7684\u5b58\u5728\uff0c\u6a21\u578b\u7684\u8f93\u51fa\u53d8\u91cf\u540d\u4e5f\u4e0d\u662f\u6700\u7ec8\u8f93\u51fa\uff0c\u56e0\u6b64\u4e5f\u9700\u8981\u5bf9\u8f93\u51fa\u8fdb\u884c transform\u3002</p>"},{"location":"zh/examples/hpinns/#33-transform","title":"3.3 transform\u6784\u5efa","text":"<p>\u8f93\u5165\u7684 transform \u4e3a\u53d8\u91cf \\((x, y)\\) \u5230 \\((\\cos(\\omega x),\\sin(\\omega x),...,\\cos(6 \\omega x),\\sin(6 \\omega x),y,\\cos(\\omega y),\\sin(\\omega y))\\) \u7684\u53d8\u6362\uff0c\u8f93\u51fa transform \u5206\u522b\u4e3a\u5bf9 \\((\\mathfrak{R} [E], \\mathfrak{I} [E], \\varepsilon)\\) \u7684\u786c\u7ea6\u675f\uff0c\u4ee3\u7801\u5982\u4e0b</p> <pre><code># transform\ndef transform_in(input):\n    # Periodic BC in x\n    P = BOX[1][0] - BOX[0][0] + 2 * DPML\n    w = 2 * np.pi / P\n    x, y = input[\"x\"], input[\"y\"]\n    input_transformed = {}\n    for t in range(1, 7):\n        input_transformed[f\"x_cos_{t}\"] = paddle.cos(t * w * x)\n        input_transformed[f\"x_sin_{t}\"] = paddle.sin(t * w * x)\n    input_transformed[\"y\"] = y\n    input_transformed[\"y_cos_1\"] = paddle.cos(OMEGA * y)\n    input_transformed[\"y_sin_1\"] = paddle.sin(OMEGA * y)\n\n    return input_transformed\n\n\ndef transform_out_all(input, var):\n    y = input[\"y\"]\n    # Zero Dirichlet BC\n    a, b = BOX[0][1] - DPML, BOX[1][1] + DPML\n    t = (1 - paddle.exp(a - y)) * (1 - paddle.exp(y - b))\n    return t * var\n\n\ndef transform_out_real_part(input, out):\n    re = out[\"e_re\"]\n    trans_out = transform_out_all(input, re)\n    return {\"e_real\": trans_out}\n\n\ndef transform_out_imaginary_part(input, out):\n    im = out[\"e_im\"]\n    trans_out = transform_out_all(input, im)\n    return {\"e_imaginary\": trans_out}\n\n\ndef transform_out_epsilon(input, out):\n    eps = out[\"eps\"]\n    # 1 &lt;= eps &lt;= 12\n    eps = F.sigmoid(eps) * 11 + 1\n    return {\"epsilon\": eps}\n</code></pre> <p>\u9700\u8981\u5bf9\u6bcf\u4e2a MLP \u6a21\u578b\u5206\u522b\u6ce8\u518c\u76f8\u5e94\u7684 transform \uff0c\u7136\u540e\u5c06 3 \u4e2a MLP \u6a21\u578b\u7ec4\u6210 Model List</p> <pre><code># register transform\nmodel_re.register_input_transform(func_module.transform_in)\nmodel_im.register_input_transform(func_module.transform_in)\nmodel_eps.register_input_transform(func_module.transform_in)\n\nmodel_re.register_output_transform(func_module.transform_out_real_part)\nmodel_im.register_output_transform(func_module.transform_out_imaginary_part)\nmodel_eps.register_output_transform(func_module.transform_out_epsilon)\n\nmodel_list = ppsci.arch.ModelList((model_re, model_im, model_eps))\n</code></pre> <p>\u8fd9\u6837\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 3 \u4e2a MLP \u6a21\u578b\uff0c\u6bcf\u4e2a MLP \u5305\u542b 4 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 48\uff0c\u4f7f\u7528 \"tanh\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5305\u542b\u8f93\u5165\u8f93\u51fa transform \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model list</code>\u3002</p>"},{"location":"zh/examples/hpinns/#34","title":"3.4 \u53c2\u6570\u548c\u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u6211\u4eec\u9700\u8981\u6307\u5b9a\u95ee\u9898\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5982\u901a\u8fc7 <code>train_mode</code> \u53c2\u6570\u6307\u5b9a\u5e94\u7528\u589e\u5f3a\u7684\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u7684\u786c\u7ea6\u675f\u8fdb\u884c\u8bad\u7ec3</p> <pre><code># open FLAG for higher order differential operator\npaddle.framework.core.set_prim_eager_enabled(True)\n\nppsci.utils.misc.set_random_seed(cfg.seed)\n# initialize logger\nlogger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n</code></pre> <pre><code># initialize params\nfunc_module.train_mode = cfg.TRAIN_MODE\nloss_log_obj = []\n</code></pre> <pre><code># define constants\nBOX = np.array([[-2, -2], [2, 3]])\nDPML = 1\nOMEGA = 2 * np.pi\nSIGMA0 = -np.log(1e-20) / (4 * DPML**3 / 3)\nl_BOX = BOX + np.array([[-DPML, -DPML], [DPML, DPML]])\nbeta = 2.0\nmu = 2\n\n# define variables which will be updated during training\nlambda_re: np.ndarray = None\nlambda_im: np.ndarray = None\nloss_weight: List[float] = None\ntrain_mode: str = None\n\n# define log variables for plotting\nloss_log = []  # record all losses, [pde, lag, obj]\nloss_obj = 0.0  # record last objective loss of each k\nlambda_log = []  # record all lambdas\n</code></pre> <p>\u7531\u4e8e\u5e94\u7528\u4e86\u589e\u5f3a\u7684\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\uff0c\u53c2\u6570 \\(\\mu\\) \u548c \\(\\lambda\\) \u4e0d\u662f\u5e38\u91cf\uff0c\u800c\u662f\u968f\u8bad\u7ec3\u8f6e\u6b21 \\(k\\) \u6539\u53d8\uff0c\u6b64\u65f6 \\(\\beta\\) \u4e3a\u6539\u53d8\u7684\u7cfb\u6570\uff0c\u5373\u6bcf\u8f6e\u8bad\u7ec3</p> <p>\\(\\mu_k = \\beta \\mu_{k-1}\\), \\(\\lambda_k = \\beta \\lambda_{k-1}\\)</p> <p>\u540c\u65f6\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\u7b49\u8d85\u53c2\u6570</p> <pre><code>    activation: \"tanh\"\n\n# training settings\nTRAIN:\n  epochs: 20000\n  iters_per_epoch: 1\n  eval_during_train: false\n  learning_rate: 0.001\n  max_iter: 15000\n</code></pre>"},{"location":"zh/examples/hpinns/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u5148\u4f7f\u7528 Adam \u4f18\u5316\u5668\u8fdb\u884c\u5927\u81f4\u8bad\u7ec3\uff0c\u518d\u4f7f\u7528 LBFGS \u4f18\u5316\u5668\u903c\u8fd1\u6700\u4f18\u70b9\uff0c\u56e0\u6b64\u9700\u8981\u4e24\u4e2a\u4f18\u5316\u5668\uff0c\u8fd9\u4e5f\u5bf9\u5e94\u4e86\u4e0a\u4e00\u90e8\u5206\u8d85\u53c2\u6570\u4e2d\u7684\u4e24\u79cd <code>EPOCHS</code> \u503c</p> <pre><code>optimizer_adam = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(\n    (model_re, model_im, model_eps)\n)\n</code></pre> <pre><code>optimizer_lbfgs = ppsci.optimizer.LBFGS(max_iter=cfg.TRAIN.max_iter)(\n    (model_re, model_im, model_eps)\n)\n</code></pre>"},{"location":"zh/examples/hpinns/#36","title":"3.6 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u91c7\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u7ea6\u675f\u4e3a\u7ed3\u679c\u9700\u8981\u6ee1\u8db3PDE\u516c\u5f0f\u3002</p> <p>\u867d\u7136\u6211\u4eec\u4e0d\u662f\u4ee5\u76d1\u7763\u5b66\u4e60\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u6b64\u5904\u4ecd\u7136\u53ef\u4ee5\u91c7\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code>\uff0c\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u76d1\u7763\u7ea6\u675f\u6307\u5b9a\u6587\u4ef6\u8def\u5f84\u7b49\u6570\u636e\u8bfb\u53d6\u914d\u7f6e\uff0c\u56e0\u4e3a\u6570\u636e\u96c6\u4e2d\u6ca1\u6709\u6807\u7b7e\u6570\u636e\uff0c\u56e0\u6b64\u5728\u6570\u636e\u8bfb\u53d6\u65f6\u6211\u4eec\u9700\u8981\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u5145\u5f53\u6807\u7b7e\u6570\u636e\uff0c\u5e76\u6ce8\u610f\u5728\u4e4b\u540e\u4e0d\u8981\u4f7f\u7528\u8fd9\u90e8\u5206\u201c\u5047\u7684\u201d\u6807\u7b7e\u6570\u636e\u3002</p> <pre><code>\"alias_dict\": {\n    \"e_real\": \"x\",\n    \"e_imaginary\": \"x\",\n    \"epsilon\": \"x\",\n    **{k: \"x\" for k in label_keys_derivative},\n},\n</code></pre> <p>\u5982\u4e0a\uff0c\u6240\u6709\u8f93\u51fa\u7684\u6807\u7b7e\u90fd\u4f1a\u8bfb\u53d6\u8f93\u5165 <code>x</code> \u7684\u503c\u3002</p> <p>\u4e0b\u9762\u662f\u7ea6\u675f\u7b49\u5177\u4f53\u5185\u5bb9\uff0c\u8981\u6ce8\u610f\u4e0a\u8ff0\u63d0\u5230\u7684\u7ed9\u5b9a\u201c\u5047\u7684\u201d\u6807\u7b7e\u6570\u636e\uff1a</p> <pre><code># manually build constraint(s)\nlabel_keys = (\"x\", \"y\", \"bound\", \"e_real\", \"e_imaginary\", \"epsilon\")\nlabel_keys_derivative = (\n    \"de_re_x\",\n    \"de_re_y\",\n    \"de_re_xx\",\n    \"de_re_yy\",\n    \"de_im_x\",\n    \"de_im_y\",\n    \"de_im_xx\",\n    \"de_im_yy\",\n)\noutput_expr = {\n    \"x\": lambda out: out[\"x\"],\n    \"y\": lambda out: out[\"y\"],\n    \"bound\": lambda out: out[\"bound\"],\n    \"e_real\": lambda out: out[\"e_real\"],\n    \"e_imaginary\": lambda out: out[\"e_imaginary\"],\n    \"epsilon\": lambda out: out[\"epsilon\"],\n    \"de_re_x\": lambda out: jacobian(out[\"e_real\"], out[\"x\"]),\n    \"de_re_y\": lambda out: jacobian(out[\"e_real\"], out[\"y\"]),\n    \"de_re_xx\": lambda out: hessian(out[\"e_real\"], out[\"x\"]),\n    \"de_re_yy\": lambda out: hessian(out[\"e_real\"], out[\"y\"]),\n    \"de_im_x\": lambda out: jacobian(out[\"e_imaginary\"], out[\"x\"]),\n    \"de_im_y\": lambda out: jacobian(out[\"e_imaginary\"], out[\"y\"]),\n    \"de_im_xx\": lambda out: hessian(out[\"e_imaginary\"], out[\"x\"]),\n    \"de_im_yy\": lambda out: hessian(out[\"e_imaginary\"], out[\"y\"]),\n}\n\nsup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH,\n            \"input_keys\": (\"x\", \"y\", \"bound\"),\n            \"label_keys\": label_keys + label_keys_derivative,\n            \"alias_dict\": {\n                \"e_real\": \"x\",\n                \"e_imaginary\": \"x\",\n                \"epsilon\": \"x\",\n                **{k: \"x\" for k in label_keys_derivative},\n            },\n        },\n    },\n    ppsci.loss.FunctionalLoss(func_module.pde_loss_fun),\n    output_expr,\n    name=\"sup_constraint_pde\",\n)\nsup_constraint_obj = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH,\n            \"input_keys\": (\"x\", \"y\", \"bound\"),\n            \"label_keys\": label_keys,\n            \"alias_dict\": {\"e_real\": \"x\", \"e_imaginary\": \"x\", \"epsilon\": \"x\"},\n        },\n    },\n    ppsci.loss.FunctionalLoss(func_module.obj_loss_fun),\n    {key: lambda out, k=key: out[k] for key in label_keys},\n    name=\"sup_constraint_obj\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u5176\u4e2d <code>\u201cdataset\u201d</code> \u5b57\u6bb5\u8868\u793a\u4f7f\u7528\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4fe1\u606f\uff0c\u5404\u4e2a\u5b57\u6bb5\u5206\u522b\u8868\u793a\uff1a</p> <ol> <li><code>name</code>\uff1a \u6570\u636e\u96c6\u7c7b\u578b\uff0c\u6b64\u5904 <code>\"IterableMatDataset\"</code> \u8868\u793a\u4e0d\u5206 batch \u987a\u5e8f\u8bfb\u53d6\u7684 <code>.mat</code> \u7c7b\u578b\u7684\u6570\u636e\u96c6\uff1b</li> <li><code>file_path</code>\uff1a \u6570\u636e\u96c6\u6587\u4ef6\u8def\u5f84\uff1b</li> <li><code>input_keys</code>\uff1a \u8f93\u5165\u53d8\u91cf\u540d\uff1b</li> <li><code>label_keys</code>\uff1a \u6807\u7b7e\u53d8\u91cf\u540d\uff1b</li> <li><code>alias_dict</code>\uff1a \u53d8\u91cf\u522b\u540d\u3002</li> </ol> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u7684 <code>FunctionalLoss</code> \u4e3a PaddleScience \u9884\u7559\u7684\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u7c7b\uff0c\u8be5\u7c7b\u652f\u6301\u7f16\u5199\u4ee3\u7801\u65f6\u81ea\u5b9a\u4e49 loss \u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u8bf8\u5982 <code>MSE</code> \u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u672c\u95ee\u9898\u4e2d\u7531\u4e8e\u5b58\u5728\u591a\u4e2a loss \u9879\uff0c\u56e0\u6b64\u9700\u8981\u5b9a\u4e49\u591a\u4e2a loss \u8ba1\u7b97\u51fd\u6570\uff0c\u8fd9\u4e5f\u662f\u9700\u8981\u6784\u5efa\u591a\u4e2a\u7ea6\u675f\u7684\u539f\u56e0\u3002\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u4ee3\u7801\u8bf7\u53c2\u8003 \u81ea\u5b9a\u4e49 loss \u548c metric\u3002</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165 <code>output_expr</code>\uff0c\u8ba1\u7b97\u540e\u7684\u503c\u5c06\u4f1a\u6309\u7167\u6307\u5b9a\u540d\u79f0\u5b58\u5165\u8f93\u51fa\u5217\u8868\u4e2d\uff0c\u4ece\u800c\u4fdd\u8bc1 loss \u8ba1\u7b97\u65f6\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b\u503c\u3002</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002</p> <p>\u5728\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code>constraint = {\n    sup_constraint_pde.name: sup_constraint_pde,\n    sup_constraint_obj.name: sup_constraint_obj,\n}\n</code></pre>"},{"location":"zh/examples/hpinns/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u4e0e\u7ea6\u675f\u540c\u7406\uff0c\u867d\u7136\u672c\u95ee\u9898\u4f7f\u7528\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u4f46\u4ecd\u53ef\u4ee5\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u672c\u95ee\u9898\u5b58\u5728\u4e24\u4e2a\u91c7\u6837\u70b9\u533a\u57df\uff0c\u4e00\u4e2a\u662f\u8f83\u5927\u7684\u5b8c\u6574\u5b9a\u4e49\u533a\u57df\uff0c\u53e6\u4e00\u4e2a\u662f\u5b9a\u4e49\u57df\u4e2d\u7684\u4e00\u5757 objective \u533a\u57df\uff0c\u8bc4\u4f30\u5668\u5206\u522b\u5bf9\u8fd9\u4e24\u4e2a\u533a\u57df\u8fdb\u884c\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e24\u4e2a\u8bc4\u4f30\u5668\u3002<code>opt</code>\u5bf9\u5e94 objective \u533a\u57df\uff0c<code>val</code> \u5bf9\u5e94\u6574\u4e2a\u5b9a\u4e49\u57df\u3002</p> <pre><code># manually build validator\nsup_validator_opt = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH_VALID,\n            \"input_keys\": (\"x\", \"y\", \"bound\"),\n            \"label_keys\": label_keys + label_keys_derivative,\n            \"alias_dict\": {\n                \"x\": \"x_opt\",\n                \"y\": \"y_opt\",\n                \"e_real\": \"x_opt\",\n                \"e_imaginary\": \"x_opt\",\n                \"epsilon\": \"x_opt\",\n                **{k: \"x_opt\" for k in label_keys_derivative},\n            },\n        },\n    },\n    ppsci.loss.FunctionalLoss(func_module.eval_loss_fun),\n    output_expr,\n    {\"mse\": ppsci.metric.FunctionalMetric(func_module.eval_metric_fun)},\n    name=\"opt_sup\",\n)\nsup_validator_val = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATASET_PATH_VALID,\n            \"input_keys\": (\"x\", \"y\", \"bound\"),\n            \"label_keys\": label_keys + label_keys_derivative,\n            \"alias_dict\": {\n                \"x\": \"x_val\",\n                \"y\": \"y_val\",\n                \"e_real\": \"x_val\",\n                \"e_imaginary\": \"x_val\",\n                \"epsilon\": \"x_val\",\n                **{k: \"x_val\" for k in label_keys_derivative},\n            },\n        },\n    },\n    ppsci.loss.FunctionalLoss(func_module.eval_loss_fun),\n    output_expr,\n    {\"mse\": ppsci.metric.FunctionalMetric(func_module.eval_metric_fun)},\n    name=\"val_sup\",\n)\nvalidator = {\n    sup_validator_opt.name: sup_validator_opt,\n    sup_validator_val.name: sup_validator_val,\n}\n</code></pre> <p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u4e3a <code>FunctionalMetric</code>\uff0c\u8fd9\u662f PaddleScience \u9884\u7559\u7684\u81ea\u5b9a\u4e49 metric \u51fd\u6570\u7c7b\uff0c\u8be5\u7c7b\u652f\u6301\u7f16\u5199\u4ee3\u7801\u65f6\u81ea\u5b9a\u4e49 metric \u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u8bf8\u5982 <code>MSE</code>\u3001 <code>L2</code> \u7b49\u73b0\u6709\u65b9\u6cd5\u3002\u81ea\u5b9a\u4e49 metric \u51fd\u6570\u4ee3\u7801\u8bf7\u53c2\u8003\u4e0b\u4e00\u90e8\u5206 \u81ea\u5b9a\u4e49 loss \u548c metric\u3002</p> <p>\u5176\u4f59\u914d\u7f6e\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>"},{"location":"zh/examples/hpinns/#38-loss-metric","title":"3.8 \u81ea\u5b9a\u4e49 loss \u548c metric","text":"<p>\u7531\u4e8e\u672c\u95ee\u9898\u91c7\u7528\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u6807\u7b7e\u6570\u636e\uff0closs \u548c metric \u6839\u636e PDE \u8ba1\u7b97\u5f97\u5230\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u5b9a\u4e49 loss \u548c metric\u3002\u65b9\u6cd5\u4e3a\u5148\u5b9a\u4e49\u76f8\u5173\u51fd\u6570\uff0c\u518d\u5c06\u51fd\u6570\u540d\u4f5c\u4e3a\u53c2\u6570\u4f20\u7ed9 <code>FunctionalLoss</code> \u548c <code>FunctionalMetric</code>\u3002</p> <p>\u9700\u8981\u6ce8\u610f\u81ea\u5b9a\u4e49 loss \u548c metric \u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u53c2\u6570\u9700\u8981\u4e0e PaddleScience \u4e2d\u5982 <code>MSE</code> \u7b49\u5176\u4ed6\u51fd\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u5373\u8f93\u5165\u4e3a\u6a21\u578b\u8f93\u51fa <code>output_dict</code> \u7b49\u5b57\u5178\u53d8\u91cf\uff0closs \u51fd\u6570\u8f93\u51fa\u4e3a loss \u503c <code>paddle.Tensor</code>\uff0cmetric \u51fd\u6570\u8f93\u51fa\u4e3a\u5b57\u5178 <code>Dict[str, paddle.Tensor]</code>\u3002</p> <pre><code>def pde_loss_fun(output_dict: Dict[str, paddle.Tensor], *args) -&gt; paddle.Tensor:\n    \"\"\"Compute pde loss and lagrangian loss.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n\n    Returns:\n        paddle.Tensor: PDE loss (and lagrangian loss if using Augmented Lagrangian method).\n    \"\"\"\n    global loss_log\n    bound = int(output_dict[\"bound\"])\n    loss_re, loss_im = compute_real_and_imaginary_loss(output_dict)\n    loss_re = loss_re[bound:]\n    loss_im = loss_im[bound:]\n\n    loss_eqs1 = paddle.mean(loss_re**2)\n    loss_eqs2 = paddle.mean(loss_im**2)\n    # augmented_Lagrangian\n    if lambda_im is None:\n        init_lambda(output_dict, bound)\n    loss_lag1 = paddle.mean(loss_re * lambda_re)\n    loss_lag2 = paddle.mean(loss_im * lambda_im)\n\n    losses = (\n        loss_weight[0] * loss_eqs1\n        + loss_weight[1] * loss_eqs2\n        + loss_weight[2] * loss_lag1\n        + loss_weight[3] * loss_lag2\n    )\n    loss_log.append(float(loss_eqs1 + loss_eqs2))  # for plotting\n    loss_log.append(float(loss_lag1 + loss_lag2))  # for plotting\n    return {\"pde\": losses}\n\n\ndef obj_loss_fun(output_dict: Dict[str, paddle.Tensor], *args) -&gt; paddle.Tensor:\n    \"\"\"Compute objective loss.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n\n    Returns:\n        paddle.Tensor: Objective loss.\n    \"\"\"\n    global loss_log, loss_obj\n    x, y = output_dict[\"x\"], output_dict[\"y\"]\n    bound = int(output_dict[\"bound\"])\n    e_re = output_dict[\"e_real\"]\n    e_im = output_dict[\"e_imaginary\"]\n\n    f1 = paddle.heaviside((x + 0.5) * (0.5 - x), paddle.to_tensor(0.5))\n    f2 = paddle.heaviside((y - 1) * (2 - y), paddle.to_tensor(0.5))\n    j = e_re[:bound] ** 2 + e_im[:bound] ** 2 - f1[:bound] * f2[:bound]\n    loss_opt_area = paddle.mean(j**2)\n\n    if lambda_im is None:\n        init_lambda(output_dict, bound)\n    losses = loss_weight[4] * loss_opt_area\n    loss_log.append(float(loss_opt_area))  # for plotting\n    loss_obj = float(loss_opt_area)  # for plotting\n    return {\"obj\": losses}\n\n\ndef eval_loss_fun(output_dict: Dict[str, paddle.Tensor], *args) -&gt; paddle.Tensor:\n    \"\"\"Compute objective loss for evaluation.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n\n    Returns:\n        paddle.Tensor: Objective loss.\n    \"\"\"\n    x, y = output_dict[\"x\"], output_dict[\"y\"]\n    e_re = output_dict[\"e_real\"]\n    e_im = output_dict[\"e_imaginary\"]\n\n    f1 = paddle.heaviside((x + 0.5) * (0.5 - x), paddle.to_tensor(0.5))\n    f2 = paddle.heaviside((y - 1) * (2 - y), paddle.to_tensor(0.5))\n    j = e_re**2 + e_im**2 - f1 * f2\n    losses = paddle.mean(j**2)\n\n    return {\"eval\": losses}\n</code></pre> <pre><code>def eval_metric_fun(\n    output_dict: Dict[str, paddle.Tensor], *args\n) -&gt; Dict[str, paddle.Tensor]:\n    \"\"\"Compute metric for evaluation.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n\n    Returns:\n        Dict[str, paddle.Tensor]: MSE metric.\n    \"\"\"\n    loss_re, loss_im = compute_real_and_imaginary_loss(output_dict)\n    eps_opt = paddle.concat([loss_re, loss_im], axis=-1)\n    metric = paddle.mean(eps_opt**2)\n\n    metric_dict = {\"eval_metric\": metric}\n    return metric_dict\n</code></pre>"},{"location":"zh/examples/hpinns/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model_list,\n    constraint,\n    cfg.output_dir,\n    optimizer_adam,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    validator=validator,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n)\n\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre> <p>\u7531\u4e8e\u672c\u95ee\u9898\u5b58\u5728\u591a\u79cd\u8bad\u7ec3\u6a21\u5f0f\uff0c\u6839\u636e\u6bcf\u4e2a\u6a21\u5f0f\u7684\u4e0d\u540c\uff0c\u5c06\u8fdb\u884c \\([2,1+k]\\) \u6b21\u5b8c\u6574\u7684\u8bad\u7ec3\u3001\u8bc4\u4f30\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u53c2\u8003 \u5b8c\u6574\u4ee3\u7801 \u4e2d holography.py \u6587\u4ef6\u3002</p>"},{"location":"zh/examples/hpinns/#310","title":"3.10 \u53ef\u89c6\u5316","text":"<p>PaddleScience \u4e2d\u63d0\u4f9b\u4e86\u53ef\u89c6\u5316\u5668\uff0c\u4f46\u7531\u4e8e\u672c\u95ee\u9898\u56fe\u7247\u6570\u91cf\u8f83\u591a\u4e14\u8f83\u4e3a\u590d\u6742\uff0c\u4ee3\u7801\u4e2d\u81ea\u5b9a\u4e49\u4e86\u53ef\u89c6\u5316\u51fd\u6570\uff0c\u8c03\u7528\u81ea\u5b9a\u4e49\u51fd\u6570\u5373\u53ef\u5b9e\u73b0\u53ef\u89c6\u5316</p> <pre><code>################# plotting ###################\n# log of loss\nloss_log = np.array(func_module.loss_log).reshape(-1, 3)\n\nplot_module.set_params(\n    cfg.TRAIN_MODE, cfg.output_dir, cfg.DATASET_PATH, cfg.DATASET_PATH_VALID\n)\nplot_module.plot_6a(loss_log)\nif cfg.TRAIN_MODE != \"soft\":\n    plot_module.prepare_data(solver, expr_dict)\n    plot_module.plot_6b(loss_log_obj)\n    plot_module.plot_6c7c(func_module.lambda_log)\n    plot_module.plot_6d(func_module.lambda_log)\n    plot_module.plot_6ef(func_module.lambda_log)\n</code></pre> <p>\u81ea\u5b9a\u4e49\u4ee3\u7801\u8bf7\u53c2\u8003 \u5b8c\u6574\u4ee3\u7801 \u4e2d plotting.py \u6587\u4ef6\u3002</p>"},{"location":"zh/examples/hpinns/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"<p>\u5b8c\u6574\u4ee3\u7801\u5305\u542b PaddleScience \u5177\u4f53\u5b9e\u73b0\u6d41\u7a0b\u4ee3\u7801 holography.py\uff0c\u6240\u6709\u81ea\u5b9a\u4e49\u51fd\u6570\u4ee3\u7801 functions.py \u548c \u81ea\u5b9a\u4e49\u53ef\u89c6\u5316\u4ee3\u7801 plotting.py\u3002</p> holography.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis module is heavily adapted from https://github.com/lululxvi/hpinn\n\"\"\"\n\nfrom os import path as osp\n\nimport functions as func_module\nimport hydra\nimport numpy as np\nimport paddle\nimport plotting as plot_module\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.autodiff import hessian\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # open FLAG for higher order differential operator\n    paddle.framework.core.set_prim_eager_enabled(True)\n\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    model_re = ppsci.arch.MLP(**cfg.MODEL.re_net)\n    model_im = ppsci.arch.MLP(**cfg.MODEL.im_net)\n    model_eps = ppsci.arch.MLP(**cfg.MODEL.eps_net)\n\n    # initialize params\n    func_module.train_mode = cfg.TRAIN_MODE\n    loss_log_obj = []\n\n    # register transform\n    model_re.register_input_transform(func_module.transform_in)\n    model_im.register_input_transform(func_module.transform_in)\n    model_eps.register_input_transform(func_module.transform_in)\n\n    model_re.register_output_transform(func_module.transform_out_real_part)\n    model_im.register_output_transform(func_module.transform_out_imaginary_part)\n    model_eps.register_output_transform(func_module.transform_out_epsilon)\n\n    model_list = ppsci.arch.ModelList((model_re, model_im, model_eps))\n\n    # initialize Adam optimizer\n    optimizer_adam = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(\n        (model_re, model_im, model_eps)\n    )\n\n    # manually build constraint(s)\n    label_keys = (\"x\", \"y\", \"bound\", \"e_real\", \"e_imaginary\", \"epsilon\")\n    label_keys_derivative = (\n        \"de_re_x\",\n        \"de_re_y\",\n        \"de_re_xx\",\n        \"de_re_yy\",\n        \"de_im_x\",\n        \"de_im_y\",\n        \"de_im_xx\",\n        \"de_im_yy\",\n    )\n    output_expr = {\n        \"x\": lambda out: out[\"x\"],\n        \"y\": lambda out: out[\"y\"],\n        \"bound\": lambda out: out[\"bound\"],\n        \"e_real\": lambda out: out[\"e_real\"],\n        \"e_imaginary\": lambda out: out[\"e_imaginary\"],\n        \"epsilon\": lambda out: out[\"epsilon\"],\n        \"de_re_x\": lambda out: jacobian(out[\"e_real\"], out[\"x\"]),\n        \"de_re_y\": lambda out: jacobian(out[\"e_real\"], out[\"y\"]),\n        \"de_re_xx\": lambda out: hessian(out[\"e_real\"], out[\"x\"]),\n        \"de_re_yy\": lambda out: hessian(out[\"e_real\"], out[\"y\"]),\n        \"de_im_x\": lambda out: jacobian(out[\"e_imaginary\"], out[\"x\"]),\n        \"de_im_y\": lambda out: jacobian(out[\"e_imaginary\"], out[\"y\"]),\n        \"de_im_xx\": lambda out: hessian(out[\"e_imaginary\"], out[\"x\"]),\n        \"de_im_yy\": lambda out: hessian(out[\"e_imaginary\"], out[\"y\"]),\n    }\n\n    sup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableMatDataset\",\n                \"file_path\": cfg.DATASET_PATH,\n                \"input_keys\": (\"x\", \"y\", \"bound\"),\n                \"label_keys\": label_keys + label_keys_derivative,\n                \"alias_dict\": {\n                    \"e_real\": \"x\",\n                    \"e_imaginary\": \"x\",\n                    \"epsilon\": \"x\",\n                    **{k: \"x\" for k in label_keys_derivative},\n                },\n            },\n        },\n        ppsci.loss.FunctionalLoss(func_module.pde_loss_fun),\n        output_expr,\n        name=\"sup_constraint_pde\",\n    )\n    sup_constraint_obj = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableMatDataset\",\n                \"file_path\": cfg.DATASET_PATH,\n                \"input_keys\": (\"x\", \"y\", \"bound\"),\n                \"label_keys\": label_keys,\n                \"alias_dict\": {\"e_real\": \"x\", \"e_imaginary\": \"x\", \"epsilon\": \"x\"},\n            },\n        },\n        ppsci.loss.FunctionalLoss(func_module.obj_loss_fun),\n        {key: lambda out, k=key: out[k] for key in label_keys},\n        name=\"sup_constraint_obj\",\n    )\n    constraint = {\n        sup_constraint_pde.name: sup_constraint_pde,\n        sup_constraint_obj.name: sup_constraint_obj,\n    }\n\n    # manually build validator\n    sup_validator_opt = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"IterableMatDataset\",\n                \"file_path\": cfg.DATASET_PATH_VALID,\n                \"input_keys\": (\"x\", \"y\", \"bound\"),\n                \"label_keys\": label_keys + label_keys_derivative,\n                \"alias_dict\": {\n                    \"x\": \"x_opt\",\n                    \"y\": \"y_opt\",\n                    \"e_real\": \"x_opt\",\n                    \"e_imaginary\": \"x_opt\",\n                    \"epsilon\": \"x_opt\",\n                    **{k: \"x_opt\" for k in label_keys_derivative},\n                },\n            },\n        },\n        ppsci.loss.FunctionalLoss(func_module.eval_loss_fun),\n        output_expr,\n        {\"mse\": ppsci.metric.FunctionalMetric(func_module.eval_metric_fun)},\n        name=\"opt_sup\",\n    )\n    sup_validator_val = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"IterableMatDataset\",\n                \"file_path\": cfg.DATASET_PATH_VALID,\n                \"input_keys\": (\"x\", \"y\", \"bound\"),\n                \"label_keys\": label_keys + label_keys_derivative,\n                \"alias_dict\": {\n                    \"x\": \"x_val\",\n                    \"y\": \"y_val\",\n                    \"e_real\": \"x_val\",\n                    \"e_imaginary\": \"x_val\",\n                    \"epsilon\": \"x_val\",\n                    **{k: \"x_val\" for k in label_keys_derivative},\n                },\n            },\n        },\n        ppsci.loss.FunctionalLoss(func_module.eval_loss_fun),\n        output_expr,\n        {\"mse\": ppsci.metric.FunctionalMetric(func_module.eval_metric_fun)},\n        name=\"val_sup\",\n    )\n    validator = {\n        sup_validator_opt.name: sup_validator_opt,\n        sup_validator_val.name: sup_validator_val,\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        constraint,\n        cfg.output_dir,\n        optimizer_adam,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        validator=validator,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n    )\n\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n    # initialize LBFGS optimizer\n    optimizer_lbfgs = ppsci.optimizer.LBFGS(max_iter=cfg.TRAIN.max_iter)(\n        (model_re, model_im, model_eps)\n    )\n\n    # train: soft constraint, epoch=1 for lbfgs\n    if cfg.TRAIN_MODE == \"soft\":\n        solver = ppsci.solver.Solver(\n            model_list,\n            constraint,\n            cfg.output_dir,\n            optimizer_lbfgs,\n            None,\n            cfg.TRAIN.epochs_lbfgs,\n            cfg.TRAIN.iters_per_epoch,\n            eval_during_train=cfg.TRAIN.eval_during_train,\n            validator=validator,\n            checkpoint_path=cfg.TRAIN.checkpoint_path,\n        )\n\n        # train model\n        solver.train()\n        # evaluate after finished training\n        solver.eval()\n\n    # append objective loss for plot\n    loss_log_obj.append(func_module.loss_obj)\n\n    # penalty and augmented Lagrangian, difference between the two is updating of lambda\n    if cfg.TRAIN_MODE != \"soft\":\n        train_dict = ppsci.utils.reader.load_mat_file(\n            cfg.DATASET_PATH, (\"x\", \"y\", \"bound\")\n        )\n        in_dict = {\"x\": train_dict[\"x\"], \"y\": train_dict[\"y\"]}\n        expr_dict = output_expr.copy()\n        expr_dict.pop(\"bound\")\n\n        func_module.init_lambda(in_dict, int(train_dict[\"bound\"]))\n        func_module.lambda_log.append(\n            [\n                func_module.lambda_re.copy().squeeze(),\n                func_module.lambda_im.copy().squeeze(),\n            ]\n        )\n\n        for i in range(1, cfg.TRAIN_K + 1):\n            pred_dict = solver.predict(\n                in_dict,\n                expr_dict,\n                batch_size=np.shape(train_dict[\"x\"])[0],\n                no_grad=False,\n            )\n            func_module.update_lambda(pred_dict, int(train_dict[\"bound\"]))\n\n            func_module.update_mu()\n            logger.message(f\"Iteration {i}: mu = {func_module.mu}\\n\")\n\n            solver = ppsci.solver.Solver(\n                model_list,\n                constraint,\n                cfg.output_dir,\n                optimizer_lbfgs,\n                None,\n                cfg.TRAIN.epochs_lbfgs,\n                cfg.TRAIN.iters_per_epoch,\n                eval_during_train=cfg.TRAIN.eval_during_train,\n                validator=validator,\n                checkpoint_path=cfg.TRAIN.checkpoint_path,\n            )\n\n            # train model\n            solver.train()\n            # evaluate\n            solver.eval()\n            # append objective loss for plot\n            loss_log_obj.append(func_module.loss_obj)\n\n    ################# plotting ###################\n    # log of loss\n    loss_log = np.array(func_module.loss_log).reshape(-1, 3)\n\n    plot_module.set_params(\n        cfg.TRAIN_MODE, cfg.output_dir, cfg.DATASET_PATH, cfg.DATASET_PATH_VALID\n    )\n    plot_module.plot_6a(loss_log)\n    if cfg.TRAIN_MODE != \"soft\":\n        plot_module.prepare_data(solver, expr_dict)\n        plot_module.plot_6b(loss_log_obj)\n        plot_module.plot_6c7c(func_module.lambda_log)\n        plot_module.plot_6d(func_module.lambda_log)\n        plot_module.plot_6ef(func_module.lambda_log)\n\n\ndef evaluate(cfg: DictConfig):\n    # open FLAG for higher order differential operator\n    paddle.framework.core.set_prim_eager_enabled(True)\n\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    model_re = ppsci.arch.MLP(**cfg.MODEL.re_net)\n    model_im = ppsci.arch.MLP(**cfg.MODEL.im_net)\n    model_eps = ppsci.arch.MLP(**cfg.MODEL.eps_net)\n\n    # initialize params\n    func_module.train_mode = cfg.TRAIN_MODE\n\n    # register transform\n    model_re.register_input_transform(func_module.transform_in)\n    model_im.register_input_transform(func_module.transform_in)\n    model_eps.register_input_transform(func_module.transform_in)\n\n    model_re.register_output_transform(func_module.transform_out_real_part)\n    model_im.register_output_transform(func_module.transform_out_imaginary_part)\n    model_eps.register_output_transform(func_module.transform_out_epsilon)\n\n    model_list = ppsci.arch.ModelList((model_re, model_im, model_eps))\n\n    # manually build constraint(s)\n    label_keys = (\"x\", \"y\", \"bound\", \"e_real\", \"e_imaginary\", \"epsilon\")\n    label_keys_derivative = (\n        \"de_re_x\",\n        \"de_re_y\",\n        \"de_re_xx\",\n        \"de_re_yy\",\n        \"de_im_x\",\n        \"de_im_y\",\n        \"de_im_xx\",\n        \"de_im_yy\",\n    )\n    output_expr = {\n        \"x\": lambda out: out[\"x\"],\n        \"y\": lambda out: out[\"y\"],\n        \"bound\": lambda out: out[\"bound\"],\n        \"e_real\": lambda out: out[\"e_real\"],\n        \"e_imaginary\": lambda out: out[\"e_imaginary\"],\n        \"epsilon\": lambda out: out[\"epsilon\"],\n        \"de_re_x\": lambda out: jacobian(out[\"e_real\"], out[\"x\"]),\n        \"de_re_y\": lambda out: jacobian(out[\"e_real\"], out[\"y\"]),\n        \"de_re_xx\": lambda out: hessian(out[\"e_real\"], out[\"x\"]),\n        \"de_re_yy\": lambda out: hessian(out[\"e_real\"], out[\"y\"]),\n        \"de_im_x\": lambda out: jacobian(out[\"e_imaginary\"], out[\"x\"]),\n        \"de_im_y\": lambda out: jacobian(out[\"e_imaginary\"], out[\"y\"]),\n        \"de_im_xx\": lambda out: hessian(out[\"e_imaginary\"], out[\"x\"]),\n        \"de_im_yy\": lambda out: hessian(out[\"e_imaginary\"], out[\"y\"]),\n    }\n\n    # manually build validator\n    sup_validator_opt = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"IterableMatDataset\",\n                \"file_path\": cfg.DATASET_PATH_VALID,\n                \"input_keys\": (\"x\", \"y\", \"bound\"),\n                \"label_keys\": label_keys + label_keys_derivative,\n                \"alias_dict\": {\n                    \"x\": \"x_opt\",\n                    \"y\": \"y_opt\",\n                    \"e_real\": \"x_opt\",\n                    \"e_imaginary\": \"x_opt\",\n                    \"epsilon\": \"x_opt\",\n                    **{k: \"x_opt\" for k in label_keys_derivative},\n                },\n            },\n        },\n        ppsci.loss.FunctionalLoss(func_module.eval_loss_fun),\n        output_expr,\n        {\"mse\": ppsci.metric.FunctionalMetric(func_module.eval_metric_fun)},\n        name=\"opt_sup\",\n    )\n    sup_validator_val = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"IterableMatDataset\",\n                \"file_path\": cfg.DATASET_PATH_VALID,\n                \"input_keys\": (\"x\", \"y\", \"bound\"),\n                \"label_keys\": label_keys + label_keys_derivative,\n                \"alias_dict\": {\n                    \"x\": \"x_val\",\n                    \"y\": \"y_val\",\n                    \"e_real\": \"x_val\",\n                    \"e_imaginary\": \"x_val\",\n                    \"epsilon\": \"x_val\",\n                    **{k: \"x_val\" for k in label_keys_derivative},\n                },\n            },\n        },\n        ppsci.loss.FunctionalLoss(func_module.eval_loss_fun),\n        output_expr,\n        {\"mse\": ppsci.metric.FunctionalMetric(func_module.eval_metric_fun)},\n        name=\"val_sup\",\n    )\n    validator = {\n        sup_validator_opt.name: sup_validator_opt,\n        sup_validator_val.name: sup_validator_val,\n    }\n\n    solver = ppsci.solver.Solver(\n        model_list,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n\n    # evaluate\n    solver.eval()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model_re = ppsci.arch.MLP(**cfg.MODEL.re_net)\n    model_im = ppsci.arch.MLP(**cfg.MODEL.im_net)\n    model_eps = ppsci.arch.MLP(**cfg.MODEL.eps_net)\n\n    # register transform\n    model_re.register_input_transform(func_module.transform_in)\n    model_im.register_input_transform(func_module.transform_in)\n    model_eps.register_input_transform(func_module.transform_in)\n\n    model_re.register_output_transform(func_module.transform_out_real_part)\n    model_im.register_output_transform(func_module.transform_out_imaginary_part)\n    model_eps.register_output_transform(func_module.transform_out_epsilon)\n\n    # wrap to a model_list\n    model_list = ppsci.arch.ModelList((model_re, model_im, model_eps))\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model_list,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in [\"x\", \"y\"]},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    valid_dict = ppsci.utils.reader.load_mat_file(\n        cfg.DATASET_PATH_VALID, (\"x_val\", \"y_val\", \"bound\")\n    )\n    input_dict = {\"x\": valid_dict[\"x_val\"], \"y\": valid_dict[\"y_val\"]}\n\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.INFER.output_keys, output_dict.keys())\n    }\n\n    # plotting E and eps\n    N = ((func_module.l_BOX[1] - func_module.l_BOX[0]) / 0.05).astype(int)\n    input_eval = np.stack((input_dict[\"x\"], input_dict[\"y\"]), axis=-1).reshape(\n        N[0], N[1], 2\n    )\n    e_re = output_dict[\"e_re\"].reshape(N[0], N[1])\n    e_im = output_dict[\"e_im\"].reshape(N[0], N[1])\n    eps = output_dict[\"eps\"].reshape(N[0], N[1])\n    v_visual = e_re**2 + e_im**2\n    field_visual = np.stack((v_visual, eps), axis=-1)\n    plot_module.field_name = [\"Fig7_E\", \"Fig7_eps\"]\n    plot_module.FIGNAME = \"hpinns_pred\"\n    plot_module.plot_field_holo(input_eval, field_visual)\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"hpinns.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> functions.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis module is heavily adapted from https://github.com/lululxvi/hpinn\n\"\"\"\n\nfrom typing import Dict\nfrom typing import List\n\nimport numpy as np\nimport paddle\nimport paddle.nn.functional as F\n\n\"\"\"All functions used in hpinns example, including functions of transform and loss.\"\"\"\n\n# define constants\nBOX = np.array([[-2, -2], [2, 3]])\nDPML = 1\nOMEGA = 2 * np.pi\nSIGMA0 = -np.log(1e-20) / (4 * DPML**3 / 3)\nl_BOX = BOX + np.array([[-DPML, -DPML], [DPML, DPML]])\nbeta = 2.0\nmu = 2\n\n# define variables which will be updated during training\nlambda_re: np.ndarray = None\nlambda_im: np.ndarray = None\nloss_weight: List[float] = None\ntrain_mode: str = None\n\n# define log variables for plotting\nloss_log = []  # record all losses, [pde, lag, obj]\nloss_obj = 0.0  # record last objective loss of each k\nlambda_log = []  # record all lambdas\n\n\n# transform\ndef transform_in(input):\n    # Periodic BC in x\n    P = BOX[1][0] - BOX[0][0] + 2 * DPML\n    w = 2 * np.pi / P\n    x, y = input[\"x\"], input[\"y\"]\n    input_transformed = {}\n    for t in range(1, 7):\n        input_transformed[f\"x_cos_{t}\"] = paddle.cos(t * w * x)\n        input_transformed[f\"x_sin_{t}\"] = paddle.sin(t * w * x)\n    input_transformed[\"y\"] = y\n    input_transformed[\"y_cos_1\"] = paddle.cos(OMEGA * y)\n    input_transformed[\"y_sin_1\"] = paddle.sin(OMEGA * y)\n\n    return input_transformed\n\n\ndef transform_out_all(input, var):\n    y = input[\"y\"]\n    # Zero Dirichlet BC\n    a, b = BOX[0][1] - DPML, BOX[1][1] + DPML\n    t = (1 - paddle.exp(a - y)) * (1 - paddle.exp(y - b))\n    return t * var\n\n\ndef transform_out_real_part(input, out):\n    re = out[\"e_re\"]\n    trans_out = transform_out_all(input, re)\n    return {\"e_real\": trans_out}\n\n\ndef transform_out_imaginary_part(input, out):\n    im = out[\"e_im\"]\n    trans_out = transform_out_all(input, im)\n    return {\"e_imaginary\": trans_out}\n\n\ndef transform_out_epsilon(input, out):\n    eps = out[\"eps\"]\n    # 1 &lt;= eps &lt;= 12\n    eps = F.sigmoid(eps) * 11 + 1\n    return {\"epsilon\": eps}\n\n\n# loss\ndef init_lambda(output_dict: Dict[str, paddle.Tensor], bound: int):\n    \"\"\"Init lambdas of Lagrangian and weights of losses.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n        bound (int): The bound of the data range that should be used.\n    \"\"\"\n    global lambda_re, lambda_im, loss_weight\n    x, y = output_dict[\"x\"], output_dict[\"y\"]\n    lambda_re = np.zeros((len(x[bound:]), 1), paddle.get_default_dtype())\n    lambda_im = np.zeros((len(y[bound:]), 1), paddle.get_default_dtype())\n    # loss_weight: [PDE loss 1, PDE loss 2, Lagrangian loss 1, Lagrangian loss 2, objective loss]\n    if train_mode == \"aug_lag\":\n        loss_weight = [0.5 * mu] * 2 + [1.0, 1.0] + [1.0]\n    else:\n        loss_weight = [0.5 * mu] * 2 + [0.0, 0.0] + [1.0]\n\n\ndef update_lambda(output_dict: Dict[str, paddle.Tensor], bound: int):\n    \"\"\"Update lambdas of Lagrangian.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n        bound (int): The bound of the data range that should be used.\n    \"\"\"\n    global lambda_re, lambda_im, lambda_log\n    loss_re, loss_im = compute_real_and_imaginary_loss(output_dict)\n    loss_re = loss_re[bound:]\n    loss_im = loss_im[bound:]\n    lambda_re += mu * loss_re.numpy()\n    lambda_im += mu * loss_im.numpy()\n    lambda_log.append([lambda_re.copy().squeeze(), lambda_im.copy().squeeze()])\n\n\ndef update_mu():\n    \"\"\"Update mu.\"\"\"\n    global mu, loss_weight\n    mu *= beta\n    loss_weight[:2] = [0.5 * mu] * 2\n\n\ndef _sigma_1(d):\n    return SIGMA0 * d**2 * np.heaviside(d, 0)\n\n\ndef _sigma_2(d):\n    return 2 * SIGMA0 * d * np.heaviside(d, 0)\n\n\ndef sigma(x, a, b):\n    \"\"\"sigma(x) = 0 if a &lt; x &lt; b, else grows cubically from zero.\"\"\"\n    return _sigma_1(a - x) + _sigma_1(x - b)\n\n\ndef dsigma(x, a, b):\n    return -_sigma_2(a - x) + _sigma_2(x - b)\n\n\ndef perfectly_matched_layers(x: paddle.Tensor, y: paddle.Tensor):\n    \"\"\"Apply the technique of perfectly matched layers(PMLs) proposed by paper arXiv:2108.05348.\n\n    Args:\n        x (paddle.Tensor): one of input contains tensor.\n        y (paddle.Tensor): one of input contains tensor.\n\n    Returns:\n        np.ndarray: Parameters of pde formula.\n    \"\"\"\n    x = x.numpy()\n    y = y.numpy()\n\n    sigma_x = sigma(x, BOX[0][0], BOX[1][0])\n    AB1 = 1 / (1 + 1j / OMEGA * sigma_x) ** 2\n    A1, B1 = AB1.real, AB1.imag\n\n    dsigma_x = dsigma(x, BOX[0][0], BOX[1][0])\n    AB2 = -1j / OMEGA * dsigma_x * AB1 / (1 + 1j / OMEGA * sigma_x)\n    A2, B2 = AB2.real, AB2.imag\n\n    sigma_y = sigma(y, BOX[0][1], BOX[1][1])\n    AB3 = 1 / (1 + 1j / OMEGA * sigma_y) ** 2\n    A3, B3 = AB3.real, AB3.imag\n\n    dsigma_y = dsigma(y, BOX[0][1], BOX[1][1])\n    AB4 = -1j / OMEGA * dsigma_y * AB3 / (1 + 1j / OMEGA * sigma_y)\n    A4, B4 = AB4.real, AB4.imag\n    return A1, B1, A2, B2, A3, B3, A4, B4\n\n\ndef obj_func_J(y):\n    # Approximate the objective function\n    y = y.numpy() + 1.5\n    h = 0.2\n    return 1 / (h * np.pi**0.5) * np.exp(-((y / h) ** 2)) * (np.abs(y) &lt; 0.5)\n\n\ndef compute_real_and_imaginary_loss(\n    output_dict: Dict[str, paddle.Tensor]\n) -&gt; paddle.Tensor:\n    \"\"\"Compute real and imaginary_loss.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n\n    Returns:\n        paddle.Tensor: Real and imaginary_loss.\n    \"\"\"\n    x, y = output_dict[\"x\"], output_dict[\"y\"]\n    e_re = output_dict[\"e_real\"]\n    e_im = output_dict[\"e_imaginary\"]\n    eps = output_dict[\"epsilon\"]\n\n    condition = np.logical_and(y.numpy() &lt; 0, y.numpy() &gt; -1).astype(\n        paddle.get_default_dtype()\n    )\n\n    eps = eps * condition + 1 - condition\n\n    de_re_x = output_dict[\"de_re_x\"]\n    de_re_y = output_dict[\"de_re_y\"]\n    de_re_xx = output_dict[\"de_re_xx\"]\n    de_re_yy = output_dict[\"de_re_yy\"]\n    de_im_x = output_dict[\"de_im_x\"]\n    de_im_y = output_dict[\"de_im_y\"]\n    de_im_xx = output_dict[\"de_im_xx\"]\n    de_im_yy = output_dict[\"de_im_yy\"]\n\n    a1, b1, a2, b2, a3, b3, a4, b4 = perfectly_matched_layers(x, y)\n\n    loss_re = (\n        (a1 * de_re_xx + a2 * de_re_x + a3 * de_re_yy + a4 * de_re_y) / OMEGA\n        - (b1 * de_im_xx + b2 * de_im_x + b3 * de_im_yy + b4 * de_im_y) / OMEGA\n        + eps * OMEGA * e_re\n    )\n    loss_im = (\n        (a1 * de_im_xx + a2 * de_im_x + a3 * de_im_yy + a4 * de_im_y) / OMEGA\n        + (b1 * de_re_xx + b2 * de_re_x + b3 * de_re_yy + b4 * de_re_y) / OMEGA\n        + eps * OMEGA * e_im\n        + obj_func_J(y)\n    )\n    return loss_re, loss_im\n\n\ndef pde_loss_fun(output_dict: Dict[str, paddle.Tensor], *args) -&gt; paddle.Tensor:\n    \"\"\"Compute pde loss and lagrangian loss.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n\n    Returns:\n        paddle.Tensor: PDE loss (and lagrangian loss if using Augmented Lagrangian method).\n    \"\"\"\n    global loss_log\n    bound = int(output_dict[\"bound\"])\n    loss_re, loss_im = compute_real_and_imaginary_loss(output_dict)\n    loss_re = loss_re[bound:]\n    loss_im = loss_im[bound:]\n\n    loss_eqs1 = paddle.mean(loss_re**2)\n    loss_eqs2 = paddle.mean(loss_im**2)\n    # augmented_Lagrangian\n    if lambda_im is None:\n        init_lambda(output_dict, bound)\n    loss_lag1 = paddle.mean(loss_re * lambda_re)\n    loss_lag2 = paddle.mean(loss_im * lambda_im)\n\n    losses = (\n        loss_weight[0] * loss_eqs1\n        + loss_weight[1] * loss_eqs2\n        + loss_weight[2] * loss_lag1\n        + loss_weight[3] * loss_lag2\n    )\n    loss_log.append(float(loss_eqs1 + loss_eqs2))  # for plotting\n    loss_log.append(float(loss_lag1 + loss_lag2))  # for plotting\n    return {\"pde\": losses}\n\n\ndef obj_loss_fun(output_dict: Dict[str, paddle.Tensor], *args) -&gt; paddle.Tensor:\n    \"\"\"Compute objective loss.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n\n    Returns:\n        paddle.Tensor: Objective loss.\n    \"\"\"\n    global loss_log, loss_obj\n    x, y = output_dict[\"x\"], output_dict[\"y\"]\n    bound = int(output_dict[\"bound\"])\n    e_re = output_dict[\"e_real\"]\n    e_im = output_dict[\"e_imaginary\"]\n\n    f1 = paddle.heaviside((x + 0.5) * (0.5 - x), paddle.to_tensor(0.5))\n    f2 = paddle.heaviside((y - 1) * (2 - y), paddle.to_tensor(0.5))\n    j = e_re[:bound] ** 2 + e_im[:bound] ** 2 - f1[:bound] * f2[:bound]\n    loss_opt_area = paddle.mean(j**2)\n\n    if lambda_im is None:\n        init_lambda(output_dict, bound)\n    losses = loss_weight[4] * loss_opt_area\n    loss_log.append(float(loss_opt_area))  # for plotting\n    loss_obj = float(loss_opt_area)  # for plotting\n    return {\"obj\": losses}\n\n\ndef eval_loss_fun(output_dict: Dict[str, paddle.Tensor], *args) -&gt; paddle.Tensor:\n    \"\"\"Compute objective loss for evaluation.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n\n    Returns:\n        paddle.Tensor: Objective loss.\n    \"\"\"\n    x, y = output_dict[\"x\"], output_dict[\"y\"]\n    e_re = output_dict[\"e_real\"]\n    e_im = output_dict[\"e_imaginary\"]\n\n    f1 = paddle.heaviside((x + 0.5) * (0.5 - x), paddle.to_tensor(0.5))\n    f2 = paddle.heaviside((y - 1) * (2 - y), paddle.to_tensor(0.5))\n    j = e_re**2 + e_im**2 - f1 * f2\n    losses = paddle.mean(j**2)\n\n    return {\"eval\": losses}\n\n\ndef eval_metric_fun(\n    output_dict: Dict[str, paddle.Tensor], *args\n) -&gt; Dict[str, paddle.Tensor]:\n    \"\"\"Compute metric for evaluation.\n\n    Args:\n        output_dict (Dict[str, paddle.Tensor]): Dict of outputs contains tensor.\n\n    Returns:\n        Dict[str, paddle.Tensor]: MSE metric.\n    \"\"\"\n    loss_re, loss_im = compute_real_and_imaginary_loss(output_dict)\n    eps_opt = paddle.concat([loss_re, loss_im], axis=-1)\n    metric = paddle.mean(eps_opt**2)\n\n    metric_dict = {\"eval_metric\": metric}\n    return metric_dict\n</code></pre> plotting.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis module is heavily adapted from https://github.com/lululxvi/hpinn\n\"\"\"\n\nimport os\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nimport functions as func_module\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import ticker\n\nimport ppsci\n\n\"\"\"All plotting functions.\"\"\"\n\n# define constants\nfont = {\"weight\": \"normal\", \"size\": 10}\ninput_name = (\"x\", \"y\")\nfield_name = [\n    \"Fig7_E\",\n    \"Fig7_eps\",\n    \"Fig_6C_lambda_re_1\",\n    \"Fig_6C_lambda_im_1\",\n    \"Fig_6C_lambda_re_4\",\n    \"Fig_6C_lambda_im_4\",\n    \"Fig_6C_lambda_re_9\",\n    \"Fig_6C_lambda_im_9\",\n]\n\n# define constants which will be assigned later\nFIGNAME: str = \"\"\nOUTPUT_DIR: str = \"\"\nDATASET_PATH: str = \"\"\nDATASET_PATH_VALID: str = \"\"\ninput_valid: np.ndarray = None\noutput_valid: np.ndarray = None\ninput_train: np.ndarray = None\n\n\ndef set_params(figname, output_dir, dataset_path, dataset_path_valid):\n    global FIGNAME, OUTPUT_DIR, DATASET_PATH, DATASET_PATH_VALID\n    FIGNAME = figname\n    OUTPUT_DIR = output_dir + \"figure/\"\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    DATASET_PATH = dataset_path\n    DATASET_PATH_VALID = dataset_path_valid\n\n\ndef prepare_data(solver: ppsci.solver.Solver, expr_dict: Dict[str, Callable]):\n    \"\"\"Prepare data of input of training and validation and generate\n        output of validation by predicting.\n\n    Args:\n        solver (ppsci.solver.Solver): Object of ppsci.solver.Solver().\n        expr_dict (Dict[str, Callable]): Expression dict, which guide to\n            compute equation variable with callable function.\n    \"\"\"\n    global input_valid, output_valid, input_train\n    # train data\n    train_dict = ppsci.utils.reader.load_mat_file(DATASET_PATH, (\"x\", \"y\", \"bound\"))\n\n    bound = int(train_dict[\"bound\"])\n    x_train = train_dict[\"x\"][bound:]\n    y_train = train_dict[\"y\"][bound:]\n    input_train = np.stack((x_train, y_train), axis=-1).reshape(-1, 2)\n\n    # valid data\n    N = ((func_module.l_BOX[1] - func_module.l_BOX[0]) / 0.05).astype(int)\n\n    valid_dict = ppsci.utils.reader.load_mat_file(\n        DATASET_PATH_VALID, (\"x_val\", \"y_val\", \"bound\")\n    )\n    in_dict_val = {\"x\": valid_dict[\"x_val\"], \"y\": valid_dict[\"y_val\"]}\n    func_module.init_lambda(in_dict_val, int(valid_dict[\"bound\"]))\n\n    pred_dict_val = solver.predict(\n        in_dict_val,\n        expr_dict,\n        batch_size=np.shape(valid_dict[\"x_val\"])[0],\n        no_grad=False,\n        return_numpy=True,\n    )\n\n    input_valid = np.stack((valid_dict[\"x_val\"], valid_dict[\"y_val\"]), axis=-1).reshape(\n        N[0], N[1], 2\n    )\n    output_valid = np.array(\n        [\n            pred_dict_val[\"e_real\"],\n            pred_dict_val[\"e_imaginary\"],\n            pred_dict_val[\"epsilon\"],\n        ]\n    ).T.reshape(N[0], N[1], 3)\n\n\ndef plot_field_holo(\n    coord_visual: np.ndarray,\n    field_visual: np.ndarray,\n    coord_lambda: Optional[np.ndarray] = None,\n    field_lambda: Optional[np.ndarray] = None,\n):\n    \"\"\"Plot fields of of holography example.\n\n    Args:\n        coord_visual (np.ndarray): The coord of epsilon and |E|**2.\n        field_visual (np.ndarray): The filed of epsilon and |E|**2.\n        coord_lambda (Optional[np.ndarray], optional): The coord of lambda. Defaults to None.\n        field_lambda (Optional[np.ndarray], optional): The filed of lambda. Defaults to None.\n    \"\"\"\n    fmin, fmax = np.array([0, 1.0]), np.array([0.6, 12])\n    cmin, cmax = coord_visual.min(axis=(0, 1)), coord_visual.max(axis=(0, 1))\n    emin, emax = np.array([-3, -1]), np.array([3, 0])\n    x_pos = coord_visual[:, :, 0]\n    y_pos = coord_visual[:, :, 1]\n\n    for fi in range(len(field_name)):\n        if fi == 0:\n            # Fig7_E\n            plt.figure(101, figsize=(8, 6))\n            plt.clf()\n            plt.rcParams[\"font.size\"] = 20\n            f_true = field_visual[..., fi]\n            plt.pcolormesh(\n                x_pos,\n                y_pos,\n                f_true,\n                cmap=\"rainbow\",\n                shading=\"gouraud\",\n                antialiased=True,\n                snap=True,\n            )\n            cb = plt.colorbar()\n            plt.axis((cmin[0], cmax[0], cmin[1], cmax[1]))\n            plt.clim(vmin=fmin[fi], vmax=fmax[fi])\n        elif fi == 1:\n            # Fig7_eps\n            plt.figure(201, figsize=(8, 1.5))\n            plt.clf()\n            plt.rcParams[\"font.size\"] = 20\n            f_true = field_visual[..., fi]\n            plt.pcolormesh(\n                x_pos,\n                y_pos,\n                f_true,\n                cmap=\"rainbow\",\n                shading=\"gouraud\",\n                antialiased=True,\n                snap=True,\n            )\n            cb = plt.colorbar()\n            plt.axis((emin[0], emax[0], emin[1], emax[1]))\n            plt.clim(vmin=fmin[fi], vmax=fmax[fi])\n        elif coord_lambda is not None and field_lambda is not None:\n            # Fig_6C_lambda_\n            plt.figure(fi * 100 + 101, figsize=(8, 6))\n            plt.clf()\n            plt.rcParams[\"font.size\"] = 20\n            f_true = field_lambda[..., fi - 2]\n            plt.scatter(\n                coord_lambda[..., 0],\n                coord_lambda[..., 1],\n                c=f_true,\n                cmap=\"rainbow\",\n                alpha=0.6,\n            )\n            cb = plt.colorbar()\n            plt.axis((cmin[0], cmax[0], cmin[1], cmax[1]))\n\n        # colorbar settings\n        cb.ax.tick_params(labelsize=20)\n        tick_locator = ticker.MaxNLocator(\n            nbins=5\n        )  # the number of scale values \u200b\u200bon the colorbar\n        cb.locator = tick_locator\n        cb.update_ticks()\n\n        plt.xlabel(f\"${str(input_name[0])}$\", fontdict=font)\n        plt.ylabel(f\"${str(input_name[1])}$\", fontdict=font)\n        plt.yticks(size=10)\n        plt.xticks(size=10)\n        plt.savefig(\n            os.path.join(\n                OUTPUT_DIR,\n                f\"{FIGNAME}_{str(field_name[fi])}.jpg\",\n            )\n        )\n\n\ndef plot_6a(log_loss: np.ndarray):\n    \"\"\"Plot Fig.6 A of paper.\n\n    Args:\n        log_loss (np.ndarray): Losses of all training's iterations.\n    \"\"\"\n    plt.figure(300, figsize=(8, 6))\n    smooth_step = 100  # how many steps of loss are squeezed to one point, num_points is epoch/smooth_step\n    if log_loss.shape[0] % smooth_step != 0:\n        vis_loss_ = log_loss[: -(log_loss.shape[0] % smooth_step), :].reshape(\n            -1, smooth_step, log_loss.shape[1]\n        )\n    else:\n        vis_loss_ = log_loss.reshape(-1, smooth_step, log_loss.shape[1])\n\n    vis_loss = vis_loss_.mean(axis=1).reshape(-1, 3)\n    vis_loss_total = vis_loss[:, :].sum(axis=1)\n    vis_loss[:, 1] = vis_loss[:, 2]\n    vis_loss[:, 2] = vis_loss_total\n    for i in range(vis_loss.shape[1]):\n        plt.semilogy(np.arange(vis_loss.shape[0]) * smooth_step, vis_loss[:, i])\n    plt.legend(\n        [\"PDE loss\", \"Objective loss\", \"Total loss\"],\n        loc=\"lower left\",\n        prop=font,\n    )\n    plt.xlabel(\"Iteration \", fontdict=font)\n    plt.ylabel(\"Loss \", fontdict=font)\n    plt.grid()\n    plt.yticks(size=10)\n    plt.xticks(size=10)\n    plt.savefig(os.path.join(OUTPUT_DIR, f\"{FIGNAME}_Fig6_A.jpg\"))\n\n\ndef plot_6b(log_loss_obj: List[float]):\n    \"\"\"Plot Fig.6 B of paper.\n\n    Args:\n        log_loss_obj (List[float]): Objective losses of last iteration of each k.\n    \"\"\"\n    plt.figure(400, figsize=(10, 6))\n    plt.clf()\n    plt.plot(np.arange(len(log_loss_obj)), log_loss_obj, \"bo-\")\n    plt.xlabel(\"k\", fontdict=font)\n    plt.ylabel(\"Objective\", fontdict=font)\n    plt.grid()\n    plt.yticks(size=10)\n    plt.xticks(size=10)\n    plt.savefig(os.path.join(OUTPUT_DIR, f\"{FIGNAME}_Fig6_B.jpg\"))\n\n\ndef plot_6c7c(log_lambda: List[np.ndarray]):\n    \"\"\"Plot Fig.6 Cs and Fig.7.Cs of paper.\n\n    Args:\n        log_lambda (List[np.ndarray]): Lambdas of each k.\n    \"\"\"\n    # plot Fig.6 Cs and Fig.7.Cs of paper\n    global input_valid, output_valid, input_train\n\n    field_lambda = np.concatenate(\n        [log_lambda[1], log_lambda[4], log_lambda[9]], axis=0\n    ).T\n    v_visual = output_valid[..., 0] ** 2 + output_valid[..., 1] ** 2\n    field_visual = np.stack((v_visual, output_valid[..., -1]), axis=-1)\n    plot_field_holo(input_valid, field_visual, input_train, field_lambda)\n\n\ndef plot_6d(log_lambda: List[np.ndarray]):\n    \"\"\"Plot Fig.6 D of paper.\n\n    Args:\n        log_lambda (List[np.ndarray]): Lambdas of each k.\n    \"\"\"\n    # lambda/mu\n    mu_ = 2 ** np.arange(1, 11)\n    log_lambda = np.array(log_lambda) / mu_[:, None, None]\n    # randomly pick 3 lambda points to represent all points of each k\n    ind = np.random.randint(low=0, high=np.shape(log_lambda)[-1], size=3)\n    la_mu_ind = log_lambda[:, :, ind]\n    marker = [\"ro-\", \"bo:\", \"r*-\", \"b*:\", \"rp-\", \"bp:\"]\n    plt.figure(500, figsize=(7, 5))\n    plt.clf()\n    for i in range(6):\n        plt.plot(\n            np.arange(0, 10),\n            la_mu_ind[:, int(i % 2), int(i / 2)],\n            marker[i],\n            linewidth=2,\n        )\n    plt.legend(\n        [\"Re, 1\", \"Im, 1\", \"Re, 2\", \"Im, 2\", \"Re, 3\", \"Im, 3\"],\n        loc=\"upper right\",\n        prop=font,\n    )\n    plt.grid()\n    plt.xlabel(\"k\", fontdict=font)\n    plt.ylabel(r\"$ \\lambda^k / \\mu^k_F$\", fontdict=font)\n    plt.yticks(size=12)\n    plt.xticks(size=12)\n    plt.savefig(os.path.join(OUTPUT_DIR, f\"{FIGNAME}_Fig6_D_lambda.jpg\"))\n\n\ndef plot_6ef(log_lambda: List[np.ndarray]):\n    \"\"\"Plot Fig.6 E and Fig.6.F of paper.\n\n    Args:\n        log_lambda (List[np.ndarray]): Lambdas of each k.\n    \"\"\"\n    # lambda/mu\n    mu_ = 2 ** np.arange(1, 11)\n    log_lambda = np.array(log_lambda) / mu_[:, None, None]\n    # pick k=1,4,6,9\n    iter_ind = [1, 4, 6, 9]\n    plt.figure(600, figsize=(5, 5))\n    plt.clf()\n    for i in iter_ind:\n        sns.kdeplot(log_lambda[i, 0, :], label=\"k = \" + str(i), cut=0, linewidth=2)\n    plt.legend(prop=font)\n    plt.grid()\n    plt.xlim([-0.1, 0.1])\n    plt.xlabel(r\"$ \\lambda^k_{Re} / \\mu^k_F$\", fontdict=font)\n    plt.ylabel(\"Frequency\", fontdict=font)\n    plt.yticks(size=12)\n    plt.xticks(size=12)\n    plt.savefig(os.path.join(OUTPUT_DIR, f\"{FIGNAME}_Fig6_E.jpg\"))\n\n    plt.figure(700, figsize=(5, 5))\n    plt.clf()\n    for i in iter_ind:\n        sns.kdeplot(log_lambda[i, 1, :], label=\"k = \" + str(i), cut=0, linewidth=2)\n    plt.legend(prop=font)\n    plt.grid()\n    plt.xlim([-0.1, 0.1])\n    plt.xlabel(r\"$ \\lambda^k_{Im} / \\mu^k_F$\", fontdict=font)\n    plt.ylabel(\"Frequency\", fontdict=font)\n    plt.yticks(size=12)\n    plt.xticks(size=12)\n    plt.savefig(os.path.join(OUTPUT_DIR, f\"{FIGNAME}_Fig6_F.jpg\"))\n</code></pre>"},{"location":"zh/examples/hpinns/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u53c2\u8003 \u95ee\u9898\u5b9a\u4e49\uff0c\u4e0b\u56fe\u5c55\u793a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d loss \u53d8\u5316\u3001\u53c2\u6570 lambda \u548c\u53c2\u6570 mu \u4e0e\u589e\u5f3a\u7684\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u4e2d\u8bad\u7ec3\u8bba\u6b21 k \u7684\u53d8\u5316\u3001\u7535\u573a E \u548c\u4ecb\u7535\u5e38\u6570 epsilon \u6700\u7ec8\u9884\u6d4b\u7684\u503c\u3002</p> <p>\u4e0b\u56fe\u5c55\u793a\u4e86\u5bf9\u4e8e\u4e00\u4e2a\u5b9a\u4e49\u7684\u65b9\u5f62\u57df\u5185\uff0c\u7535\u78c1\u6ce2\u4f20\u64ad\u7684\u60c5\u51b5\u7684\u9884\u6d4b\u3002\u9884\u6d4b\u7ed3\u679c\u4e0e\u6709\u9650\u5dee\u5206\u9891\u57df(FDFD)\u65b9\u6cd5\u7684\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p> <p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684 loss \u503c\u53d8\u5316\uff1a</p> <p> </p>  \u8bad\u7ec3\u8fc7\u7a0b loss \u503c\u968f iteration \u53d8\u5316 <p>objective loss \u503c\u968f\u8bad\u7ec3\u8f6e\u6b21 k \u7684\u53d8\u5316\uff1a</p> <p> </p>  k \u503c\u5bf9\u5e94 objective loss \u503c <p>k=1,4,9 \u65f6\u5bf9\u5e94\u53c2\u6570 lambda \u5b9e\u90e8\u548c\u865a\u90e8\u7684\u503c\uff1a</p> <p> </p>  k=1,4,9 \u65f6\u5bf9\u5e94 lambda \u503c <p>\u53c2\u6570 lambda \u548c\u53c2\u6570 mu \u7684\u6bd4\u503c\u968f\u8bad\u7ec3\u8f6e\u6b21 k \u7684\u53d8\u5316\uff1a</p> <p> </p>  k \u503c\u5bf9\u5e94 lambda/mu \u503c <p>\u53c2\u6570 lambda \u548c\u53c2\u6570 mu \u5b9e\u90e8\u7684\u6bd4\u503c\u968f\u8bad\u7ec3\u8f6e\u6b21 k=1,4,6,9 \u65f6\u51fa\u73b0\u7684\u9891\u7387\uff0c\u66f2\u7ebf\u8d8a\u201c\u5c16\u201d\u8bf4\u660e\u503c\u8d8a\u8d8b\u4e8e\u7edf\u4e00\uff0c\u6536\u655b\u7684\u8d8a\u597d\uff1a</p> <p> </p>  k=1,4,6,9 \u65f6\u5bf9\u5e94\u5b9e\u90e8 lambda/mu \u503c\u51fa\u73b0\u9891\u7387 <p>\u53c2\u6570 lambda \u548c\u53c2\u6570 mu \u865a\u90e8\u7684\u6bd4\u503c\u968f\u8bad\u7ec3\u8f6e\u6b21 k=1,4,6,9 \u65f6\u51fa\u73b0\u7684\u9891\u7387\uff0c\u66f2\u7ebf\u8d8a\u201c\u5c16\u201d\u8bf4\u660e\u503c\u8d8a\u8d8b\u4e8e\u7edf\u4e00\uff0c\u6536\u655b\u7684\u8d8a\u597d\uff1a</p> <p> </p>  k=1,4,6,9 \u65f6\u5bf9\u5e94\u865a\u90e8 lambda/mu \u503c\u51fa\u73b0\u9891\u7387 <p>\u7535\u573a E \u503c\uff1a</p> <p> </p>  E \u503c <p>\u4ecb\u7535\u5e38\u6570 epsilon \u503c\uff1a</p> <p> </p>  epsilon \u503c"},{"location":"zh/examples/hpinns/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li> <p>PHYSICS-INFORMED NEURAL NETWORKS WITH HARD CONSTRAINTS FOR INVERSE DESIGN</p> </li> <li> <p>\u53c2\u8003\u4ee3\u7801</p> </li> </ul>"},{"location":"zh/examples/labelfree_DNN_surrogate/","title":"Labelfree_DNN_surrogate","text":""},{"location":"zh/examples/labelfree_DNN_surrogate/#labelfree-dnn-surrogate-aneurysm-flow-pipe-flow","title":"LabelFree-DNN-Surrogate (Aneurysm flow &amp; Pipe flow)","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <p>\u6848\u4f8b\u4e00\uff1aPipe Flow</p> <pre><code>python poiseuille_flow.py\n</code></pre> <p>\u6848\u4f8b\u4e8c\uff1aAneurysm Flow</p> <pre><code>wget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/LabelFree-DNN-Surrogate/LabelFree-DNN-Surrogate_data.zip\nunzip LabelFree-DNN-Surrogate_data.zip\n\npython aneurysm_flow.py\n</code></pre> <p>\u6848\u4f8b\u4e00\uff1aPipe Flow</p> <pre><code>python poiseuille_flow.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/LabelFree-DNN-Surrogate/poiseuille_flow_pretrained.pdparams\n</code></pre> <p>\u6848\u4f8b\u4e8c\uff1aAneurysm Flow</p> <pre><code>wget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/LabelFree-DNN-Surrogate/LabelFree-DNN-Surrogate_data.zip\nunzip LabelFree-DNN-Surrogate_data.zip\n\npython aneurysm_flow.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/LabelFree-DNN-Surrogate/aneurysm_flow.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 aneurysm_flow.pdparams L-2 error u : 2.548e-4  L-2 error v : 7.169e-5"},{"location":"zh/examples/labelfree_DNN_surrogate/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u7684\u6570\u503c\u6a21\u62df\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4f7f\u7528\u591a\u9879\u5f0f\u5c06\u63a7\u5236\u65b9\u7a0b\u5728\u7a7a\u95f4\u6216/\u548c\u65f6\u95f4\u4e0a\u79bb\u6563\u5316\u4e3a\u6709\u9650\u7ef4\u4ee3\u6570\u7cfb\u7edf\u3002\u7531\u4e8e\u7269\u7406\u7684\u591a\u5c3a\u5ea6\u7279\u6027\u548c\u5bf9\u590d\u6742\u51e0\u4f55\u4f53\u8fdb\u884c\u7f51\u683c\u5212\u5206\u7684\u654f\u611f\u6027\uff0c\u8fd9\u6837\u7684\u8fc7\u7a0b\u5bf9\u4e8e\u5927\u591a\u6570\u5b9e\u65f6\u5e94\u7528\u7a0b\u5e8f\uff08\u4f8b\u5982\uff0c\u4e34\u5e8a\u8bca\u65ad\u548c\u624b\u672f\u8ba1\u5212\uff09\u548c\u591a\u67e5\u8be2\u5206\u6790\uff08\u4f8b\u5982\uff0c\u4f18\u5316\u8bbe\u8ba1\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u79cd\u7269\u7406\u7ea6\u675f\u7684 DL \u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e0d\u4f9d\u8d56\u4efb\u4f55\u6a21\u62df\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5bf9\u6d41\u4f53\u6d41\u52a8\u8fdb\u884c\u4ee3\u7406\u5efa\u6a21\u3002 \u5177\u4f53\u6765\u8bf4\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u67b6\u6784\u6765\u5f3a\u5236\u6267\u884c\u521d\u59cb\u6761\u4ef6\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u5c06\u63a7\u5236\u504f\u5fae\u5206\u65b9\u7a0b\uff08\u5373 Navier-Stokes \u65b9\u7a0b\uff09\u7eb3\u5165 DNN\u7684\u635f\u5931\u4e2d\u4ee5\u9a71\u52a8\u8bad\u7ec3\u3002 \u5bf9\u4e0e\u8840\u6db2\u52a8\u529b\u5b66\u5e94\u7528\u76f8\u5173\u7684\u8bb8\u591a\u5185\u90e8\u6d41\u52a8\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c\uff0c\u5e76\u7814\u7a76\u4e86\u6d41\u4f53\u7279\u6027\u548c\u57df\u51e0\u4f55\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u524d\u5411\u4f20\u64ad\u3002\u7ed3\u679c\u8868\u660e\uff0cDL \u4ee3\u7406\u8fd1\u4f3c\u4e0e\u7b2c\u4e00\u539f\u7406\u6570\u503c\u6a21\u62df\u4e4b\u95f4\u7684\u6d41\u573a\u548c\u524d\u5411\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\u975e\u5e38\u543b\u5408\u3002</p>"},{"location":"zh/examples/labelfree_DNN_surrogate/#2-pipeflow","title":"2. \u6848\u4f8b\u4e00\uff1aPipeFlow","text":""},{"location":"zh/examples/labelfree_DNN_surrogate/#21","title":"2.1 \u95ee\u9898\u5b9a\u4e49","text":"<p>\u7ba1\u9053\u6d41\u4f53\u662f\u4e00\u7c7b\u975e\u5e38\u5e38\u89c1\u548c\u5e38\u7528\u7684\u6d41\u4f53\u7cfb\u7edf\uff0c\u4f8b\u5982\u52a8\u8109\u4e2d\u7684\u8840\u6db2\u6216\u6c14\u7ba1\u4e2d\u7684\u6c14\u6d41\uff0c\u4e00\u822c\u7ba1\u9053\u6d41\u53d7\u5230\u7ba1\u9053\u4e24\u7aef\u7684\u538b\u529b\u5dee\u9a71\u52a8\uff0c\u6216\u8005\u91cd\u529b\u4f53\u79ef\u529b\u9a71\u52a8\u3002 \u5728\u5fc3\u8840\u7ba1\u7cfb\u7edf\u4e2d\uff0c\u524d\u8005\u66f4\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u56e0\u4e3a\u8840\u6d41\u4e3b\u8981\u53d7\u5fc3\u810f\u6cf5\u9001\u5f15\u8d77\u7684\u538b\u964d\u63a7\u5236\u3002 \u4e00\u822c\u6765\u8bf4\uff0c\u6a21\u62df\u7ba1\u4e2d\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u9700\u8981\u7528\u6570\u503c\u65b9\u6cd5\u6c42\u89e3\u5b8c\u6574\u7684 Navier-Stokes \u65b9\u7a0b\uff0c\u4f46\u5982\u679c\u7ba1\u662f\u76f4\u7684\u5e76\u4e14\u5177\u6709\u6052\u5b9a\u7684\u5706\u5f62\u6a2a\u622a\u9762\uff0c\u5219\u53ef\u4ee5\u83b7\u5f97\u5b8c\u5168\u53d1\u5c55\u7684\u7a33\u6001\u6d41\u52a8\u7684\u89e3\u6790\u89e3\uff0c\u5373 \u4e00\u4e2a\u7406\u60f3\u7684\u57fa\u51c6\u6765\u9a8c\u8bc1\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6027\u80fd\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u9996\u5148\u7814\u7a76\u4e8c\u7ef4\u5706\u7ba1\u4e2d\u7684\u6d41\u52a8\uff08\u4e5f\u79f0\u4e3a\u6cca\u8083\u53f6\u6d41\uff09\u3002</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ u\\dfrac{\\partial u}{\\partial x} + v\\dfrac{\\partial u}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial x} + \\nu(\\dfrac{\\partial ^2 u}{\\partial x ^2} + \\dfrac{\\partial ^2 u}{\\partial y ^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ u\\dfrac{\\partial v}{\\partial x} + v\\dfrac{\\partial v}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial y} + \\nu(\\dfrac{\\partial ^2 v}{\\partial x ^2} + \\dfrac{\\partial ^2 v}{\\partial y ^2}) \\] <p>\u6211\u4eec\u53ea\u5173\u6ce8\u8fd9\u79cd\u5b8c\u5168\u53d1\u5c55\u7684\u6d41\u52a8\u5e76\u4e14\u5728\u8fb9\u754c\u65bd\u52a0\u4e86\u65e0\u6ed1\u79fb\u8fb9\u754c\u6761\u4ef6\u3002\u4e0e\u4f20\u7edfPINNs\u65b9\u6cd5\u4e0d\u540c\u7684\u662f\uff0c\u6211\u4eec\u5c06\u65e0\u6ed1\u52a8\u8fb9\u754c\u6761\u4ef6\u901a\u8fc7\u901f\u5ea6\u51fd\u6570\u5047\u8bbe\u7684\u65b9\u5f0f\u5f3a\u5236\u65bd\u52a0\u5728\u8fb9\u754c\u4e0a\uff1a \u5bf9\u4e8e\u6d41\u4f53\u57df\u8fb9\u754c\u548c\u6d41\u4f53\u57df\u5185\u90e8\u5706\u5468\u8fb9\u754c\uff0c\u5219\u9700\u65bd\u52a0 Dirichlet \u8fb9\u754c\u6761\u4ef6\uff1a</p> <p> </p> \u6d41\u573a\u793a\u610f\u56fe <p>\u6d41\u4f53\u57df\u5165\u53e3\u8fb9\u754c\uff1a</p> \\[ p=0.1 \\] <p>\u6d41\u4f53\u57df\u51fa\u53e3\u8fb9\u754c\uff1a</p> \\[ p=0 \\] <p>\u6d41\u4f53\u57df\u4e0a\u4e0b\u8fb9\u754c\uff1a</p> \\[ u=0, v=0 \\]"},{"location":"zh/examples/labelfree_DNN_surrogate/#22","title":"2.2 \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/labelfree_DNN_surrogate/#221","title":"2.2.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9\u548c\u8be5\u70b9\u7684\u52a8\u529b\u7c98\u6027\u7cfb\u6570\u4e09\u5143\u7ec4 \\((x, y, \\nu)\\) \u90fd\u6709\u81ea\u8eab\u7684\u6a2a\u5411\u901f\u5ea6 \\(u\\)\u3001\u7eb5\u5411\u901f\u5ea6 \\(v\\)\u3001\u538b\u529b \\(p\\) \u4e09\u4e2a\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684\u4e09\u4e2a MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((x, y, \\nu)\\) \u5230 \\((u, v, p)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f_1, f_2, f_3: \\mathbb{R}^3 \\to \\mathbb{R}^3\\) \uff0c\u5373\uff1a</p> \\[ u= transform_{output}(f_1(transform_{input}(x, y, \\nu))) \\] \\[ v= transform_{output}(f_2(transform_{input}(x, y, \\nu))) \\] \\[ p= transform_{output}(f_3(transform_{input}(x, y, \\nu))) \\] <p>\u4e0a\u5f0f\u4e2d \\(f_1, f_2, f_3\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\\(transform_{input}, transform_{output}\\), \u8868\u793a\u65bd\u52a0\u989d\u5916\u7684\u7ed3\u6784\u5316\u81ea\u5b9a\u4e49\u5c42\uff0c\u7528\u4e8e\u65bd\u52a0\u7ea6\u675f\u548c\u4e30\u5bcc\u8f93\u5165\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b:</p> <pre><code># set model\nmodel_u = ppsci.arch.MLP(**cfg.MODEL.u_net)\nmodel_v = ppsci.arch.MLP(**cfg.MODEL.v_net)\nmodel_p = ppsci.arch.MLP(**cfg.MODEL.p_net)\n\ndef input_trans(input):\n    x, y = input[\"x\"], input[\"y\"]\n    nu = input[\"nu\"]\n    b = 2 * np.pi / (X_OUT - cfg.X_IN)\n    c = np.pi * (cfg.X_IN + X_OUT) / (cfg.X_IN - X_OUT)\n    sin_x = cfg.X_IN * paddle.sin(b * x + c)\n    cos_x = cfg.X_IN * paddle.cos(b * x + c)\n    return {\"sin(x)\": sin_x, \"cos(x)\": cos_x, \"x\": x, \"y\": y, \"nu\": nu}\n\ndef output_trans_u(input, out):\n    return {\"u\": out[\"u\"] * (cfg.R**2 - input[\"y\"] ** 2)}\n\ndef output_trans_v(input, out):\n    return {\"v\": (cfg.R**2 - input[\"y\"] ** 2) * out[\"v\"]}\n\ndef output_trans_p(input, out):\n    return {\n        \"p\": (\n            (cfg.P_IN - cfg.P_OUT) * (X_OUT - input[\"x\"]) / cfg.L\n            + (cfg.X_IN - input[\"x\"]) * (X_OUT - input[\"x\"]) * out[\"p\"]\n        )\n    }\n\nmodel_u.register_input_transform(input_trans)\nmodel_v.register_input_transform(input_trans)\nmodel_p.register_input_transform(input_trans)\nmodel_u.register_output_transform(output_trans_u)\nmodel_v.register_output_transform(output_trans_v)\nmodel_p.register_output_transform(output_trans_p)\nmodel = ppsci.arch.ModelList((model_u, model_v, model_p))\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>[\"x\"\u3001 \"y\"\u3001 \"nu\"]</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>[\"u\"\u3001 \"v\"\u3001 \"p\"]</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\u4ee5\u53ca\u6fc0\u6d3b\u51fd\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e09\u4e2a\u62e5\u6709 3 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\u548c 1 \u5c42\u8f93\u51fa\u5c42\u795e\u7ecf\u5143\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 50\uff0c\u4f7f\u7528 \"swish\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model_u</code> <code>model_v</code> <code>model_p</code>\u3002</p>"},{"location":"zh/examples/labelfree_DNN_surrogate/#222","title":"2.2.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e\u672c\u6848\u4f8b\u4f7f\u7528\u7684\u662f Navier-Stokes \u65b9\u7a0b\u76842\u7ef4\u7a33\u6001\u5f62\u5f0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>NavierStokes</code>\u3002</p> <pre><code># set euqation\nequation = {\n    \"NavierStokes\": ppsci.equation.NavierStokes(\n        nu=\"nu\", rho=cfg.RHO, dim=2, time=False\n    )\n}\n</code></pre> <p>\u5728\u5b9e\u4f8b\u5316 <code>NavierStokes</code> \u7c7b\u65f6\u9700\u6307\u5b9a\u5fc5\u8981\u7684\u53c2\u6570\uff1a\u52a8\u529b\u7c98\u5ea6 \\(\\nu\\) \u4e3a\u7f51\u7edc\u8f93\u51fa, \u6d41\u4f53\u5bc6\u5ea6 \\(\\rho=1.0\\)\u3002</p>"},{"location":"zh/examples/labelfree_DNN_surrogate/#223","title":"2.2.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d\u672c\u6848\u4f8b\u7684\u8ba1\u7b97\u57df\u548c\u53c2\u6570\u81ea\u53d8\u91cf \\(\\nu\\) \u7531<code>numpy</code>\u968f\u673a\u6570\u751f\u6210\u7684\u70b9\u4e91\u6784\u6210\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u70b9\u4e91\u51e0\u4f55 <code>PointCloud</code> \u7ec4\u5408\u6210\u7a7a\u95f4\u7684 <code>Geometry</code> \u8ba1\u7b97\u57df\u3002</p> <pre><code>## prepare data with (?, 2)\ndata_1d_x = np.linspace(\n    cfg.X_IN, X_OUT, cfg.N_x, endpoint=True, dtype=paddle.get_default_dtype()\n)\ndata_1d_y = np.linspace(\n    Y_START, Y_END, cfg.N_y, endpoint=True, dtype=paddle.get_default_dtype()\n)\ndata_1d_nu = np.linspace(\n    NU_START, NU_END, cfg.N_p, endpoint=True, dtype=paddle.get_default_dtype()\n)\n\ndata_2d_xy = (\n    np.array(np.meshgrid(data_1d_x, data_1d_y, data_1d_nu)).reshape(3, -1).T\n)\ndata_2d_xy_shuffle = copy.deepcopy(data_2d_xy)\nnp.random.shuffle(data_2d_xy_shuffle)\n\ninput_x = data_2d_xy_shuffle[:, 0].reshape(data_2d_xy_shuffle.shape[0], 1)\ninput_y = data_2d_xy_shuffle[:, 1].reshape(data_2d_xy_shuffle.shape[0], 1)\ninput_nu = data_2d_xy_shuffle[:, 2].reshape(data_2d_xy_shuffle.shape[0], 1)\n\ninterior_geom = ppsci.geometry.PointCloud(\n    interior={\"x\": input_x, \"y\": input_y, \"nu\": input_nu},\n    coord_keys=(\"x\", \"y\", \"nu\"),\n)\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#224","title":"2.2.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u6839\u636e 2.1 \u95ee\u9898\u5b9a\u4e49 \u5f97\u5230\u7684\u516c\u5f0f\u548c\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u5bf9\u5e94\u4e86\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u7684\u51e0\u4e2a\u7ea6\u675f\u6761\u4ef6\uff0c\u5373\uff1a</p> <ul> <li> <p>\u65bd\u52a0\u5728\u6d41\u4f53\u57df\u5185\u90e8\u70b9\u4e0a\u7684Navier-Stokes \u65b9\u7a0b\u7ea6\u675f</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ u\\dfrac{\\partial u}{\\partial x} + v\\dfrac{\\partial u}{\\partial y} +\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial x} - \\nu(\\dfrac{\\partial ^2 u}{\\partial x ^2} + \\dfrac{\\partial ^2 u}{\\partial y ^2}) = 0 \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ u\\dfrac{\\partial v}{\\partial x} + v\\dfrac{\\partial v}{\\partial y} +\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial y} - \\nu(\\dfrac{\\partial ^2 v}{\\partial x ^2} + \\dfrac{\\partial ^2 v}{\\partial y ^2}) = 0 \\] <p>\u4e3a\u4e86\u65b9\u4fbf\u83b7\u53d6\u4e2d\u95f4\u53d8\u91cf\uff0c<code>NavierStokes</code> \u7c7b\u5185\u90e8\u5c06\u4e0a\u5f0f\u5de6\u4fa7\u7684\u7ed3\u679c\u5206\u522b\u547d\u540d\u4e3a <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code>\u3002</p> </li> <li> <p>\u65bd\u52a0\u5728\u6d41\u4f53\u57df\u5165\u51fa\u53e3\u3001\u6d41\u4f53\u57df\u4e0a\u4e0b\u8840\u7ba1\u58c1\u8fb9\u754c\u7684\u7684 Dirichlet \u8fb9\u754c\u6761\u4ef6\u7ea6\u675f\u3002\u4f5c\u4e3a\u672c\u6587\u521b\u65b0\u70b9\u4e4b\u4e00\uff0c\u6b64\u6848\u4f8b\u521b\u65b0\u6027\u7684\u4f7f\u7528\u4e86\u7ed3\u6784\u5316\u8fb9\u754c\u6761\u4ef6\uff0c\u5373\u901a\u8fc7\u7f51\u7edc\u7684\u8f93\u51fa\u5c42\u540e\u9762\uff0c\u589e\u52a0\u4e00\u5c42\u516c\u5f0f\u5c42\uff0c\u6765\u65bd\u52a0\u8fb9\u754c\u6761\u4ef6\uff08\u516c\u5f0f\u5728\u8fb9\u754c\u5904\u503c\u4e3a\u96f6\uff09\u3002\u907f\u514d\u4e86\u6570\u636e\u70b9\u4f5c\u4e3a\u8fb9\u754c\u6761\u4ef6\u65e0\u6cd5\u6709\u6548\u7ea6\u675f\u7684\u4e0d\u8db3\u3002\u7edf\u4e00\u4f7f\u7528\u7528\u7c7b\u51fd\u6570<code>Transform()</code>\u8fdb\u884c\u521d\u59cb\u5316\u548c\u7ba1\u7406\u3002\u5177\u4f53\u7684\u63a8\u7406\u8fc7\u7a0b\u4e3a\uff1a</p> <p>\u6d41\u4f53\u57df\u4e0a\u4e0b\u8fb9\u754c(\u8840\u7ba1\u58c1)\u4fee\u6b63\u51fd\u6570\u7684\u516c\u5f0f\u5f62\u5f0f\u4e3a:</p> \\[ \\hat{u}(t,x,\\theta;W,b) = u_{par}(t,x,\\theta) + D(t,x,\\theta)\\tilde{u}(t,x,\\theta;W,b) \\] \\[ \\hat{p}(t,x,\\theta;W,b) = p_{par}(t,x,\\theta) + D(t,x,\\theta)\\tilde{p}(t,x,\\theta;W,b) \\] <p>\u5176\u4e2d\\(u_{par}\\)\u548c\\(p_{par}\\)\u662f\u6ee1\u8db3\u8fb9\u754c\u6761\u4ef6\u548c\u521d\u59cb\u6761\u4ef6\u7684\u7279\u89e3\uff0c\u5177\u4f53\u7684\u4fee\u6b63\u51fd\u6570\u5e26\u5165\u540e\u5f97\u5230\uff1a</p> \\[ \\hat{u} = (\\dfrac{d^2}{4} - y^2) \\tilde{u} \\] \\[ \\hat{v} = (\\dfrac{d^2}{4} - y^2) \\tilde{v} \\] \\[ \\hat{p} = \\dfrac{x - x_{in}}{x_{out} - x_{in}}p_{out} + \\dfrac{x_{out} - x}{x_{out} - x_{in}}p_{in} + (x - x_{in})(x_{out} - x) \\tilde{p} \\] </li> </ul> <p>\u63a5\u4e0b\u6765\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>InteriorConstraint</code> \u548c\u6a21\u578b<code>Transform</code>\u81ea\u5b9a\u4e49\u5c42\uff0c\u6784\u5efa\u4e0a\u8ff0\u4e24\u79cd\u7ea6\u675f\u6761\u4ef6\u3002</p> <ul> <li> <p>\u5185\u90e8\u70b9\u7ea6\u675f</p> <p>\u4ee5\u4f5c\u7528\u5728\u6d41\u4f53\u57df\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>pde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"NavierStokes\"].equations,\n    {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n    geom=interior_geom,\n    dataloader_cfg={\n        \"dataset\": \"NamedArrayDataset\",\n        \"num_workers\": 1,\n        \"batch_size\": cfg.TRAIN.batch_size.pde_constraint,\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"shuffle\": False,\n            \"drop_last\": False,\n        },\n    },\n    loss=ppsci.loss.MSELoss(\"mean\"),\n    evenly=True,\n    name=\"EQ\",\n)\n# wrap constraints together\nconstraint = {pde_constraint.name: pde_constraint}\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 2.2.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"NavierStokes\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u6211\u4eec\u5e0c\u671b Navier-Stokes \u65b9\u7a0b\u4ea7\u751f\u7684\u4e09\u4e2a\u4e2d\u95f4\u7ed3\u679c <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code> \u88ab\u4f18\u5316\u81f3 0\uff0c\u56e0\u6b64\u5c06\u5b83\u4eec\u7684\u76ee\u6807\u503c\u5168\u90e8\u8bbe\u4e3a 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 2.2.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>interior_geom</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528\u5206\u6279\u6b21\u6570\u636e\u70b9\u8bad\u7ec3\uff0c\u56e0\u6b64 <code>dataset</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e3a <code>NamedArrayDataset</code> \u4e14 <code>iters_per_epoch</code> \u4e5f\u8bbe\u7f6e\u4e3a 1\uff0c\u91c7\u6837\u70b9\u6570 <code>batch_size</code> \u8bbe\u4e3a 128\uff1b</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\u53d6\u5e73\u5747\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"EQ\" \u5373\u53ef\u3002</p> </li> </ul>"},{"location":"zh/examples/labelfree_DNN_surrogate/#225","title":"2.2.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u4f7f\u75283000\u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u5b66\u4e60\u7387\u8bbe\u4e3a 0.005\u3002</p>"},{"location":"zh/examples/labelfree_DNN_surrogate/#226","title":"2.2.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#227","title":"2.2.7 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    epochs=cfg.TRAIN.epochs,\n    iters_per_epoch=ITERS_PER_EPOCH,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    save_freq=cfg.TRAIN.save_freq,\n    equation=equation,\n)\nsolver.train()\n</code></pre> <p>\u53e6\u4e00\u65b9\u9762\uff0c\u6b64\u6848\u4f8b\u7684\u53ef\u89c6\u5316\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8e\uff1a</p> <ol> <li> <p>\u5728 \\(x=0\\) \u622a\u9762\u901f\u5ea6 \\(u(y)\\) \u968f \\(y\\) \u5728\u56db\u79cd\u4e0d\u540c\u7684\u52a8\u529b\u7c98\u6027\u7cfb\u6570 \\({\\nu}\\) \u91c7\u6837\u4e0b\u7684\u66f2\u7ebf\u548c\u89e3\u6790\u89e3\u7684\u5bf9\u6bd4</p> </li> <li> <p>\u5f53\u6211\u4eec\u9009\u53d6\u622a\u65ad\u9ad8\u65af\u5206\u5e03\u7684\u52a8\u529b\u7c98\u6027\u7cfb\u6570 \\({\\nu}\\) \u91c7\u6837(\u5747\u503c\u4e3a \\(\\hat{\\nu} = 10^{\u22123}\\)\uff0c \u65b9\u5dee \\(\\sigma_{\\nu}\u200b=2.67 \\times 10^{\u22124}\\))\uff0c\u4e2d\u5fc3\u5904\u901f\u5ea6\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u548c\u89e3\u6790\u89e3\u5bf9\u6bd4</p> </li> </ol> <pre><code>def evaluate(cfg: DictConfig):\n    NU_MEAN = 0.001\n    NU_STD = 0.9\n    L = 1.0  # length of pipe\n    R = 0.05  # radius of pipe\n    RHO = 1  # density\n    P_OUT = 0  # pressure at the outlet of pipe\n    P_IN = 0.1  # pressure at the inlet of pipe\n    N_x = 10\n    N_y = 50\n    N_p = 50\n    X_IN = 0\n    X_OUT = X_IN + L\n    Y_START = -R\n    Y_END = Y_START + 2 * R\n    NU_START = NU_MEAN - NU_MEAN * NU_STD  # 0.0001\n    NU_END = NU_MEAN + NU_MEAN * NU_STD  # 0.1\n\n    ## prepare data with (?, 2)\n    data_1d_x = np.linspace(\n        X_IN, X_OUT, N_x, endpoint=True, dtype=paddle.get_default_dtype()\n    )\n    data_1d_y = np.linspace(\n        Y_START, Y_END, N_y, endpoint=True, dtype=paddle.get_default_dtype()\n    )\n    data_1d_nu = np.linspace(\n        NU_START, NU_END, N_p, endpoint=True, dtype=paddle.get_default_dtype()\n    )\n    data_2d_xy = (\n        np.array(np.meshgrid(data_1d_x, data_1d_y, data_1d_nu)).reshape(3, -1).T\n    )\n\n    # set model\n    model_u = ppsci.arch.MLP((\"sin(x)\", \"cos(x)\", \"y\", \"nu\"), (\"u\",), 3, 50, \"swish\")\n    model_v = ppsci.arch.MLP((\"sin(x)\", \"cos(x)\", \"y\", \"nu\"), (\"v\",), 3, 50, \"swish\")\n    model_p = ppsci.arch.MLP((\"sin(x)\", \"cos(x)\", \"y\", \"nu\"), (\"p\",), 3, 50, \"swish\")\n\n    class Transform:\n        def input_trans(self, input):\n            self.input = input\n            x, y = input[\"x\"], input[\"y\"]\n            nu = input[\"nu\"]\n            b = 2 * np.pi / (X_OUT - X_IN)\n            c = np.pi * (X_IN + X_OUT) / (X_IN - X_OUT)\n            sin_x = X_IN * paddle.sin(b * x + c)\n            cos_x = X_IN * paddle.cos(b * x + c)\n            return {\"sin(x)\": sin_x, \"cos(x)\": cos_x, \"y\": y, \"nu\": nu}\n\n        def output_trans_u(self, input, out):\n            return {\"u\": out[\"u\"] * (R**2 - self.input[\"y\"] ** 2)}\n\n        def output_trans_v(self, input, out):\n            return {\"v\": (R**2 - self.input[\"y\"] ** 2) * out[\"v\"]}\n\n        def output_trans_p(self, input, out):\n            return {\n                \"p\": (\n                    (P_IN - P_OUT) * (X_OUT - self.input[\"x\"]) / L\n                    + (X_IN - self.input[\"x\"]) * (X_OUT - self.input[\"x\"]) * out[\"p\"]\n                )\n            }\n\n    transform = Transform()\n    model_u.register_input_transform(transform.input_trans)\n    model_v.register_input_transform(transform.input_trans)\n    model_p.register_input_transform(transform.input_trans)\n    model_u.register_output_transform(transform.output_trans_u)\n    model_v.register_output_transform(transform.output_trans_v)\n    model_p.register_output_transform(transform.output_trans_p)\n    model = ppsci.arch.ModelList((model_u, model_v, model_p))\n\n    # Validator vel\n    input_dict = {\n        \"x\": data_2d_xy[:, 0:1],\n        \"y\": data_2d_xy[:, 1:2],\n        \"nu\": data_2d_xy[:, 2:3],\n    }\n    u_analytical = np.zeros([N_y, N_x, N_p])\n    dP = P_IN - P_OUT\n\n    for i in range(N_p):\n        uy = (R**2 - data_1d_y**2) * dP / (2 * L * data_1d_nu[i] * RHO)\n        u_analytical[:, :, i] = np.tile(uy.reshape([N_y, 1]), N_x)\n\n    label_dict = {\"u\": np.ones_like(input_dict[\"x\"])}\n    weight_dict = {\"u\": np.ones_like(input_dict[\"x\"])}\n\n    # Validator KL\n    num_test = 500\n    data_1d_nu_distribution = np.random.normal(NU_MEAN, 0.2 * NU_MEAN, num_test)\n    data_2d_xy_test = (\n        np.array(\n            np.meshgrid((X_IN - X_OUT) / 2.0, 0, data_1d_nu_distribution), np.float32\n        )\n        .reshape(3, -1)\n        .T\n    )\n    input_dict_KL = {\n        \"x\": data_2d_xy_test[:, 0:1],\n        \"y\": data_2d_xy_test[:, 1:2],\n        \"nu\": data_2d_xy_test[:, 2:3],\n    }\n    u_max_a = (R**2) * dP / (2 * L * data_1d_nu_distribution * RHO)\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#23","title":"2.3 \u5b8c\u6574\u4ee3\u7801","text":"poiseuille_flow.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nReference: https://github.com/Jianxun-Wang/LabelFree-DNN-Surrogate\n\"\"\"\n\nimport copy\nimport os\nfrom os import path as osp\n\nimport hydra\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import checker\n\nif not checker.dynamic_import_to_globals(\"seaborn\"):\n    raise ModuleNotFoundError(\"Please install seaborn with `pip install seaborn&gt;=0.13.0`.\")  # fmt: skip\n\nimport seaborn as sns\n\n\ndef train(cfg: DictConfig):\n    X_OUT = cfg.X_IN + cfg.L\n    Y_START = -cfg.R\n    Y_END = Y_START + 2 * cfg.R\n    NU_START = cfg.NU_MEAN - cfg.NU_MEAN * cfg.NU_STD  # 0.0001\n    NU_END = cfg.NU_MEAN + cfg.NU_MEAN * cfg.NU_STD  # 0.1\n\n    ## prepare data with (?, 2)\n    data_1d_x = np.linspace(\n        cfg.X_IN, X_OUT, cfg.N_x, endpoint=True, dtype=paddle.get_default_dtype()\n    )\n    data_1d_y = np.linspace(\n        Y_START, Y_END, cfg.N_y, endpoint=True, dtype=paddle.get_default_dtype()\n    )\n    data_1d_nu = np.linspace(\n        NU_START, NU_END, cfg.N_p, endpoint=True, dtype=paddle.get_default_dtype()\n    )\n\n    data_2d_xy = (\n        np.array(np.meshgrid(data_1d_x, data_1d_y, data_1d_nu)).reshape(3, -1).T\n    )\n    data_2d_xy_shuffle = copy.deepcopy(data_2d_xy)\n    np.random.shuffle(data_2d_xy_shuffle)\n\n    input_x = data_2d_xy_shuffle[:, 0].reshape(data_2d_xy_shuffle.shape[0], 1)\n    input_y = data_2d_xy_shuffle[:, 1].reshape(data_2d_xy_shuffle.shape[0], 1)\n    input_nu = data_2d_xy_shuffle[:, 2].reshape(data_2d_xy_shuffle.shape[0], 1)\n\n    interior_geom = ppsci.geometry.PointCloud(\n        interior={\"x\": input_x, \"y\": input_y, \"nu\": input_nu},\n        coord_keys=(\"x\", \"y\", \"nu\"),\n    )\n\n    # set model\n    model_u = ppsci.arch.MLP(**cfg.MODEL.u_net)\n    model_v = ppsci.arch.MLP(**cfg.MODEL.v_net)\n    model_p = ppsci.arch.MLP(**cfg.MODEL.p_net)\n\n    def input_trans(input):\n        x, y = input[\"x\"], input[\"y\"]\n        nu = input[\"nu\"]\n        b = 2 * np.pi / (X_OUT - cfg.X_IN)\n        c = np.pi * (cfg.X_IN + X_OUT) / (cfg.X_IN - X_OUT)\n        sin_x = cfg.X_IN * paddle.sin(b * x + c)\n        cos_x = cfg.X_IN * paddle.cos(b * x + c)\n        return {\"sin(x)\": sin_x, \"cos(x)\": cos_x, \"x\": x, \"y\": y, \"nu\": nu}\n\n    def output_trans_u(input, out):\n        return {\"u\": out[\"u\"] * (cfg.R**2 - input[\"y\"] ** 2)}\n\n    def output_trans_v(input, out):\n        return {\"v\": (cfg.R**2 - input[\"y\"] ** 2) * out[\"v\"]}\n\n    def output_trans_p(input, out):\n        return {\n            \"p\": (\n                (cfg.P_IN - cfg.P_OUT) * (X_OUT - input[\"x\"]) / cfg.L\n                + (cfg.X_IN - input[\"x\"]) * (X_OUT - input[\"x\"]) * out[\"p\"]\n            )\n        }\n\n    model_u.register_input_transform(input_trans)\n    model_v.register_input_transform(input_trans)\n    model_p.register_input_transform(input_trans)\n    model_u.register_output_transform(output_trans_u)\n    model_v.register_output_transform(output_trans_v)\n    model_p.register_output_transform(output_trans_p)\n    model = ppsci.arch.ModelList((model_u, model_v, model_p))\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    # set euqation\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(\n            nu=\"nu\", rho=cfg.RHO, dim=2, time=False\n        )\n    }\n\n    # set constraint\n    ITERS_PER_EPOCH = int(\n        (cfg.N_x * cfg.N_y * cfg.N_p) / cfg.TRAIN.batch_size.pde_constraint\n    )\n\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n        geom=interior_geom,\n        dataloader_cfg={\n            \"dataset\": \"NamedArrayDataset\",\n            \"num_workers\": 1,\n            \"batch_size\": cfg.TRAIN.batch_size.pde_constraint,\n            \"iters_per_epoch\": ITERS_PER_EPOCH,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"shuffle\": False,\n                \"drop_last\": False,\n            },\n        },\n        loss=ppsci.loss.MSELoss(\"mean\"),\n        evenly=True,\n        name=\"EQ\",\n    )\n    # wrap constraints together\n    constraint = {pde_constraint.name: pde_constraint}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        epochs=cfg.TRAIN.epochs,\n        iters_per_epoch=ITERS_PER_EPOCH,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        save_freq=cfg.TRAIN.save_freq,\n        equation=equation,\n    )\n    solver.train()\n\n\ndef evaluate(cfg: DictConfig):\n    NU_MEAN = 0.001\n    NU_STD = 0.9\n    L = 1.0  # length of pipe\n    R = 0.05  # radius of pipe\n    RHO = 1  # density\n    P_OUT = 0  # pressure at the outlet of pipe\n    P_IN = 0.1  # pressure at the inlet of pipe\n    N_x = 10\n    N_y = 50\n    N_p = 50\n    X_IN = 0\n    X_OUT = X_IN + L\n    Y_START = -R\n    Y_END = Y_START + 2 * R\n    NU_START = NU_MEAN - NU_MEAN * NU_STD  # 0.0001\n    NU_END = NU_MEAN + NU_MEAN * NU_STD  # 0.1\n\n    ## prepare data with (?, 2)\n    data_1d_x = np.linspace(\n        X_IN, X_OUT, N_x, endpoint=True, dtype=paddle.get_default_dtype()\n    )\n    data_1d_y = np.linspace(\n        Y_START, Y_END, N_y, endpoint=True, dtype=paddle.get_default_dtype()\n    )\n    data_1d_nu = np.linspace(\n        NU_START, NU_END, N_p, endpoint=True, dtype=paddle.get_default_dtype()\n    )\n    data_2d_xy = (\n        np.array(np.meshgrid(data_1d_x, data_1d_y, data_1d_nu)).reshape(3, -1).T\n    )\n\n    # set model\n    model_u = ppsci.arch.MLP((\"sin(x)\", \"cos(x)\", \"y\", \"nu\"), (\"u\",), 3, 50, \"swish\")\n    model_v = ppsci.arch.MLP((\"sin(x)\", \"cos(x)\", \"y\", \"nu\"), (\"v\",), 3, 50, \"swish\")\n    model_p = ppsci.arch.MLP((\"sin(x)\", \"cos(x)\", \"y\", \"nu\"), (\"p\",), 3, 50, \"swish\")\n\n    class Transform:\n        def input_trans(self, input):\n            self.input = input\n            x, y = input[\"x\"], input[\"y\"]\n            nu = input[\"nu\"]\n            b = 2 * np.pi / (X_OUT - X_IN)\n            c = np.pi * (X_IN + X_OUT) / (X_IN - X_OUT)\n            sin_x = X_IN * paddle.sin(b * x + c)\n            cos_x = X_IN * paddle.cos(b * x + c)\n            return {\"sin(x)\": sin_x, \"cos(x)\": cos_x, \"y\": y, \"nu\": nu}\n\n        def output_trans_u(self, input, out):\n            return {\"u\": out[\"u\"] * (R**2 - self.input[\"y\"] ** 2)}\n\n        def output_trans_v(self, input, out):\n            return {\"v\": (R**2 - self.input[\"y\"] ** 2) * out[\"v\"]}\n\n        def output_trans_p(self, input, out):\n            return {\n                \"p\": (\n                    (P_IN - P_OUT) * (X_OUT - self.input[\"x\"]) / L\n                    + (X_IN - self.input[\"x\"]) * (X_OUT - self.input[\"x\"]) * out[\"p\"]\n                )\n            }\n\n    transform = Transform()\n    model_u.register_input_transform(transform.input_trans)\n    model_v.register_input_transform(transform.input_trans)\n    model_p.register_input_transform(transform.input_trans)\n    model_u.register_output_transform(transform.output_trans_u)\n    model_v.register_output_transform(transform.output_trans_v)\n    model_p.register_output_transform(transform.output_trans_p)\n    model = ppsci.arch.ModelList((model_u, model_v, model_p))\n\n    # Validator vel\n    input_dict = {\n        \"x\": data_2d_xy[:, 0:1],\n        \"y\": data_2d_xy[:, 1:2],\n        \"nu\": data_2d_xy[:, 2:3],\n    }\n    u_analytical = np.zeros([N_y, N_x, N_p])\n    dP = P_IN - P_OUT\n\n    for i in range(N_p):\n        uy = (R**2 - data_1d_y**2) * dP / (2 * L * data_1d_nu[i] * RHO)\n        u_analytical[:, :, i] = np.tile(uy.reshape([N_y, 1]), N_x)\n\n    label_dict = {\"u\": np.ones_like(input_dict[\"x\"])}\n    weight_dict = {\"u\": np.ones_like(input_dict[\"x\"])}\n\n    # Validator KL\n    num_test = 500\n    data_1d_nu_distribution = np.random.normal(NU_MEAN, 0.2 * NU_MEAN, num_test)\n    data_2d_xy_test = (\n        np.array(\n            np.meshgrid((X_IN - X_OUT) / 2.0, 0, data_1d_nu_distribution), np.float32\n        )\n        .reshape(3, -1)\n        .T\n    )\n    input_dict_KL = {\n        \"x\": data_2d_xy_test[:, 0:1],\n        \"y\": data_2d_xy_test[:, 1:2],\n        \"nu\": data_2d_xy_test[:, 2:3],\n    }\n    u_max_a = (R**2) * dP / (2 * L * data_1d_nu_distribution * RHO)\n    label_dict_KL = {\"u\": np.ones_like(input_dict_KL[\"x\"])}\n    weight_dict_KL = {\"u\": np.ones_like(input_dict_KL[\"x\"])}\n\n    class Cross_section_velocity_profile_metric(ppsci.metric.base.Metric):\n        def __init__(self, keep_batch: bool = False):\n            super().__init__(keep_batch)\n\n        @paddle.no_grad()\n        def forward(self, output_dict, label_dict):\n            u_pred = output_dict[\"u\"].numpy().reshape(N_y, N_x, N_p)\n            metric_dict = {}\n            for nu in range(N_p):\n                err = (\n                    u_analytical[:, int(round(N_x / 2)), nu]\n                    - u_pred[:, int(round(N_x / 2)), nu]\n                )\n                metric_dict[f\"nu = {data_1d_nu[nu]:.2g}\"] = np.abs(err).sum()\n            return metric_dict\n\n    # Kullback-Leibler Divergence\n    class KL_divergence(ppsci.metric.base.Metric):\n        def __init__(self, keep_batch: bool = False):\n            super().__init__(keep_batch)\n\n        @paddle.no_grad()\n        def forward(self, output_dict, label_dict):\n            u_max_pred = output_dict[\"u\"].numpy().flatten()\n            import scipy\n\n            print(f\"KL = {scipy.stats.entropy(u_max_a, u_max_pred)}\")\n            return {\"KL divergence\": scipy.stats.entropy(u_max_a, u_max_pred)}\n\n    dataset_vel = {\n        \"name\": \"NamedArrayDataset\",\n        \"input\": input_dict,\n        \"label\": label_dict,\n        \"weight\": weight_dict,\n    }\n    dataset_kl = {\n        \"name\": \"NamedArrayDataset\",\n        \"input\": input_dict_KL,\n        \"label\": label_dict_KL,\n        \"weight\": weight_dict_KL,\n    }\n    eval_cfg = {\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"shuffle\": False,\n            \"drop_last\": False,\n        },\n        \"batch_size\": 2000,\n    }\n    eval_cfg[\"dataset\"] = dataset_vel\n    velocity_validator = ppsci.validate.SupervisedValidator(\n        eval_cfg,\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"u\": lambda out: out[\"u\"]},\n        {\"Cross_section_velocity_profile_MAE\": Cross_section_velocity_profile_metric()},\n        name=\"Cross_section_velocity_profile_MAE\",\n    )\n    eval_cfg[\"dataset\"] = dataset_kl\n    kl_validator = ppsci.validate.SupervisedValidator(\n        eval_cfg,\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"u\": lambda out: out[\"u\"]},\n        {\"Cross_section_velocity_profile_MAE\": KL_divergence()},\n        name=\"KL_divergence\",\n    )\n    validator = {\n        velocity_validator.name: velocity_validator,\n        kl_validator.name: kl_validator,\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    solver.eval()\n\n    output_dict = solver.predict(input_dict, return_numpy=True)\n    u_pred = output_dict[\"u\"].reshape(N_y, N_x, N_p)\n    fontsize = 16\n    idx_X = int(round(N_x / 2))  # pipe velocity section at L/2\n    nu_index = [3, 6, 9, 12, 14, 20, 49]  # pick 7 nu samples\n    ytext = [0.55, 0.5, 0.4, 0.28, 0.1, 0.05, 0.001]  # text y position\n\n    # Plot\n    PLOT_DIR = osp.join(cfg.output_dir, \"visu\")\n    os.makedirs(PLOT_DIR, exist_ok=True)\n    plt.figure(1)\n    plt.clf()\n    for idxP in range(len(nu_index)):\n        ax1 = plt.subplot(111)\n        plt.plot(\n            data_1d_y,\n            u_analytical[:, idx_X, nu_index[idxP]],\n            color=\"darkblue\",\n            linestyle=\"-\",\n            lw=3.0,\n            alpha=1.0,\n        )\n        plt.plot(\n            data_1d_y,\n            u_pred[:, idx_X, nu_index[idxP]],\n            color=\"red\",\n            linestyle=\"--\",\n            dashes=(5, 5),\n            lw=2.0,\n            alpha=1.0,\n        )\n        plt.text(\n            -0.012,\n            ytext[idxP],\n            rf\"$\\nu = $ {data_1d_nu[nu_index[idxP]]:.2g}\",\n            {\"color\": \"k\", \"fontsize\": fontsize - 4},\n        )\n\n    plt.ylabel(r\"$u(y)$\", fontsize=fontsize)\n    plt.xlabel(r\"$y$\", fontsize=fontsize)\n    ax1.tick_params(axis=\"x\", labelsize=fontsize)\n    ax1.tick_params(axis=\"y\", labelsize=fontsize)\n    ax1.set_xlim([-0.05, 0.05])\n    ax1.set_ylim([0.0, 0.62])\n    plt.savefig(osp.join(PLOT_DIR, \"pipe_uProfiles.png\"), bbox_inches=\"tight\")\n\n    # Distribution of center velocity\n    # Predicted result\n    input_dict_test = {\n        \"x\": data_2d_xy_test[:, 0:1],\n        \"y\": data_2d_xy_test[:, 1:2],\n        \"nu\": data_2d_xy_test[:, 2:3],\n    }\n    output_dict_test = solver.predict(input_dict_test, return_numpy=True)\n    u_max_pred = output_dict_test[\"u\"]\n\n    # Analytical result, y = 0\n    u_max_a = (R**2) * dP / (2 * L * data_1d_nu_distribution * RHO)\n\n    # Plot\n    plt.figure(2)\n    plt.clf()\n    ax1 = plt.subplot(111)\n    sns.kdeplot(\n        u_max_a,\n        fill=True,\n        color=\"black\",\n        label=\"Analytical\",\n        linestyle=\"-\",\n        linewidth=3,\n    )\n    sns.kdeplot(\n        u_max_pred,\n        fill=False,\n        color=\"red\",\n        label=\"DNN\",\n        linestyle=\"--\",\n        linewidth=3.5,\n    )\n    plt.legend(prop={\"size\": fontsize})\n    plt.xlabel(r\"$u_c$\", fontsize=fontsize)\n    plt.ylabel(r\"PDF\", fontsize=fontsize)\n    ax1.tick_params(axis=\"x\", labelsize=fontsize)\n    ax1.tick_params(axis=\"y\", labelsize=fontsize)\n    plt.savefig(osp.join(PLOT_DIR, \"pipe_unformUQ.png\"), bbox_inches=\"tight\")\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"poiseuille_flow.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#24","title":"2.4 \u7ed3\u679c\u5c55\u793a","text":"(\u5de6)\u5728 x=0 \u622a\u9762\u901f\u5ea6 u(y) \u968f y \u5728\u56db\u79cd\u4e0d\u540c\u7684\u52a8\u529b\u7c98\u6027\u7cfb\u6570\u91c7\u6837\u4e0b\u7684\u66f2\u7ebf\u548c\u89e3\u6790\u89e3\u7684\u5bf9\u6bd4 (\u53f3)\u5f53\u6211\u4eec\u9009\u53d6\u622a\u65ad\u9ad8\u65af\u5206\u5e03\u7684\u52a8\u529b\u7c98\u6027\u7cfb\u6570 nu \u91c7\u6837(\u5747\u503c\u4e3a nu=0.001\uff0c \u65b9\u5dee sigma\u200b=2.67 x 10e\u22124)\uff0c\u4e2d\u5fc3\u5904\u901f\u5ea6\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u548c\u89e3\u6790\u89e3\u5bf9\u6bd4 <p>DNN\u4ee3\u7406\u6a21\u578b\u7684\u7ed3\u679c\u5982\u5de6\u56fe\u6240\u793a\uff0c\u548c\u6cca\u8083\u53f6\u6d41\u52a8\u7684\u7cbe\u786e\u89e3(\u8bba\u6587\u516c\u5f0f13)\u8fdb\u884c\u6bd4\u8f83\uff1a</p> \\[ u_a = \\dfrac{\\delta p}{2 \\nu \\rho L} + (\\dfrac{d^2}{4} - y^2) \\] <p>\u516c\u5f0f\u548c\u56fe\u7247\u4e2d\u7684 \\(y\\) \u8868\u793a\u5c55\u5411\u5750\u6807\uff0c\\(\\delta p\\)\uff0c\u4ece\u56fe\u7247\u4e2d\u6211\u4eec\u53ef\u4ee5\u89c2\u5bdf\u5230DNN\u9884\u6d4b\u7684\uff0c4\u79cd\u4e0d\u540c\u7c98\u5ea6\u91c7\u6837\u4e0b\u7684\u901f\u5ea6\u66f2\u7ebf\uff08\u7ea2\u8272\u865a\u7ebf\uff09\uff0c\u51e0\u4e4e\u5b8c\u7f8e\u7b26\u5408\u89e3\u6790\u89e3\u7684\u901f\u5ea6\u66f2\u7ebf\uff08\u84dd\u8272\u5b9e\u7ebf\uff09\uff0c\u5176\u4e2d\uff0c4\u4e2acase\u7684\u96f7\u8bfa\u6570\uff08\\(Re\\)\uff09\u5206\u522b\u4e3a283\uff0c121\uff0c33\uff0c3\u3002\u5b9e\u9645\u4e0a\uff0c\u53ea\u8981\u96f7\u8bfa\u6570\u9002\u4e2d\uff0cDNN\u80fd\u7cbe\u786e\u9884\u6d4b\u4efb\u610f\u7ed9\u5b9a\u52a8\u529b\u5b66\u7c98\u6027\u7cfb\u6570\u7684\u7ba1\u9053\u6d41\u3002</p> <p>\u53f3\u56fe\u5c55\u793a\u4e86\u4e2d\u5fc3\u7ebf(x\u65b9\u5411\u7ba1\u9053\u4e2d\u5fc3)\u901f\u5ea6\uff0c\u5728\u7ed9\u5b9a\u52a8\u529b\u5b66\u7c98\u6027\u7cfb\u6570\uff08\u9ad8\u65af\u5206\u5e03\uff09\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u52a8\u529b\u5b66\u7c98\u6027\u7cfb\u6570\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u5e73\u5747\u503c\u4e3a\\(1e^{-3}\\)\uff0c\u65b9\u5dee\u4e3a\\(2.67e^{-4}\\)\uff0c\u8fd9\u6837\u4fdd\u8bc1\u4e86\u52a8\u529b\u5b66\u7c98\u6027\u7cfb\u6570\u662f\u4e00\u4e2a\u6b63\u968f\u673a\u53d8\u91cf\u3002\u6b64\u5916\uff0c\u8fd9\u4e2a\u9ad8\u65af\u5206\u5e03\u7684\u533a\u95f4\u4e3a\\((0,+\\infty)\\)\uff0c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u4e3a\uff1a</p> \\[ f(\\nu ; \\bar{\\nu}, \\sigma_{\\nu}) = \\dfrac{\\dfrac{1}{\\sigma_{\\nu}} N(\\dfrac{(\\nu - \\bar{\\nu})}{\\sigma_{\\nu}})}{1 - \\phi(-\\dfrac{\\bar{\\nu}}{\\sigma_{\\nu}})} \\] <p>\u66f4\u591a\u7ec6\u8282\u8bf7\u53c2\u8003\u8bba\u6587\u7b2c\u4e5d\u9875</p>"},{"location":"zh/examples/labelfree_DNN_surrogate/#3-aneurysm-flow","title":"3. \u6848\u4f8b\u4e8c: Aneurysm Flow","text":""},{"location":"zh/examples/labelfree_DNN_surrogate/#31","title":"3.1 \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u6587\u4e3b\u8981\u7814\u7a76\u4e86\u4e24\u79cd\u7c7b\u578b\u7684\u5178\u578b\u8840\u7ba1\u6d41\uff08\u5177\u6709\u6807\u51c6\u5316\u7684\u8840\u7ba1\u51e0\u4f55\u5f62\u72b6\uff09\uff0c\u72ed\u7a84\u6d41\u548c\u52a8\u8109\u7624\u6d41\u3002 \u72ed\u7a84\u8840\u6d41\u662f\u6307\u6d41\u8fc7\u8840\u7ba1\u7684\u8840\u6d41\uff0c\u5176\u4e2d\u8840\u7ba1\u58c1\u53d8\u7a84\u548c\u518d\u6269\u5f20\u3002 \u8840\u7ba1\u7684\u8fd9\u79cd\u5c40\u90e8\u9650\u5236\u4e0e\u8bb8\u591a\u5fc3\u8840\u7ba1\u75be\u75c5\u6709\u5173\uff0c\u4f8b\u5982\u52a8\u8109\u786c\u5316\u3001\u4e2d\u98ce\u548c\u5fc3\u810f\u75c5\u53d1\u4f5c \u3002 \u52a8\u8109\u7624\u5185\u7684\u8840\u7ba1\u8840\u6d41\uff0c\u5373\u7531\u4e8e\u8840\u7ba1\u58c1\u8584\u5f31\u5bfc\u81f4\u7684\u52a8\u8109\u6269\u5f20\uff0c\u79f0\u4e3a\u52a8\u8109\u7624\u8840\u6d41\u3002 \u52a8\u8109\u7624\u7834\u88c2\u53ef\u80fd\u5bfc\u81f4\u5371\u53ca\u751f\u547d\u7684\u60c5\u51b5\uff0c\u4f8b\u5982\uff0c\u7531\u4e8e\u8111\u52a8\u8109\u7624\u7834\u88c2\u5f15\u8d77\u7684\u86db\u7f51\u819c\u4e0b\u8154\u51fa\u8840 (SAH)\uff0c\u800c\u8840\u6db2\u52a8\u529b\u5b66\u7684\u7814\u7a76\u53ef\u4ee5\u63d0\u9ad8\u8bca\u65ad\u548c\u5bf9\u52a8\u8109\u7624\u8fdb\u5c55\u548c\u7834\u88c2\u7684\u57fa\u672c\u4e86\u89e3 \u3002</p> <p>\u867d\u7136\u73b0\u5b9e\u7684\u8840\u7ba1\u51e0\u4f55\u5f62\u72b6\u901a\u5e38\u662f\u4e0d\u89c4\u5219\u548c\u590d\u6742\u7684\uff0c\u5305\u62ec\u66f2\u7387\u3001\u5206\u53c9\u548c\u8fde\u63a5\u70b9\uff0c\u4f46\u8fd9\u91cc\u7814\u7a76\u7406\u60f3\u5316\u7684\u72ed\u7a84\u548c\u52a8\u8109\u7624\u6a21\u578b\u4ee5\u8fdb\u884c\u6982\u5ff5\u9a8c\u8bc1\u3002 \u5373\uff0c\u72ed\u7a84\u8840\u7ba1\u548c\u52a8\u8109\u7624\u8840\u7ba1\u90fd\u88ab\u7406\u60f3\u5316\u4e3a\u5177\u6709\u4e0d\u540c\u6a2a\u622a\u9762\u534a\u5f84\u7684\u8f74\u5bf9\u79f0\u7ba1\uff0c\u5176\u7531\u4ee5\u4e0b\u51fd\u6570\u53c2\u6570\u5316\uff0c</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ u\\dfrac{\\partial u}{\\partial x} + v\\dfrac{\\partial u}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial x} + \\nu(\\dfrac{\\partial ^2 u}{\\partial x ^2} + \\dfrac{\\partial ^2 u}{\\partial y ^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ u\\dfrac{\\partial v}{\\partial x} + v\\dfrac{\\partial v}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial y} + \\nu(\\dfrac{\\partial ^2 v}{\\partial x ^2} + \\dfrac{\\partial ^2 v}{\\partial y ^2}) \\] <p>\u6211\u4eec\u53ea\u5173\u6ce8\u8fd9\u79cd\u5b8c\u5168\u53d1\u5c55\u7684\u6d41\u52a8\u5e76\u4e14\u5728\u8fb9\u754c\u65bd\u52a0\u4e86\u65e0\u6ed1\u79fb\u8fb9\u754c\u6761\u4ef6\u3002\u4e0e\u4f20\u7edfPINNs\u65b9\u6cd5\u4e0d\u540c\u7684\u662f\uff0c\u6211\u4eec\u5c06\u65e0\u6ed1\u52a8\u8fb9\u754c\u6761\u4ef6\u901a\u8fc7\u901f\u5ea6\u51fd\u6570\u5047\u8bbe\u7684\u65b9\u5f0f\u5f3a\u5236\u65bd\u52a0\u5728\u8fb9\u754c\u4e0a\uff1a \u5bf9\u4e8e\u6d41\u4f53\u57df\u8fb9\u754c\u548c\u6d41\u4f53\u57df\u5185\u90e8\u5706\u5468\u8fb9\u754c\uff0c\u5219\u9700\u65bd\u52a0 Dirichlet \u8fb9\u754c\u6761\u4ef6\uff1a</p> <p> </p> \u6d41\u573a\u793a\u610f\u56fe <p>\u6d41\u4f53\u57df\u5165\u53e3\u8fb9\u754c\uff1a</p> \\[ p=0.1 \\] <p>\u6d41\u4f53\u57df\u51fa\u53e3\u8fb9\u754c\uff1a</p> \\[ p=0 \\] <p>\u6d41\u4f53\u57df\u4e0a\u4e0b\u8fb9\u754c\uff1a</p> \\[ u=0, v=0 \\]"},{"location":"zh/examples/labelfree_DNN_surrogate/#32","title":"3.2 \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/labelfree_DNN_surrogate/#321","title":"3.2.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9\u548c\u51e0\u4f55\u653e\u5927\u7cfb\u6570 \\((x, y, scale)\\) \u90fd\u6709\u81ea\u8eab\u7684\u6a2a\u5411\u901f\u5ea6 \\(u\\)\u3001\u7eb5\u5411\u901f\u5ea6 \\(v\\)\u3001\u538b\u529b \\(p\\) \u4e09\u4e2a\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684\u4e09\u4e2a MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((x, y, scale)\\) \u5230 \\((u, v, p)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f_1, f_2, f_3: \\mathbb{R}^3 \\to \\mathbb{R}^3\\) \uff0c\u5373\uff1a</p> \\[ u= transform_{output}(f_1(transform_{input}(x, y, scale))) \\] \\[ v= transform_{output}(f_2(transform_{input}(x, y, scale))) \\] \\[ p= transform_{output}(f_3(transform_{input}(x, y, scale))) \\] <p>\u4e0a\u5f0f\u4e2d \\(f_1, f_2, f_3\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\\(transform_{input}, transform_{output}\\), \u8868\u793a\u65bd\u52a0\u989d\u5916\u7684\u7ed3\u6784\u5316\u81ea\u5b9a\u4e49\u5c42\uff0c\u7528\u4e8e\u65bd\u52a0\u7ea6\u675f\u548c\u94fe\u63a5\u8f93\u5165\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b:</p> <pre><code>class Transform:\n    def __init__(self) -&gt; None:\n        pass\n\n    def output_transform_u(self, in_, out):\n        x, y, scale = in_[\"x\"], in_[\"y\"], in_[\"scale\"]\n        r_func = (\n            scale\n            / np.sqrt(2 * np.pi * SIGMA**2)\n            * paddle.exp(-((x - mu) ** 2) / (2 * SIGMA**2))\n        )\n        self.h = R_INLET - r_func\n        u = out[\"u\"]\n        # The no-slip condition of velocity on the wall\n        return {\"u\": u * (self.h**2 - y**2)}\n\n    def output_transform_v(self, in_, out):\n        y = in_[\"y\"]\n        v = out[\"v\"]\n        # The no-slip condition of velocity on the wall\n        return {\"v\": (self.h**2 - y**2) * v}\n\n    def output_transform_p(self, in_, out):\n        x = in_[\"x\"]\n        p = out[\"p\"]\n        # The pressure inlet [p_in = 0.1] and outlet [p_out = 0]\n        return {\n            \"p\": ((P_IN - P_OUT) * (X_OUT - x) / L + (X_IN - x) * (X_OUT - x) * p)\n        }\n\ntransform = Transform()\nmodel_1.register_output_transform(transform.output_transform_u)\nmodel_2.register_output_transform(transform.output_transform_v)\nmodel_3.register_output_transform(transform.output_transform_p)\nmodel = ppsci.arch.ModelList((model_1, model_2, model_3))\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>[\"x\"\u3001 \"y\"\u3001 \"scale\"]</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>[\"u\"\u3001 \"v\"\u3001 \"p\"]</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\u4ee5\u53ca\u6fc0\u6d3b\u51fd\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e09\u4e2a\u62e5\u6709 3 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\u548c 1 \u5c42\u8f93\u51fa\u5c42\u795e\u7ecf\u5143\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 20\uff0c\u4f7f\u7528 \"silu\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model_1</code> <code>model_2</code> <code>model_3</code>\u3002</p> <p>\u6b64\u5916\uff0c\u4f7f\u7528<code>kaiming normal</code>\u65b9\u6cd5\u5bf9\u6743\u91cd\u548c\u504f\u7f6e\u521d\u59cb\u5316\u3002</p> <pre><code>def init_func(m):\n    if misc.typename(m) == \"Linear\":\n        ppsci.utils.initializer.kaiming_normal_(m.weight, reverse=True)\n\nmodel_1 = ppsci.arch.MLP((\"x\", \"y\", \"scale\"), (\"u\",), 3, 20, \"silu\")\nmodel_2 = ppsci.arch.MLP((\"x\", \"y\", \"scale\"), (\"v\",), 3, 20, \"silu\")\nmodel_3 = ppsci.arch.MLP((\"x\", \"y\", \"scale\"), (\"p\",), 3, 20, \"silu\")\nmodel_1.apply(init_func)\nmodel_2.apply(init_func)\nmodel_3.apply(init_func)\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#322","title":"3.2.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e\u672c\u6848\u4f8b\u4f7f\u7528\u7684\u662f Navier-Stokes \u65b9\u7a0b\u76842\u7ef4\u7a33\u6001\u5f62\u5f0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>NavierStokes</code>\u3002</p> <pre><code>equation = {\"NavierStokes\": ppsci.equation.NavierStokes(NU, RHO, 2, False)}\n</code></pre> <p>\u5728\u5b9e\u4f8b\u5316 <code>NavierStokes</code> \u7c7b\u65f6\u9700\u6307\u5b9a\u5fc5\u8981\u7684\u53c2\u6570\uff1a\u52a8\u529b\u7c98\u5ea6 \\(\\nu = 0.001\\), \u6d41\u4f53\u5bc6\u5ea6 \\(\\rho = 1.0\\)\u3002</p> <pre><code># Physic properties\nP_OUT = 0  # pressure at the outlet of pipe\nP_IN = 0.1  # pressure at the inlet of pipe\nNU = 1e-3\nRHO = 1\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#323","title":"3.2.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d\u672c\u6848\u4f8b\u7684\u8ba1\u7b97\u57df\u548c\u53c2\u6570\u81ea\u53d8\u91cf\\(scale\\)\u7531<code>numpy</code>\u968f\u673a\u6570\u751f\u6210\u7684\u70b9\u4e91\u6784\u6210\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u70b9\u4e91\u51e0\u4f55 <code>PointCloud</code> \u7ec4\u5408\u6210\u7a7a\u95f4\u7684 <code>Geometry</code> \u8ba1\u7b97\u57df\u3002</p> <pre><code># Geometry\nL = 1\nX_IN = 0\nX_OUT = X_IN + L\nR_INLET = 0.05\nmu = 0.5 * (X_OUT - X_IN)\nx_initial = np.linspace(X_IN, X_OUT, 100, dtype=paddle.get_default_dtype()).reshape(\n    100, 1\n)\nx_20_copy = np.tile(x_initial, (20, 1))  # duplicate 20 times of x for dataloader\nSIGMA = 0.1\nSCALE_START = -0.02\nSCALE_END = 0\nscale_initial = np.linspace(\n    SCALE_START, SCALE_END, 50, endpoint=True, dtype=paddle.get_default_dtype()\n).reshape(50, 1)\nscale = np.tile(scale_initial, (len(x_20_copy), 1))\nx = np.array([np.tile(val, len(scale_initial)) for val in x_20_copy]).reshape(\n    len(scale), 1\n)\n\n# Axisymmetric boundary\nr_func = (\n    scale\n    / math.sqrt(2 * np.pi * SIGMA**2)\n    * np.exp(-((x - mu) ** 2) / (2 * SIGMA**2))\n)\n\n# Visualize stenosis(scale == 0.2)\nPLOT_DIR = osp.join(cfg.output_dir, \"visu\")\nos.makedirs(PLOT_DIR, exist_ok=True)\ny_up = (R_INLET - r_func) * np.ones_like(x)\ny_down = (-R_INLET + r_func) * np.ones_like(x)\nidx = np.where(scale == 0)  # plot vessel which scale is 0.2 by finding its indices\nplt.figure()\nplt.scatter(x[idx], y_up[idx])\nplt.scatter(x[idx], y_down[idx])\nplt.axis(\"equal\")\nplt.savefig(osp.join(PLOT_DIR, \"idealized_stenotic_vessel\"), bbox_inches=\"tight\")\n\n# Points and shuffle(for alignment)\ny = np.zeros([len(x), 1], dtype=paddle.get_default_dtype())\nfor x0 in x_initial:\n    index = np.where(x[:, 0] == x0)[0]\n    # y is linear to scale, so we place linspace to get 1000 x, it corresponds to vessels\n    y[index] = np.linspace(\n        -max(y_up[index]),\n        max(y_up[index]),\n        len(index),\n        dtype=paddle.get_default_dtype(),\n    ).reshape(len(index), -1)\n\nidx = np.where(scale == 0)  # plot vessel which scale is 0.2 by finding its indices\nplt.figure()\nplt.scatter(x[idx], y[idx])\nplt.axis(\"equal\")\nplt.savefig(osp.join(PLOT_DIR, \"one_scale_sample\"), bbox_inches=\"tight\")\ninterior_geom = ppsci.geometry.PointCloud(\n    interior={\"x\": x, \"y\": y, \"scale\": scale},\n    coord_keys=(\"x\", \"y\", \"scale\"),\n)\ngeom = {\"interior\": interior_geom}\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#324","title":"3.2.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u6839\u636e 3.1 \u95ee\u9898\u5b9a\u4e49 \u5f97\u5230\u7684\u516c\u5f0f\u548c\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u5bf9\u5e94\u4e86\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u7684\u51e0\u4e2a\u7ea6\u675f\u6761\u4ef6\uff0c\u5373\uff1a</p> <ul> <li> <p>\u65bd\u52a0\u5728\u6d41\u4f53\u57df\u5185\u90e8\u70b9\u4e0a\u7684Navier-Stokes \u65b9\u7a0b\u7ea6\u675f</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ u\\dfrac{\\partial u}{\\partial x} + v\\dfrac{\\partial u}{\\partial y} +\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial x} - \\nu(\\dfrac{\\partial ^2 u}{\\partial x ^2} + \\dfrac{\\partial ^2 u}{\\partial y ^2}) = 0 \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ u\\dfrac{\\partial v}{\\partial x} + v\\dfrac{\\partial v}{\\partial y} +\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial y} - \\nu(\\dfrac{\\partial ^2 v}{\\partial x ^2} + \\dfrac{\\partial ^2 v}{\\partial y ^2}) = 0 \\] <p>\u4e3a\u4e86\u65b9\u4fbf\u83b7\u53d6\u4e2d\u95f4\u53d8\u91cf\uff0c<code>NavierStokes</code> \u7c7b\u5185\u90e8\u5c06\u4e0a\u5f0f\u5de6\u4fa7\u7684\u7ed3\u679c\u5206\u522b\u547d\u540d\u4e3a <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code>\u3002</p> </li> <li> <p>\u65bd\u52a0\u5728\u6d41\u4f53\u57df\u5165\u51fa\u53e3\u3001\u6d41\u4f53\u57df\u4e0a\u4e0b\u8840\u7ba1\u58c1\u8fb9\u754c\u7684\u7684 Dirichlet \u8fb9\u754c\u6761\u4ef6\u7ea6\u675f\u3002\u4f5c\u4e3a\u672c\u6587\u521b\u65b0\u70b9\u4e4b\u4e00\uff0c\u6b64\u6848\u4f8b\u521b\u65b0\u6027\u7684\u4f7f\u7528\u4e86\u7ed3\u6784\u5316\u8fb9\u754c\u6761\u4ef6\uff0c\u5373\u901a\u8fc7\u7f51\u7edc\u7684\u8f93\u51fa\u5c42\u540e\u9762\uff0c\u589e\u52a0\u4e00\u5c42\u516c\u5f0f\u5c42\uff0c\u6765\u65bd\u52a0\u8fb9\u754c\u6761\u4ef6\uff08\u516c\u5f0f\u5728\u8fb9\u754c\u5904\u503c\u4e3a\u96f6\uff09\u3002\u907f\u514d\u4e86\u6570\u636e\u70b9\u4f5c\u4e3a\u8fb9\u754c\u6761\u4ef6\u65e0\u6cd5\u6709\u6548\u7ea6\u675f\u3002\u7edf\u4e00\u4f7f\u7528\u7528\u7c7b\u51fd\u6570<code>Transform()</code>\u8fdb\u884c\u521d\u59cb\u5316\u548c\u7ba1\u7406\u3002\u5177\u4f53\u7684\u63a8\u7406\u8fc7\u7a0b\u4e3a\uff1a</p> <p>\u8bbe\u72ed\u7a84\u7f29\u653e\u7cfb\u6570\u4e3a\\(A\\):</p> \\[ R(x) = R_{0} - A\\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}) \\] \\[ d = R(x) \\] <p>\u5177\u4f53\u7684\u4fee\u6b63\u51fd\u6570\u5e26\u5165\u540e\u5f97\u5230\uff1a</p> \\[ \\hat{u} = (\\dfrac{d^2}{4} - y^2) \\tilde{u} \\] \\[ \\hat{v} = (\\dfrac{d^2}{4} - y^2) \\tilde{v} \\] \\[ \\hat{p} = \\dfrac{x - x_{in}}{x_{out} - x_{in}}p_{out} + \\dfrac{x_{out} - x}{x_{out} - x_{in}}p_{in} + (x - x_{in})(x_{out} - x) \\tilde{p} \\] </li> </ul> <p>\u63a5\u4e0b\u6765\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>InteriorConstraint</code> \u548c\u6a21\u578b<code>Transform</code>\u81ea\u5b9a\u4e49\u5c42\uff0c\u6784\u5efa\u4e0a\u8ff0\u4e24\u79cd\u7ea6\u675f\u6761\u4ef6\u3002</p> <ul> <li> <p>\u5185\u90e8\u70b9\u7ea6\u675f</p> <p>\u4ee5\u4f5c\u7528\u5728\u6d41\u4f53\u57df\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>pde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"NavierStokes\"].equations,\n    {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n    geom=geom[\"interior\"],\n    dataloader_cfg={\n        \"dataset\": \"NamedArrayDataset\",\n        \"num_workers\": 1,\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"iters_per_epoch\": int(x.shape[0] / cfg.TRAIN.batch_size),\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"shuffle\": True,\n            \"drop_last\": False,\n        },\n    },\n    loss=ppsci.loss.MSELoss(\"mean\"),\n    evenly=True,\n    name=\"EQ\",\n)\nconstraint = {pde_constraint.name: pde_constraint}\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"NavierStokes\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u6211\u4eec\u5e0c\u671b Navier-Stokes \u65b9\u7a0b\u4ea7\u751f\u7684\u4e09\u4e2a\u4e2d\u95f4\u7ed3\u679c <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code> \u88ab\u4f18\u5316\u81f3 0\uff0c\u56e0\u6b64\u5c06\u5b83\u4eec\u7684\u76ee\u6807\u503c\u5168\u90e8\u8bbe\u4e3a 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>interior_geom</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528\u5206\u6279\u6b21\u6570\u636e\u70b9\u8bad\u7ec3\uff0c\u56e0\u6b64 <code>dataset</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e3a <code>NamedArrayDataset</code> \u4e14 <code>iters_per_epoch</code> \u4e5f\u8bbe\u7f6e\u4e3a 1\uff0c\u91c7\u6837\u70b9\u6570 <code>batch_size</code> \u8bbe\u4e3a 128\uff1b</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\u53d6\u5e73\u5747\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"EQ\" \u5373\u53ef\u3002</p> </li> </ul>"},{"location":"zh/examples/labelfree_DNN_surrogate/#325","title":"3.2.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u4f7f\u7528400\u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u5b66\u4e60\u7387\u8bbe\u4e3a 0.005\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 400\n  learning_rate: 1e-3\n  beta1: 0.9\n  beta2: 0.99\n  epsilon: 1e-15\n  batch_size: 50\n  pretrained_model_path: null\n  checkpoint_path: null\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#326","title":"3.2.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code>optimizer_1 = ppsci.optimizer.Adam(\n    cfg.TRAIN.learning_rate,\n    beta1=cfg.TRAIN.beta1,\n    beta2=cfg.TRAIN.beta2,\n    epsilon=cfg.TRAIN.epsilon,\n)(model_1)\noptimizer_2 = ppsci.optimizer.Adam(\n    cfg.TRAIN.learning_rate,\n    beta1=cfg.TRAIN.beta1,\n    beta2=cfg.TRAIN.beta2,\n    epsilon=cfg.TRAIN.epsilon,\n)(model_2)\noptimizer_3 = ppsci.optimizer.Adam(\n    cfg.TRAIN.learning_rate,\n    beta1=cfg.TRAIN.beta1,\n    beta2=cfg.TRAIN.beta2,\n    epsilon=cfg.TRAIN.epsilon,\n)(model_3)\noptimizer = ppsci.optimizer.OptimizerList((optimizer_1, optimizer_2, optimizer_3))\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#327","title":"3.2.7 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316(\u9700\u8981\u4e0b\u8f7d\u6570\u636e)","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u63a8\u7406\u3002</p> <pre><code>def evaluate(cfg: DictConfig):\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#33","title":"3.3 \u5b8c\u6574\u4ee3\u7801","text":"aneurysm_flow.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nReference: https://github.com/Jianxun-Wang/LabelFree-DNN-Surrogate\n\"\"\"\n\nimport math\nimport os\nimport os.path as osp\n\nimport hydra\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\nfrom ppsci.utils import misc\n\npaddle.framework.core.set_prim_eager_enabled(True)\n\n\ndef train(cfg: DictConfig):\n    # Physic properties\n    P_OUT = 0  # pressure at the outlet of pipe\n    P_IN = 0.1  # pressure at the inlet of pipe\n    NU = 1e-3\n    RHO = 1\n\n    # Geometry\n    L = 1\n    X_IN = 0\n    X_OUT = X_IN + L\n    R_INLET = 0.05\n    mu = 0.5 * (X_OUT - X_IN)\n    x_initial = np.linspace(X_IN, X_OUT, 100, dtype=paddle.get_default_dtype()).reshape(\n        100, 1\n    )\n    x_20_copy = np.tile(x_initial, (20, 1))  # duplicate 20 times of x for dataloader\n    SIGMA = 0.1\n    SCALE_START = -0.02\n    SCALE_END = 0\n    scale_initial = np.linspace(\n        SCALE_START, SCALE_END, 50, endpoint=True, dtype=paddle.get_default_dtype()\n    ).reshape(50, 1)\n    scale = np.tile(scale_initial, (len(x_20_copy), 1))\n    x = np.array([np.tile(val, len(scale_initial)) for val in x_20_copy]).reshape(\n        len(scale), 1\n    )\n\n    # Axisymmetric boundary\n    r_func = (\n        scale\n        / math.sqrt(2 * np.pi * SIGMA**2)\n        * np.exp(-((x - mu) ** 2) / (2 * SIGMA**2))\n    )\n\n    # Visualize stenosis(scale == 0.2)\n    PLOT_DIR = osp.join(cfg.output_dir, \"visu\")\n    os.makedirs(PLOT_DIR, exist_ok=True)\n    y_up = (R_INLET - r_func) * np.ones_like(x)\n    y_down = (-R_INLET + r_func) * np.ones_like(x)\n    idx = np.where(scale == 0)  # plot vessel which scale is 0.2 by finding its indices\n    plt.figure()\n    plt.scatter(x[idx], y_up[idx])\n    plt.scatter(x[idx], y_down[idx])\n    plt.axis(\"equal\")\n    plt.savefig(osp.join(PLOT_DIR, \"idealized_stenotic_vessel\"), bbox_inches=\"tight\")\n\n    # Points and shuffle(for alignment)\n    y = np.zeros([len(x), 1], dtype=paddle.get_default_dtype())\n    for x0 in x_initial:\n        index = np.where(x[:, 0] == x0)[0]\n        # y is linear to scale, so we place linspace to get 1000 x, it corresponds to vessels\n        y[index] = np.linspace(\n            -max(y_up[index]),\n            max(y_up[index]),\n            len(index),\n            dtype=paddle.get_default_dtype(),\n        ).reshape(len(index), -1)\n\n    idx = np.where(scale == 0)  # plot vessel which scale is 0.2 by finding its indices\n    plt.figure()\n    plt.scatter(x[idx], y[idx])\n    plt.axis(\"equal\")\n    plt.savefig(osp.join(PLOT_DIR, \"one_scale_sample\"), bbox_inches=\"tight\")\n    interior_geom = ppsci.geometry.PointCloud(\n        interior={\"x\": x, \"y\": y, \"scale\": scale},\n        coord_keys=(\"x\", \"y\", \"scale\"),\n    )\n    geom = {\"interior\": interior_geom}\n\n    def init_func(m):\n        if misc.typename(m) == \"Linear\":\n            ppsci.utils.initializer.kaiming_normal_(m.weight, reverse=True)\n\n    model_1 = ppsci.arch.MLP((\"x\", \"y\", \"scale\"), (\"u\",), 3, 20, \"silu\")\n    model_2 = ppsci.arch.MLP((\"x\", \"y\", \"scale\"), (\"v\",), 3, 20, \"silu\")\n    model_3 = ppsci.arch.MLP((\"x\", \"y\", \"scale\"), (\"p\",), 3, 20, \"silu\")\n    model_1.apply(init_func)\n    model_2.apply(init_func)\n    model_3.apply(init_func)\n\n    class Transform:\n        def __init__(self) -&gt; None:\n            pass\n\n        def output_transform_u(self, in_, out):\n            x, y, scale = in_[\"x\"], in_[\"y\"], in_[\"scale\"]\n            r_func = (\n                scale\n                / np.sqrt(2 * np.pi * SIGMA**2)\n                * paddle.exp(-((x - mu) ** 2) / (2 * SIGMA**2))\n            )\n            self.h = R_INLET - r_func\n            u = out[\"u\"]\n            # The no-slip condition of velocity on the wall\n            return {\"u\": u * (self.h**2 - y**2)}\n\n        def output_transform_v(self, in_, out):\n            y = in_[\"y\"]\n            v = out[\"v\"]\n            # The no-slip condition of velocity on the wall\n            return {\"v\": (self.h**2 - y**2) * v}\n\n        def output_transform_p(self, in_, out):\n            x = in_[\"x\"]\n            p = out[\"p\"]\n            # The pressure inlet [p_in = 0.1] and outlet [p_out = 0]\n            return {\n                \"p\": ((P_IN - P_OUT) * (X_OUT - x) / L + (X_IN - x) * (X_OUT - x) * p)\n            }\n\n    transform = Transform()\n    model_1.register_output_transform(transform.output_transform_u)\n    model_2.register_output_transform(transform.output_transform_v)\n    model_3.register_output_transform(transform.output_transform_p)\n    model = ppsci.arch.ModelList((model_1, model_2, model_3))\n    optimizer_1 = ppsci.optimizer.Adam(\n        cfg.TRAIN.learning_rate,\n        beta1=cfg.TRAIN.beta1,\n        beta2=cfg.TRAIN.beta2,\n        epsilon=cfg.TRAIN.epsilon,\n    )(model_1)\n    optimizer_2 = ppsci.optimizer.Adam(\n        cfg.TRAIN.learning_rate,\n        beta1=cfg.TRAIN.beta1,\n        beta2=cfg.TRAIN.beta2,\n        epsilon=cfg.TRAIN.epsilon,\n    )(model_2)\n    optimizer_3 = ppsci.optimizer.Adam(\n        cfg.TRAIN.learning_rate,\n        beta1=cfg.TRAIN.beta1,\n        beta2=cfg.TRAIN.beta2,\n        epsilon=cfg.TRAIN.epsilon,\n    )(model_3)\n    optimizer = ppsci.optimizer.OptimizerList((optimizer_1, optimizer_2, optimizer_3))\n\n    equation = {\"NavierStokes\": ppsci.equation.NavierStokes(NU, RHO, 2, False)}\n\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n        geom=geom[\"interior\"],\n        dataloader_cfg={\n            \"dataset\": \"NamedArrayDataset\",\n            \"num_workers\": 1,\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"iters_per_epoch\": int(x.shape[0] / cfg.TRAIN.batch_size),\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"shuffle\": True,\n                \"drop_last\": False,\n            },\n        },\n        loss=ppsci.loss.MSELoss(\"mean\"),\n        evenly=True,\n        name=\"EQ\",\n    )\n    constraint = {pde_constraint.name: pde_constraint}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        log_freq=cfg.log_freq,\n        epochs=cfg.TRAIN.epochs,\n        iters_per_epoch=int(x.shape[0] / cfg.TRAIN.batch_size),\n        save_freq=cfg.save_freq,\n        equation=equation,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n    )\n    solver.train()\n\n\ndef evaluate(cfg: DictConfig):\n    PLOT_DIR = osp.join(cfg.output_dir, \"visu\")\n    os.makedirs(PLOT_DIR, exist_ok=True)\n\n    # Physic properties\n    P_OUT = 0  # pressure at the outlet of pipe\n    P_IN = 0.1  # pressure at the inlet of pipe\n    NU = 1e-3\n\n    # Geometry\n    L = 1\n    X_IN = 0\n    X_OUT = X_IN + L\n    R_INLET = 0.05\n    mu = 0.5 * (X_OUT - X_IN)\n    SIGMA = 0.1\n\n    def init_func(m):\n        if misc.typename(m) == \"Linear\":\n            ppsci.utils.initializer.kaiming_normal_(m.weight, reverse=True)\n\n    model_1 = ppsci.arch.MLP((\"x\", \"y\", \"scale\"), (\"u\",), 3, 20, \"silu\")\n    model_2 = ppsci.arch.MLP((\"x\", \"y\", \"scale\"), (\"v\",), 3, 20, \"silu\")\n    model_3 = ppsci.arch.MLP((\"x\", \"y\", \"scale\"), (\"p\",), 3, 20, \"silu\")\n    model_1.apply(init_func)\n    model_2.apply(init_func)\n    model_3.apply(init_func)\n\n    class Transform:\n        def __init__(self) -&gt; None:\n            pass\n\n        def output_transform_u(self, in_, out):\n            x, y, scale = in_[\"x\"], in_[\"y\"], in_[\"scale\"]\n            r_func = (\n                scale\n                / np.sqrt(2 * np.pi * SIGMA**2)\n                * paddle.exp(-((x - mu) ** 2) / (2 * SIGMA**2))\n            )\n            self.h = R_INLET - r_func\n            u = out[\"u\"]\n            # The no-slip condition of velocity on the wall\n            return {\"u\": u * (self.h**2 - y**2)}\n\n        def output_transform_v(self, in_, out):\n            y = in_[\"y\"]\n            v = out[\"v\"]\n            # The no-slip condition of velocity on the wall\n            return {\"v\": (self.h**2 - y**2) * v}\n\n        def output_transform_p(self, in_, out):\n            x = in_[\"x\"]\n            p = out[\"p\"]\n            # The pressure inlet [p_in = 0.1] and outlet [p_out = 0]\n            return {\n                \"p\": ((P_IN - P_OUT) * (X_OUT - x) / L + (X_IN - x) * (X_OUT - x) * p)\n            }\n\n    transform = Transform()\n    model_1.register_output_transform(transform.output_transform_u)\n    model_2.register_output_transform(transform.output_transform_v)\n    model_3.register_output_transform(transform.output_transform_p)\n    model = ppsci.arch.ModelList((model_1, model_2, model_3))\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n\n    def model_predict(\n        x: np.ndarray, y: np.ndarray, scale: np.ndarray, solver: ppsci.solver.Solver\n    ):\n        xt = paddle.to_tensor(x)\n        yt = paddle.to_tensor(y)\n        scalet = paddle.full_like(xt, scale)\n        input_dict = {\"x\": xt, \"y\": yt, \"scale\": scalet}\n        output_dict = solver.predict(input_dict, batch_size=100, return_numpy=True)\n        return output_dict\n\n    scale_test = np.load(\"./data/aneurysm_scale0005to002_eval0to002mean001_3sigma.npz\")[\n        \"scale\"\n    ]\n    CASE_SELECTED = [1, 151, 486]\n    PLOT_X = 0.8\n    PLOT_Y = 0.06\n    FONTSIZE = 14\n    axis_limit = [0, 1, -0.15, 0.15]\n    path = \"./data/cases/\"\n    D_P = 0.1\n    error_u = []\n    error_v = []\n    N_CL = 200  # number of sampling points in centerline (confused about centerline, but the paper did not explain)\n    x_centerline = np.linspace(\n        X_IN, X_OUT, N_CL, dtype=paddle.get_default_dtype()\n    ).reshape(N_CL, 1)\n    for case_id in CASE_SELECTED:\n        scale = scale_test[case_id - 1]\n        data_CFD = np.load(osp.join(path, f\"{case_id}CFD_contour.npz\"))\n        x = data_CFD[\"x\"].astype(paddle.get_default_dtype())\n        y = data_CFD[\"y\"].astype(paddle.get_default_dtype())\n        u_cfd = data_CFD[\"U\"].astype(paddle.get_default_dtype())\n        # p_cfd = data_CFD[\"P\"].astype(paddle.get_default_dtype()) # missing data\n\n        n = len(x)\n        output_dict = model_predict(\n            x.reshape(n, 1),\n            y.reshape(n, 1),\n            np.full((n, 1), scale, dtype=paddle.get_default_dtype()),\n            solver,\n        )\n        u, v, _ = (\n            output_dict[\"u\"],\n            output_dict[\"v\"],\n            output_dict[\"p\"],\n        )\n        w = np.zeros_like(u)\n        u_vec = np.concatenate([u, v, w], axis=1)\n        error_u.append(\n            np.linalg.norm(u_vec[:, 0] - u_cfd[:, 0]) / (D_P * len(u_vec[:, 0]))\n        )\n        error_v.append(\n            np.linalg.norm(u_vec[:, 1] - u_cfd[:, 1]) / (D_P * len(u_vec[:, 0]))\n        )\n\n        # Stream-wise velocity component u\n        plt.figure()\n        plt.subplot(212)\n        plt.scatter(x, y, c=u_vec[:, 0], vmin=min(u_cfd[:, 0]), vmax=max(u_cfd[:, 0]))\n        plt.text(PLOT_X, PLOT_Y, r\"DNN\", {\"color\": \"b\", \"fontsize\": FONTSIZE})\n        plt.axis(axis_limit)\n        plt.colorbar()\n        plt.subplot(211)\n        plt.scatter(x, y, c=u_cfd[:, 0], vmin=min(u_cfd[:, 0]), vmax=max(u_cfd[:, 0]))\n        plt.colorbar()\n        plt.text(PLOT_X, PLOT_Y, r\"CFD\", {\"color\": \"b\", \"fontsize\": FONTSIZE})\n        plt.axis(axis_limit)\n        plt.savefig(\n            osp.join(PLOT_DIR, f\"{case_id}_scale_{scale}_uContour_test.png\"),\n            bbox_inches=\"tight\",\n        )\n\n        # Span-wise velocity component v\n        plt.figure()\n        plt.subplot(212)\n        plt.scatter(x, y, c=u_vec[:, 1], vmin=min(u_cfd[:, 1]), vmax=max(u_cfd[:, 1]))\n        plt.text(PLOT_X, PLOT_Y, r\"DNN\", {\"color\": \"b\", \"fontsize\": FONTSIZE})\n        plt.axis(axis_limit)\n        plt.colorbar()\n        plt.subplot(211)\n        plt.scatter(x, y, c=u_cfd[:, 1], vmin=min(u_cfd[:, 1]), vmax=max(u_cfd[:, 1]))\n        plt.colorbar()\n        plt.text(PLOT_X, PLOT_Y, r\"CFD\", {\"color\": \"b\", \"fontsize\": FONTSIZE})\n        plt.axis(axis_limit)\n        plt.savefig(\n            osp.join(PLOT_DIR, f\"{case_id}_scale_{scale}_vContour_test.png\"),\n            bbox_inches=\"tight\",\n        )\n        plt.close(\"all\")\n\n        # Centerline wall shear profile tau_c (downside)\n        data_CFD_wss = np.load(osp.join(path, f\"{case_id}CFD_wss.npz\"))\n        x_initial = data_CFD_wss[\"x\"]\n        wall_shear_mag_up = data_CFD_wss[\"wss\"]\n\n        D_H = 0.001  # The span-wise distance is approximately the height of the wall\n        r_cl = (\n            scale\n            / np.sqrt(2 * np.pi * SIGMA**2)\n            * np.exp(-((x_centerline - mu) ** 2) / (2 * SIGMA**2))\n        )\n        y_wall = (-R_INLET + D_H) * np.ones_like(x_centerline) + r_cl\n        output_dict_wss = model_predict(\n            x_centerline,\n            y_wall,\n            np.full((N_CL, 1), scale, dtype=paddle.get_default_dtype()),\n            solver,\n        )\n        v_cl_total = np.zeros_like(\n            x_centerline\n        )  # assuming normal velocity along the wall is zero\n        u_cl = output_dict_wss[\"u\"]\n        v_cl = output_dict_wss[\"v\"]\n        v_cl_total = np.sqrt(u_cl**2 + v_cl**2)\n        tau_c = NU * v_cl_total / D_H\n        plt.figure()\n        plt.plot(\n            x_initial,\n            wall_shear_mag_up,\n            label=\"CFD\",\n            color=\"darkblue\",\n            linestyle=\"-\",\n            lw=3.0,\n            alpha=1.0,\n        )\n        plt.plot(\n            x_initial,\n            tau_c,\n            label=\"DNN\",\n            color=\"red\",\n            linestyle=\"--\",\n            dashes=(5, 5),\n            lw=2.0,\n            alpha=1.0,\n        )\n        plt.xlabel(r\"x\", fontsize=16)\n        plt.ylabel(r\"$\\tau_{c}$\", fontsize=16)\n        plt.legend(prop={\"size\": 16})\n        plt.savefig(\n            osp.join(PLOT_DIR, f\"{case_id}_nu__{scale}_wallshear_test.png\"),\n            bbox_inches=\"tight\",\n        )\n        plt.close(\"all\")\n    logger.message(\n        f\"Table 1 : Aneurysm - Geometry error u : {sum(error_u) / len(error_u): .3e}\"\n    )\n    logger.message(\n        f\"Table 1 : Aneurysm - Geometry error v : {sum(error_v) / len(error_v): .3e}\"\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"aneurysm_flow.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/labelfree_DNN_surrogate/#34","title":"3.4 \u7ed3\u679c\u5c55\u793a","text":"\u7b2c\u4e00\u884c\u4e3ax\u65b9\u5411\u901f\u5ea6\uff0c\u7b2c\u4e8c\u884c\u4e3ay\u65b9\u5411\u901f\u5ea6\uff0c\u7b2c\u4e09\u884c\u4e3a\u58c1\u9762\u526a\u5207\u5e94\u529b\u66f2\u7ebf <p>\u56fe\u7247\u5c55\u793a\u4e86\u5bf9\u4e8e\u51e0\u4f55\u53d8\u5316\u7684\u52a8\u8109\u7624\u6d41\u52a8\u7684\u6c42\u89e3\u80fd\u529b\uff0c\u5176\u4e2d\u8bad\u7ec3\u662f\u901a\u8fc7\uff0c\u5bf9\u51e0\u4f55\u7f29\u653e\u7cfb\u6570\\(A\\)\u4ece\\(0\\)\u5230\\(-2e^{-2}\\)\u533a\u95f4\u91c7\u6837\u8fdb\u884c\u7684\u3002\u4e09\u79cd\u4e0d\u540c\u51e0\u4f55\u7684\u6d41\u573a\u9884\u6d4b\u5982\u56fe\u6240\u793a\uff0c\u52a8\u8109\u7624\u7684\u5927\u5c0f\u4ece\u5de6\u5230\u53f3\u589e\u52a0\uff0c\u6d41\u52a8\u901f\u5ea6\u5728\u8840\u7ba1\u6269\u5f20\u533a\u57df\u51cf\u5c0f\uff0c\u5728\u52a8\u8109\u7624\u4e2d\u5fc3\u5904\u8870\u51cf\u6700\u591a\u3002\u4ece\u524d\u4e24\u884c\u56fe\u7247\u53ef\u4ee5\u770b\u51faCFD\u7ed3\u679c\u548c\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7b26\u5408\u8f83\u597d\u3002\u5bf9\u4e8eWSS\u58c1\u9762\u526a\u5207\u5e94\u529b\uff0c\u66f2\u7ebf\u968f\u7740\u51e0\u4f55\u7684\u53d8\u5316\u4e5f\u88ab\u6a21\u578b\u7cbe\u786e\u6355\u83b7\u3002</p> <p>\u66f4\u591a\u7ec6\u8282\u53c2\u8003\u8bba\u658713\u9875\u3002</p>"},{"location":"zh/examples/labelfree_DNN_surrogate/#4","title":"4. \u53c2\u8003\u6587\u732e","text":"<p>\u53c2\u8003\u6587\u732e\uff1a Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data</p> <p>\u53c2\u8003\u4ee3\u7801\uff1a LabelFree-DNN-Surrogate</p>"},{"location":"zh/examples/laplace2d/","title":"Laplace2D","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#2d-laplace","title":"2D-Laplace","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code>python laplace2d.py\n</code></pre> <pre><code>python laplace2d.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/laplace2d/laplace2d_pretrained.pdparams\n</code></pre> <pre><code>python laplace2d.py mode=export\n</code></pre> <pre><code>python laplace2d.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 laplace2d_pretrained.pdparams loss(MSE_Metric): 0.00002MSE.u(MSE_Metric): 0.00002","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u62c9\u666e\u62c9\u65af\u65b9\u7a0b\u7531\u6cd5\u56fd\u6570\u5b66\u5bb6\u62c9\u666e\u62c9\u65af\u9996\u5148\u63d0\u51fa\u800c\u5f97\u540d\uff0c\u8be5\u65b9\u7a0b\u5728\u8bb8\u591a\u9886\u57df\u90fd\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f8b\u5982\u7535\u78c1\u5b66\u3001\u5929\u6587\u5b66\u548c\u6d41\u4f53\u529b\u5b66\u7b49\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u62c9\u666e\u62c9\u65af\u65b9\u7a0b\u7684\u6c42\u89e3\u5f80\u5f80\u662f\u4e00\u4e2a\u590d\u6742\u7684\u6570\u5b66\u95ee\u9898\u3002\u5bf9\u4e8e\u4e00\u4e9b\u5177\u6709\u7279\u5b9a\u8fb9\u754c\u6761\u4ef6\u548c\u521d\u59cb\u6761\u4ef6\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u7684\u6570\u503c\u65b9\u6cd5\uff08\u5982\u6709\u9650\u5143\u65b9\u6cd5\u3001\u6709\u9650\u5dee\u5206\u65b9\u6cd5\u7b49\uff09\u6765\u6c42\u89e3\u62c9\u666e\u62c9\u65af\u65b9\u7a0b\u3002\u5bf9\u4e8e\u4e00\u4e9b\u590d\u6742\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u9700\u8981\u91c7\u7528\u66f4\u9ad8\u7ea7\u7684\u6570\u503c\u65b9\u6cd5\u6216\u8005\u501f\u52a9\u9ad8\u6027\u80fd\u8ba1\u7b97\u673a\u8fdb\u884c\u8ba1\u7b97\u3002</p> <p>\u672c\u6848\u4f8b\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u5f0f\u5bf9\u62c9\u666e\u62c9\u65af\u65b9\u7a0b\u76842\u7ef4\u5f62\u5f0f\u8fdb\u884c\u6c42\u89e3\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u62c9\u666e\u62c9\u65af\u65b9\u7a0b\uff082\u7ef4\u5f62\u5f0f\uff09\uff1a</p> \\[ \\dfrac{\\partial^{2} u}{\\partial x^{2}} + \\dfrac{\\partial^{2} u}{\\partial y^{2}} = 0, x \\in (0, 1), y \\in (0, 1) \\]","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 2D-Laplace \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y)\\) \u90fd\u6709\u5bf9\u5e94\u7684\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf \\(u\\) \uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((x, y)\\) \u5230 \\((u)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^1\\) \uff0c\u5373\uff1a</p> \\[ u = f(x, y) \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"x\", \"y\")</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>(\"u\",)</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 5 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 20 \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e 2D-Laplace \u4f7f\u7528\u7684\u662f Laplace \u65b9\u7a0b\u76842\u7ef4\u5f62\u5f0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>Laplace</code>\uff0c\u6307\u5b9a\u8be5\u7c7b\u7684\u53c2\u6570 <code>dim</code> \u4e3a2\u3002</p> <pre><code># set equation\nequation = {\"laplace\": ppsci.equation.Laplace(dim=2)}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d 2D Laplace \u95ee\u9898\u4f5c\u7528\u5728\u4ee5 (0.0, 0.0),  (1.0, 1.0) \u4e3a\u5bf9\u89d2\u7ebf\u7684\u4e8c\u7ef4\u77e9\u5f62\u533a\u57df\uff0c \u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u7a7a\u95f4\u51e0\u4f55 <code>Rectangle</code> \u4f5c\u4e3a\u8ba1\u7b97\u57df\u3002</p> <pre><code># set geometry\ngeom = {\n    \"rect\": ppsci.geometry.Rectangle(\n        cfg.DIAGONAL_COORD.xmin, cfg.DIAGONAL_COORD.xmax\n    )\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e24\u4e2a\u7ea6\u675f\u6761\u4ef6\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u7684\u8bad\u7ec3\u5206\u522b\u662f\u4f5c\u7528\u4e8e\u91c7\u6837\u70b9\u4e0a\u7684 Laplace \u65b9\u7a0b\u7ea6\u675f\u548c\u4f5c\u7528\u4e8e\u8fb9\u754c\u70b9\u4e0a\u7684\u7ea6\u675f\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u79cd\u7ea6\u675f\u6307\u5b9a\u91c7\u6837\u70b9\u4e2a\u6570\uff0c\u8868\u793a\u6bcf\u4e00\u79cd\u7ea6\u675f\u5728\u5176\u5bf9\u5e94\u8ba1\u7b97\u57df\u5185\u91c7\u6837\u6570\u636e\u7684\u6570\u91cf\uff0c\u4ee5\u53ca\u901a\u7528\u7684\u91c7\u6837\u914d\u7f6e\u3002</p> <pre><code>NPOINT_INTERIOR: 9801\nNPOINT_BC: 400\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set constraint\npde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"laplace\"].equations,\n    {\"laplace\": 0},\n    geom[\"rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_TOTAL},\n    ppsci.loss.MSELoss(\"sum\"),\n    evenly=True,\n    name=\"EQ\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"laplace\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u6211\u4eec\u5e0c\u671b Laplace \u65b9\u7a0b\u4ea7\u751f\u7684\u7ed3\u679c <code>laplace</code> \u88ab\u4f18\u5316\u81f3 0\uff0c\u56e0\u6b64\u5c06\u5b83\u7684\u76ee\u6807\u503c\u5168\u8bbe\u4e3a 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"rect\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528\u5168\u91cf\u6570\u636e\u70b9\u8bad\u7ec3\uff0c\u56e0\u6b64 <code>dataset</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e3a \"IterableNamedArrayDataset\" \u4e14 <code>iters_per_epoch</code> \u4e5f\u8bbe\u7f6e\u4e3a 1\uff0c\u91c7\u6837\u70b9\u6570 <code>batch_size</code> \u8bbe\u4e3a 10201(\u8868\u793a99x99\u7684\u7b49\u95f4\u9694\u7f51\u683c\u52a0400\u4e2a\u8fb9\u754c\u70b9)\uff1b</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"sum\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u9009\u62e9\u662f\u5426\u5728\u8ba1\u7b97\u57df\u4e0a\u8fdb\u884c\u7b49\u95f4\u9694\u91c7\u6837\uff0c\u6b64\u5904\u6211\u4eec\u9009\u62e9\u5f00\u542f\u7b49\u95f4\u9694\u91c7\u6837\uff0c\u8fd9\u6837\u80fd\u8ba9\u8bad\u7ec3\u70b9\u5747\u5300\u5206\u5e03\u5728\u8ba1\u7b97\u57df\u4e0a\uff0c\u6709\u5229\u4e8e\u8bad\u7ec3\u6536\u655b\uff1b</p> <p>\u7b2c\u4e03\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"EQ\" \u5373\u53ef\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u540c\u7406\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u6784\u5efa\u77e9\u5f62\u7684\u56db\u4e2a\u8fb9\u754c\u7684\u7ea6\u675f\u3002\u4f46\u4e0e\u6784\u5efa <code>InteriorConstraint</code> \u7ea6\u675f\u4e0d\u540c\u7684\u662f\uff0c\u7531\u4e8e\u4f5c\u7528\u533a\u57df\u662f\u8fb9\u754c\uff0c\u56e0\u6b64\u6211\u4eec\u4f7f\u7528 <code>BoundaryConstraint</code> \u7c7b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>bc = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"]},\n    {\"u\": u_solution_func},\n    geom[\"rect\"],\n    {**train_dataloader_cfg, \"batch_size\": cfg.NPOINT_BC},\n    ppsci.loss.MSELoss(\"sum\"),\n    name=\"BC\",\n)\n# wrap constraints together\nconstraint = {\n    pde_constraint.name: pde_constraint,\n    bc.name: bc,\n}\n</code></pre> <p><code>BoundaryConstraint</code> \u7c7b\u7b2c\u4e00\u4e2a\u53c2\u6570\u8868\u793a\u6211\u4eec\u76f4\u63a5\u5bf9\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u51fa\u7ed3\u679c <code>out[\"u\"]</code> \u4f5c\u4e3a\u7a0b\u5e8f\u8fd0\u884c\u65f6\u7684\u7ea6\u675f\u5bf9\u8c61\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u6307\u6211\u4eec\u7ea6\u675f\u5bf9\u8c61\u7684\u771f\u503c\u5982\u4f55\u83b7\u5f97\uff0c\u8fd9\u91cc\u6211\u4eec\u76f4\u63a5\u901a\u8fc7\u5176\u89e3\u6790\u89e3\u8fdb\u884c\u8ba1\u7b97\uff0c\u5b9a\u4e49\u89e3\u6790\u89e3\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># compute ground truth function\ndef u_solution_func(out):\n    \"\"\"compute ground truth for u as label data\"\"\"\n    x, y = out[\"x\"], out[\"y\"]\n    return np.cos(x) * np.cosh(y)\n</code></pre> <p><code>BoundaryConstraint</code> \u7c7b\u5176\u4ed6\u53c2\u6570\u7684\u542b\u4e49\u4e0e <code>InteriorConstraint</code> \u57fa\u672c\u4e00\u81f4\uff0c\u8fd9\u91cc\u4e0d\u518d\u4ecb\u7ecd\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528\u4e24\u4e07\u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u8bc4\u4f30\u95f4\u9694\u4e3a\u4e24\u767e\u8f6e\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 20000\n  iters_per_epoch: 1\n  eval_during_train: true\n  eval_freq: 200\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(learning_rate=cfg.TRAIN.learning_rate)(model)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.GeometryValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\nmse_metric = ppsci.validate.GeometryValidator(\n    {\"u\": lambda out: out[\"u\"]},\n    {\"u\": u_solution_func},\n    geom[\"rect\"],\n    {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"total_size\": NPOINT_TOTAL,\n    },\n    ppsci.loss.MSELoss(),\n    evenly=True,\n    metric={\"MSE\": ppsci.metric.MSE()},\n    with_initial=True,\n    name=\"MSE_Metric\",\n)\nvalidator = {mse_metric.name: mse_metric}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#38","title":"3.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u672c\u6587\u4e2d\u7684\u8f93\u51fa\u6570\u636e\u662f\u4e00\u4e2a\u533a\u57df\u5185\u7684\u4e8c\u7ef4\u70b9\u96c6\uff0c\u56e0\u6b64\u6211\u4eec\u53ea\u9700\u8981\u5c06\u8bc4\u4f30\u7684\u8f93\u51fa\u6570\u636e\u4fdd\u5b58\u6210 vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u6700\u540e\u7528\u53ef\u89c6\u5316\u8f6f\u4ef6\u6253\u5f00\u67e5\u770b\u5373\u53ef\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set visualizer(optional)\nvis_points = geom[\"rect\"].sample_interior(NPOINT_TOTAL, evenly=True)\nvisualizer = {\n    \"visualize_u\": ppsci.visualize.VisualizerVtu(\n        vis_points,\n        {\"u\": lambda d: d[\"u\"]},\n        num_timestamps=1,\n        prefix=\"result_u\",\n    )\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    epochs=cfg.TRAIN.epochs,\n    iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"laplace2d.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hydra\nimport numpy as np\nfrom omegaconf import DictConfig\n\nimport ppsci\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"laplace\": ppsci.equation.Laplace(dim=2)}\n\n    # set geometry\n    geom = {\n        \"rect\": ppsci.geometry.Rectangle(\n            cfg.DIAGONAL_COORD.xmin, cfg.DIAGONAL_COORD.xmax\n        )\n    }\n\n    # compute ground truth function\n    def u_solution_func(out):\n        \"\"\"compute ground truth for u as label data\"\"\"\n        x, y = out[\"x\"], out[\"y\"]\n        return np.cos(x) * np.cosh(y)\n\n    # set train dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    }\n\n    NPOINT_TOTAL = cfg.NPOINT_INTERIOR + cfg.NPOINT_BC\n\n    # set constraint\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"laplace\"].equations,\n        {\"laplace\": 0},\n        geom[\"rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_TOTAL},\n        ppsci.loss.MSELoss(\"sum\"),\n        evenly=True,\n        name=\"EQ\",\n    )\n    bc = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": u_solution_func},\n        geom[\"rect\"],\n        {**train_dataloader_cfg, \"batch_size\": cfg.NPOINT_BC},\n        ppsci.loss.MSELoss(\"sum\"),\n        name=\"BC\",\n    )\n    # wrap constraints together\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        bc.name: bc,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(learning_rate=cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    mse_metric = ppsci.validate.GeometryValidator(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": u_solution_func},\n        geom[\"rect\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"total_size\": NPOINT_TOTAL,\n        },\n        ppsci.loss.MSELoss(),\n        evenly=True,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        with_initial=True,\n        name=\"MSE_Metric\",\n    )\n    validator = {mse_metric.name: mse_metric}\n\n    # set visualizer(optional)\n    vis_points = geom[\"rect\"].sample_interior(NPOINT_TOTAL, evenly=True)\n    visualizer = {\n        \"visualize_u\": ppsci.visualize.VisualizerVtu(\n            vis_points,\n            {\"u\": lambda d: d[\"u\"]},\n            num_timestamps=1,\n            prefix=\"result_u\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        epochs=cfg.TRAIN.epochs,\n        iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"laplace\": ppsci.equation.Laplace(dim=2)}\n\n    # set geometry\n    geom = {\n        \"rect\": ppsci.geometry.Rectangle(\n            cfg.DIAGONAL_COORD.xmin, cfg.DIAGONAL_COORD.xmax\n        )\n    }\n\n    # compute ground truth function\n    def u_solution_func(out):\n        \"\"\"compute ground truth for u as label data\"\"\"\n        x, y = out[\"x\"], out[\"y\"]\n        return np.cos(x) * np.cosh(y)\n\n    NPOINT_TOTAL = cfg.NPOINT_INTERIOR + cfg.NPOINT_BC\n\n    # set validator\n    mse_metric = ppsci.validate.GeometryValidator(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": u_solution_func},\n        geom[\"rect\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"total_size\": NPOINT_TOTAL,\n        },\n        ppsci.loss.MSELoss(),\n        evenly=True,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        with_initial=True,\n        name=\"MSE_Metric\",\n    )\n    validator = {mse_metric.name: mse_metric}\n\n    # set visualizer(optional)\n    vis_points = geom[\"rect\"].sample_interior(NPOINT_TOTAL, evenly=True)\n    visualizer = {\n        \"visualize_u\": ppsci.visualize.VisualizerVtu(\n            vis_points,\n            {\"u\": lambda d: d[\"u\"]},\n            num_timestamps=1,\n            prefix=\"result_u\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n    # visualize prediction\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # set geometry\n    geom = {\n        \"rect\": ppsci.geometry.Rectangle(\n            cfg.DIAGONAL_COORD.xmin, cfg.DIAGONAL_COORD.xmax\n        )\n    }\n    NPOINT_TOTAL = cfg.NPOINT_INTERIOR + cfg.NPOINT_BC\n    input_dict = geom[\"rect\"].sample_interior(NPOINT_TOTAL, evenly=True)\n\n    output_dict = predictor.predict(\n        {key: input_dict[key] for key in cfg.MODEL.input_keys}, cfg.INFER.batch_size\n    )\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n\n    # save result\n    ppsci.visualize.save_vtu_from_dict(\n        \"./laplace2d_pred.vtu\",\n        {**input_dict, **output_dict},\n        input_dict.keys(),\n        cfg.MODEL.output_keys,\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"laplace2d.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/laplace2d/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4f7f\u7528\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u5bf9\u4e0a\u8ff0\u8ba1\u7b97\u57df\u4e2d\u5747\u5300\u53d6\u7684\u5171 <code>NPOINT_TOTAL</code> \u4e2a\u70b9 \\((x_i,y_i)\\) \u8fdb\u884c\u9884\u6d4b\uff0c\u9884\u6d4b\u7ed3\u679c\u5982\u4e0b\u6240\u793a\u3002\u56fe\u50cf\u4e2d\u6bcf\u4e2a\u70b9 \\((x_i,y_i)\\) \u7684\u503c\u4ee3\u8868\u5bf9\u5e94\u5750\u6807\u4e0a\u6a21\u578b\u5bf9 2D-Laplace \u95ee\u9898\u9884\u6d4b\u7684\u89e3 \\(u(x_i,y_i)\\)\u3002</p> <p> </p> \u6a21\u578b\u9884\u6d4b\u7ed3\u679c","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Laplace\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/","title":"LDC2D_steady","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#2d-ldc2d-lid-driven-cavity-flow","title":"2D-LDC(2D Lid Driven Cavity Flow)","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> Re=1000Re=3200 \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc -P ./data/ \\\n    https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re100.mat \\\n    https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re400.mat \\\n    https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1000.mat \\\n    https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re3200.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re100.mat --create-dirs -o ./data/ldc_Re100.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re400.mat --create-dirs -o ./data/ldc_Re400.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1000.mat --create-dirs -o ./data/ldc_Re1000.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re3200.mat --create-dirs -o ./data/ldc_Re3200.mat\npython ldc_2d_Re3200_sota.py\n</code></pre> <pre><code># linux\nwget -nc -P ./data/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1000.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1000.mat --create-dirs -o ./data/ldc_Re1000.mat\npython ldc_2d_Re3200_sota.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/ldc/ldc_re1000_sota_pretrained.pdparams\n</code></pre> <pre><code>python ldc_2d_Re3200_sota.py mode=export\n</code></pre> <pre><code># linux\nwget -nc -P ./data/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1000.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1000.mat --create-dirs -o ./data/ldc_Re1000.mat\npython ldc_2d_Re3200_sota.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \\(Re\\) \u6307\u6807 - 100 U_validator/loss: 0.00017U_validator/L2Rel.U: 0.04875 - 400 U_validator/loss: 0.00047U_validator/L2Rel.U: 0.07554 ldc_re1000_sota_pretrained.pdparams 1000 U_validator/loss: 0.00053U_validator/L2Rel.U: 0.07777 - 3200 U_validator/loss: 0.00227U_validator/L2Rel.U: 0.15440 \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc -P ./data/ \\\n    https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re100.mat \\\n    https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re400.mat \\\n    https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1000.mat \\\n    https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1600.mat \\\n    https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re3200.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re100.mat --create-dirs -o ./data/ldc_Re100.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re400.mat --create-dirs -o ./data/ldc_Re400.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1000.mat --create-dirs -o ./data/ldc_Re1000.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re1600.mat --create-dirs -o ./data/ldc_Re1600.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re3200.mat --create-dirs -o ./data/ldc_Re3200.mat\npython ldc_2d_Re3200_piratenet.py\n</code></pre> <pre><code># linux\nwget -nc -P ./data/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re3200.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re3200.mat --create-dirs -o ./data/ldc_Re3200.mat\npython ldc_2d_Re3200_piratenet.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/ldc/ldc_re3200_piratenet_pretrained.pdparams\n</code></pre> <pre><code>python ldc_2d_Re3200_piratenet.py mode=export\n</code></pre> <pre><code># linux\nwget -nc -P ./data/ https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re3200.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/ldc/ldc_Re3200.mat --create-dirs -o ./data/ldc_Re3200.mat\npython ldc_2d_Re3200_piratenet.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \\(Re\\) \u6307\u6807 - 100 U_validator/loss: 0.00016U_validator/L2Rel.U: 0.04741 - 400 U_validator/loss: 0.00071U_validator/L2Rel.U: 0.09288 - 1000 U_validator/loss: 0.00191U_validator/L2Rel.U: 0.14797 - 1600 U_validator/loss: 0.00276U_validator/L2Rel.U: 0.17360 ldc_re3200_piratenet_pretrained.pdparams 3200 U_validator/loss: 0.00016U_validator/L2Rel.U: 0.04166 <p>\u8bf4\u660e</p> <p>\u672c\u6848\u4f8b\u4ec5\u63d0\u4f9b \\(Re=1000/3200\\) \u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u82e5\u9700\u8981\u5176\u4ed6\u96f7\u8bfa\u6570\u4e0b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8bf7\u6267\u884c\u8bad\u7ec3\u547d\u4ee4\u624b\u52a8\u8bad\u7ec3\u5373\u53ef\u5f97\u5230\u5404\u96f7\u8bfa\u6570\u4e0b\u7684\u6a21\u578b\u6743\u91cd\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u9876\u76d6\u65b9\u8154\u9a71\u52a8\u6d41LDC\u95ee\u9898\u5728\u8bb8\u591a\u9886\u57df\u4e2d\u90fd\u6709\u5e94\u7528\u3002\u4f8b\u5982\uff0c\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u7528\u4e8e\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\uff08CFD\uff09\u9886\u57df\u4e2d\u9a8c\u8bc1\u8ba1\u7b97\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u867d\u7136\u8fd9\u4e2a\u95ee\u9898\u7684\u8fb9\u754c\u6761\u4ef6\u76f8\u5bf9\u7b80\u5355\uff0c\u4f46\u662f\u5176\u6d41\u52a8\u7279\u6027\u5374\u975e\u5e38\u590d\u6742\u3002\u5728\u9876\u76d6\u9a71\u52a8\u6d41LDC\u4e2d\uff0c\u9876\u58c1\u671dx\u65b9\u5411\u4ee5U=1\u7684\u901f\u5ea6\u79fb\u52a8\uff0c\u800c\u5176\u4ed6\u4e09\u4e2a\u58c1\u5219\u88ab\u5b9a\u4e49\u4e3a\u65e0\u6ed1\u79fb\u8fb9\u754c\u6761\u4ef6\uff0c\u5373\u901f\u5ea6\u4e3a\u96f6\u3002</p> <p>\u6b64\u5916\uff0c\u9876\u76d6\u65b9\u8154\u9a71\u52a8\u6d41LDC\u95ee\u9898\u4e5f\u88ab\u7528\u4e8e\u7814\u7a76\u548c\u9884\u6d4b\u7a7a\u6c14\u52a8\u529b\u5b66\u4e2d\u7684\u6d41\u52a8\u73b0\u8c61\u3002\u4f8b\u5982\uff0c\u5728\u6c7d\u8f66\u5de5\u4e1a\u4e2d\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u5206\u6790\u8f66\u4f53\u5185\u90e8\u7684\u7a7a\u6c14\u6d41\u52a8\uff0c\u53ef\u4ee5\u5e2e\u52a9\u4f18\u5316\u8f66\u8f86\u7684\u8bbe\u8ba1\u548c\u6027\u80fd\u3002</p> <p>\u603b\u7684\u6765\u8bf4\uff0c\u9876\u76d6\u65b9\u8154\u9a71\u52a8\u6d41LDC\u95ee\u9898\u5728\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\u3001\u7a7a\u6c14\u52a8\u529b\u5b66\u4ee5\u53ca\u76f8\u5173\u9886\u57df\u4e2d\u90fd\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u5bf9\u4e8e\u7814\u7a76\u548c\u9884\u6d4b\u6d41\u52a8\u73b0\u8c61\u3001\u4f18\u5316\u4ea7\u54c1\u8bbe\u8ba1\u7b49\u65b9\u9762\u90fd\u8d77\u5230\u4e86\u91cd\u8981\u7684\u4f5c\u7528\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u6848\u4f8b\u5047\u8bbe \\(Re=3200\\)\uff0c\u8ba1\u7b97\u57df\u4e3a\u4e00\u4e2a\u957f\u5bbd\u5747\u4e3a 1 \u7684\u65b9\u8154\uff0c\u5e94\u7528\u4ee5\u4e0b\u516c\u5f0f\u8fdb\u884c\u9876\u76d6\u9a71\u52a8\u65b9\u8154\u6d41\u7814\u7a76\u7a33\u6001\u6d41\u573a\u95ee\u9898\uff1a</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[  u\\dfrac{\\partial u}{\\partial x} + v\\dfrac{\\partial u}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial x} + \\nu(\\dfrac{\\partial ^2 u}{\\partial x ^2} + \\dfrac{\\partial ^2 u}{\\partial y ^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ u\\dfrac{\\partial v}{\\partial x} + v\\dfrac{\\partial v}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial y} + \\nu(\\dfrac{\\partial ^2 v}{\\partial x ^2} + \\dfrac{\\partial ^2 v}{\\partial y ^2}) \\] <p>\u4ee4\uff1a</p> <p>\\(t^* = \\dfrac{L}{U_0}\\)</p> <p>\\(x^*=y^* = L\\)</p> <p>\\(u^*=v^* = U_0\\)</p> <p>\\(p^* = \\rho {U_0}^2\\)</p> <p>\u5b9a\u4e49\uff1a</p> <p>\u65e0\u91cf\u7eb2\u5750\u6807 \\(x\uff1aX = \\dfrac{x}{x^*}\\)\uff1b\u65e0\u91cf\u7eb2\u5750\u6807 \\(y\uff1aY = \\dfrac{y}{y^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u901f\u5ea6 \\(x\uff1aU = \\dfrac{u}{u^*}\\)\uff1b\u65e0\u91cf\u7eb2\u901f\u5ea6 \\(y\uff1aV = \\dfrac{v}{u^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u538b\u529b \\(P = \\dfrac{p}{p^*}\\)</p> <p>\u96f7\u8bfa\u6570 \\(Re = \\dfrac{L U_0}{\\nu}\\)</p> <p>\u5219\u53ef\u83b7\u5f97\u5982\u4e0b\u65e0\u91cf\u7eb2Navier-Stokes\u65b9\u7a0b\uff0c\u65bd\u52a0\u4e8e\u65b9\u8154\u5185\u90e8\uff1a</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial U}{\\partial X} + \\dfrac{\\partial U}{\\partial Y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ U\\dfrac{\\partial U}{\\partial X} + V\\dfrac{\\partial U}{\\partial Y} = -\\dfrac{\\partial P}{\\partial X} + \\dfrac{1}{Re}(\\dfrac{\\partial ^2 U}{\\partial X^2} + \\dfrac{\\partial ^2 U}{\\partial Y^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ U\\dfrac{\\partial V}{\\partial X} + V\\dfrac{\\partial V}{\\partial Y} = -\\dfrac{\\partial P}{\\partial Y} + \\dfrac{1}{Re}(\\dfrac{\\partial ^2 V}{\\partial X^2} + \\dfrac{\\partial ^2 V}{\\partial Y^2}) \\] <p>\u5bf9\u4e8e\u65b9\u8154\u8fb9\u754c\uff0c\u5219\u9700\u65bd\u52a0 Dirichlet \u8fb9\u754c\u6761\u4ef6\uff1a</p> <p>\u4e0a\u8fb9\u754c\uff1a</p> \\[ u(x, y) = 1 \u2212 \\dfrac{\\cosh (C_0(x \u2212 0.5))} {\\cosh (0.5C_0)} , \\] <p>\u5de6\u8fb9\u754c\u3001\u4e0b\u8fb9\u754c\u3001\u53f3\u8fb9\u754c\uff1a</p> \\[ u=0, v=0 \\]","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 2D-LDC \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((x, y)\\) \u90fd\u6709\u81ea\u8eab\u7684\u6a2a\u5411\u901f\u5ea6 \\(u\\)\u3001\u7eb5\u5411\u901f\u5ea6 \\(v\\)\u3001\u538b\u529b \\(p\\) \u4e09\u4e2a\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((x, y)\\) \u5230 \\((u, v, p)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^3\\) \uff0c\u5373\uff1a</p> \\[ u, v, p = f(x, y) \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.PirateNet(**cfg.MODEL)\n</code></pre> <p>\u5176\u4e2d <code>cfg.MODEL</code> \u914d\u7f6e\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code># model settings\nMODEL:\n  input_keys: [\"x\", \"y\"]\n  output_keys: [\"u\", \"v\", \"p\"]\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>[\"x\", \"y\"]</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>[\"u\", \"v\", \"p\"]</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u5982\u4e0a\u6240\u793a\uff0c\u901a\u8fc7\u6307\u5b9a <code>PirateNet</code> \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\u4ee5\u53ca\u6fc0\u6d3b\u51fd\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 12 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 256\uff0c\u4f7f\u7528 \"tanh\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#32-curriculum-learning","title":"3.2 Curriculum Learning","text":"<p>\u4e3a\u4e86\u52a0\u5feb\u6536\u655b\u901f\u5ea6\uff0c\u6211\u4eec\u4f7f\u7528 Curriculum learning \u7684\u65b9\u6cd5\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u5373\u5148\u8bad\u7ec3\u6a21\u578b\u5728\u4f4e\u96f7\u8bfa\u6570\u4e0b\uff0c\u7136\u540e\u9010\u6b65\u589e\u52a0\u96f7\u8bfa\u6570\uff0c\u6700\u7ec8\u8fbe\u5230\u9ad8\u96f7\u8bfa\u6570\u4e0b\u7684\u6536\u655b\u3002</p> <pre><code>for idx in range(len(cfg.Re)):\n    train_curriculum(cfg, idx)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#33","title":"3.3 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e 2D-LDC \u4f7f\u7528\u7684\u662f Navier-Stokes \u65b9\u7a0b\u76842\u7ef4\u7a33\u6001\u5f62\u5f0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>NavierStokes</code>\u3002</p> <pre><code># set equation\nequation = {\n    \"NavierStokes\": ppsci.equation.NavierStokes(1 / Re, 1, dim=2, time=False)\n}\n</code></pre> <p>\u5728\u8bfe\u7a0b\u5b66\u4e60\u7684\u51fd\u6570\u4e2d\uff0c\u6211\u4eec\u5728\u5b9e\u4f8b\u5316 <code>NavierStokes</code> \u7c7b\u65f6\u9700\u6307\u5b9a\u5fc5\u8981\u7684\u53c2\u6570\uff1a\u52a8\u529b\u7c98\u5ea6 \\(\\nu=\\frac{1}{Re}\\), \u6d41\u4f53\u5bc6\u5ea6 \\(\\rho=1.0\\)\uff0c\u5176\u4e2d \\(Re\\) \u662f\u4e00\u4e2a\u968f\u7740\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u9010\u6b65\u589e\u5927\u7684\u53d8\u91cf\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#34","title":"3.4 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d 2D-LDC \u95ee\u9898\u8bad\u7ec3\u3001\u8bc4\u4f30\u6240\u9700\u7684\u6570\u636e\uff0c\u901a\u8fc7\u8bfb\u53d6\u5bf9\u5e94\u96f7\u8bfa\u6570\u7684\u6587\u4ef6\u5f97\u5230\u3002</p> <pre><code># load data\ndata = sio.loadmat(f\"./data/ldc_Re{Re}.mat\")\nu_ref = data[\"u\"].astype(dtype)\nv_ref = data[\"v\"].astype(dtype)\nU_ref = np.sqrt(u_ref**2 + v_ref**2).reshape(-1, 1)\nx_star = data[\"x\"].flatten().astype(dtype)\ny_star = data[\"y\"].flatten().astype(dtype)\nx0 = x_star[0]\nx1 = x_star[-1]\ny0 = y_star[0]\ny1 = y_star[-1]\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#35","title":"3.5 \u7ea6\u675f\u6784\u5efa","text":"<p>\u6839\u636e 2. \u95ee\u9898\u5b9a\u4e49 \u5f97\u5230\u7684\u65e0\u91cf\u7eb2\u516c\u5f0f\u548c\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u5bf9\u5e94\u4e86\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u7684\u4e24\u4e2a\u7ea6\u675f\u6761\u4ef6\uff0c\u5373\uff1a</p> <ol> <li> <p>\u65bd\u52a0\u5728\u77e9\u5f62\u5185\u90e8\u70b9\u4e0a\u7684\u65e0\u91cf\u7eb2 Navier-Stokes \u65b9\u7a0b\u7ea6\u675f\uff08\u7ecf\u8fc7\u7b80\u5355\u79fb\u9879\uff09</p> \\[ \\dfrac{\\partial U}{\\partial X} + \\dfrac{\\partial U}{\\partial Y} = 0 \\] \\[ U\\dfrac{\\partial U}{\\partial X} + V\\dfrac{\\partial U}{\\partial Y} + \\dfrac{\\partial P}{\\partial X} - \\dfrac{1}{Re}(\\dfrac{\\partial ^2 U}{\\partial X^2} + \\dfrac{\\partial ^2 U}{\\partial Y^2}) = 0 \\] \\[ U\\dfrac{\\partial V}{\\partial X} + V\\dfrac{\\partial V}{\\partial Y} + \\dfrac{\\partial P}{\\partial Y} - \\dfrac{1}{Re}(\\dfrac{\\partial ^2 V}{\\partial X^2} + \\dfrac{\\partial ^2 V}{\\partial Y^2}) = 0 \\] <p>\u4e3a\u4e86\u65b9\u4fbf\u83b7\u53d6\u4e2d\u95f4\u53d8\u91cf\uff0c<code>NavierStokes</code> \u7c7b\u5185\u90e8\u5c06\u4e0a\u5f0f\u5de6\u4fa7\u7684\u7ed3\u679c\u5206\u522b\u547d\u540d\u4e3a <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code>\u3002</p> </li> <li> <p>\u65bd\u52a0\u5728\u77e9\u5f62\u4e0a\u3001\u4e0b\u3001\u5de6\u3001\u53f3\u8fb9\u754c\u4e0a\u7684 Dirichlet \u8fb9\u754c\u6761\u4ef6\u7ea6\u675f</p> <p>\u4e0a\u8fb9\u754c\uff1a</p> \\[ u(x, y) = 1 \u2212 \\dfrac{\\cosh (C_0(x \u2212 0.5))} {\\cosh (0.5C_0)} , \\] <p>\u5de6\u8fb9\u754c\u3001\u4e0b\u8fb9\u754c\u3001\u53f3\u8fb9\u754c\uff1a</p> \\[ u=0, v=0 \\] </li> </ol> <p>\u63a5\u4e0b\u6765\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u4e0a\u8ff0\u4e24\u79cd\u7ea6\u675f\u6761\u4ef6\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#351","title":"3.5.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u77e9\u5f62\u5185\u90e8\u70b9\u4e0a\u7684 <code>SupervisedConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set N-S pde constraint\ndef gen_input_batch():\n    tx = np.random.uniform(\n        [x0, y0],\n        [x1, y1],\n        (cfg_t.TRAIN.batch_size.pde, 2),\n    ).astype(dtype)\n    return {\"x\": tx[:, 0:1], \"y\": tx[:, 1:2]}\n\ndef gen_label_batch(input_batch):\n    return {\n        \"continuity\": np.zeros([cfg_t.TRAIN.batch_size.pde, 1], dtype),\n        \"momentum_x\": np.zeros([cfg_t.TRAIN.batch_size.pde, 1], dtype),\n        \"momentum_y\": np.zeros([cfg_t.TRAIN.batch_size.pde, 1], dtype),\n    }\n\npde_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"ContinuousNamedArrayDataset\",\n            \"input\": gen_input_batch,\n            \"label\": gen_label_batch,\n        },\n    },\n    output_expr=equation[\"NavierStokes\"].equations,\n    loss=ppsci.loss.MSELoss(\"mean\"),\n    name=\"PDE\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u6570\u636e\u96c6\u914d\u7f6e\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u6784\u5efa\u8f93\u5165\u6570\u636e\uff0c\u6b64\u5904\u586b\u5165\u8f93\u5165\u6570\u636e\u548c\u6807\u7b7e\u6570\u636e\u7684\u6784\u9020\u51fd\u6570\u51fd\u6570 <code>gen_input_batch</code> \u548c <code>gen_label_batch</code>\uff0c\u4ee5\u53ca\u6570\u636e\u96c6\u7684\u540d\u79f0 <code>ContinuousNamedArrayDataset</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"NavierStokes\"].equations</code>\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684 <code>MSE</code> \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u5e73\u5747\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"PDE\" \u5373\u53ef\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#352","title":"3.5.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u5c06\u4e0a\u8fb9\u754c\u7684\u6807\u7b7e\u6570\u636e\u6309\u7167\u4e0a\u8ff0\u5bf9\u5e94\u516c\u5f0f\u8fdb\u884c\u5904\u7406\uff0c\u5176\u4f59\u70b9\u7684\u6807\u7b7e\u6570\u636e\u8bbe\u7f6e\u4e3a 0\u3002\u7136\u540e\u7ee7\u7eed\u6784\u5efa\u65b9\u8154\u8fb9\u754c\u7684 Dirichlet \u7ea6\u675f\uff0c\u6211\u4eec\u4ecd\u7136\u4f7f\u7528 <code>SupervisedConstraint</code> \u7c7b\u3002</p> <pre><code># set boundary conditions\nx_bc = sample_points_on_square_boundary(\n    cfg_t.TRAIN.batch_size.bc, eps=0.0\n).astype(\n    dtype\n)  # avoid singularity a right corner for u velocity\nv_bc = np.zeros((cfg_t.TRAIN.batch_size.bc * 4, 1), dtype)\nu_bc = copy.deepcopy(v_bc)\nlid_bc_fn = lambda x: 1 - np.cosh(50 * (x - 0.5)) / np.cosh(50 * 0.5)\nu_bc[: cfg_t.TRAIN.batch_size.bc] = lid_bc_fn(\n    x_bc[: cfg_t.TRAIN.batch_size.bc, 0:1]\n)\nbc = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableNamedArrayDataset\",\n            \"input\": {\n                \"x\": x_bc[:, 0:1],\n                \"y\": x_bc[:, 1:2],\n            },\n            \"label\": {\"u\": u_bc, \"v\": v_bc},\n        },\n    },\n    output_expr={\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n    loss=ppsci.loss.MSELoss(\"mean\"),\n    name=\"BC\",\n)\n</code></pre> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u3001\u8fb9\u754c\u7ea6\u675f\u3001\u521d\u503c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    pde_constraint.name: pde_constraint,\n    bc.name: bc,\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#36","title":"3.6 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u5206\u522b\u5728 Re=100, 400, 1000, 1600, 3200 \u4e0a\u8bad\u7ec3 10, 20, 50, 50, 500 \u8f6e\uff0c\u6bcf\u8f6e\u8fed\u4ee3\u6b21\u6570\u4e3a 1000\uff0c</p> <pre><code># working conditions\nRe: [100, 400, 1000, 1600, 3200]\nepochs: [10, 20, 50, 50, 500]\n</code></pre> <p>\u5176\u6b21\uff0c\u8bbe\u7f6e\u5408\u9002\u7684\u5b66\u4e60\u7387\u8870\u51cf\u7b56\u7565\uff0c</p> <pre><code># training settings\nTRAIN:\n  epochs: 10\n  iters_per_epoch: 1000\n  save_freq: 100\n  eval_during_train: true\n  eval_freq: 1\n  lr_scheduler:\n    epochs: ${sum:${epochs}}\n    iters_per_epoch: ${TRAIN.iters_per_epoch}\n    learning_rate: 1.0e-3\n    gamma: 0.9\n    decay_steps: 10000\n    warmup_epoch: 5\n    by_epoch: false\n</code></pre> <p>\u6700\u540e\uff0c\u8bbe\u7f6e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u635f\u5931\u81ea\u52a8\u5e73\u8861\u7b56\u7565\u4e3a <code>GradNorm</code>\uff0c</p> <pre><code>grad_norm:\n  update_freq: 1000\n  momentum: 0.9\n  init_weights: [10, 1, 1, 100, 100]\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#37","title":"3.7 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n    **cfg.TRAIN.lr_scheduler\n)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#38","title":"3.8 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\nxy_star = misc.cartesian_product(x_star, y_star).astype(dtype)\neval_data = {\"x\": xy_star[:, 0:1], \"y\": xy_star[:, 1:2]}\neval_label = {\"U\": U_ref.reshape([-1, 1])}\nU_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": eval_data,\n            \"label\": eval_label,\n        },\n        \"batch_size\": cfg_t.EVAL.batch_size,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    {\"U\": lambda out: (out[\"u\"] ** 2 + out[\"v\"] ** 2).sqrt()},\n    metric={\"L2Rel\": ppsci.metric.L2Rel()},\n    name=\"U_validator\",\n)\nvalidator = {U_validator.name: U_validator}\n</code></pre> <p>\u6b64\u5904\u8ba1\u7b97 \\(U=\\sqrt{u^2+v^2}\\) \u7684\u9884\u6d4b\u8bef\u5dee\uff1b</p> <p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u9009\u62e9 <code>ppsci.metric.L2Rel</code> \u5373\u53ef\uff1b</p> <p>\u5176\u4f59\u914d\u7f6e\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    optimizer=optimizer,\n    equation=equation,\n    validator=validator,\n    loss_aggregator=grad_norm,\n    cfg=cfg_t,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\npred_dict = solver.predict(\n    eval_data, batch_size=cfg_t.EVAL.batch_size, return_numpy=True\n)\nU_pred = np.sqrt(pred_dict[\"u\"] ** 2 + pred_dict[\"v\"] ** 2).reshape(\n    [len(x_star), len(y_star)]\n)\nplot(U_pred, cfg_t.output_dir)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"ldc_2d_Re3200_piratenet.py<pre><code>\"\"\"\nReference: https://github.com/PredictiveIntelligenceLab/jaxpi/tree/main/examples/ldc\n\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport os\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle\nimport scipy.io as sio\nfrom matplotlib import pyplot as plt\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.loss import mtl\nfrom ppsci.utils import misc\n\ndtype = paddle.get_default_dtype()\n\n\ndef plot(U_pred: np.ndarray, output_dir: str):\n    os.makedirs(output_dir, exist_ok=True)\n    fig_path = osp.join(output_dir, \"ac.png\")\n\n    fig = plt.figure()\n    plt.pcolor(U_pred.T, cmap=\"jet\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.colorbar()\n    plt.title(r\"Prediction of $U=\\sqrt{{u^2+v^2}}$\")\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    ppsci.utils.logger.info(f\"Saving figure to {fig_path}\")\n    plt.close()\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.PirateNet(**cfg.MODEL)\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n        **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n    grad_norm = mtl.GradNorm(\n        model,\n        5,\n        update_freq=cfg.TRAIN.grad_norm.update_freq,\n        momentum=cfg.TRAIN.grad_norm.momentum,\n        init_weights=list(cfg.TRAIN.grad_norm.init_weights),\n    )\n\n    def sample_points_on_square_boundary(num_pts_per_side, eps):\n        # Sample points along the top side (x=1 to x=0, y=1)\n        top_coords = np.linspace(0, 1, num_pts_per_side)\n        top = np.column_stack((top_coords, np.ones_like(top_coords)))\n\n        # Sample points along the bottom side (x=0 to x=1, y=0)\n        bottom_coords = np.linspace(0, 1, num_pts_per_side)\n        bottom = np.column_stack((bottom_coords, np.zeros_like(bottom_coords)))\n\n        # Sample points along the left side (x=0, y=1 to y=0)\n        left_coords = np.linspace(0, 1 - eps, num_pts_per_side)\n        left = np.column_stack((np.zeros_like(left_coords), left_coords))\n\n        # Sample points along the right side (x=1, y=0 to y=1)\n        right_coords = np.linspace(0, 1 - eps, num_pts_per_side)\n        right = np.column_stack((np.ones_like(right_coords), right_coords))\n\n        # Combine the points from all sides\n        points = np.vstack((top, bottom, left, right))\n\n        return points\n\n    def train_curriculum(cfg, idx):\n        cfg_t = copy.deepcopy(cfg)\n        Re = cfg_t.Re[idx]\n        cfg_t.output_dir = osp.join(cfg_t.output_dir, f\"Re_{int(Re)}\")\n        cfg_t.TRAIN.epochs = cfg_t.epochs[idx]\n        ppsci.utils.logger.message(\n            f\"Training curriculum {idx + 1}/{len(cfg_t.epochs)} Re={Re:.5g} epochs={cfg_t.epochs[idx]}\"\n        )\n\n        # set equation\n        equation = {\n            \"NavierStokes\": ppsci.equation.NavierStokes(1 / Re, 1, dim=2, time=False)\n        }\n\n        # load data\n        data = sio.loadmat(f\"./data/ldc_Re{Re}.mat\")\n        u_ref = data[\"u\"].astype(dtype)\n        v_ref = data[\"v\"].astype(dtype)\n        U_ref = np.sqrt(u_ref**2 + v_ref**2).reshape(-1, 1)\n        x_star = data[\"x\"].flatten().astype(dtype)\n        y_star = data[\"y\"].flatten().astype(dtype)\n        x0 = x_star[0]\n        x1 = x_star[-1]\n        y0 = y_star[0]\n        y1 = y_star[-1]\n\n        # set N-S pde constraint\n        def gen_input_batch():\n            tx = np.random.uniform(\n                [x0, y0],\n                [x1, y1],\n                (cfg_t.TRAIN.batch_size.pde, 2),\n            ).astype(dtype)\n            return {\"x\": tx[:, 0:1], \"y\": tx[:, 1:2]}\n\n        def gen_label_batch(input_batch):\n            return {\n                \"continuity\": np.zeros([cfg_t.TRAIN.batch_size.pde, 1], dtype),\n                \"momentum_x\": np.zeros([cfg_t.TRAIN.batch_size.pde, 1], dtype),\n                \"momentum_y\": np.zeros([cfg_t.TRAIN.batch_size.pde, 1], dtype),\n            }\n\n        pde_constraint = ppsci.constraint.SupervisedConstraint(\n            {\n                \"dataset\": {\n                    \"name\": \"ContinuousNamedArrayDataset\",\n                    \"input\": gen_input_batch,\n                    \"label\": gen_label_batch,\n                },\n            },\n            output_expr=equation[\"NavierStokes\"].equations,\n            loss=ppsci.loss.MSELoss(\"mean\"),\n            name=\"PDE\",\n        )\n\n        # set boundary conditions\n        x_bc = sample_points_on_square_boundary(\n            cfg_t.TRAIN.batch_size.bc, eps=0.0\n        ).astype(\n            dtype\n        )  # avoid singularity a right corner for u velocity\n        v_bc = np.zeros((cfg_t.TRAIN.batch_size.bc * 4, 1), dtype)\n        u_bc = copy.deepcopy(v_bc)\n        lid_bc_fn = lambda x: 1 - np.cosh(50 * (x - 0.5)) / np.cosh(50 * 0.5)\n        u_bc[: cfg_t.TRAIN.batch_size.bc] = lid_bc_fn(\n            x_bc[: cfg_t.TRAIN.batch_size.bc, 0:1]\n        )\n        bc = ppsci.constraint.SupervisedConstraint(\n            {\n                \"dataset\": {\n                    \"name\": \"IterableNamedArrayDataset\",\n                    \"input\": {\n                        \"x\": x_bc[:, 0:1],\n                        \"y\": x_bc[:, 1:2],\n                    },\n                    \"label\": {\"u\": u_bc, \"v\": v_bc},\n                },\n            },\n            output_expr={\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n            loss=ppsci.loss.MSELoss(\"mean\"),\n            name=\"BC\",\n        )\n        # wrap constraints together\n        constraint = {\n            pde_constraint.name: pde_constraint,\n            bc.name: bc,\n        }\n\n        # set validator\n        xy_star = misc.cartesian_product(x_star, y_star).astype(dtype)\n        eval_data = {\"x\": xy_star[:, 0:1], \"y\": xy_star[:, 1:2]}\n        eval_label = {\"U\": U_ref.reshape([-1, 1])}\n        U_validator = ppsci.validate.SupervisedValidator(\n            {\n                \"dataset\": {\n                    \"name\": \"NamedArrayDataset\",\n                    \"input\": eval_data,\n                    \"label\": eval_label,\n                },\n                \"batch_size\": cfg_t.EVAL.batch_size,\n            },\n            ppsci.loss.MSELoss(\"mean\"),\n            {\"U\": lambda out: (out[\"u\"] ** 2 + out[\"v\"] ** 2).sqrt()},\n            metric={\"L2Rel\": ppsci.metric.L2Rel()},\n            name=\"U_validator\",\n        )\n        validator = {U_validator.name: U_validator}\n\n        # initialize solver\n        solver = ppsci.solver.Solver(\n            model,\n            constraint,\n            optimizer=optimizer,\n            equation=equation,\n            validator=validator,\n            loss_aggregator=grad_norm,\n            cfg=cfg_t,\n        )\n        # train model\n        solver.train()\n        # evaluate after finished training\n        solver.eval()\n        # visualize prediction after finished training\n        pred_dict = solver.predict(\n            eval_data, batch_size=cfg_t.EVAL.batch_size, return_numpy=True\n        )\n        U_pred = np.sqrt(pred_dict[\"u\"] ** 2 + pred_dict[\"v\"] ** 2).reshape(\n            [len(x_star), len(y_star)]\n        )\n        plot(U_pred, cfg_t.output_dir)\n\n    for idx in range(len(cfg.Re)):\n        train_curriculum(cfg, idx)\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.PirateNet(**cfg.MODEL)\n\n    data = sio.loadmat(cfg.EVAL_DATA_PATH)\n    data = dict(data)\n    u_ref = data[\"u\"].astype(dtype)\n    v_ref = data[\"v\"].astype(dtype)\n    U_ref = np.sqrt(u_ref**2 + v_ref**2).reshape(-1, 1)\n    x_star = data[\"x\"].flatten().astype(dtype)  # [nx, ]\n    y_star = data[\"y\"].flatten().astype(dtype)  # [ny, ]\n\n    # set validator\n    xy_star = misc.cartesian_product(x_star, y_star).astype(dtype)\n    eval_data = {\"x\": xy_star[:, 0:1], \"y\": xy_star[:, 1:2]}\n    eval_label = {\"U\": U_ref.reshape([-1, 1])}\n    U_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": eval_data,\n                \"label\": eval_label,\n            },\n            \"batch_size\": cfg.EVAL.batch_size,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"U\": lambda out: (out[\"u\"] ** 2 + out[\"v\"] ** 2).sqrt()},\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"U_validator\",\n    )\n    validator = {U_validator.name: U_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        validator=validator,\n        cfg=cfg,\n    )\n\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    pred_dict = solver.predict(\n        eval_data, batch_size=cfg.EVAL.batch_size, return_numpy=True\n    )\n    U_pred = np.sqrt(pred_dict[\"u\"] ** 2 + pred_dict[\"v\"] ** 2).reshape(\n        [len(x_star), len(y_star)]\n    )\n    # plot\n    plot(U_pred, cfg.output_dir)\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.PirateNet(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(model, cfg=cfg)\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path, with_onnx=False)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n    data = sio.loadmat(cfg.EVAL_DATA_PATH)\n    data = dict(data)\n    x_star = data[\"x\"].flatten().astype(dtype)  # [nx, ]\n    y_star = data[\"y\"].flatten().astype(dtype)  # [ny, ]\n    xy_star = misc.cartesian_product(x_star, y_star).astype(dtype)\n    input_dict = {\"x\": xy_star[:, 0:1], \"y\": xy_star[:, 1:2]}\n\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n    U_pred = np.sqrt(output_dict[\"u\"] ** 2 + output_dict[\"v\"] ** 2).reshape(\n        [len(x_star), len(y_star)]\n    )\n    plot(U_pred, cfg.output_dir)\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"ldc_2d_Re3200_piratenet.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u65b9\u5c55\u793a\u4e86\u6a21\u578b\u5bf9\u4e8e\u8fb9\u957f\u4e3a 1 \u7684\u6b63\u65b9\u5f62\u8ba1\u7b97\u57df\u7684\u5185\u90e8\u70b9\u8fdb\u884c\u9884\u6d4b\u7684\u7ed3\u679c \\(U=\\sqrt{u^2+v^2}\\)\u3002</p> Re=1000Re=3200 <p> </p> <p>\u53ef\u4ee5\u770b\u5230\u5728 \\(Re=1000\\) \u4e0b\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u6c42\u89e3\u5668\u7684\u7ed3\u679c\u57fa\u672c\u76f8\u540c\uff08L2 \u76f8\u5bf9\u8bef\u5dee\u4e3a 7.7%\uff09\u3002</p> <p> </p> <p>\u53ef\u4ee5\u770b\u5230\u5728 \\(Re=3200\\) \u4e0b\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u6c42\u89e3\u5668\u7684\u7ed3\u679c\u57fa\u672c\u76f8\u540c\uff08L2 \u76f8\u5bf9\u8bef\u5dee\u4e3a 4.1%\uff09\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_steady/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>PIRATENETS: PHYSICS-INFORMED DEEP LEARNING WITHRESIDUAL ADAPTIVE NETWORKS</li> <li>jaxpi LDC example</li> </ul>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","Navier-Stokes\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/ldc2d_unsteady/","title":"LDC2D_unsteady","text":""},{"location":"zh/examples/ldc2d_unsteady/#2d-ldc2d-lid-driven-cavity-flow","title":"2D-LDC(2D Lid Driven Cavity Flow)","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code>python ldc2d_unsteady_Re10.py\n</code></pre> <pre><code>python ldc2d_unsteady_Re10.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/ldc2d_unsteady_Re10/ldc2d_unsteady_Re10_pretrained.pdparams\n</code></pre> <pre><code>python ldc2d_unsteady_Re10.py mode=export\n</code></pre> <pre><code>python ldc2d_unsteady_Re10.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 ldc2d_unsteady_Re10_pretrained.pdparams loss(Residual): 155652.67530MSE.momentum_x(Residual): 6.78030MSE.continuity(Residual): 0.16590MSE.momentum_y(Residual): 12.05981"},{"location":"zh/examples/ldc2d_unsteady/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u9876\u76d6\u65b9\u8154\u9a71\u52a8\u6d41LDC\u95ee\u9898\u5728\u8bb8\u591a\u9886\u57df\u4e2d\u90fd\u6709\u5e94\u7528\u3002\u4f8b\u5982\uff0c\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u7528\u4e8e\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\uff08CFD\uff09\u9886\u57df\u4e2d\u9a8c\u8bc1\u8ba1\u7b97\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u867d\u7136\u8fd9\u4e2a\u95ee\u9898\u7684\u8fb9\u754c\u6761\u4ef6\u76f8\u5bf9\u7b80\u5355\uff0c\u4f46\u662f\u5176\u6d41\u52a8\u7279\u6027\u5374\u975e\u5e38\u590d\u6742\u3002\u5728\u9876\u76d6\u9a71\u52a8\u6d41LDC\u4e2d\uff0c\u9876\u58c1\u671dx\u65b9\u5411\u4ee5U=1\u7684\u901f\u5ea6\u79fb\u52a8\uff0c\u800c\u5176\u4ed6\u4e09\u4e2a\u58c1\u5219\u88ab\u5b9a\u4e49\u4e3a\u65e0\u6ed1\u79fb\u8fb9\u754c\u6761\u4ef6\uff0c\u5373\u901f\u5ea6\u4e3a\u96f6\u3002</p> <p>\u6b64\u5916\uff0c\u9876\u76d6\u65b9\u8154\u9a71\u52a8\u6d41LDC\u95ee\u9898\u4e5f\u88ab\u7528\u4e8e\u7814\u7a76\u548c\u9884\u6d4b\u7a7a\u6c14\u52a8\u529b\u5b66\u4e2d\u7684\u6d41\u52a8\u73b0\u8c61\u3002\u4f8b\u5982\uff0c\u5728\u6c7d\u8f66\u5de5\u4e1a\u4e2d\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u5206\u6790\u8f66\u4f53\u5185\u90e8\u7684\u7a7a\u6c14\u6d41\u52a8\uff0c\u53ef\u4ee5\u5e2e\u52a9\u4f18\u5316\u8f66\u8f86\u7684\u8bbe\u8ba1\u548c\u6027\u80fd\u3002</p> <p>\u603b\u7684\u6765\u8bf4\uff0c\u9876\u76d6\u65b9\u8154\u9a71\u52a8\u6d41LDC\u95ee\u9898\u5728\u8ba1\u7b97\u6d41\u4f53\u529b\u5b66\u3001\u7a7a\u6c14\u52a8\u529b\u5b66\u4ee5\u53ca\u76f8\u5173\u9886\u57df\u4e2d\u90fd\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u5bf9\u4e8e\u7814\u7a76\u548c\u9884\u6d4b\u6d41\u52a8\u73b0\u8c61\u3001\u4f18\u5316\u4ea7\u54c1\u8bbe\u8ba1\u7b49\u65b9\u9762\u90fd\u8d77\u5230\u4e86\u91cd\u8981\u7684\u4f5c\u7528\u3002</p>"},{"location":"zh/examples/ldc2d_unsteady/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u6848\u4f8b\u4e2d\u6211\u4eec\u5bf9\u4e8e 16 \u4e2a\u65f6\u523b\u5185\u957f\u5bbd\u5747\u4e3a 1 \u7684\u65b9\u8154\u5185\u90e8\u4f5c\u4e3a\u8ba1\u7b97\u57df\uff0c\u5e76\u5e94\u7528\u4ee5\u4e0b\u516c\u5f0f\u8fdb\u884c\u9876\u76d6\u9a71\u52a8\u65b9\u8154\u6d41\u7814\u7a76\u77ac\u6001\u6d41\u573a\u95ee\u9898\uff1a</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial v}{\\partial y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial u}{\\partial t} + u\\dfrac{\\partial u}{\\partial x} + v\\dfrac{\\partial u}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial x} + \\nu(\\dfrac{\\partial ^2 u}{\\partial x ^2} + \\dfrac{\\partial ^2 u}{\\partial y ^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial v}{\\partial t} + u\\dfrac{\\partial v}{\\partial x} + v\\dfrac{\\partial v}{\\partial y} = -\\dfrac{1}{\\rho}\\dfrac{\\partial p}{\\partial y} + \\nu(\\dfrac{\\partial ^2 v}{\\partial x ^2} + \\dfrac{\\partial ^2 v}{\\partial y ^2}) \\] <p>\u4ee4\uff1a</p> <p>\\(t^* = \\dfrac{L}{U_0}\\)</p> <p>\\(x^*=y^* = L\\)</p> <p>\\(u^*=v^* = U_0\\)</p> <p>\\(p^* = \\rho {U_0}^2\\)</p> <p>\u5b9a\u4e49\uff1a</p> <p>\u65e0\u91cf\u7eb2\u65f6\u95f4 \\(\\tau = \\dfrac{t}{t^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u5750\u6807 \\(x\uff1aX = \\dfrac{x}{x^*}\\)\uff1b\u65e0\u91cf\u7eb2\u5750\u6807 \\(y\uff1aY = \\dfrac{y}{y^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u901f\u5ea6 \\(x\uff1aU = \\dfrac{u}{u^*}\\)\uff1b\u65e0\u91cf\u7eb2\u901f\u5ea6 \\(y\uff1aV = \\dfrac{v}{u^*}\\)</p> <p>\u65e0\u91cf\u7eb2\u538b\u529b \\(P = \\dfrac{p}{p^*}\\)</p> <p>\u96f7\u8bfa\u6570 \\(Re = \\dfrac{L U_0}{\\nu}\\)</p> <p>\u5219\u53ef\u83b7\u5f97\u5982\u4e0b\u65e0\u91cf\u7eb2Navier-Stokes\u65b9\u7a0b\uff0c\u65bd\u52a0\u4e8e\u65b9\u8154\u5185\u90e8\uff1a</p> <p>\u8d28\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial U}{\\partial X} + \\dfrac{\\partial U}{\\partial Y} = 0 \\] <p>\\(x\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial U}{\\partial \\tau} + U\\dfrac{\\partial U}{\\partial X} + V\\dfrac{\\partial U}{\\partial Y} = -\\dfrac{\\partial P}{\\partial X} + \\dfrac{1}{Re}(\\dfrac{\\partial ^2 U}{\\partial X^2} + \\dfrac{\\partial ^2 U}{\\partial Y^2}) \\] <p>\\(y\\) \u52a8\u91cf\u5b88\u6052\uff1a</p> \\[ \\dfrac{\\partial V}{\\partial \\tau} + U\\dfrac{\\partial V}{\\partial X} + V\\dfrac{\\partial V}{\\partial Y} = -\\dfrac{\\partial P}{\\partial Y} + \\dfrac{1}{Re}(\\dfrac{\\partial ^2 V}{\\partial X^2} + \\dfrac{\\partial ^2 V}{\\partial Y^2}) \\] <p>\u5bf9\u4e8e\u65b9\u8154\u8fb9\u754c\uff0c\u5219\u9700\u65bd\u52a0 Dirichlet \u8fb9\u754c\u6761\u4ef6\uff1a</p> <p>\u4e0a\u8fb9\u754c\uff1a</p> \\[ u=1, v=0 \\] <p>\u4e0b\u8fb9\u754c</p> \\[ u=0, v=0 \\] <p>\u5de6\u8fb9\u754c\uff1a</p> \\[ u=0, v=0 \\] <p>\u53f3\u8fb9\u754c\uff1a</p> \\[ u=0, v=0 \\]"},{"location":"zh/examples/ldc2d_unsteady/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/ldc2d_unsteady/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 2D-LDC \u95ee\u9898\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u5df2\u77e5\u7684\u5750\u6807\u70b9 \\((t, x, y)\\) \u90fd\u6709\u81ea\u8eab\u7684\u6a2a\u5411\u901f\u5ea6 \\(u\\)\u3001\u7eb5\u5411\u901f\u5ea6 \\(v\\)\u3001\u538b\u529b \\(p\\) \u4e09\u4e2a\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((t, x, y)\\) \u5230 \\((u, v, p)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(f: \\mathbb{R}^3 \\to \\mathbb{R}^3\\) \uff0c\u5373\uff1a</p> \\[ u, v, p = f(t, x, y) \\] <p>\u4e0a\u5f0f\u4e2d \\(f\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>[\"t\", \"x\", \"y\"]</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>[\"u\", \"v\", \"p\"]</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\u4ee5\u53ca\u6fc0\u6d3b\u51fd\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 9 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 50\uff0c\u4f7f\u7528 \"tanh\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>"},{"location":"zh/examples/ldc2d_unsteady/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e 2D-LDC \u4f7f\u7528\u7684\u662f Navier-Stokes \u65b9\u7a0b\u76842\u7ef4\u77ac\u6001\u5f62\u5f0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>NavierStokes</code>\u3002</p> <pre><code># set equation\nequation = {\"NavierStokes\": ppsci.equation.NavierStokes(cfg.NU, cfg.RHO, 2, True)}\n</code></pre> <p>\u5728\u5b9e\u4f8b\u5316 <code>NavierStokes</code> \u7c7b\u65f6\u9700\u6307\u5b9a\u5fc5\u8981\u7684\u53c2\u6570\uff1a\u52a8\u529b\u7c98\u5ea6 \\(\\nu=0.01\\), \u6d41\u4f53\u5bc6\u5ea6 \\(\\rho=1.0\\)\u3002</p>"},{"location":"zh/examples/ldc2d_unsteady/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d 2D-LDC \u95ee\u9898\u4f5c\u7528\u5728\u4ee5 [-0.05, -0.05], [0.05, 0.05] \u4e3a\u5bf9\u89d2\u7ebf\u7684\u4e8c\u7ef4\u77e9\u5f62\u533a\u57df\uff0c\u4e14\u65f6\u95f4\u57df\u4e3a 16 \u4e2a\u65f6\u523b [0.0, 0.1, ..., 1.4, 1.5]\uff0c \u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u7a7a\u95f4\u51e0\u4f55 <code>Rectangle</code> \u548c\u65f6\u95f4\u57df <code>TimeDomain</code>\uff0c\u7ec4\u5408\u6210\u65f6\u95f4-\u7a7a\u95f4\u7684 <code>TimeXGeometry</code> \u8ba1\u7b97\u57df\u3002</p> <pre><code># set timestamps(including initial t0)\ntimestamps = np.linspace(0.0, 1.5, cfg.NTIME_ALL, endpoint=True)\n# set time-geometry\ngeom = {\n    \"time_rect\": ppsci.geometry.TimeXGeometry(\n        ppsci.geometry.TimeDomain(0.0, 1.5, timestamps=timestamps),\n        ppsci.geometry.Rectangle((-0.05, -0.05), (0.05, 0.05)),\n    )\n}\n</code></pre> \u63d0\u793a <p><code>Rectangle</code> \u548c <code>TimeDomain</code> \u662f\u4e24\u79cd\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\u7684 <code>Geometry</code> \u6d3e\u751f\u7c7b\u3002</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e8e\u4e8c\u7ef4\u77e9\u5f62\u51e0\u4f55\u57df\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.Rectangle(...)</code> \u521b\u5efa\u7a7a\u95f4\u51e0\u4f55\u57df\u5bf9\u8c61\uff1b</p> <p>\u5982\u8f93\u5165\u6570\u636e\u53ea\u6765\u81ea\u4e00\u7ef4\u65f6\u95f4\u57df\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ppsci.geometry.TimeDomain(...)</code> \u6784\u5efa\u65f6\u95f4\u57df\u5bf9\u8c61\u3002</p>"},{"location":"zh/examples/ldc2d_unsteady/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u6839\u636e 2. \u95ee\u9898\u5b9a\u4e49 \u5f97\u5230\u7684\u65e0\u91cf\u7eb2\u516c\u5f0f\u548c\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u5bf9\u5e94\u4e86\u5728\u8ba1\u7b97\u57df\u4e2d\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u7684\u4e24\u4e2a\u7ea6\u675f\u6761\u4ef6\uff0c\u5373\uff1a</p> <ol> <li> <p>\u65bd\u52a0\u5728\u77e9\u5f62\u5185\u90e8\u70b9\u4e0a\u7684\u65e0\u91cf\u7eb2 Navier-Stokes \u65b9\u7a0b\u7ea6\u675f\uff08\u7ecf\u8fc7\u7b80\u5355\u79fb\u9879\uff09</p> \\[ \\dfrac{\\partial U}{\\partial X} + \\dfrac{\\partial U}{\\partial Y} = 0 \\] \\[ \\dfrac{\\partial U}{\\partial \\tau} + U\\dfrac{\\partial U}{\\partial X} + V\\dfrac{\\partial U}{\\partial Y} + \\dfrac{\\partial P}{\\partial X} - \\dfrac{1}{Re}(\\dfrac{\\partial ^2 U}{\\partial X^2} + \\dfrac{\\partial ^2 U}{\\partial Y^2}) = 0 \\] \\[ \\dfrac{\\partial V}{\\partial \\tau} + U\\dfrac{\\partial V}{\\partial X} + V\\dfrac{\\partial V}{\\partial Y} + \\dfrac{\\partial P}{\\partial Y} - \\dfrac{1}{Re}(\\dfrac{\\partial ^2 V}{\\partial X^2} + \\dfrac{\\partial ^2 V}{\\partial Y^2}) = 0 \\] <p>\u4e3a\u4e86\u65b9\u4fbf\u83b7\u53d6\u4e2d\u95f4\u53d8\u91cf\uff0c<code>NavierStokes</code> \u7c7b\u5185\u90e8\u5c06\u4e0a\u5f0f\u5de6\u4fa7\u7684\u7ed3\u679c\u5206\u522b\u547d\u540d\u4e3a <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code>\u3002</p> </li> <li> <p>\u65bd\u52a0\u5728\u77e9\u5f62\u4e0a\u3001\u4e0b\u3001\u5de6\u3001\u53f3\u8fb9\u754c\u4e0a\u7684 Dirichlet \u8fb9\u754c\u6761\u4ef6\u7ea6\u675f</p> \\[ \u4e0a\u8fb9\u754c\uff1au=1,v=0 \\] \\[ \u4e0b\u8fb9\u754c\uff1au=0,v=0 \\] \\[ \u5de6\u8fb9\u754c\uff1au=0,v=0 \\] \\[ \u53f3\u8fb9\u754c\uff1au=0,v=0 \\] </li> </ol> <p>\u63a5\u4e0b\u6765\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>InteriorConstraint</code> \u548c <code>BoundaryConstraint</code> \u6784\u5efa\u4e0a\u8ff0\u4e24\u79cd\u7ea6\u675f\u6761\u4ef6\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u79cd\u7ea6\u675f\u6307\u5b9a\u91c7\u6837\u70b9\u4e2a\u6570\uff0c\u8fd9\u8868\u793a\u67d0\u79cd\u7ea6\u675f\u5728\u5176\u5bf9\u5e94\u8ba1\u7b97\u57df\u5185\u91c7\u6837\u6570\u636e\u7684\u6570\u91cf\uff0c\u4ee5\u53ca\u6307\u5b9a\u901a\u7528\u7684\u91c7\u6837\u914d\u7f6e\u3002</p> <pre><code># set dataloader config\ntrain_dataloader_cfg = {\n    \"dataset\": \"IterableNamedArrayDataset\",\n    \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n}\n\n# pde/bc constraint use t1~tn, initial constraint use t0\nNPOINT_PDE, NTIME_PDE = 99**2, cfg.NTIME_ALL - 1\nNPOINT_TOP, NTIME_TOP = 101, cfg.NTIME_ALL - 1\nNPOINT_DOWN, NTIME_DOWN = 101, cfg.NTIME_ALL - 1\nNPOINT_LEFT, NTIME_LEFT = 99, cfg.NTIME_ALL - 1\nNPOINT_RIGHT, NTIME_RIGHT = 99, cfg.NTIME_ALL - 1\nNPOINT_IC, NTIME_IC = 99**2, 1\n</code></pre>"},{"location":"zh/examples/ldc2d_unsteady/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u77e9\u5f62\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set constraint\npde = ppsci.constraint.InteriorConstraint(\n    equation[\"NavierStokes\"].equations,\n    {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n    geom[\"time_rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_PDE * NTIME_PDE},\n    ppsci.loss.MSELoss(\"sum\"),\n    evenly=True,\n    weight_dict=cfg.TRAIN.weight.pde,  # (1)\n    name=\"EQ\",\n)\n</code></pre> <ol> <li>\u672c\u6848\u4f8b\u4e2dPDE\u7ea6\u675f\u635f\u5931\u7684\u6570\u91cf\u7ea7\u8fdc\u5927\u4e8e\u8fb9\u754c\u7ea6\u675f\u635f\u5931\uff0c\u56e0\u6b64\u9700\u8981\u7ed9PDE\u7ea6\u675f\u6743\u91cd\u8bbe\u7f6e\u4e00\u4e2a\u8f83\u5c0f\u7684\u503c\uff0c\u6709\u5229\u4e8e\u6a21\u578b\u6536\u655b</li> </ol> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"NavierStokes\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u6211\u4eec\u5e0c\u671b Navier-Stokes \u65b9\u7a0b\u4ea7\u751f\u7684\u4e09\u4e2a\u4e2d\u95f4\u7ed3\u679c <code>continuity</code>, <code>momentum_x</code>, <code>momentum_y</code> \u88ab\u4f18\u5316\u81f3 0\uff0c\u56e0\u6b64\u5c06\u5b83\u4eec\u7684\u76ee\u6807\u503c\u5168\u90e8\u8bbe\u4e3a 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"time_rect\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u6211\u4eec\u4f7f\u7528\u5168\u91cf\u6570\u636e\u70b9\u8bad\u7ec3\uff0c\u56e0\u6b64 <code>dataset</code> \u5b57\u6bb5\u8bbe\u7f6e\u4e3a \"IterableNamedArrayDataset\" \u4e14 <code>iters_per_epoch</code> \u4e5f\u8bbe\u7f6e\u4e3a 1\uff0c\u91c7\u6837\u70b9\u6570 <code>batch_size</code> \u8bbe\u4e3a 9801 * 15(\u8868\u793a99x99\u7684\u7b49\u95f4\u9694\u7f51\u683c\uff0c\u5171\u6709 15 \u4e2a\u65f6\u523b\u7684\u7f51\u683c)\uff1b</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"sum\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u9009\u62e9\u662f\u5426\u5728\u8ba1\u7b97\u57df\u4e0a\u8fdb\u884c\u7b49\u95f4\u9694\u91c7\u6837\uff0c\u6b64\u5904\u6211\u4eec\u9009\u62e9\u5f00\u542f\u7b49\u95f4\u9694\u91c7\u6837\uff0c\u8fd9\u6837\u80fd\u8ba9\u8bad\u7ec3\u70b9\u5747\u5300\u5206\u5e03\u5728\u8ba1\u7b97\u57df\u4e0a\uff0c\u6709\u5229\u4e8e\u8bad\u7ec3\u6536\u655b\uff1b</p> <p>\u7b2c\u4e03\u4e2a\u53c2\u6570\u662f\u6743\u91cd\u7cfb\u6570\uff0c\u8be5\u914d\u7f6e\u53ef\u4ee5\u7cbe\u786e\u8c03\u6574\u6bcf\u4e00\u4e2a\u53d8\u91cf\u53c2\u4e0e\u635f\u5931\u8ba1\u7b97\u65f6\u7684\u6743\u91cd\uff0c\u8bbe\u7f6e\u4e3a 0.0001 \u662f\u4e00\u4e2a\u6bd4\u8f83\u5408\u9002\u7684\u503c\uff1b</p> <p>\u7b2c\u516b\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"EQ\" \u5373\u53ef\u3002</p>"},{"location":"zh/examples/ldc2d_unsteady/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u540c\u7406\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u6784\u5efa\u77e9\u5f62\u7684\u4e0a\u3001\u4e0b\u3001\u5de6\u3001\u53f3\u56db\u4e2a\u8fb9\u754c\u7684 Dirichlet \u8fb9\u754c\u7ea6\u675f\u3002\u4f46\u4e0e\u6784\u5efa <code>InteriorConstraint</code> \u7ea6\u675f\u4e0d\u540c\u7684\u662f\uff0c\u7531\u4e8e\u4f5c\u7528\u533a\u57df\u662f\u8fb9\u754c\uff0c\u56e0\u6b64\u6211\u4eec\u4f7f\u7528 <code>BoundaryConstraint</code> \u7c7b\u3002</p> <p>\u5176\u6b21\u7ea6\u675f\u7684\u76ee\u6807\u53d8\u91cf\u4e5f\u4e0d\u540c\uff0cDirichlet \u6761\u4ef6\u7ea6\u675f\u5bf9\u8c61\u662f MLP \u6a21\u578b\u8f93\u51fa\u7684 \\(u\\) \u548c \\(v\\)\uff08\u672c\u6587\u4e0d\u5bf9 \\(p\\) \u505a\u7ea6\u675f\uff09\uff0c\u56e0\u6b64\u7b2c\u4e00\u4e2a\u53c2\u6570\u4f7f\u7528 lambda \u8868\u8fbe\u5f0f\u76f4\u63a5\u8fd4\u56de MLP \u7684\u8f93\u51fa\u7ed3\u679c <code>out[\"u\"]</code> \u548c <code>out[\"v\"]</code> \u4f5c\u4e3a\u7a0b\u5e8f\u8fd0\u884c\u65f6\u7684\u7ea6\u675f\u5bf9\u8c61\u3002</p> <p>\u7136\u540e\u7ed9 \\(u\\) \u548c \\(v\\) \u8bbe\u7f6e\u7ea6\u675f\u76ee\u6807\u503c\uff0c\u8bf7\u6ce8\u610f\u5728 <code>bc_top</code> \u4e0a\u8fb9\u754c\u4e2d\uff0c\\(u\\) \u7684\u7ea6\u675f\u76ee\u6807\u503c\u8981\u8bbe\u7f6e\u4e3a 1\u3002</p> <p>\u91c7\u6837\u70b9\u4e0e\u635f\u5931\u51fd\u6570\u914d\u7f6e\u548c <code>InteriorConstraint</code> \u7c7b\u4f3c\uff0c\u5355\u4e2a\u65f6\u523b\u7684\u70b9\u6570\u8bbe\u7f6e\u4e3a 100 \u5de6\u53f3\u3002</p> <p>\u7531\u4e8e <code>BoundaryConstraint</code> \u9ed8\u8ba4\u4f1a\u5728\u6240\u6709\u8fb9\u754c\u4e0a\u8fdb\u884c\u91c7\u6837\uff0c\u800c\u6211\u4eec\u9700\u8981\u5bf9\u56db\u4e2a\u8fb9\u754c\u5206\u522b\u65bd\u52a0\u7ea6\u675f\uff0c\u56e0\u6b64\u9700\u901a\u8fc7\u8bbe\u7f6e <code>criteria</code> \u53c2\u6570\uff0c\u8fdb\u4e00\u6b65\u7ec6\u5316\u51fa\u56db\u4e2a\u8fb9\u754c\uff0c\u5982\u4e0a\u8fb9\u754c\u5c31\u662f\u7b26\u5408 \\(y = 0.05\\) \u7684\u8fb9\u754c\u70b9\u96c6</p> <pre><code>bc_top = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n    {\"u\": 1, \"v\": 0},\n    geom[\"time_rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_TOP * NTIME_TOP},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda t, x, y: np.isclose(y, 0.05),\n    name=\"BC_top\",\n)\nbc_down = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n    {\"u\": 0, \"v\": 0},\n    geom[\"time_rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_DOWN * NTIME_DOWN},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda t, x, y: np.isclose(y, -0.05),\n    name=\"BC_down\",\n)\nbc_left = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n    {\"u\": 0, \"v\": 0},\n    geom[\"time_rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_LEFT * NTIME_LEFT},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda t, x, y: np.isclose(x, -0.05),\n    name=\"BC_left\",\n)\nbc_right = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n    {\"u\": 0, \"v\": 0},\n    geom[\"time_rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_RIGHT * NTIME_RIGHT},\n    ppsci.loss.MSELoss(\"sum\"),\n    criteria=lambda t, x, y: np.isclose(x, 0.05),\n    name=\"BC_right\",\n)\n</code></pre>"},{"location":"zh/examples/ldc2d_unsteady/#343","title":"3.4.3 \u521d\u503c\u7ea6\u675f","text":"<p>\u6700\u540e\u6211\u4eec\u8fd8\u9700\u8981\u5bf9 \\(t=t_0\\) \u65f6\u523b\u7684\u77e9\u5f62\u5185\u90e8\u70b9\u65bd\u52a0 N-S \u65b9\u7a0b\u7ea6\u675f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>ic = ppsci.constraint.InitialConstraint(\n    {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n    {\"u\": 0, \"v\": 0},\n    geom[\"time_rect\"],\n    {**train_dataloader_cfg, \"batch_size\": NPOINT_IC * NTIME_IC},\n    ppsci.loss.MSELoss(\"sum\"),\n    evenly=True,\n    name=\"IC\",\n)\n</code></pre> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u3001\u8fb9\u754c\u7ea6\u675f\u3001\u521d\u503c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    pde.name: pde,\n    bc_top.name: bc_top,\n    bc_down.name: bc_down,\n    bc_left.name: bc_left,\n    bc_right.name: bc_right,\n    ic.name: ic,\n}\n</code></pre>"},{"location":"zh/examples/ldc2d_unsteady/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528\u4e24\u4e07\u8f6e\u8bad\u7ec3\u8f6e\u6570\u548c\u5e26\u6709 warmup \u7684 Cosine \u4f59\u5f26\u8870\u51cf\u5b66\u4e60\u7387\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 20000\n  iters_per_epoch: 1\n</code></pre>"},{"location":"zh/examples/ldc2d_unsteady/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>"},{"location":"zh/examples/ldc2d_unsteady/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.GeometryValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\nNPOINT_EVAL = NPOINT_PDE * cfg.NTIME_ALL\nresidual_validator = ppsci.validate.GeometryValidator(\n    equation[\"NavierStokes\"].equations,\n    {\"momentum_x\": 0, \"continuity\": 0, \"momentum_y\": 0},\n    geom[\"time_rect\"],\n    {\n        \"dataset\": \"NamedArrayDataset\",\n        \"total_size\": NPOINT_EVAL,\n        \"batch_size\": cfg.EVAL.batch_size.residual_validator,\n        \"sampler\": {\"name\": \"BatchSampler\"},\n    },\n    ppsci.loss.MSELoss(\"sum\"),\n    evenly=True,\n    metric={\"MSE\": ppsci.metric.MSE()},\n    with_initial=True,\n    name=\"Residual\",\n)\nvalidator = {residual_validator.name: residual_validator}\n</code></pre> <p>\u65b9\u7a0b\u8bbe\u7f6e\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u76f8\u540c\uff0c\u8868\u793a\u5982\u4f55\u8ba1\u7b97\u6240\u9700\u8bc4\u4f30\u7684\u76ee\u6807\u53d8\u91cf\uff1b</p> <p>\u6b64\u5904\u6211\u4eec\u4e3a <code>momentum_x</code>, <code>continuity</code>, <code>momentum_y</code> \u4e09\u4e2a\u76ee\u6807\u53d8\u91cf\u8bbe\u7f6e\u6807\u7b7e\u503c\u4e3a 0\uff1b</p> <p>\u8ba1\u7b97\u57df\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u76f8\u540c\uff0c\u8868\u793a\u5728\u6307\u5b9a\u8ba1\u7b97\u57df\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff1b</p> <p>\u91c7\u6837\u70b9\u914d\u7f6e\u5219\u9700\u8981\u6307\u5b9a\u603b\u7684\u8bc4\u4f30\u70b9\u6570 <code>total_size</code>\uff0c\u6b64\u5904\u6211\u4eec\u8bbe\u7f6e\u4e3a 9801 * 16(99x99\u7b49\u95f4\u9694\u7f51\u683c\uff0c\u5171 16 \u4e2a\u8bc4\u4f30\u65f6\u523b)\uff1b</p> <p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u9009\u62e9 <code>ppsci.metric.MSE</code> \u5373\u53ef\uff1b</p> <p>\u5176\u4f59\u914d\u7f6e\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>"},{"location":"zh/examples/ldc2d_unsteady/#38","title":"3.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u672c\u6587\u4e2d\u7684\u8f93\u51fa\u6570\u636e\u662f\u4e00\u4e2a\u533a\u57df\u5185\u7684\u4e8c\u7ef4\u70b9\u96c6\uff0c\u6bcf\u4e2a\u65f6\u523b \\(t\\) \u7684\u5750\u6807\u662f \\((x^t_i,y^t_i)\\)\uff0c\u5bf9\u5e94\u503c\u662f \\((u^t_i, v^t_i, p^t_i)\\)\uff0c\u56e0\u6b64\u6211\u4eec\u53ea\u9700\u8981\u5c06\u8bc4\u4f30\u7684\u8f93\u51fa\u6570\u636e\u6309\u65f6\u523b\u4fdd\u5b58\u6210 16 \u4e2a vtu\u683c\u5f0f \u6587\u4ef6\uff0c\u6700\u540e\u7528\u53ef\u89c6\u5316\u8f6f\u4ef6\u6253\u5f00\u67e5\u770b\u5373\u53ef\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set visualizer(optional)\nNPOINT_BC = NPOINT_TOP + NPOINT_DOWN + NPOINT_LEFT + NPOINT_RIGHT\nvis_initial_points = geom[\"time_rect\"].sample_initial_interior(\n    (NPOINT_IC + NPOINT_BC), evenly=True\n)\nvis_pde_points = geom[\"time_rect\"].sample_interior(\n    (NPOINT_PDE + NPOINT_BC) * NTIME_PDE, evenly=True\n)\nvis_points = vis_initial_points\n# manually collate input data for visualization,\n# (interior+boundary) x all timestamps\nfor t in range(NTIME_PDE):\n    for key in geom[\"time_rect\"].dim_keys:\n        vis_points[key] = np.concatenate(\n            (\n                vis_points[key],\n                vis_pde_points[key][\n                    t\n                    * (NPOINT_PDE + NPOINT_BC) : (t + 1)\n                    * (NPOINT_PDE + NPOINT_BC)\n                ],\n            )\n        )\n\nvisualizer = {\n    \"visualize_u_v\": ppsci.visualize.VisualizerVtu(\n        vis_points,\n        {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"p\": lambda d: d[\"p\"]},\n        num_timestamps=cfg.NTIME_ALL,\n        prefix=\"result_u_v\",\n    )\n}\n</code></pre>"},{"location":"zh/examples/ldc2d_unsteady/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>"},{"location":"zh/examples/ldc2d_unsteady/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"ldc2d_steady_Re10.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"NavierStokes\": ppsci.equation.NavierStokes(cfg.NU, cfg.RHO, 2, True)}\n\n    # set timestamps(including initial t0)\n    timestamps = np.linspace(0.0, 1.5, cfg.NTIME_ALL, endpoint=True)\n    # set time-geometry\n    geom = {\n        \"time_rect\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(0.0, 1.5, timestamps=timestamps),\n            ppsci.geometry.Rectangle((-0.05, -0.05), (0.05, 0.05)),\n        )\n    }\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    }\n\n    # pde/bc constraint use t1~tn, initial constraint use t0\n    NPOINT_PDE, NTIME_PDE = 99**2, cfg.NTIME_ALL - 1\n    NPOINT_TOP, NTIME_TOP = 101, cfg.NTIME_ALL - 1\n    NPOINT_DOWN, NTIME_DOWN = 101, cfg.NTIME_ALL - 1\n    NPOINT_LEFT, NTIME_LEFT = 99, cfg.NTIME_ALL - 1\n    NPOINT_RIGHT, NTIME_RIGHT = 99, cfg.NTIME_ALL - 1\n    NPOINT_IC, NTIME_IC = 99**2, 1\n\n    # set constraint\n    pde = ppsci.constraint.InteriorConstraint(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n        geom[\"time_rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_PDE * NTIME_PDE},\n        ppsci.loss.MSELoss(\"sum\"),\n        evenly=True,\n        weight_dict=cfg.TRAIN.weight.pde,  # (1)\n        name=\"EQ\",\n    )\n    bc_top = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n        {\"u\": 1, \"v\": 0},\n        geom[\"time_rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_TOP * NTIME_TOP},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda t, x, y: np.isclose(y, 0.05),\n        name=\"BC_top\",\n    )\n    bc_down = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n        {\"u\": 0, \"v\": 0},\n        geom[\"time_rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_DOWN * NTIME_DOWN},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda t, x, y: np.isclose(y, -0.05),\n        name=\"BC_down\",\n    )\n    bc_left = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n        {\"u\": 0, \"v\": 0},\n        geom[\"time_rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_LEFT * NTIME_LEFT},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda t, x, y: np.isclose(x, -0.05),\n        name=\"BC_left\",\n    )\n    bc_right = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n        {\"u\": 0, \"v\": 0},\n        geom[\"time_rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_RIGHT * NTIME_RIGHT},\n        ppsci.loss.MSELoss(\"sum\"),\n        criteria=lambda t, x, y: np.isclose(x, 0.05),\n        name=\"BC_right\",\n    )\n    ic = ppsci.constraint.InitialConstraint(\n        {\"u\": lambda out: out[\"u\"], \"v\": lambda out: out[\"v\"]},\n        {\"u\": 0, \"v\": 0},\n        geom[\"time_rect\"],\n        {**train_dataloader_cfg, \"batch_size\": NPOINT_IC * NTIME_IC},\n        ppsci.loss.MSELoss(\"sum\"),\n        evenly=True,\n        name=\"IC\",\n    )\n    # wrap constraints together\n    constraint = {\n        pde.name: pde,\n        bc_top.name: bc_top,\n        bc_down.name: bc_down,\n        bc_left.name: bc_left,\n        bc_right.name: bc_right,\n        ic.name: ic,\n    }\n\n    # set training hyper-parameters\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Cosine(\n        **cfg.TRAIN.lr_scheduler,\n        warmup_epoch=int(0.05 * cfg.TRAIN.epochs),\n    )()\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    # set validator\n    NPOINT_EVAL = NPOINT_PDE * cfg.NTIME_ALL\n    residual_validator = ppsci.validate.GeometryValidator(\n        equation[\"NavierStokes\"].equations,\n        {\"momentum_x\": 0, \"continuity\": 0, \"momentum_y\": 0},\n        geom[\"time_rect\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"total_size\": NPOINT_EVAL,\n            \"batch_size\": cfg.EVAL.batch_size.residual_validator,\n            \"sampler\": {\"name\": \"BatchSampler\"},\n        },\n        ppsci.loss.MSELoss(\"sum\"),\n        evenly=True,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        with_initial=True,\n        name=\"Residual\",\n    )\n    validator = {residual_validator.name: residual_validator}\n\n    # set visualizer(optional)\n    NPOINT_BC = NPOINT_TOP + NPOINT_DOWN + NPOINT_LEFT + NPOINT_RIGHT\n    vis_initial_points = geom[\"time_rect\"].sample_initial_interior(\n        (NPOINT_IC + NPOINT_BC), evenly=True\n    )\n    vis_pde_points = geom[\"time_rect\"].sample_interior(\n        (NPOINT_PDE + NPOINT_BC) * NTIME_PDE, evenly=True\n    )\n    vis_points = vis_initial_points\n    # manually collate input data for visualization,\n    # (interior+boundary) x all timestamps\n    for t in range(NTIME_PDE):\n        for key in geom[\"time_rect\"].dim_keys:\n            vis_points[key] = np.concatenate(\n                (\n                    vis_points[key],\n                    vis_pde_points[key][\n                        t\n                        * (NPOINT_PDE + NPOINT_BC) : (t + 1)\n                        * (NPOINT_PDE + NPOINT_BC)\n                    ],\n                )\n            )\n\n    visualizer = {\n        \"visualize_u_v\": ppsci.visualize.VisualizerVtu(\n            vis_points,\n            {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"p\": lambda d: d[\"p\"]},\n            num_timestamps=cfg.NTIME_ALL,\n            prefix=\"result_u_v\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"NavierStokes\": ppsci.equation.NavierStokes(cfg.NU, cfg.RHO, 2, True)}\n\n    # set timestamps(including initial t0)\n    timestamps = np.linspace(0.0, 1.5, cfg.NTIME_ALL, endpoint=True)\n    # set time-geometry\n    geom = {\n        \"time_rect\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(0.0, 1.5, timestamps=timestamps),\n            ppsci.geometry.Rectangle((-0.05, -0.05), (0.05, 0.05)),\n        )\n    }\n\n    # pde/bc constraint use t1~tn, initial constraint use t0\n    NPOINT_PDE = 99**2\n    NPOINT_TOP = 101\n    NPOINT_DOWN = 101\n    NPOINT_LEFT = 99\n    NPOINT_RIGHT = 99\n    NPOINT_IC = 99**2\n    NTIME_PDE = cfg.NTIME_ALL - 1\n\n    # set validator\n    NPOINT_EVAL = NPOINT_PDE * cfg.NTIME_ALL\n    residual_validator = ppsci.validate.GeometryValidator(\n        equation[\"NavierStokes\"].equations,\n        {\"momentum_x\": 0, \"continuity\": 0, \"momentum_y\": 0},\n        geom[\"time_rect\"],\n        {\n            \"dataset\": \"NamedArrayDataset\",\n            \"total_size\": NPOINT_EVAL,\n            \"batch_size\": cfg.EVAL.batch_size.residual_validator,\n            \"sampler\": {\"name\": \"BatchSampler\"},\n        },\n        ppsci.loss.MSELoss(\"sum\"),\n        evenly=True,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        with_initial=True,\n        name=\"Residual\",\n    )\n    validator = {residual_validator.name: residual_validator}\n\n    # set visualizer(optional)\n    NPOINT_BC = NPOINT_TOP + NPOINT_DOWN + NPOINT_LEFT + NPOINT_RIGHT\n    vis_initial_points = geom[\"time_rect\"].sample_initial_interior(\n        (NPOINT_IC + NPOINT_BC), evenly=True\n    )\n    vis_pde_points = geom[\"time_rect\"].sample_interior(\n        (NPOINT_PDE + NPOINT_BC) * NTIME_PDE, evenly=True\n    )\n    vis_points = vis_initial_points\n    # manually collate input data for visualization,\n    # (interior+boundary) x all timestamps\n    for t in range(NTIME_PDE):\n        for key in geom[\"time_rect\"].dim_keys:\n            vis_points[key] = np.concatenate(\n                (\n                    vis_points[key],\n                    vis_pde_points[key][\n                        t\n                        * (NPOINT_PDE + NPOINT_BC) : (t + 1)\n                        * (NPOINT_PDE + NPOINT_BC)\n                    ],\n                )\n            )\n\n    visualizer = {\n        \"visualize_u_v\": ppsci.visualize.VisualizerVtu(\n            vis_points,\n            {\"u\": lambda d: d[\"u\"], \"v\": lambda d: d[\"v\"], \"p\": lambda d: d[\"p\"]},\n            num_timestamps=cfg.NTIME_ALL,\n            prefix=\"result_u_v\",\n        )\n    }\n\n    # directly evaluate pretrained model(optional)\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n    # visualize prediction for pretrained model(optional)\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # set timestamps(including initial t0)\n    timestamps = np.linspace(0.0, 1.5, cfg.NTIME_ALL, endpoint=True)\n    # set time-geometry\n    geom = {\n        \"time_rect\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(0.0, 1.5, timestamps=timestamps),\n            ppsci.geometry.Rectangle((-0.05, -0.05), (0.05, 0.05)),\n        )\n    }\n    # manually collate input data for inference\n    NPOINT_PDE = 99**2\n    NPOINT_TOP = 101\n    NPOINT_DOWN = 101\n    NPOINT_LEFT = 99\n    NPOINT_RIGHT = 99\n    NPOINT_IC = 99**2\n    NTIME_PDE = cfg.NTIME_ALL - 1\n    NPOINT_BC = NPOINT_TOP + NPOINT_DOWN + NPOINT_LEFT + NPOINT_RIGHT\n    input_dict = geom[\"time_rect\"].sample_initial_interior(\n        (NPOINT_IC + NPOINT_BC), evenly=True\n    )\n    input_pde_dict = geom[\"time_rect\"].sample_interior(\n        (NPOINT_PDE + NPOINT_BC) * NTIME_PDE, evenly=True\n    )\n    # (interior+boundary) x all timestamps\n    for t in range(NTIME_PDE):\n        for key in geom[\"time_rect\"].dim_keys:\n            input_dict[key] = np.concatenate(\n                (\n                    input_dict[key],\n                    input_pde_dict[key][\n                        t\n                        * (NPOINT_PDE + NPOINT_BC) : (t + 1)\n                        * (NPOINT_PDE + NPOINT_BC)\n                    ],\n                )\n            )\n    output_dict = predictor.predict(\n        {key: input_dict[key] for key in cfg.MODEL.input_keys}, cfg.INFER.batch_size\n    )\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n\n    ppsci.visualize.save_vtu_from_dict(\n        \"./ldc2d_unsteady_Re10_pred.vtu\",\n        {**input_dict, **output_dict},\n        input_dict.keys(),\n        cfg.MODEL.output_keys,\n        cfg.NTIME_ALL,\n    )\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"ldc2d_unsteady_Re10.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/ldc2d_unsteady/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u65b9\u5c55\u793a\u4e86\u6a21\u578b\u5bf9\u4e8e\u6700\u540e\u4e00\u4e2a\u65f6\u523b\uff0c\u8fb9\u957f\u4e3a 1 \u7684\u6b63\u65b9\u5f62\u8ba1\u7b97\u57df\u7684\u5185\u90e8\u70b9\u8fdb\u884c\u9884\u6d4b\u7684\u7ed3\u679c\u3001OpeFOAM\u6c42\u89e3\u7ed3\u679c\uff0c\u5305\u62ec\u6bcf\u4e2a\u70b9\u7684\u6c34\u5e73(x)\u65b9\u5411\u6d41\u901f\\(u(x,y)\\)\u3001\u5782\u76f4(y)\u65b9\u5411\u6d41\u901f\\(v(x,y)\\)\u3001\u538b\u529b\\(p(x,y)\\)\u3002</p> \u8bf4\u660e <p>\u672c\u6848\u4f8b\u53ea\u4f5c\u4e3ademo\u5c55\u793a\uff0c\u5c1a\u672a\u8fdb\u884c\u5145\u5206\u8c03\u4f18\uff0c\u4e0b\u65b9\u90e8\u5206\u5c55\u793a\u7ed3\u679c\u53ef\u80fd\u4e0e OpenFOAM \u5b58\u5728\u4e00\u5b9a\u5dee\u522b\u3002</p> <p> </p> \u5de6\uff1a\u6a21\u578b\u9884\u6d4b\u7ed3\u679c u \uff0c\u53f3\uff1aOpenFOAM\u7ed3\u679c u  <p> </p> \u5de6\uff1a\u6a21\u578b\u9884\u6d4b\u7ed3\u679c v \uff0c\u53f3\uff1aOpenFOAM\u7ed3\u679c v  <p> </p> \u5de6\uff1a\u6a21\u578b\u9884\u6d4b\u7ed3\u679c p \uff0c\u53f3\uff1aOpenFOAM\u7ed3\u679c p  <p>\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e0eOpenFOAM\u7684\u9884\u6d4b\u7ed3\u679c\u5927\u81f4\u76f8\u540c\u3002</p>"},{"location":"zh/examples/lorenz/","title":"Lorenz_transform_physx","text":""},{"location":"zh/examples/lorenz/#lorenz-system","title":"Lorenz System","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_training_rk.hdf5 -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_valid_rk.hdf5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_training_rk.hdf5 --create-dirs -o ./datasets/lorenz_training_rk.hdf5\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_valid_rk.hdf5 --create-dirs -o ./datasets/lorenz_valid_rk.hdf5\npython train_enn.py\npython train_transformer.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_training_rk.hdf5 -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_valid_rk.hdf5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_training_rk.hdf5 --create-dirs -o ./datasets/lorenz_training_rk.hdf5\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_valid_rk.hdf5 --create-dirs -o ./datasets/lorenz_valid_rk.hdf5\npython train_enn.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/lorenz/lorenz_pretrained.pdparams\npython train_transformer.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/lorenz/lorenz_transformer_pretrained.pdparams EMBEDDING_MODEL_PATH=https://paddle-org.bj.bcebos.com/paddlescience/models/lorenz/lorenz_pretrained.pdparams\n</code></pre> <pre><code>python train_transformer.py mode=export EMBEDDING_MODEL_PATH=https://paddle-org.bj.bcebos.com/paddlescience/models/lorenz/lorenz_pretrained.pdparams\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_training_rk.hdf5 -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_valid_rk.hdf5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_training_rk.hdf5 --create-dirs -o ./datasets/lorenz_training_rk.hdf5\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/lorenz_valid_rk.hdf5 --create-dirs -o ./datasets/lorenz_valid_rk.hdf5\npython train_transformer.py mode=infer\n</code></pre> \u6a21\u578b MSE lorenz_transformer_pretrained.pdparams 0.054"},{"location":"zh/examples/lorenz/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>Lorenz System\uff0c\u4e2d\u6587\u540d\u79f0\u53ef\u8bd1\u4f5c\u201c\u6d1b\u4f26\u5179\u7cfb\u7edf\u201d\uff0c\u53c8\u79f0\u201c\u6d1b\u4f26\u5179\u6df7\u6c8c\u7cfb\u7edf\u201d\uff0c\u6700\u65e9\u7531\u7f8e\u56fd\u6c14\u8c61\u5b66\u5bb6\u7231\u5fb7\u534e\u00b7\u6d1b\u4f26\u5179\uff08Edward N.Lorenz\uff09\u57281963\u5e74\u7684\u4e00\u7bc7\u6587\u7ae0\u4e2d\u63d0\u51fa\u3002\u8457\u540d\u7684\u201c\u8774\u8776\u6548\u5e94\u201d\uff0c\u5373\u201c\u4e00\u53ea\u5357\u7f8e\u6d32\u4e9a\u9a6c\u900a\u6cb3\u6d41\u57df\u70ed\u5e26\u96e8\u6797\u4e2d\u7684\u8774\u8776\uff0c\u5076\u5c14\u6247\u52a8\u51e0\u4e0b\u7fc5\u8180\uff0c\u53ef\u4ee5\u5728\u4e24\u5468\u4ee5\u540e\u5f15\u8d77\u7f8e\u56fd\u5f97\u514b\u8428\u65af\u5dde\u7684\u4e00\u573a\u9f99\u5377\u98ce\u201d\uff0c\u4e5f\u662f\u6700\u65e9\u8d77\u6e90\u4e8e\u8fd9\u7bc7\u6587\u7ae0\u3002\u6d1b\u4f26\u5179\u7cfb\u7edf\u7684\u7279\u70b9\u662f\u5728\u4e00\u5b9a\u53c2\u6570\u6761\u4ef6\u4e0b\u5c55\u73b0\u51fa\u590d\u6742\u3001\u4e0d\u786e\u5b9a\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5305\u62ec\u5bf9\u521d\u59cb\u6761\u4ef6\u7684\u654f\u611f\u6027\u548c\u957f\u671f\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u3002\u8fd9\u79cd\u6df7\u6c8c\u884c\u4e3a\u5728\u81ea\u7136\u754c\u548c\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u9886\u57df\u4e2d\u90fd\u5b58\u5728\uff0c\u4f8b\u5982\u6c14\u5019\u53d8\u5316\u3001\u80a1\u7968\u5e02\u573a\u6ce2\u52a8\u7b49\u3002\u6d1b\u4f26\u5179\u7cfb\u7edf\u5bf9\u6570\u503c\u6270\u52a8\u6781\u4e3a\u654f\u611f\uff0c\u662f\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\uff08\u6df1\u5ea6\u5b66\u4e60\uff09\u6a21\u578b\u51c6\u786e\u6027\u7684\u826f\u597d\u57fa\u51c6\u3002</p>"},{"location":"zh/examples/lorenz/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u6d1b\u4f26\u5179\u7cfb\u7edf\u7684\u72b6\u6001\u65b9\u7a0b\uff1a</p> \\[ \\begin{cases}   \\dfrac{\\partial x}{\\partial t} = \\sigma(y - x), &amp; \\\\   \\dfrac{\\partial y}{\\partial t} = x(\\rho - z) - y, &amp; \\\\   \\dfrac{\\partial z}{\\partial t} = xy - \\beta z \\end{cases} \\] <p>\u5f53\u53c2\u6570\u53d6\u4ee5\u4e0b\u503c\u65f6\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u7ecf\u5178\u7684\u6df7\u6c8c\u7279\u6027\uff1a</p> \\[\\rho = 28, \\sigma = 10, \\beta = \\frac{8}{3}\\] <p>\u5728\u8fd9\u4e2a\u6848\u4f8b\u4e2d\uff0c\u8981\u6c42\u7ed9\u5b9a\u521d\u59cb\u65f6\u523b\u70b9\u7684\u5750\u6807\uff0c\u9884\u6d4b\u672a\u6765\u4e00\u6bb5\u65f6\u95f4\u5185\u70b9\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002</p>"},{"location":"zh/examples/lorenz/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u57fa\u4e8e PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002\u672c\u6848\u4f8b\u57fa\u4e8e\u8bba\u6587 Transformers for Modeling Physical Systems \u65b9\u6cd5\u8fdb\u884c\u6c42\u89e3\uff0c\u63a5\u4e0b\u6765\u9996\u5148\u4f1a\u5bf9\u8be5\u8bba\u6587\u7684\u7406\u8bba\u65b9\u6cd5\u8fdb\u884c\u7b80\u5355\u4ecb\u7ecd\uff0c\u7136\u540e\u5bf9\u4f7f\u7528\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4ecb\u7ecd\uff0c\u6700\u540e\u5bf9\u8be5\u65b9\u6cd5\u4e24\u4e2a\u8bad\u7ec3\u6b65\u9aa4\uff08Embedding \u6a21\u578b\u8bad\u7ec3\u3001Transformer \u6a21\u578b\u8bad\u7ec3\uff09\u7684\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u3001\u6a21\u578b\u6784\u5efa\u7b49\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/lorenz/#31","title":"3.1 \u65b9\u6cd5\u4ecb\u7ecd","text":"<p>Transformer \u7ed3\u6784\u5728 NLP\u3001CV \u9886\u57df\u4e2d\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u6210\u529f\uff0c\u4f46\u662f\u5176\u5728\u5efa\u6a21\u7269\u7406\u7cfb\u7edf\u65b9\u9762\u8fd8\u6ca1\u6709\u5f97\u5230\u66f4\u591a\u7684\u63a2\u7d22\u3002\u5728 Transformers for Modeling Physical Systems \u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e Transformer \u7684\u7f51\u7edc\u7ed3\u6784\u7528\u4e8e\u5efa\u6a21\u7269\u7406\u7cfb\u7edf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u7684\u5efa\u6a21\u4e0d\u540c\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u5e76\u4e14\u6bd4\u5176\u4ed6\u4f20\u7edf\u7684\u65b9\u6cd5\u66f4\u597d\u3002</p> <p>\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u8be5\u65b9\u6cd5\u4e3b\u8981\u5305\u542b\u4e24\u4e2a\u7f51\u7edc\u6a21\u578b\uff1aEmbedding \u6a21\u578b\u548c Transformer \u6a21\u578b\u3002\u5176\u4e2d\uff0cEmbedding \u6a21\u578b\u7684 Encoder \u6a21\u5757\u8d1f\u8d23\u5c06\u7269\u7406\u72b6\u6001\u53d8\u91cf\u8fdb\u884c\u7f16\u7801\u6620\u5c04\u4e3a\u7f16\u7801\u5411\u91cf\uff0cDecoder \u6a21\u5757\u5219\u8d1f\u8d23\u5c06\u7f16\u7801\u5411\u91cf\u6620\u5c04\u4e3a\u7269\u7406\u72b6\u6001\u53d8\u91cf\uff1bTransformer \u6a21\u578b\u4f5c\u7528\u4e8e\u7f16\u7801\u7a7a\u95f4\uff0c\u5176\u8f93\u5165\u662f Embedding \u6a21\u578b Encoder \u6a21\u5757\u7684\u8f93\u51fa\uff0c\u5229\u7528\u5f53\u524d\u65f6\u523b\u7684\u7f16\u7801\u5411\u91cf\u9884\u6d4b\u4e0b\u4e00\u65f6\u523b\u7684\u7f16\u7801\u5411\u91cf\uff0c\u9884\u6d4b\u5f97\u5230\u7684\u7f16\u7801\u5411\u91cf\u53ef\u4ee5\u88ab Embedding \u6a21\u578b\u7684 Decoder \u6a21\u5757\u89e3\u7801\uff0c\u5f97\u5230\u5bf9\u5e94\u7684\u7269\u7406\u72b6\u6001\u53d8\u91cf\u3002\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c\u9996\u5148\u8bad\u7ec3 Embedding \u6a21\u578b\uff0c\u7136\u540e\u5c06 Embedding \u6a21\u578b\u7684\u53c2\u6570\u51bb\u7ed3\u8bad\u7ec3 Transformer \u6a21\u578b\u3002\u5173\u4e8e\u8be5\u65b9\u6cd5\u7684\u7ec6\u8282\u8bf7\u53c2\u8003\u8bba\u6587 Transformers for Modeling Physical Systems\u3002</p> <p> </p> \u5de6\uff1aEmbedding \u7f51\u7edc\u7ed3\u6784\uff0c\u53f3\uff1aTransformer \u7f51\u7edc\u7ed3\u6784"},{"location":"zh/examples/lorenz/#32","title":"3.2 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u6570\u636e\u96c6\u91c7\u7528\u4e86 Transformer-Physx \u4e2d\u63d0\u4f9b\u7684\u6570\u636e\u3002\u8be5\u6570\u636e\u96c6\u4f7f\u7528\u9f99\u683c\uff0d\u5e93\u5854\uff08Runge-Kutta\uff09\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u65b9\u6cd5\u5f97\u5230\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u5927\u5c0f\u4e3a0.01\uff0c\u521d\u59cb\u4f4d\u7f6e\u4ece\u4ee5\u4e0b\u8303\u56f4\u4e2d\u968f\u673a\u9009\u53d6\uff1a</p> \\[x_{0} \\sim(-20, 20), y_{0} \\sim(-20, 20), z_{0} \\sim(10, 40)\\] <p>\u6570\u636e\u96c6\u7684\u5212\u5206\u5982\u4e0b\uff1a</p> \u6570\u636e\u96c6 \u65f6\u95f4\u5e8f\u5217\u7684\u6570\u91cf \u65f6\u95f4\u6b65\u7684\u6570\u91cf \u4e0b\u8f7d\u5730\u5740 \u8bad\u7ec3\u96c6 2048 256 lorenz_training_rk.hdf5 \u9a8c\u8bc1\u96c6 64 1024 lorenz_valid_rk.hdf5 <p>\u6570\u636e\u96c6\u5b98\u7f51\u4e3a\uff1ahttps://zenodo.org/record/5148524#.ZDe77-xByrc</p>"},{"location":"zh/examples/lorenz/#33-embedding","title":"3.3 Embedding \u6a21\u578b","text":"<p>\u9996\u5148\u5c55\u793a\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u5404\u4e2a\u53c2\u6570\u53d8\u91cf\uff0c\u6bcf\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u4f1a\u5728\u4e0b\u9762\u4f7f\u7528\u5230\u65f6\u8fdb\u884c\u89e3\u91ca\u3002</p> examples/conf/enn.yaml<pre><code>TRAIN_BLOCK_SIZE: 16\nVALID_BLOCK_SIZE: 32\nTRAIN_FILE_PATH: ./datasets/lorenz_training_rk.hdf5\nVALID_FILE_PATH: ./datasets/lorenz_valid_rk.hdf5\n\n# model settings\nMODEL:\n  input_keys: [\"states\"]\n  output_keys: [\"pred_states\", \"recover_states\"]\n</code></pre>"},{"location":"zh/examples/lorenz/#331","title":"3.3.1 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/lorenz/train_enn.py<pre><code>train_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"LorenzDataset\",\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n        \"stride\": 16,\n        \"weight_dict\": {\n            key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n        },\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 4,\n}\n</code></pre> <p>\u5176\u4e2d\uff0c\"dataset\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Dataset</code> \u7c7b\u540d\u4e3a <code>LorenzDataset</code>\uff0c\u53e6\u5916\u8fd8\u6307\u5b9a\u4e86\u8be5\u7c7b\u521d\u59cb\u5316\u65f6\u53c2\u6570\u7684\u53d6\u503c\uff1a</p> <ol> <li><code>file_path</code>\uff1a\u4ee3\u8868\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6587\u4ef6\u8def\u5f84\uff0c\u6307\u5b9a\u4e3a\u53d8\u91cf <code>train_file_path</code> \u7684\u503c\uff1b</li> <li><code>input_keys</code>\uff1a\u4ee3\u8868\u6a21\u578b\u8f93\u5165\u6570\u636e\u7684\u53d8\u91cf\u540d\u79f0\uff0c\u6b64\u5904\u586b\u5165\u53d8\u91cf <code>input_keys</code>\uff1b</li> <li><code>label_keys</code>\uff1a\u4ee3\u8868\u771f\u5b9e\u6807\u7b7e\u7684\u53d8\u91cf\u540d\u79f0\uff0c\u6b64\u5904\u586b\u5165\u53d8\u91cf <code>output_keys</code>\uff1b</li> <li><code>block_size</code>\uff1a\u4ee3\u8868\u4f7f\u7528\u591a\u957f\u7684\u65f6\u95f4\u6b65\u8fdb\u884c\u8bad\u7ec3\uff0c\u6307\u5b9a\u4e3a\u53d8\u91cf <code>train_block_size</code> \u7684\u503c\uff1b</li> <li><code>stride</code>\uff1a\u4ee3\u8868\u8fde\u7eed\u7684\u4e24\u4e2a\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u7684\u65f6\u95f4\u6b65\u95f4\u9694\uff0c\u6307\u5b9a\u4e3a16\uff1b</li> <li><code>weight_dict</code>\uff1a\u4ee3\u8868\u6a21\u578b\u8f93\u51fa\u5404\u4e2a\u53d8\u91cf\u4e0e\u771f\u5b9e\u6807\u7b7e\u635f\u5931\u51fd\u6570\u7684\u6743\u91cd\uff0c\u6b64\u5904\u4f7f\u7528 <code>output_keys</code>\u3001<code>weights</code> \u751f\u6210\u3002</li> </ol> <p>\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u53e6\u5916\u8fd8\u6307\u5b9a\u4e86\u8be5\u7c7b\u521d\u59cb\u5316\u65f6\u53c2\u6570 <code>drop_last</code>\u3001<code>shuffle</code> \u5747\u4e3a <code>True</code>\u3002</p> <p><code>train_dataloader_cfg</code> \u8fd8\u5b9a\u4e49\u4e86 <code>batch_size</code>\u3001<code>num_workers</code> \u7684\u503c\u3002</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/lorenz/train_enn.py<pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.MSELossWithL2Decay(\n        regularization_dict={\n            regularization_key: 1.0e-1 * (cfg.TRAIN_BLOCK_SIZE - 1)\n        }\n    ),\n    {\n        key: lambda out, k=key: out[k]\n        for key in cfg.MODEL.output_keys + (regularization_key,)\n    },\n    name=\"Sup\",\n)\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u6570\u636e\u7684\u52a0\u8f7d\u65b9\u5f0f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e0a\u6587\u4e2d\u5b9a\u4e49\u7684 <code>train_dataloader_cfg</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u4f7f\u7528\u5e26\u6709 L2Decay \u7684 MSELoss\uff0c\u7c7b\u540d\u4e3a <code>MSELossWithL2Decay</code>\uff0c<code>regularization_dict</code> \u8bbe\u7f6e\u4e86\u6b63\u5219\u5316\u7684\u53d8\u91cf\u540d\u79f0\u548c\u5bf9\u5e94\u7684\u6743\u91cd\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u8868\u793a\u5728\u8bad\u7ec3\u65f6\u5982\u4f55\u8ba1\u7b97\u9700\u8981\u88ab\u7ea6\u675f\u7684\u4e2d\u95f4\u53d8\u91cf\uff0c\u6b64\u5904\u6211\u4eec\u7ea6\u675f\u7684\u53d8\u91cf\u5c31\u662f\u7f51\u7edc\u7684\u8f93\u51fa\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"Sup\"\u3002</p>"},{"location":"zh/examples/lorenz/#332","title":"3.3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0cEmbedding \u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u90fd\u662f\u7269\u7406\u7a7a\u95f4\u4e2d\u70b9\u7684\u4f4d\u7f6e\u5750\u6807 \\((x, y, z)\\) \uff0c\u4f7f\u7528\u4e86\u5168\u8fde\u63a5\u5c42\u5b9e\u73b0 Embedding \u6a21\u578b\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002</p> <p> </p> Embedding \u7f51\u7edc\u6a21\u578b <p>\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/lorenz/train_enn.py<pre><code>data_mean, data_std = get_mean_std(sup_constraint.data_loader.dataset.data)\nmodel = ppsci.arch.LorenzEmbedding(\n    cfg.MODEL.input_keys,\n    cfg.MODEL.output_keys + (regularization_key,),\n    data_mean,\n    data_std,\n)\n</code></pre> <p>\u5176\u4e2d\uff0c<code>LorenzEmbedding</code> \u7684\u524d\u4e24\u4e2a\u53c2\u6570\u5728\u524d\u6587\u4e2d\u5df2\u6709\u63cf\u8ff0\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\uff0c\u7f51\u7edc\u6a21\u578b\u7684\u7b2c\u4e09\u3001\u56db\u4e2a\u53c2\u6570\u662f\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5747\u503c\u548c\u65b9\u5dee\uff0c\u7528\u4e8e\u5f52\u4e00\u5316\u8f93\u5165\u6570\u636e\u3002\u8ba1\u7b97\u5747\u503c\u3001\u65b9\u5dee\u7684\u7684\u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/lorenz/train_enn.py<pre><code>def get_mean_std(data: np.ndarray):\n    mean = np.asarray(\n        [np.mean(data[:, :, 0]), np.mean(data[:, :, 1]), np.mean(data[:, :, 2])]\n    ).reshape(1, 3)\n    std = np.asarray(\n        [np.std(data[:, :, 0]), np.std(data[:, :, 1]), np.std(data[:, :, 2])]\n    ).reshape(1, 3)\n    return mean, std\n</code></pre>"},{"location":"zh/examples/lorenz/#333","title":"3.3.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>ExponentialDecay</code> \uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a0.001\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>Adam</code>\uff0c\u68af\u5ea6\u88c1\u526a\u4f7f\u7528\u4e86 Paddle \u5185\u7f6e\u7684 <code>ClipGradByGlobalNorm</code> \u65b9\u6cd5\u3002\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> examples/lorenz/train_enn.py<pre><code># init optimizer and lr scheduler\nclip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\nlr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n    iters_per_epoch=ITERS_PER_EPOCH,\n    decay_steps=ITERS_PER_EPOCH,\n    **cfg.TRAIN.lr_scheduler,\n)()\noptimizer = ppsci.optimizer.Adam(\n    lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n)(model)\n</code></pre>"},{"location":"zh/examples/lorenz/#334","title":"3.3.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/lorenz/train_enn.py<pre><code>eval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"LorenzDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 32,\n        \"weight_dict\": {\n            key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n        },\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"num_workers\": 4,\n}\n\nmse_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"MSE_Validator\",\n)\nvalidator = {mse_validator.name: mse_validator}\n</code></pre> <p><code>SupervisedValidator</code> \u8bc4\u4f30\u5668\u4e0e <code>SupervisedConstraint</code> \u6bd4\u8f83\u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u8bc4\u4f30\u5668\u9700\u8981\u8bbe\u7f6e\u8bc4\u4ef7\u6307\u6807 <code>metric</code>\uff0c\u5728\u8fd9\u91cc\u4f7f\u7528 <code>ppsci.metric.MSE</code> \u3002</p>"},{"location":"zh/examples/lorenz/#335","title":"3.3.5 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/lorenz/train_enn.py<pre><code>solver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=True,\n    validator=validator,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/lorenz/#34-transformer","title":"3.4 Transformer \u6a21\u578b","text":"<p>\u4e0a\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u6784\u5efa Embedding \u6a21\u578b\u7684\u8bad\u7ec3\u3001\u8bc4\u4f30\uff0c\u5728\u672c\u8282\u4e2d\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528\u8bad\u7ec3\u597d\u7684 Embedding \u6a21\u578b\u8bad\u7ec3 Transformer \u6a21\u578b\u3002\u56e0\u4e3a\u8bad\u7ec3 Transformer \u6a21\u578b\u7684\u6b65\u9aa4\u4e0e\u8bad\u7ec3 Embedding \u6a21\u578b\u7684\u6b65\u9aa4\u57fa\u672c\u76f8\u4f3c\uff0c\u56e0\u6b64\u672c\u8282\u5728\u4e24\u8005\u7684\u91cd\u590d\u90e8\u5206\u7684\u5404\u4e2a\u53c2\u6570\u4e0d\u518d\u8be6\u7ec6\u4ecb\u7ecd\u3002\u9996\u5148\u5c06\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u5404\u4e2a\u53c2\u6570\u53d8\u91cf\u5c55\u793a\u5982\u4e0b\uff0c\u6bcf\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u4f1a\u5728\u4e0b\u9762\u4f7f\u7528\u5230\u65f6\u8fdb\u884c\u89e3\u91ca\u3002</p> examples/lorenz/conf/transformer.yaml<pre><code># model settings\nMODEL:\n  input_keys: [\"embeds\"]\n  output_keys: [\"pred_embeds\"]\n  num_layers: 4\n  num_ctx: 64\n  embed_size: 32\n</code></pre>"},{"location":"zh/examples/lorenz/#341","title":"3.4.1 \u7ea6\u675f\u6784\u5efa","text":"<p>Transformer \u6a21\u578b\u540c\u6837\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/lorenz/train_transformer.py<pre><code>train_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"LorenzDataset\",\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n        \"stride\": 64,\n        \"embedding_model\": embedding_model,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 4,\n}\n</code></pre> <p>\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\u4e0e Embedding \u6a21\u578b\u4e2d\u7684\u57fa\u672c\u4e00\u81f4\uff0c\u4e0d\u518d\u8d58\u8ff0\u3002\u9700\u8981\u8bf4\u660e\u7684\u662f\u7531\u4e8e Transformer \u6a21\u578b\u8bad\u7ec3\u7684\u8f93\u5165\u6570\u636e\u662f Embedding \u6a21\u578b Encoder \u6a21\u5757\u7684\u8f93\u51fa\u6570\u636e\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u8bad\u7ec3\u597d\u7684 Embedding \u6a21\u578b\u4f5c\u4e3a <code>LorenzDataset</code> \u7684\u4e00\u4e2a\u53c2\u6570\uff0c\u5728\u521d\u59cb\u5316\u65f6\u9996\u5148\u5c06\u8bad\u7ec3\u6570\u636e\u6620\u5c04\u5230\u7f16\u7801\u7a7a\u95f4\u3002</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/lorenz/train_transformer.py<pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    name=\"Sup\",\n)\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre>"},{"location":"zh/examples/lorenz/#342","title":"3.4.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0cTransformer \u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u90fd\u662f\u7f16\u7801\u7a7a\u95f4\u4e2d\u7684\u5411\u91cf\uff0c\u4f7f\u7528\u7684 Transformer \u7ed3\u6784\u5982\u4e0b\uff1a</p> <p> </p> Transformer \u7f51\u7edc\u6a21\u578b <p>\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/lorenz/train_transformer.py<pre><code>model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n</code></pre> <p>\u7c7b <code>PhysformerGPT2</code> \u9664\u4e86\u9700\u8981\u586b\u5165 <code>input_keys</code>\u3001<code>output_keys</code> \u5916\uff0c\u8fd8\u9700\u8981\u8bbe\u7f6e Transformer \u6a21\u578b\u7684\u5c42\u6570 <code>num_layers</code>\u3001\u4e0a\u4e0b\u6587\u7684\u5927\u5c0f <code>num_ctx</code>\u3001\u8f93\u5165\u7684 Embedding \u5411\u91cf\u7684\u957f\u5ea6 <code>embed_size</code>\u3001\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u53c2\u6570 <code>num_heads</code>\uff0c\u5728\u8fd9\u91cc\u586b\u5165\u7684\u6570\u503c\u4e3a4\u300164\u300132\u30014\u3002</p>"},{"location":"zh/examples/lorenz/#343","title":"3.4.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>CosineWarmRestarts</code>\uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a0.001\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>Adam</code>\uff0c\u68af\u5ea6\u88c1\u526a\u4f7f\u7528\u4e86 Paddle \u5185\u7f6e\u7684 <code>ClipGradByGlobalNorm</code> \u65b9\u6cd5\u3002\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/lorenz/train_transformer.py<pre><code>clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\nlr_scheduler = ppsci.optimizer.lr_scheduler.CosineWarmRestarts(\n    iters_per_epoch=ITERS_PER_EPOCH, **cfg.TRAIN.lr_scheduler\n)()\noptimizer = ppsci.optimizer.Adam(\n    lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n)(model)\n</code></pre>"},{"location":"zh/examples/lorenz/#344","title":"3.4.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/lorenz/train_transformer.py<pre><code>eval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"LorenzDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 1024,\n        \"embedding_model\": embedding_model,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"num_workers\": 4,\n}\n\nmse_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"MSE_Validator\",\n)\nvalidator = {mse_validator.name: mse_validator}\n</code></pre>"},{"location":"zh/examples/lorenz/#345","title":"3.4.5 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u53ef\u4ee5\u901a\u8fc7\u6784\u5efa\u53ef\u89c6\u5316\u5668\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\u5c06\u8bc4\u4f30\u7ed3\u679c\u53ef\u89c6\u5316\u51fa\u6765\uff0c\u7531\u4e8e Transformer \u6a21\u578b\u7684\u8f93\u51fa\u6570\u636e\u662f\u9884\u6d4b\u7684\u7f16\u7801\u7a7a\u95f4\u7684\u6570\u636e\u65e0\u6cd5\u76f4\u63a5\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u56e0\u6b64\u9700\u8981\u989d\u5916\u5c06\u8f93\u51fa\u6570\u636e\u4f7f\u7528 Embedding \u7f51\u7edc\u7684 Decoder \u6a21\u5757\u53d8\u6362\u5230\u7269\u7406\u72b6\u6001\u7a7a\u95f4\u3002</p> <p>\u5728\u672c\u6587\u4e2d\u9996\u5148\u5b9a\u4e49\u4e86\u5bf9 Transformer \u6a21\u578b\u8f93\u51fa\u6570\u636e\u53d8\u6362\u5230\u7269\u7406\u72b6\u6001\u7a7a\u95f4\u7684\u4ee3\u7801\uff1a</p> examples/lorenz/train_transformer.py<pre><code>def build_embedding_model(embedding_model_path: str) -&gt; ppsci.arch.LorenzEmbedding:\n    input_keys = (\"states\",)\n    output_keys = (\"pred_states\", \"recover_states\")\n    regularization_key = \"k_matrix\"\n    model = ppsci.arch.LorenzEmbedding(input_keys, output_keys + (regularization_key,))\n    save_load.load_pretrain(model, embedding_model_path)\n    return model\n\n\nclass OutputTransform(object):\n    def __init__(self, model: base.Arch):\n        self.model = model\n        self.model.eval()\n\n    def __call__(self, x: Dict[str, paddle.Tensor]):\n        pred_embeds = x[\"pred_embeds\"]\n        pred_states = self.model.decoder(pred_embeds)\n\n        return pred_states\n</code></pre> examples/lorenz/train_transformer.py<pre><code>embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\noutput_transform = OutputTransform(embedding_model)\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230\uff0c\u7a0b\u5e8f\u9996\u5148\u8f7d\u5165\u4e86\u8bad\u7ec3\u597d\u7684 Embedding \u6a21\u578b\uff0c\u7136\u540e\u5728 <code>OutputTransform</code> \u7684 <code>__call__</code> \u51fd\u6570\u5185\u5b9e\u73b0\u4e86\u7f16\u7801\u5411\u91cf\u5230\u7269\u7406\u72b6\u6001\u7a7a\u95f4\u7684\u53d8\u6362\u3002</p> <p>\u5728\u5b9a\u4e49\u597d\u4e86\u4ee5\u4e0a\u4ee3\u7801\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u5b9e\u73b0\u53ef\u89c6\u5316\u5668\u4ee3\u7801\u7684\u6784\u5efa\u4e86\uff1a</p> examples/lorenz/train_transformer.py<pre><code>states = mse_validator.data_loader.dataset.data\nembedding_data = mse_validator.data_loader.dataset.embedding_data\nvis_data = {\n    \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1, :],\n    \"states\": states[: cfg.VIS_DATA_NUMS, 1:, :],\n}\n\nvisualizer = {\n    \"visualize_states\": ppsci.visualize.VisualizerScatter3D(\n        vis_data,\n        {\n            \"pred_states\": lambda d: output_transform(d),\n            \"states\": lambda d: d[\"states\"],\n        },\n        num_timestamps=1,\n        prefix=\"result_states\",\n    )\n}\n</code></pre> <p>\u9996\u5148\u4f7f\u7528\u4e0a\u6587\u4e2d\u7684 <code>mse_validator</code> \u4e2d\u7684\u6570\u636e\u96c6\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u53e6\u5916\u8fd8\u5f15\u5165\u4e86 <code>vis_data_nums</code> \u53d8\u91cf\u7528\u4e8e\u63a7\u5236\u9700\u8981\u53ef\u89c6\u5316\u6837\u672c\u7684\u6570\u91cf\u3002\u6700\u540e\u901a\u8fc7 <code>VisualizerScatter3D</code> \u6784\u5efa\u53ef\u89c6\u5316\u5668\u3002</p>"},{"location":"zh/examples/lorenz/#346","title":"3.4.6 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/lorenz/train_transformer.py<pre><code>solver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    validator=validator,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>"},{"location":"zh/examples/lorenz/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"lorenz/train_enn.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Two-stage training\n# 1. Train a embedding model by running train_enn.py.\n# 2. Load pretrained embedding model and freeze it, then train a transformer model by running train_transformer.py.\n\n# This file is for step1: training a embedding model.\n# This file is based on PaddleScience/ppsci API.\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef get_mean_std(data: np.ndarray):\n    mean = np.asarray(\n        [np.mean(data[:, :, 0]), np.mean(data[:, :, 1]), np.mean(data[:, :, 2])]\n    ).reshape(1, 3)\n    std = np.asarray(\n        [np.std(data[:, :, 0]), np.std(data[:, :, 1]), np.std(data[:, :, 2])]\n    ).reshape(1, 3)\n    return mean, std\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    weights = (1.0 * (cfg.TRAIN_BLOCK_SIZE - 1), 1.0e4 * cfg.TRAIN_BLOCK_SIZE)\n    regularization_key = \"k_matrix\"\n    # manually build constraint(s)\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"LorenzDataset\",\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n            \"stride\": 16,\n            \"weight_dict\": {\n                key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n            },\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 4,\n    }\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELossWithL2Decay(\n            regularization_dict={\n                regularization_key: 1.0e-1 * (cfg.TRAIN_BLOCK_SIZE - 1)\n            }\n        ),\n        {\n            key: lambda out, k=key: out[k]\n            for key in cfg.MODEL.output_keys + (regularization_key,)\n        },\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(sup_constraint.data_loader)\n\n    # manually init model\n    data_mean, data_std = get_mean_std(sup_constraint.data_loader.dataset.data)\n    model = ppsci.arch.LorenzEmbedding(\n        cfg.MODEL.input_keys,\n        cfg.MODEL.output_keys + (regularization_key,),\n        data_mean,\n        data_std,\n    )\n\n    # init optimizer and lr scheduler\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\n    lr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n        iters_per_epoch=ITERS_PER_EPOCH,\n        decay_steps=ITERS_PER_EPOCH,\n        **cfg.TRAIN.lr_scheduler,\n    )()\n    optimizer = ppsci.optimizer.Adam(\n        lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n    )(model)\n\n    # manually build validator\n    weights = (1.0 * (cfg.VALID_BLOCK_SIZE - 1), 1.0e4 * cfg.VALID_BLOCK_SIZE)\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"LorenzDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 32,\n            \"weight_dict\": {\n                key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n            },\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=True,\n        validator=validator,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    weights = (1.0 * (cfg.TRAIN_BLOCK_SIZE - 1), 1.0e4 * cfg.TRAIN_BLOCK_SIZE)\n    regularization_key = \"k_matrix\"\n    # manually build constraint(s)\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"LorenzDataset\",\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n            \"stride\": 16,\n            \"weight_dict\": {\n                key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n            },\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 4,\n    }\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELossWithL2Decay(\n            regularization_dict={\n                regularization_key: 1.0e-1 * (cfg.TRAIN_BLOCK_SIZE - 1)\n            }\n        ),\n        {\n            key: lambda out, k=key: out[k]\n            for key in cfg.MODEL.output_keys + (regularization_key,)\n        },\n        name=\"Sup\",\n    )\n\n    # manually init model\n    data_mean, data_std = get_mean_std(sup_constraint.data_loader.dataset.data)\n    model = ppsci.arch.LorenzEmbedding(\n        cfg.MODEL.input_keys,\n        cfg.MODEL.output_keys + (regularization_key,),\n        data_mean,\n        data_std,\n    )\n\n    # manually build validator\n    weights = (1.0 * (cfg.VALID_BLOCK_SIZE - 1), 1.0e4 * cfg.VALID_BLOCK_SIZE)\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"LorenzDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 32,\n            \"weight_dict\": {\n                key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n            },\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"enn.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> lorenz/train_transformer.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Two-stage training\n# 1. Train a embedding model by running train_enn.py.\n# 2. Load pretrained embedding model and freeze it, then train a transformer model by running train_transformer.py.\n\n# This file is for step2: training a transformer model, based on frozen pretrained embedding model.\n# This file is based on PaddleScience/ppsci API.\nfrom os import path as osp\nfrom typing import Dict\n\nimport hydra\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.arch import base\nfrom ppsci.utils import logger\nfrom ppsci.utils import save_load\n\n\ndef build_embedding_model(embedding_model_path: str) -&gt; ppsci.arch.LorenzEmbedding:\n    input_keys = (\"states\",)\n    output_keys = (\"pred_states\", \"recover_states\")\n    regularization_key = \"k_matrix\"\n    model = ppsci.arch.LorenzEmbedding(input_keys, output_keys + (regularization_key,))\n    save_load.load_pretrain(model, embedding_model_path)\n    return model\n\n\nclass OutputTransform(object):\n    def __init__(self, model: base.Arch):\n        self.model = model\n        self.model.eval()\n\n    def __call__(self, x: Dict[str, paddle.Tensor]):\n        pred_embeds = x[\"pred_embeds\"]\n        pred_states = self.model.decoder(pred_embeds)\n\n        return pred_states\n\n\ndef train(cfg: DictConfig):\n    # train time-series: 2048    time-steps: 256    block-size: 64  stride: 64\n    # valid time-series: 64      time-steps: 1024   block-size: 256 stride: 1024\n    # test  time-series: 256     time-steps: 1024\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    output_transform = OutputTransform(embedding_model)\n\n    # manually build constraint(s)\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"LorenzDataset\",\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n            \"stride\": 64,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 4,\n    }\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(constraint[\"Sup\"].data_loader)\n\n    # manually init model\n    model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n\n    # init optimizer and lr scheduler\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\n    lr_scheduler = ppsci.optimizer.lr_scheduler.CosineWarmRestarts(\n        iters_per_epoch=ITERS_PER_EPOCH, **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(\n        lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n    )(model)\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"LorenzDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 1024,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    # set visualizer(optional)\n    states = mse_validator.data_loader.dataset.data\n    embedding_data = mse_validator.data_loader.dataset.embedding_data\n    vis_data = {\n        \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1, :],\n        \"states\": states[: cfg.VIS_DATA_NUMS, 1:, :],\n    }\n\n    visualizer = {\n        \"visualize_states\": ppsci.visualize.VisualizerScatter3D(\n            vis_data,\n            {\n                \"pred_states\": lambda d: output_transform(d),\n                \"states\": lambda d: d[\"states\"],\n            },\n            num_timestamps=1,\n            prefix=\"result_states\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        validator=validator,\n        visualizer=visualizer,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # directly evaluate pretrained model(optional)\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    output_transform = OutputTransform(embedding_model)\n\n    # manually init model\n    model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"LorenzDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 1024,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    # set visualizer(optional)\n    states = mse_validator.data_loader.dataset.data\n    embedding_data = mse_validator.data_loader.dataset.embedding_data\n    vis_datas = {\n        \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1, :],\n        \"states\": states[: cfg.VIS_DATA_NUMS, 1:, :],\n    }\n\n    visualizer = {\n        \"visulzie_states\": ppsci.visualize.VisualizerScatter3D(\n            vis_datas,\n            {\n                \"pred_states\": lambda d: output_transform(d),\n                \"states\": lambda d: d[\"states\"],\n            },\n            num_timestamps=1,\n            prefix=\"result_states\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n    # visualize prediction for pretrained model(optional)\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    model_cfg = {\n        **cfg.MODEL,\n        \"embedding_model\": embedding_model,\n        \"input_keys\": [\"states\"],\n        \"output_keys\": [\"pred_states\"],\n    }\n    model = ppsci.arch.PhysformerGPT2(**model_cfg)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {\n            key: InputSpec([None, 255, 3], \"float32\", name=key)\n            for key in model.input_keys\n        },\n    ]\n\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    dataset_cfg = {\n        \"name\": \"LorenzDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 1024,\n    }\n\n    dataset = ppsci.data.dataset.build_dataset(dataset_cfg)\n\n    input_dict = {\n        \"states\": dataset.data[: cfg.VIS_DATA_NUMS, :-1, :],\n    }\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_keys = [\"pred_states\"]\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(output_keys, output_dict.keys())\n    }\n\n    input_dict = {\n        \"states\": dataset.data[: cfg.VIS_DATA_NUMS, 1:, :],\n    }\n\n    data_dict = {**input_dict, **output_dict}\n    for i in range(cfg.VIS_DATA_NUMS):\n        ppsci.visualize.save_plot_from_3d_dict(\n            f\"./lorenz_transformer_pred_{i}\",\n            {key: value[i] for key, value in data_dict.items()},\n            (\"states\", \"pred_states\"),\n        )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"transformer.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/lorenz/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u56fe\u4e2d\u5c55\u793a\u4e86\u4e24\u4e2a\u4e0d\u540c\u521d\u59cb\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u548c\u4f20\u7edf\u6570\u503c\u5fae\u5206\u7684\u9884\u6d4b\u7ed3\u679c\u3002</p> <p> </p> \u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff08\"pred_states\"\uff09\u4e0e\u4f20\u7edf\u6570\u503c\u5fae\u5206\u7ed3\u679c\uff08\"states\"\uff09 <p> </p> \u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff08\"pred_states\"\uff09\u4e0e\u4f20\u7edf\u6570\u503c\u5fae\u5206\u7ed3\u679c\uff08\"states\"\uff09"},{"location":"zh/examples/nlsmb/","title":"NLSMB","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#nls-mb","title":"NLS-MB","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># soliton\npython NLS-MB_optical_soliton.py\n# rogue wave\npython NLS-MB_optical_rogue_wave.py\n</code></pre> <pre><code># soliton\npython NLS-MB_optical_soliton.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/NLS-MB/NLS-MB_soliton_pretrained.pdparams\n# rogue wave\npython NLS-MB_optical_rogue_wave.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/NLS-MB/NLS-MB_rogue_wave_pretrained.pdparams\n</code></pre> <pre><code># soliton\npython NLS-MB_optical_soliton.py mode=export\n# rogue wave\npython NLS-MB_optical_rogue_wave.py mode=export\n</code></pre> <pre><code># soliton\npython NLS-MB_optical_soliton.py mode=infer\n# rogue wave\npython NLS-MB_optical_rogue_wave.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 NLS-MB_soliton_pretrained.pdparams Residual/loss: 0.00000Residual/MSE.Schrodinger_1: 0.00000Residual/MSE.Schrodinger_2: 0.00000Residual/MSE.Maxwell_1: 0.00000Residual/MSE.Maxwell_2: 0.00000Residual/MSE.Bloch: 0.00000 NLS-MB_optical_rogue_wave.pdparams Residual/loss: 0.00001Residual/MSE.Schrodinger_1: 0.00000Residual/MSE.Schrodinger_2: 0.00000Residual/MSE.Maxwell_1: 0.00000Residual/MSE.Maxwell_2: 0.00000Residual/MSE.Bloch: 0.00000","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u975e\u7ebf\u6027\u5c40\u57df\u6ce2\u52a8\u529b\u5b66\uff0c\u4f5c\u4e3a\u975e\u7ebf\u6027\u79d1\u5b66\u7684\u91cd\u8981\u5206\u652f\uff0c\u6db5\u76d6\u4e86\u5b64\u5b50\u3001\u547c\u5438\u5b50\u548c\u602a\u6ce2\u7b49\u57fa\u672c\u5f62\u5f0f\u7684\u975e\u7ebf\u6027\u5c40\u57df\u6ce2\u3002\u6fc0\u5149\u9501\u6a21\u6280\u672f\u4e3a\u8fd9\u4e9b\u7406\u8bba\u9884\u8a00\u7684\u975e\u7ebf\u6027\u5c40\u57df\u6ce2\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u5e73\u53f0\uff0c\u4eba\u4eec\u901a\u8fc7\u6b64\u6280\u672f\u89c2\u5bdf\u5230\u4e86\u5b64\u5b50\u5206\u5b50\u548c\u602a\u6ce2\u7b49\u4e30\u5bcc\u7684\u975e\u7ebf\u6027\u73b0\u8c61\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8\u4e86\u975e\u7ebf\u6027\u5c40\u57df\u6ce2\u7684\u7814\u7a76\u3002\u76ee\u524d\uff0c\u8be5\u9886\u57df\u7684\u7814\u7a76\u5df2\u6df1\u5165\u6d41\u4f53\u529b\u5b66\u3001\u975e\u7ebf\u6027\u5149\u5b66\u3001\u73bb\u8272-\u7231\u56e0\u65af\u5766\u51dd\u805a(BEC)\u3001\u7b49\u79bb\u5b50\u4f53\u7269\u7406\u7b49\u591a\u4e2a\u7269\u7406\u9886\u57df\u3002\u5728\u5149\u7ea4\u9886\u57df\uff0c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u7814\u7a76\u57fa\u4e8e\u5149\u7ea4\u7684\u5149\u5b66\u5668\u4ef6\u3001\u4fe1\u606f\u5904\u7406\u3001\u6750\u6599\u8bbe\u8ba1\u4ee5\u53ca\u4fe1\u53f7\u4f20\u8f93\u7684\u539f\u7406\uff0c\u5bf9\u5149\u7ea4\u6fc0\u5149\u5668\u3001\u653e\u5927\u5668\u3001\u6ce2\u5bfc\u548c\u901a\u4fe1\u6280\u672f\u7684\u53d1\u5c55\u8d77\u5230\u4e86\u5173\u952e\u4f5c\u7528\u3002\u5149\u8109\u51b2\u5728\u5149\u7ea4\u4e2d\u7684\u4f20\u64ad\u52a8\u529b\u5b66\u53d7\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\uff08\u5982\u975e\u7ebf\u6027\u859b\u5b9a\u8c14\u65b9\u7a0bNLSE\uff09\u7684\u8c03\u63a7\u3002\u5f53\u8272\u6563\u4e0e\u975e\u7ebf\u6027\u6548\u5e94\u5171\u5b58\u65f6\uff0c\u8fd9\u4e9b\u65b9\u7a0b\u5f80\u5f80\u96be\u4ee5\u89e3\u6790\u6c42\u89e3\u3002\u56e0\u6b64\uff0c\u5206\u6b65\u5085\u7acb\u53f6\u65b9\u6cd5\u53ca\u5176\u6539\u8fdb\u7248\u672c\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u7814\u7a76\u5149\u7ea4\u4e2d\u7684\u975e\u7ebf\u6027\u6548\u5e94\uff0c\u5176\u4f18\u52bf\u5728\u4e8e\u5b9e\u73b0\u7b80\u5355\u4e14\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5bf9\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u957f\u8ddd\u79bb\u4e14\u9ad8\u5ea6\u975e\u7ebf\u6027\u7684\u573a\u666f\uff0c\u4e3a\u6ee1\u8db3\u7cbe\u5ea6\u9700\u6c42\uff0c\u5fc5\u987b\u5927\u5e45\u51cf\u5c11\u5206\u6b65\u5085\u7acb\u53f6\u65b9\u6cd5\u7684\u6b65\u957f\uff0c\u8fd9\u65e0\u7591\u589e\u52a0\u4e86\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u65f6\u57df\u4e2d\u7f51\u683c\u70b9\u96c6\u6570\u91cf\u5e9e\u5927\uff0c\u8ba1\u7b97\u8fc7\u7a0b\u8017\u65f6\u8f83\u957f\u3002PINN\u6bd4\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5728\u6570\u636e\u5c11\u5f97\u591a\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8ba1\u7b97\u590d\u6742\u6027\uff08\u4ee5\u500d\u6570\u8868\u793a\uff09\u901a\u5e38\u6bd4SFM\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u5728\u63ba\u94d2\u5149\u7ea4\u4e2d\uff0c\u5149\u8109\u51b2\u7684\u4f20\u64ad\u6027\u8d28\u53ef\u4ee5\u7531\u8026\u5408NLS-MB\u65b9\u7a0b\u6765\u63cf\u8ff0\uff0c\u5176\u5f62\u5f0f\u4e3a</p> \\[ \\begin{cases}    \\dfrac{\\partial E}{\\partial x} = i \\alpha_1 \\dfrac{\\partial^2 E}{\\partial t ^2} - i \\alpha_2 |E|^2 E+2 p \\\\    \\dfrac{\\partial p}{\\partial t} = 2 i \\omega_0 p+2 E \\eta \\\\    \\dfrac{\\partial \\eta}{\\partial t} = -(E p^* + E^* p) \\end{cases} \\] <p>\u5176\u4e2d\uff0cx, t\u5206\u522b\u8868\u793a\u5f52\u4e00\u5316\u7684\u4f20\u64ad\u8ddd\u79bb\u548c\u65f6\u95f4\uff0c\u590d\u5305\u7edcE\u662f\u6162\u53d8\u7684\u7535\u573a\uff0cp\u662f\u5171\u632f\u4ecb\u8d28\u504f\u632f\u7684\u91cf\u5ea6\uff0c\\(\\eta\\)\u8868\u793a\u7c92\u5b50\u6570\u53cd\u8f6c\u7684\u7a0b\u5ea6\uff0c\u7b26\u53f7*\u8868\u793a\u590d\u5171\u8f6d\u3002\\(\\alpha_1\\)\u662f\u7fa4\u901f\u5ea6\u8272\u6563\u53c2\u6570\uff0c\\(\\alpha_2\\)\u200b\u200b\u662fKerr\u975e\u7ebf\u6027\u53c2\u6570\uff0c\u662f\u6d4b\u91cf\u5171\u632f\u9891\u7387\u7684\u504f\u79fb\u3002NLS-MB\u7cfb\u7edf\u662f\u7531Maimistov\u548cManykin\u9996\u6b21\u63d0\u51fa\u6765\u7684,\u7528\u6765\u63cf\u8ff0\u6781\u77ed\u7684\u8109\u51b2\u5728Kerr\u975e\u7ebf\u6027\u4ecb\u8d28\u4e2d\u7684\u4f20\u64ad.\u8be5\u7cfb\u7edf\u5728\u89e3\u51b3\u5149\u7ea4\u635f\u8017\u4f7f\u5f97\u5176\u4f20\u8f93\u8ddd\u79bb\u53d7\u9650\u8fd9\u4e00\u95ee\u9898\u4e0a,\u4e5f\u626e\u6f14\u7740\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5728\u8fd9\u4e2a\u65b9\u7a0b\u4e2d\uff0c\u5b83\u63cf\u8ff0\u7684\u662f\u81ea\u611f\u5e94\u900f\u660e\u5b64\u5b50\u548cNLS\u5b64\u5b50\u7684\u6df7\u5408\u72b6\u6001\uff0c\u79f0\u4f5cSIT-NLS\u5b64\u5b50\uff0c\u8fd9\u4e24\u79cd\u5b64\u5b50\u53ef\u4ee5\u5171\u5b58\uff0c\u5e76\u4e14\u5df2\u7ecf\u6709\u5f88\u591a\u5173\u4e8e\u5176\u5728\u5149\u7ea4\u901a\u4fe1\u4e2d\u7684\u7814\u7a76.</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#21-optical-soliton","title":"2.1 Optical soliton","text":"<p>\u5728\u5149\u7ea4\u7684\u53cd\u5e38\u8272\u6563\u533a\uff0c\u7531\u4e8e\u8272\u6563\u548c\u975e\u7ebf\u6027\u6548\u5e94\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u53ef\u4ea7\u751f\u4e00\u79cd\u975e\u5e38\u5f15\u4eba\u6ce8\u76ee\u7684\u73b0\u8c61\u2014\u2014\u5149\u5b64\u5b50\u3002\u201c\u5b64\u5b50\u201d(soliton)\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u6ce2\u5305\uff0c\u5b83\u53ef\u4ee5\u4f20\u8f93\u5f88\u957f\u8ddd\u79bb\u800c\u4e0d\u53d8\u5f62\u5b64\u5b50\u5728\u7269\u7406\u5b66\u7684\u8bb8\u591a\u5206\u652f\u5df2\u5f97\u5230\u5e7f\u7684\u7814\u7a76\uff0c\u672c\u6848\u4f8b\u8ba8\u8bba\u7684\u5149\u7ea4\u4e2d\u7684\u5b64\u5b50\u4e0d\u4ec5\u5177\u6709\u57fa\u7840\u7406\u8bba\u7814\u7a76\u4ef7\u503c\uff0c\u800c\u4e14\u5728\u5149\u7ea4\u901a\u4fe1\u65b9\u9762\u4e5f\u6709\u5b9e\u9645\u5e94\u7528\u3002</p> \\[ \\begin{gathered}   E(x,t) = \\frac{{2\\exp ( - 2it)}}{{\\cosh (2t + 6x)}},  \\\\   p(x,t) = \\frac{{\\exp ( - 2it)\\left\\{ {\\exp ( - 2t - 6x) - \\exp (2t + 6x)} \\right\\}}}{{\\cosh {{(2t + 6x)}^2}}},  \\\\   \\eta (x,t) = \\frac{{\\cosh {{(2t + 6x)}^2} - 2}}{{\\cosh {{(2t + 6x)}^2}}}. \\end{gathered} \\] <p>\u6211\u4eec\u8003\u8651\u8ba1\u7b97\u57df\u4e3a \\([\u22121, 1] \u00d7 [\u22121, 1]\\)\u3002 \u6211\u4eec\u9996\u5148\u786e\u5b9a\u4f18\u5316\u7b56\u7565\u3002 \u6bcf\u4e2a\u8fb9\u754c\u4e0a\u6709 \\(200\\) \u4e2a\u70b9,\u5373 \\(N_b = 2 \u00d7 200\\)\u3002\u4e3a\u4e86\u8ba1\u7b97 NLS-MB \u7684\u65b9\u7a0b\u635f\u5931,\u5728\u57df\u5185\u968f\u673a\u9009\u62e9 \\(20,000\\) \u4e2a\u70b9\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#22-optical-rogue-wave","title":"2.2 Optical rogue wave","text":"<p>\u5149\u5b66\u602a\u6ce2\uff08Optical rogue waves\uff09\u662f\u5149\u5b66\u4e2d\u7684\u4e00\u79cd\u73b0\u8c61\uff0c\u7c7b\u4f3c\u4e8e\u6d77\u6d0b\u4e2d\u7684\u5b64\u7acb\u6d77\u6d6a\uff0c\u4f46\u5728\u5149\u5b66\u7cfb\u7edf\u4e2d\u3002\u5b83\u4eec\u662f\u7a81\u7136\u51fa\u73b0\u5e76\u4e14\u5e45\u5ea6\u5f02\u5e38\u9ad8\u7684\u5149\u6ce2\uff0c\u5149\u5b66\u5b64\u7acb\u5b50\u6ce2\u6709\u4e00\u4e9b\u6f5c\u5728\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u5149\u901a\u4fe1\u548c\u6fc0\u5149\u6280\u672f\u9886\u57df\u3002\u4e00\u4e9b\u7814\u7a76\u8868\u660e\uff0c\u5b83\u4eec\u53ef\u4ee5\u7528\u4e8e\u589e\u5f3a\u5149\u4fe1\u53f7\u7684\u4f20\u8f93\u548c\u5904\u7406\uff0c\u6216\u8005\u7528\u4e8e\u4ea7\u751f\u8d85\u77ed\u8109\u51b2\u6fc0\u5149\u3002 \u6211\u4eec\u8003\u8651\u8ba1\u7b97\u57df\u4e3a \\([\u22120.5, 0.5] \u00d7 [\u22122.5, 2.5]\\)</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u672c\u6587\u4f7f\u7528PINN\u7ecf\u5178\u7684MLP\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002</p> <pre><code>model = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e Optical soliton \u4f7f\u7528\u7684\u662f NLS-MB \u65b9\u7a0b\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>NLSMB</code>\u3002</p> <pre><code>equation = {\n    \"NLS-MB\": ppsci.equation.NLSMB(alpha_1=0.5, alpha_2=-1, omega_0=-1, time=True)\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d Optical soliton \u95ee\u9898\u4f5c\u7528\u5728\u4ee5\u7a7a\u95f4(-1.0, 1.0),  \u65f6\u95f4(-1.0, 1.0) \u7684\u65f6\u7a7a\u533a\u57df\uff0c \u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684\u65f6\u7a7a\u51e0\u4f55 <code>time_interval</code> \u4f5c\u4e3a\u8ba1\u7b97\u57df\u3002</p> <pre><code>geom = {\n    \"time_interval\": ppsci.geometry.TimeXGeometry(\n        ppsci.geometry.TimeDomain(t_lower, t_upper, timestamps=timestamps),\n        ppsci.geometry.Interval(x_lower, x_upper),\n    )\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u56e0\u6570\u636e\u96c6\u4e3a\u89e3\u6790\u89e3,\u6211\u4eec\u5148\u6784\u9020\u89e3\u6790\u89e3\u51fd\u6570</p> <pre><code>def analytic_solution(out):\n    t, x = out[\"t\"], out[\"x\"]\n    Eu_true = 2 * np.cos(2 * t) / np.cosh(2 * t + 6 * x)\n\n    Ev_true = -2 * np.sin(2 * t) / np.cosh(2 * t + 6 * x)\n\n    pu_true = (\n        (np.exp(-2 * t - 6 * x) - np.exp(2 * t + 6 * x))\n        * np.cos(2 * t)\n        / np.cosh(2 * t + 6 * x) ** 2\n    )\n    pv_true = (\n        -(np.exp(-2 * t - 6 * x) - np.exp(2 * t + 6 * x))\n        * np.sin(2 * t)\n        / np.cosh(2 * t + 6 * x) ** 2\n    )\n    eta_true = (np.cosh(2 * t + 6 * x) ** 2 - 2) / np.cosh(2 * t + 6 * x) ** 2\n\n    return Eu_true, Ev_true, pu_true, pv_true, eta_true\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u4ee5\u4f5c\u7528\u5728\u5185\u90e8\u70b9\u4e0a\u7684 <code>InteriorConstraint</code> \u4e3a\u4f8b\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>pde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"NLS-MB\"].equations,\n    {\n        \"Schrodinger_1\": 0,\n        \"Schrodinger_2\": 0,\n        \"Maxwell_1\": 0,\n        \"Maxwell_2\": 0,\n        \"Bloch\": 0,\n    },\n    geom[\"time_interval\"],\n    {\n        \"dataset\": {\"name\": \"IterableNamedArrayDataset\"},\n        \"batch_size\": 20000,\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    },\n    ppsci.loss.MSELoss(),\n    evenly=True,\n    name=\"EQ\",\n)\n</code></pre> <p><code>InteriorConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\uff08\u7ec4\uff09\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"NLS-MB\"].equations</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u53d8\u91cf\u7684\u76ee\u6807\u503c\uff0c\u5728\u672c\u95ee\u9898\u4e2d\u5e0c\u671b NLS-MB \u6bcf\u4e2a\u65b9\u7a0b\u5747\u88ab\u4f18\u5316\u81f3 0\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u65b9\u7a0b\u4f5c\u7528\u7684\u8ba1\u7b97\u57df\uff0c\u6b64\u5904\u586b\u5165\u5728 3.3 \u8ba1\u7b97\u57df\u6784\u5efa \u7ae0\u8282\u5b9e\u4f8b\u5316\u597d\u7684 <code>geom[\"time_interval\"]</code> \u5373\u53ef\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u5728\u8ba1\u7b97\u57df\u4e0a\u7684\u91c7\u6837\u914d\u7f6e\uff0c\u6b64\u5904\u8bbe\u7f6e <code>batch_size</code> \u4e3a <code>20000</code>\u3002</p> <p>\u7b2c\u4e94\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u9009\u7528\u5e38\u7528\u7684 MSE \u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u7684\u5747\u65b9\u8bef\u5dee\uff1b</p> <p>\u7b2c\u516d\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"EQ\" \u5373\u53ef\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u7531\u4e8e\u6211\u4eec\u8fb9\u754c\u70b9\u548c\u521d\u503c\u70b9\u5177\u6709\u89e3\u6790\u89e3,\u56e0\u6b64\u6211\u4eec\u4f7f\u7528\u76d1\u7763\u7ea6\u675f</p> <pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"Sup\",\n)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 50000 \u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c0.001 \u7684\u521d\u59cb\u5b66\u4e60\u7387\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 50000\n  iters_per_epoch: 1\n  lbfgs:\n    iters_per_epoch: ${TRAIN.iters_per_epoch}\n    output_dir: ${output_dir}LBFGS\n    learning_rate: 1.0\n    max_iter: 1\n    eval_freq: ${TRAIN.eval_freq}\n    eval_during_train: ${TRAIN.eval_during_train}\n  eval_during_train: true\n  eval_freq: 1000\n  learning_rate: 0.001\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code>optimizer = ppsci.optimizer.Adam(learning_rate=cfg.TRAIN.learning_rate)(model)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.GeometryValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code>residual_validator = ppsci.validate.GeometryValidator(\n    equation[\"NLS-MB\"].equations,\n    {\n        \"Schrodinger_1\": 0,\n        \"Schrodinger_2\": 0,\n        \"Maxwell_1\": 0,\n        \"Maxwell_2\": 0,\n        \"Bloch\": 0,\n    },\n    geom[\"time_interval\"],\n    {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"total_size\": 20600,\n    },\n    ppsci.loss.MSELoss(),\n    evenly=True,\n    metric={\"MSE\": ppsci.metric.MSE()},\n    with_initial=True,\n    name=\"Residual\",\n)\nvalidator = {residual_validator.name: residual_validator}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#38","title":"3.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u8ba1\u7b97\u57df\u53d6\u70b9\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u624b\u52a8\u8ba1\u7b97\u51fa\u632f\u5e45\uff0c\u5e76\u53ef\u89c6\u5316\u7ed3\u679c\u3002</p> <pre><code>vis_points = geom[\"time_interval\"].sample_interior(20000, evenly=True)\nEu_true, Ev_true, pu_true, pv_true, eta_true = analytic_solution(vis_points)\npred = solver.predict(vis_points, return_numpy=True)\nt = vis_points[\"t\"][:, 0]\nx = vis_points[\"x\"][:, 0]\nE_ref = np.sqrt(Eu_true**2 + Ev_true**2)[:, 0]\nE_pred = np.sqrt(pred[\"Eu\"] ** 2 + pred[\"Ev\"] ** 2)[:, 0]\np_ref = np.sqrt(pu_true**2 + pv_true**2)[:, 0]\np_pred = np.sqrt(pred[\"pu\"] ** 2 + pred[\"pv\"] ** 2)[:, 0]\neta_ref = eta_true[:, 0]\neta_pred = pred[\"eta\"][:, 0]\n\n# plot\nplot(t, x, E_ref, E_pred, p_ref, p_pred, eta_ref, eta_pred, cfg.output_dir)\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#391-adam","title":"3.9.1 \u4f7f\u7528 Adam \u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code>solver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    epochs=cfg.TRAIN.epochs,\n    iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#392-l-bfgs","title":"3.9.2 \u4f7f\u7528 L-BFGS \u5fae\u8c03[\u53ef\u9009]","text":"<p>\u5728\u4f7f\u7528 <code>Adam</code> \u4f18\u5316\u5668\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u4f18\u5316\u5668\u66f4\u6362\u6210\u4e8c\u9636\u4f18\u5316\u5668 <code>L-BFGS</code> \u7ee7\u7eed\u8bad\u7ec3\u5c11\u91cf\u8f6e\u6570\uff08\u6b64\u5904\u6211\u4eec\u4f7f\u7528 <code>Adam</code> \u4f18\u5316\u8f6e\u6570\u7684 10% \u5373\u53ef\uff09\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002</p> <pre><code>OUTPUT_DIR = cfg.TRAIN.lbfgs.output_dir\nlogger.init_logger(\"ppsci\", osp.join(OUTPUT_DIR, f\"{cfg.mode}.log\"), \"info\")\nEPOCHS = cfg.TRAIN.epochs // 10\noptimizer_lbfgs = ppsci.optimizer.LBFGS(\n    cfg.TRAIN.lbfgs.learning_rate, cfg.TRAIN.lbfgs.max_iter\n)(model)\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    OUTPUT_DIR,\n    optimizer_lbfgs,\n    None,\n    EPOCHS,\n    cfg.TRAIN.lbfgs.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.lbfgs.eval_during_train,\n    eval_freq=cfg.TRAIN.lbfgs.eval_freq,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre> \u63d0\u793a <p>\u5728\u5e38\u89c4\u4f18\u5316\u5668\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4f7f\u7528 <code>L-BFGS</code> \u5fae\u8c03\u5c11\u91cf\u8f6e\u6570\u7684\u65b9\u6cd5\uff0c\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u90fd\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"NLS-MB_optical_soliton.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef analytic_solution(out):\n    t, x = out[\"t\"], out[\"x\"]\n    Eu_true = 2 * np.cos(2 * t) / np.cosh(2 * t + 6 * x)\n\n    Ev_true = -2 * np.sin(2 * t) / np.cosh(2 * t + 6 * x)\n\n    pu_true = (\n        (np.exp(-2 * t - 6 * x) - np.exp(2 * t + 6 * x))\n        * np.cos(2 * t)\n        / np.cosh(2 * t + 6 * x) ** 2\n    )\n    pv_true = (\n        -(np.exp(-2 * t - 6 * x) - np.exp(2 * t + 6 * x))\n        * np.sin(2 * t)\n        / np.cosh(2 * t + 6 * x) ** 2\n    )\n    eta_true = (np.cosh(2 * t + 6 * x) ** 2 - 2) / np.cosh(2 * t + 6 * x) ** 2\n\n    return Eu_true, Ev_true, pu_true, pv_true, eta_true\n\n\ndef plot(\n    t: np.ndarray,\n    x: np.ndarray,\n    E_ref: np.ndarray,\n    E_pred: np.ndarray,\n    p_ref: np.ndarray,\n    p_pred: np.ndarray,\n    eta_ref: np.ndarray,\n    eta_pred: np.ndarray,\n    output_dir: str,\n):\n    fig = plt.figure(figsize=(10, 10))\n    plt.subplot(3, 3, 1)\n    plt.title(\"E_ref\")\n    plt.tricontourf(x, t, E_ref, levels=256, cmap=\"jet\")\n    plt.subplot(3, 3, 2)\n    plt.title(\"E_pred\")\n    plt.tricontourf(x, t, E_pred, levels=256, cmap=\"jet\")\n    plt.subplot(3, 3, 3)\n    plt.title(\"E_diff\")\n    plt.tricontourf(x, t, np.abs(E_ref - E_pred), levels=256, cmap=\"jet\")\n    plt.subplot(3, 3, 4)\n    plt.title(\"p_ref\")\n    plt.tricontourf(x, t, p_ref, levels=256, cmap=\"jet\")\n    plt.subplot(3, 3, 5)\n    plt.title(\"p_pred\")\n    plt.tricontourf(x, t, p_pred, levels=256, cmap=\"jet\")\n    plt.subplot(3, 3, 6)\n    plt.title(\"p_diff\")\n    plt.tricontourf(x, t, np.abs(p_ref - p_pred), levels=256, cmap=\"jet\")\n    plt.subplot(3, 3, 7)\n    plt.title(\"eta_ref\")\n    plt.tricontourf(x, t, eta_ref, levels=256, cmap=\"jet\")\n    plt.subplot(3, 3, 8)\n    plt.title(\"eta_pred\")\n    plt.tricontourf(x, t, eta_pred, levels=256, cmap=\"jet\")\n    plt.subplot(3, 3, 9)\n    plt.title(\"eta_diff\")\n    plt.tricontourf(x, t, np.abs(eta_ref - eta_pred), levels=256, cmap=\"jet\")\n    fig_path = osp.join(output_dir, \"pred_optical_soliton.png\")\n    print(f\"Saving figure to {fig_path}\")\n    fig.savefig(fig_path, bbox_inches=\"tight\", dpi=400)\n    plt.close()\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\n        \"NLS-MB\": ppsci.equation.NLSMB(alpha_1=0.5, alpha_2=-1, omega_0=-1, time=True)\n    }\n\n    x_lower = -1\n    x_upper = 1\n    t_lower = -1\n    t_upper = 1\n    # set timestamps(including initial t0)\n    timestamps = np.linspace(t_lower, t_upper, cfg.NTIME_ALL, endpoint=True)\n    # set time-geometry\n    geom = {\n        \"time_interval\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(t_lower, t_upper, timestamps=timestamps),\n            ppsci.geometry.Interval(x_lower, x_upper),\n        )\n    }\n\n    X, T = np.meshgrid(\n        np.linspace(x_lower, x_upper, 256), np.linspace(t_lower, t_upper, 256)\n    )\n    X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n\n    # Boundary and Initial conditions\n    ic = X_star[:, 1] == t_lower\n    idx_ic = np.random.choice(np.where(ic)[0], 200, replace=False)\n    lb = X_star[:, 0] == x_lower\n    idx_lb = np.random.choice(np.where(lb)[0], 200, replace=False)\n    ub = X_star[:, 0] == x_upper\n    idx_ub = np.random.choice(np.where(ub)[0], 200, replace=False)\n    icbc_idx = np.hstack((idx_lb, idx_ic, idx_ub))\n    X_u_train = X_star[icbc_idx].astype(\"float32\")\n    X_u_train = {\"t\": X_u_train[:, 1:2], \"x\": X_u_train[:, 0:1]}\n\n    Eu_train, Ev_train, pu_train, pv_train, eta_train = analytic_solution(X_u_train)\n\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"t\": X_u_train[\"t\"], \"x\": X_u_train[\"x\"]},\n            \"label\": {\n                \"Eu\": Eu_train,\n                \"Ev\": Ev_train,\n                \"pu\": pu_train,\n                \"pv\": pv_train,\n                \"eta\": eta_train,\n            },\n        },\n        \"batch_size\": 600,\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    }\n\n    # set constraint\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"NLS-MB\"].equations,\n        {\n            \"Schrodinger_1\": 0,\n            \"Schrodinger_2\": 0,\n            \"Maxwell_1\": 0,\n            \"Maxwell_2\": 0,\n            \"Bloch\": 0,\n        },\n        geom[\"time_interval\"],\n        {\n            \"dataset\": {\"name\": \"IterableNamedArrayDataset\"},\n            \"batch_size\": 20000,\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        },\n        ppsci.loss.MSELoss(),\n        evenly=True,\n        name=\"EQ\",\n    )\n\n    # supervised constraint s.t ||u-u_0||\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"Sup\",\n    )\n\n    # wrap constraints together\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        sup_constraint.name: sup_constraint,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(learning_rate=cfg.TRAIN.learning_rate)(model)\n\n    # set validator\n    residual_validator = ppsci.validate.GeometryValidator(\n        equation[\"NLS-MB\"].equations,\n        {\n            \"Schrodinger_1\": 0,\n            \"Schrodinger_2\": 0,\n            \"Maxwell_1\": 0,\n            \"Maxwell_2\": 0,\n            \"Bloch\": 0,\n        },\n        geom[\"time_interval\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"total_size\": 20600,\n        },\n        ppsci.loss.MSELoss(),\n        evenly=True,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        with_initial=True,\n        name=\"Residual\",\n    )\n    validator = {residual_validator.name: residual_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        epochs=cfg.TRAIN.epochs,\n        iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n    # fine-tuning pretrained model with L-BFGS\n    OUTPUT_DIR = cfg.TRAIN.lbfgs.output_dir\n    logger.init_logger(\"ppsci\", osp.join(OUTPUT_DIR, f\"{cfg.mode}.log\"), \"info\")\n    EPOCHS = cfg.TRAIN.epochs // 10\n    optimizer_lbfgs = ppsci.optimizer.LBFGS(\n        cfg.TRAIN.lbfgs.learning_rate, cfg.TRAIN.lbfgs.max_iter\n    )(model)\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        OUTPUT_DIR,\n        optimizer_lbfgs,\n        None,\n        EPOCHS,\n        cfg.TRAIN.lbfgs.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.lbfgs.eval_during_train,\n        eval_freq=cfg.TRAIN.lbfgs.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n    # visualize prediction\n    vis_points = geom[\"time_interval\"].sample_interior(20000, evenly=True)\n    Eu_true, Ev_true, pu_true, pv_true, eta_true = analytic_solution(vis_points)\n    pred = solver.predict(vis_points, return_numpy=True)\n    t = vis_points[\"t\"][:, 0]\n    x = vis_points[\"x\"][:, 0]\n    E_ref = np.sqrt(Eu_true**2 + Ev_true**2)[:, 0]\n    E_pred = np.sqrt(pred[\"Eu\"] ** 2 + pred[\"Ev\"] ** 2)[:, 0]\n    p_ref = np.sqrt(pu_true**2 + pv_true**2)[:, 0]\n    p_pred = np.sqrt(pred[\"pu\"] ** 2 + pred[\"pv\"] ** 2)[:, 0]\n    eta_ref = eta_true[:, 0]\n    eta_pred = pred[\"eta\"][:, 0]\n\n    # plot\n    plot(t, x, E_ref, E_pred, p_ref, p_pred, eta_ref, eta_pred, cfg.output_dir)\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\n        \"NLS-MB\": ppsci.equation.NLSMB(alpha_1=0.5, alpha_2=-1, omega_0=-1, time=True)\n    }\n\n    # set geometry\n    x_lower = -1\n    x_upper = 1\n    t_lower = -1\n    t_upper = 1\n    # set timestamps(including initial t0)\n    timestamps = np.linspace(t_lower, t_upper, cfg.NTIME_ALL, endpoint=True)\n    # set time-geometry\n    geom = {\n        \"time_interval\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(t_lower, t_upper, timestamps=timestamps),\n            ppsci.geometry.Interval(x_lower, x_upper),\n        )\n    }\n\n    # set validator\n    residual_validator = ppsci.validate.GeometryValidator(\n        equation[\"NLS-MB\"].equations,\n        {\n            \"Schrodinger_1\": 0,\n            \"Schrodinger_2\": 0,\n            \"Maxwell_1\": 0,\n            \"Maxwell_2\": 0,\n            \"Bloch\": 0,\n        },\n        geom[\"time_interval\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"total_size\": 20600,\n        },\n        ppsci.loss.MSELoss(),\n        evenly=True,\n        metric={\"MSE\": ppsci.metric.MSE()},\n        with_initial=True,\n        name=\"Residual\",\n    )\n    validator = {residual_validator.name: residual_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        eval_freq=cfg.TRAIN.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n\n    # visualize prediction\n    vis_points = geom[\"time_interval\"].sample_interior(20000, evenly=True)\n    Eu_true, Ev_true, pu_true, pv_true, eta_true = analytic_solution(vis_points)\n    pred = solver.predict(vis_points, return_numpy=True)\n    t = vis_points[\"t\"][:, 0]\n    x = vis_points[\"x\"][:, 0]\n    E_ref = np.sqrt(Eu_true**2 + Ev_true**2)[:, 0]\n    E_pred = np.sqrt(pred[\"Eu\"] ** 2 + pred[\"Ev\"] ** 2)[:, 0]\n    p_ref = np.sqrt(pu_true**2 + pv_true**2)[:, 0]\n    p_pred = np.sqrt(pred[\"pu\"] ** 2 + pred[\"pv\"] ** 2)[:, 0]\n    eta_ref = eta_true[:, 0]\n    eta_pred = pred[\"eta\"][:, 0]\n\n    # plot\n    plot(t, x, E_ref, E_pred, p_ref, p_pred, eta_ref, eta_pred, cfg.output_dir)\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # set geometry\n    x_lower = -1\n    x_upper = 1\n    t_lower = -1\n    t_upper = 1\n    # set timestamps(including initial t0)\n    timestamps = np.linspace(t_lower, t_upper, cfg.NTIME_ALL, endpoint=True)\n    # set time-geometry\n    geom = {\n        \"time_interval\": ppsci.geometry.TimeXGeometry(\n            ppsci.geometry.TimeDomain(t_lower, t_upper, timestamps=timestamps),\n            ppsci.geometry.Interval(x_lower, x_upper),\n        )\n    }\n\n    NPOINT_TOTAL = cfg.NPOINT_INTERIOR + cfg.NPOINT_BC\n    input_dict = geom[\"time_interval\"].sample_interior(NPOINT_TOTAL, evenly=True)\n\n    output_dict = predictor.predict(\n        {key: input_dict[key] for key in cfg.MODEL.input_keys}, cfg.INFER.batch_size\n    )\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n\n    # visualize prediction\n    Eu_true, Ev_true, pu_true, pv_true, eta_true = analytic_solution(input_dict)\n    t = input_dict[\"t\"][:, 0]\n    x = input_dict[\"x\"][:, 0]\n    E_ref = np.sqrt(Eu_true**2 + Ev_true**2)[:, 0]\n    E_pred = np.sqrt(output_dict[\"Eu\"] ** 2 + output_dict[\"Ev\"] ** 2)[:, 0]\n    p_ref = np.sqrt(pu_true**2 + pv_true**2)[:, 0]\n    p_pred = np.sqrt(output_dict[\"pu\"] ** 2 + output_dict[\"pv\"] ** 2)[:, 0]\n    eta_ref = eta_true[:, 0]\n    eta_pred = output_dict[\"eta\"][:, 0]\n\n    # plot\n    plot(t, x, E_ref, E_pred, p_ref, p_pred, eta_ref, eta_pred, cfg.output_dir)\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"NLS-MB_soliton.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#51-optical_soliton","title":"5.1 optical_soliton","text":"\u89e3\u6790\u89e3\u7ed3\u679c\u4e0e PINN \u9884\u6d4b\u7ed3\u679c\u5bf9\u6bd4\uff0c\u4ece\u4e0a\u5230\u4e0b\u5206\u522b\u4e3a\uff1a\u6162\u53d8\u7535\u573a\uff08E\uff09\uff0c\u5171\u632f\u504f\u91cf\uff08p\uff09\u4ee5\u53ca\u7c92\u5b50\u6570\u53cd\u8f6c\u7a0b\u5ea6\uff08eta\uff09","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#52-optical_rogue_wave","title":"5.2 optical_rogue_wave","text":"\u89e3\u6790\u89e3\u7ed3\u679c\u4e0e PINN \u9884\u6d4b\u7ed3\u679c\u5bf9\u6bd4\uff0c\u4ece\u4e0a\u5230\u4e0b\u5206\u522b\u4e3a\uff1a\u6162\u53d8\u7535\u573a\uff08E\uff09\uff0c\u5171\u632f\u504f\u91cf\uff08p\uff09\u4ee5\u53ca\u7c92\u5b50\u6570\u53cd\u8f6c\u7a0b\u5ea6\uff08eta\uff09 <p>\u53ef\u4ee5\u770b\u5230PINN\u9884\u6d4b\u4e0e\u89e3\u6790\u89e3\u7684\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nlsmb/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<p>[1] S.-Y. Xu, Q. Zhou, and W. Liu, Prediction of Soliton Evolution and Equation Parameters for NLS\u2013MB Equation Based on the phPINN Algorithm, Nonlinear Dyn (2023).</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","NLSMB\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206","\u590d\u6570","\u4e8c\u9636\u4f18\u5316"]},{"location":"zh/examples/nowcastnet/","title":"NowcastNet","text":""},{"location":"zh/examples/nowcastnet/#nowcastnet","title":"NowcastNet","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <p>\u6682\u65e0</p> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/nowcastnet/mrms.tar\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/nowcastnet/mrms.tar -o mrms.tar\nmkdir ./datasets\ntar -xvf mrms.tar -C ./datasets/\npython nowcastnet.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/nowcastnet/nowcastnet_pretrained.pdparams\n</code></pre> <pre><code>python nowcastnet.py mode=export\n</code></pre> <pre><code>python nowcastnet.py mode=infer\n</code></pre>"},{"location":"zh/examples/nowcastnet/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5df2\u88ab\u5e94\u7528\u4e8e\u5929\u6c14\u9884\u62a5\uff0c\u5c24\u5176\u662f\u96f7\u8fbe\u89c2\u6d4b\u7684\u964d\u6c34\u9884\u62a5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5229\u7528\u5927\u91cf\u96f7\u8fbe\u590d\u5408\u89c2\u6d4b\u6570\u636e\u6765\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u660e\u786e\u53c2\u8003\u964d\u6c34\u8fc7\u7a0b\u7684\u7269\u7406\u5b9a\u5f8b\u3002 \u8fd9\u91cc\u590d\u73b0\u4e86\u4e00\u4e2a\u9488\u5bf9\u6781\u7aef\u964d\u6c34\u7684\u975e\u7ebf\u6027\u77ed\u4e34\u9884\u62a5\u6a21\u578b\u2014\u2014NowcastNet\uff0c\u8be5\u6a21\u578b\u5c06\u7269\u7406\u6f14\u53d8\u65b9\u6848\u548c\u6761\u4ef6\u5b66\u4e60\u6cd5\u7edf\u4e00\u5230\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u4f18\u5316\u3002</p>"},{"location":"zh/examples/nowcastnet/#2","title":"2. \u6a21\u578b\u539f\u7406","text":"<p>\u672c\u7ae0\u8282\u4ec5\u5bf9 NowcastNet \u7684\u6a21\u578b\u539f\u7406\u8fdb\u884c\u7b80\u5355\u5730\u4ecb\u7ecd\uff0c\u8be6\u7ec6\u7684\u7406\u8bba\u63a8\u5bfc\u8bf7\u9605\u8bfb Skilful nowcasting of extreme precipitation with NowcastNet\u3002</p> <p>\u6a21\u578b\u7684\u603b\u4f53\u7ed3\u6784\u5982\u56fe\u6240\u793a\uff1a</p> <p> </p> NowcastNet \u7f51\u7edc\u6a21\u578b <p>\u6a21\u578b\u4f7f\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u63a8\u7406\uff0c\u63a5\u4e0b\u6765\u5c06\u4ecb\u7ecd\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002</p>"},{"location":"zh/examples/nowcastnet/#3","title":"3. \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/nowcastnet/nowcastnet.py<pre><code>if cfg.CASE_TYPE == \"large\":\n    dataset_path = cfg.LARGE_DATASET_PATH\n    model_cfg = cfg.MODEL.large\n    output_dir = osp.join(cfg.output_dir, \"large\")\nelif cfg.CASE_TYPE == \"normal\":\n    dataset_path = cfg.NORMAL_DATASET_PATH\n    model_cfg = cfg.MODEL.normal\n    output_dir = osp.join(cfg.output_dir, \"normal\")\nelse:\n    raise ValueError(\n        f\"cfg.CASE_TYPE should in ['normal', 'large'], but got '{cfg.mode}'\"\n    )\nmodel = ppsci.arch.NowcastNet(**model_cfg)\n</code></pre> examples/nowcastnet/conf/nowcastnet.yaml<pre><code>CPU_WORKER: 1\n\n# model settings\nMODEL:\n  normal:\n    input_keys: [\"input\"]\n    output_keys: [\"output\"]\n    input_length: 9\n    total_length: 29\n    image_width: 512\n    image_height: 512\n    image_ch: 2\n    ngf: 32\n  large:\n    input_keys: [\"input\"]\n    output_keys: [\"output\"]\n    input_length: 9\n    total_length: 29\n    image_width: 1024\n</code></pre> <p>\u5176\u4e2d\uff0c<code>input_keys</code> \u548c <code>output_keys</code> \u5206\u522b\u4ee3\u8868\u7f51\u7edc\u6a21\u578b\u8f93\u5165\u3001\u8f93\u51fa\u53d8\u91cf\u7684\u540d\u79f0\u3002</p>"},{"location":"zh/examples/nowcastnet/#4","title":"4. \u6a21\u578b\u8bc4\u4f30\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff1a</p> examples/nowcastnet/nowcastnet.py<pre><code>solver = ppsci.solver.Solver(\n    model,\n    output_dir=output_dir,\n    pretrained_model_path=cfg.EVAL.pretrained_model_path,\n)\n</code></pre> <p>\u7136\u540e\u6784\u5efa VisualizerRadar \u751f\u6210\u56fe\u7247\u7ed3\u679c\uff1a</p> examples/nowcastnet/nowcastnet.py<pre><code>visualizer = {\n    \"v_nowcastnet\": ppsci.visualize.VisualizerRadar(\n        {\"input\": frames_tensor},\n        {\n            \"output\": lambda out: out[\"output\"],\n        },\n        prefix=\"v_nowcastnet\",\n        case_type=cfg.CASE_TYPE,\n        total_length=model_cfg.total_length,\n    )\n}\nsolver.visualizer = visualizer\n# visualize prediction\nsolver.visualize(batch_id)\n</code></pre>"},{"location":"zh/examples/nowcastnet/#5","title":"5. \u5b8c\u6574\u4ee3\u7801","text":"examples/nowcastnet/nowcastnet.py<pre><code>\"\"\"\nReference: https://codeocean.com/capsule/3935105/tree/v1\n\"\"\"\nfrom os import path as osp\n\nimport hydra\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    print(\"Not supported.\")\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    if cfg.CASE_TYPE == \"large\":\n        dataset_path = cfg.LARGE_DATASET_PATH\n        model_cfg = cfg.MODEL.large\n        output_dir = osp.join(cfg.output_dir, \"large\")\n    elif cfg.CASE_TYPE == \"normal\":\n        dataset_path = cfg.NORMAL_DATASET_PATH\n        model_cfg = cfg.MODEL.normal\n        output_dir = osp.join(cfg.output_dir, \"normal\")\n    else:\n        raise ValueError(\n            f\"cfg.CASE_TYPE should in ['normal', 'large'], but got '{cfg.mode}'\"\n        )\n    model = ppsci.arch.NowcastNet(**model_cfg)\n\n    input_keys = (\"radar_frames\",)\n    dataset_param = {\n        \"input_keys\": input_keys,\n        \"label_keys\": (),\n        \"image_width\": model_cfg.image_width,\n        \"image_height\": model_cfg.image_height,\n        \"total_length\": model_cfg.total_length,\n        \"dataset_path\": dataset_path,\n        \"data_type\": paddle.get_default_dtype(),\n    }\n    test_data_loader = paddle.io.DataLoader(\n        ppsci.data.dataset.RadarDataset(**dataset_param),\n        batch_size=1,\n        shuffle=False,\n        num_workers=cfg.CPU_WORKER,\n        drop_last=True,\n    )\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=output_dir,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n\n    for batch_id, test_ims in enumerate(test_data_loader):\n        test_ims = test_ims[0][input_keys[0]].numpy()\n        frames_tensor = paddle.to_tensor(\n            data=test_ims, dtype=paddle.get_default_dtype()\n        )\n        if batch_id &lt;= cfg.NUM_SAVE_SAMPLES:\n            visualizer = {\n                \"v_nowcastnet\": ppsci.visualize.VisualizerRadar(\n                    {\"input\": frames_tensor},\n                    {\n                        \"output\": lambda out: out[\"output\"],\n                    },\n                    prefix=\"v_nowcastnet\",\n                    case_type=cfg.CASE_TYPE,\n                    total_length=model_cfg.total_length,\n                )\n            }\n            solver.visualizer = visualizer\n            # visualize prediction\n            solver.visualize(batch_id)\n\n\ndef export(cfg: DictConfig):\n    from paddle.static import InputSpec\n\n    # set models\n    if cfg.CASE_TYPE == \"large\":\n        model_cfg = cfg.MODEL.large\n    elif cfg.CASE_TYPE == \"normal\":\n        model_cfg = cfg.MODEL.normal\n    else:\n        raise ValueError(\n            f\"cfg.CASE_TYPE should in ['normal', 'large'], but got '{cfg.mode}'\"\n        )\n    model = ppsci.arch.NowcastNet(**model_cfg)\n\n    # load pretrained model\n    solver = ppsci.solver.Solver(\n        model=model, pretrained_model_path=cfg.INFER.pretrained_model_path\n    )\n    # export models\n    input_spec = [\n        {\n            key: InputSpec(\n                [None, 29, model_cfg.image_width, model_cfg.image_height, 2],\n                \"float32\",\n                name=key,\n            )\n            for key in model_cfg.input_keys\n        },\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    import os.path as osp\n\n    from deploy.python_infer import pinn_predictor\n\n    # set model predictor\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    if cfg.CASE_TYPE == \"large\":\n        dataset_path = cfg.LARGE_DATASET_PATH\n        model_cfg = cfg.MODEL.large\n        output_dir = osp.join(cfg.output_dir, \"large\")\n    elif cfg.CASE_TYPE == \"normal\":\n        dataset_path = cfg.NORMAL_DATASET_PATH\n        model_cfg = cfg.MODEL.normal\n        output_dir = osp.join(cfg.output_dir, \"normal\")\n    else:\n        raise ValueError(\n            f\"cfg.CASE_TYPE should in ['normal', 'large'], but got '{cfg.mode}'\"\n        )\n\n    input_keys = (\"radar_frames\",)\n    dataset_param = {\n        \"input_keys\": input_keys,\n        \"label_keys\": (),\n        \"image_width\": model_cfg.image_width,\n        \"image_height\": model_cfg.image_height,\n        \"total_length\": model_cfg.total_length,\n        \"dataset_path\": dataset_path,\n    }\n    test_data_loader = paddle.io.DataLoader(\n        ppsci.data.dataset.RadarDataset(**dataset_param),\n        batch_size=cfg.INFER.batch_size,\n        num_workers=cfg.CPU_WORKER,\n        drop_last=True,\n    )\n    for batch_id, test_ims in enumerate(test_data_loader):\n        if batch_id &gt; cfg.NUM_SAVE_SAMPLES:\n            break\n        test_ims = {\"input\": test_ims[0][input_keys[0]].numpy()}\n        output_dict = predictor.predict(test_ims, cfg.INFER.batch_size)\n        # mapping data to model_cfg.output_keys\n        output_dict = {\n            store_key: output_dict[infer_key]\n            for store_key, infer_key in zip(model_cfg.output_keys, output_dict.keys())\n        }\n\n        visualizer = ppsci.visualize.VisualizerRadar(\n            test_ims,\n            {\n                \"output\": lambda out: out[\"output\"],\n            },\n            prefix=\"v_nowcastnet\",\n            case_type=cfg.CASE_TYPE,\n            total_length=model_cfg.total_length,\n        )\n        test_ims.update(output_dict)\n        visualizer.save(osp.join(output_dir, f\"epoch_{batch_id}\"), test_ims)\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"nowcastnet.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/nowcastnet/#6","title":"6. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u56fe\u5c55\u793a\u4e86\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u548c\u771f\u503c\u7ed3\u679c\u3002</p> <p> </p> \u6a21\u578b\u9884\u6d4b\u7ed3\u679c <p> </p> \u6a21\u578b\u771f\u503c\u7ed3\u679c"},{"location":"zh/examples/nowcastnet/#7","title":"7. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>Skilful nowcasting of extreme precipitation with NowcastNet</li> </ul>"},{"location":"zh/examples/nsfnet/","title":"NSFNets","text":""},{"location":"zh/examples/nsfnet/#nsfnets","title":"NSFNets","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4 <pre><code># VP_NSFNet1\npython VP_NSFNet1.py    mode=eval  pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/nsfnet/nsfnet1.pdparams\n\n# VP_NSFNet2\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/cylinder_nektar_wake.mat -P ./data/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/cylinder_nektar_wake.mat --create-dirs -o ./data/cylinder_nektar_wake.mat\n\npython VP_NSFNet2.py    mode=eval  data_dir=./data/cylinder_nektar_wake.mat  pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/nsfnet/nsfnet2.pdparams\n\n# VP_NSFNet3\npython VP_NSFNet3.py    mode=eval  pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/nsfnet/nsfnet3.pdparams\n</code></pre> <pre><code># VP_NSFNet1\npython VP_NSFNet1.py\n\n# VP_NSFNet2\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/cylinder_nektar_wake.mat -P ./data/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/cylinder_nektar_wake.mat --create-dirs -o ./data/cylinder_nektar_wake.mat\npython VP_NSFNet2.py data_dir=./data/cylinder_nektar_wake.mat\n\n# VP_NSFNet3\npython VP_NSFNet3.py\n</code></pre>"},{"location":"zh/examples/nsfnet/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6700\u8fd1\u51e0\u5e74,\u6df1\u5ea6\u5b66\u4e60\u5728\u5f88\u591a\u9886\u57df\u53d6\u5f97\u4e86\u975e\u51e1\u7684\u6210\u5c31,\u5c24\u5176\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762,\u800c\u53d7\u542f\u53d1\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55,\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u5f3a\u5927\u7684\u51fd\u6570\u903c\u8fd1\u80fd\u529b,\u795e\u7ecf\u7f51\u7edc\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u4e5f\u53d6\u5f97\u4e86\u6210\u529f,\u73b0\u9636\u6bb5\u7684\u7814\u7a76\u4e3b\u8981\u5206\u4e3a\u4e24\u5927\u7c7b,\u4e00\u7c7b\u662f\u5c06\u7269\u7406\u4fe1\u606f\u4ee5\u53ca\u7269\u7406\u9650\u5236\u52a0\u5165\u635f\u5931\u51fd\u6570\u6765\u5bf9\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3, \u5176\u4ee3\u8868\u6709 PINN \u4ee5\u53ca Deep Retz Net,\u53e6\u4e00\u7c7b\u662f\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50,\u5176\u4ee3\u8868\u6709 FNO \u4ee5\u53ca DeepONet\u3002\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u5728\u79d1\u5b66\u5b9e\u8df5\u4e2d\u83b7\u5f97\u4e86\u5e7f\u6cdb\u5e94\u7528,\u6bd4\u5982\u5929\u6c14\u9884\u6d4b,\u91cf\u5b50\u5316\u5b66,\u751f\u7269\u5de5\u7a0b,\u4ee5\u53ca\u8ba1\u7b97\u6d41\u4f53\u7b49\u9886\u57df\u3002\u800c\u4e3a\u5145\u5206\u63a2\u7d22PINN\u5bf9\u6d41\u4f53\u65b9\u7a0b\u7684\u6c42\u89e3\u80fd\u529b,\u672c\u6b21\u590d\u73b0\u8bba\u6587\u4f5c\u8005\u8bbe\u8ba1\u4e86NSFNets\uff0c\u5e76\u4e14\u5148\u540e\u4f7f\u7528\u5177\u6709\u89e3\u6790\u89e3\u6216\u6570\u503c\u89e3\u7684\u4e8c\u7ef4\u3001\u4e09\u7ef4\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u4ee5\u53ca\u4f7f\u7528DNS\u65b9\u6cd5\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u6c42\u89e3\u7684\u6570\u636e\u96c6\u4f5c\u4e3a\u53c2\u8003, \u8fdb\u884c\u6b63\u95ee\u9898\u6c42\u89e3\u8bad\u7ec3\u3002\u8bba\u6587\u5b9e\u9a8c\u8868\u660ePINN\u5bf9\u4e0d\u53ef\u538b\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u5177\u6709\u4f18\u79c0\u7684\u6570\u503c\u6c42\u89e3\u80fd\u529b, \u672c\u9879\u76ee\u4e3b\u8981\u76ee\u6807\u662f\u4f7f\u7528PaddleScience\u590d\u73b0\u8bba\u6587\u6240\u5b9e\u73b0\u7684\u9ad8\u7cbe\u5ea6\u6c42\u89e3\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u7684\u4ee3\u7801\u3002</p>"},{"location":"zh/examples/nsfnet/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u95ee\u9898\u6240\u4f7f\u7528\u7684\u4e3a\u6700\u7ecf\u5178\u7684PINN\u6a21\u578b,\u5bf9\u6b64\u4e0d\u518d\u8d58\u8ff0\u3002</p> <p>\u4e3b\u8981\u4ecb\u7ecd\u6240\u6c42\u89e3\u7684\u51e0\u7c7b\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\uff1a</p> <p>\u4e0d\u53ef\u538b\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> \\[\\frac{\\partial \\mathbf{u}}{\\partial t}+(\\mathbf{u} \\cdot \\nabla) \\mathbf{u} =-\\nabla p+\\frac{1}{Re} \\nabla^2 \\mathbf{u} \\quad \\text { in } \\Omega,\\] \\[\\nabla \\cdot \\mathbf{u} =0 \\quad  \\text { in } \\Omega,\\] \\[\\mathbf{u} =\\mathbf{u}_{\\Gamma} \\quad \\text { on } \\Gamma_D,\\] \\[\\frac{\\partial \\mathbf{u}}{\\partial n} =0 \\quad \\text { on } \\Gamma_N.\\]"},{"location":"zh/examples/nsfnet/#21-kovasznay-flownsfnet1","title":"2.1 Kovasznay flow(NSFNet1)","text":"<p>\u6211\u4eec\u4f7f\u7528 Kovasznay \u6d41\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u6765\u6f14\u793a NSFnets \u7684\u6027\u80fd\u3002 \u8be5\u4e8c\u7ef4\u7a33\u6001\u7eb3\u7ef4-\u65af\u6258\u514b\u65af\u6d41\u5177\u6709\u4ee5\u4e0b\u89e3\u6790\u89e3:</p> \\[u(x, y)=1-e^{\\lambda x} \\cos (2 \\pi y),\\] \\[v(x, y)=\\frac{\\lambda}{2 \\pi} e^{\\lambda x} \\sin (2 \\pi y),\\] \\[p(x, y)=\\frac{1}{2}\\left(1-e^{2 \\lambda x}\\right),\\] <p>\u5176\u4e2d</p> \\[\\lambda=\\frac{1}{2 \\nu}-\\sqrt{\\frac{1}{4 \\nu^2}+4 \\pi^2}, \\quad \\nu=\\frac{1}{Re}=\\frac{1}{40} .\\] <p>\u6211\u4eec\u8003\u8651\u8ba1\u7b97\u57df\u4e3a \\([\u22120.5, 1.0] \u00d7 [\u22120.5, 1.5]\\)\u3002 \u6211\u4eec\u9996\u5148\u786e\u5b9a\u4f18\u5316\u7b56\u7565\u3002 \u6bcf\u4e2a\u8fb9\u754c\u4e0a\u6709 \\(101\\) \u4e2a\u5177\u6709\u56fa\u5b9a\u7a7a\u95f4\u5750\u6807\u7684\u70b9,\u5373 \\(Nb = 4 \u00d7 101\\)\u3002\u4e3a\u4e86\u8ba1\u7b97 NSFnet \u7684\u65b9\u7a0b\u635f\u5931,\u5728\u57df\u5185\u968f\u673a\u9009\u62e9 \\(2,601\\) \u4e2a\u70b9\u3002 \u8fd9\u79cd\u7a33\u5b9a\u6d41\u52a8\u6ca1\u6709\u521d\u59cb\u6761\u4ef6\u3002 \u6211\u4eec\u4f7f\u7528 Adam \u4f18\u5316\u5668\u6765\u63d0\u4f9b\u4e00\u7ec4\u66f4\u597d\u7684\u521d\u59cb\u795e\u7ecf\u7f51\u7edc\u53ef\u5b66\u4e60\u53d8\u91cf\u3002 \u7136\u540e,\u4f7f\u7528L-BFGS-B\u5bf9\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u4ee5\u83b7\u5f97\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002 L-BFGS-B\u7684\u8bad\u7ec3\u8fc7\u7a0b\u6839\u636e\u589e\u91cf\u5bb9\u5dee\u81ea\u52a8\u7ec8\u6b62\u3002 \u5728\u672c\u8282\u4e2d,\u6211\u4eec\u5728 L-BFGS-B \u8bad\u7ec3\u4e4b\u524d\u4f7f\u7528 \\(3 \u00d7 10^4\\) Adam \u8fed\u4ee3,\u5b66\u4e60\u7387\u4e3a \\(10^{\u22123}\\)\u3002 Adam \u8fed\u4ee3\u6b21\u6570\u7684\u5f71\u54cd\u5728\u8bba\u6587\u9644\u5f55 A \u7684\u56fe A.1 \u4e2d\u8ba8\u8bba,\u6211\u4eec\u8fd8\u7814\u7a76\u4e86 NSFnet \u5728\u91c7\u6837\u70b9\u548c\u8fb9\u754c\u70b9\u6570\u91cf\u65b9\u9762\u7684\u6027\u80fd\u3002</p>"},{"location":"zh/examples/nsfnet/#22-cylinder-wake-nsfnet2","title":"2.2 Cylinder wake (NSFNet2)","text":"<p>\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 NSFnets \u6a21\u62df \\(Re = 100\\) \u65f6\u5706\u67f1\u4f53\u540e\u9762\u7684 \\(2D\\) \u6da1\u65cb\u8131\u843d\u3002\u5706\u67f1\u4f53\u653e\u7f6e\u5728 \\((x, y) = (0, 0)\\) \u5904,\u76f4\u5f84 \\(D = 1\\)\u3002\u9ad8\u4fdd\u771f DNS \u6570\u636e\u6765\u81ea \\(M. Raissi 2019\\) \u7528\u4f5c\u53c2\u8003\u5e76\u4e3a NSFnet \u8bad\u7ec3\u63d0\u4f9b\u8fb9\u754c\u548c\u521d\u59cb\u6570\u636e\u3002 \u6211\u4eec\u8003\u8651\u7531 \\([1, 8] \u00d7 [\u22122, 2]\\) \u5b9a\u4e49\u7684\u57df,\u65f6\u95f4\u95f4\u9694\u4e3a \\([0, 7]\\)\uff08\u8d85\u8fc7\u4e00\u4e2a\u8131\u843d\u5468\u671f\uff09,\u65f6\u95f4\u6b65\u957f \\(\u0394t = 0.1\\)\u3002 \u5bf9\u4e8e\u8bad\u7ec3\u6570\u636e,\u6211\u4eec\u6cbf \\(x\\) \u65b9\u5411\u8fb9\u754c\u653e\u7f6e \\(100\\) \u4e2a\u70b9,\u6cbf y \u65b9\u5411\u8fb9\u754c\u653e\u7f6e \\(50\\) \u4e2a\u70b9\u6765\u63a7\u5236\u8fb9\u754c\u6761\u4ef6,\u5e76\u4f7f\u7528\u57df\u5185\u7684 \\(140,000\\) \u4e2a\u65f6\u7a7a\u5206\u6563\u70b9\u6765\u8ba1\u7b97\u6b8b\u5dee\u3002 NSFnet \u5305\u542b \\(10\\) \u4e2a\u9690\u85cf\u5c42,\u6bcf\u5c42\u6709 \\(100\\) \u4e2a\u795e\u7ecf\u5143\u3002Cylinder wake AIstudio\u6570\u636e\u96c6\u94fe\u63a5\u3002</p>"},{"location":"zh/examples/nsfnet/#23-beltrami-flow-nsfnet3","title":"2.3 Beltrami flow (NSFNet3)","text":"\\[u(x, y, z, t)= -a\\left[e^{a x} \\sin (a y+d z)+e^{a z} \\cos (a x+d y)\\right] e^{-d^2 t}, \\] \\[v(x, y, z, t)= -a\\left[e^{a y} \\sin (a z+d x)+e^{a x} \\cos (a y+d z)\\right] e^{-d^2 t}, \\] \\[w(x, y, z, t)= -a\\left[e^{a z} \\sin (a x+d y)+e^{a y} \\cos (a z+d x)\\right] e^{-d^2 t}, \\] \\[p(x, y, z, t)= -\\frac{1}{2} a^2\\left[e^{2 a x}+e^{2 a y}+e^{2 a z}+2 \\sin (a x+d y) \\cos (a z+d x) e^{a(y+z)} +2 \\sin (a y+d z) \\cos (a x+d y) e^{a(z+x)} +2 \\sin (a z+d x) \\cos (a y+d z) e^{a(x+y)}\\right] e^{-2 d^2 t}.\\]"},{"location":"zh/examples/nsfnet/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":""},{"location":"zh/examples/nsfnet/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u672c\u6587\u4f7f\u7528PINN\u7ecf\u5178\u7684MLP\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002 </p><pre><code>model = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre>"},{"location":"zh/examples/nsfnet/#32","title":"3.2 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u6307\u5b9a\u6b8b\u5dee\u70b9\u3001\u8fb9\u754c\u70b9\u3001\u521d\u503c\u70b9\u7684\u4e2a\u6570,\u4ee5\u53ca\u53ef\u4ee5\u6307\u5b9a\u8fb9\u754c\u635f\u5931\u51fd\u6570\u548c\u521d\u503c\u635f\u5931\u51fd\u6570\u7684\u6743\u91cd </p><pre><code>N_TRAIN = cfg.ntrain\n\n# set the number of boundary samples\nNB_TRAIN = cfg.nb_train\n\n# set the number of initial samples\nN0_TRAIN = cfg.n0_train\nALPHA = cfg.alpha\nBETA = cfg.beta\n</code></pre>"},{"location":"zh/examples/nsfnet/#33","title":"3.3 \u6570\u636e\u751f\u6210","text":"<p>\u56e0\u6570\u636e\u96c6\u4e3a\u89e3\u6790\u89e3,\u6211\u4eec\u5148\u6784\u9020\u89e3\u6790\u89e3\u51fd\u6570 </p><pre><code>def analytic_solution_generate(x, y, z, t):\n    a, d = 1, 1\n    u = (\n        -a\n        * (\n            np.exp(a * x) * np.sin(a * y + d * z)\n            + np.exp(a * z) * np.cos(a * x + d * y)\n        )\n        * np.exp(-d * d * t)\n    )\n    v = (\n        -a\n        * (\n            np.exp(a * y) * np.sin(a * z + d * x)\n            + np.exp(a * x) * np.cos(a * y + d * z)\n        )\n        * np.exp(-d * d * t)\n    )\n    w = (\n        -a\n        * (\n            np.exp(a * z) * np.sin(a * x + d * y)\n            + np.exp(a * y) * np.cos(a * z + d * x)\n        )\n        * np.exp(-d * d * t)\n    )\n    p = (\n        -0.5\n        * a\n        * a\n        * (\n            np.exp(2 * a * x)\n            + np.exp(2 * a * y)\n            + np.exp(2 * a * z)\n            + 2 * np.sin(a * x + d * y) * np.cos(a * z + d * x) * np.exp(a * (y + z))\n            + 2 * np.sin(a * y + d * z) * np.cos(a * x + d * y) * np.exp(a * (z + x))\n            + 2 * np.sin(a * z + d * x) * np.cos(a * y + d * z) * np.exp(a * (x + y))\n        )\n        * np.exp(-2 * d * d * t)\n    )\n\n    return u, v, w, p\n</code></pre> <p>\u7136\u540e\u5148\u540e\u53d6\u8fb9\u754c\u70b9\u3001\u521d\u503c\u70b9\u3001\u4ee5\u53ca\u7528\u4e8e\u8ba1\u7b97\u6b8b\u5dee\u7684\u5185\u90e8\u70b9\uff08\u5177\u4f53\u53d6\u6cd5\u89c1\u8bba\u6587\u82823.3\uff09\u4ee5\u53ca\u751f\u6210\u6d4b\u8bd5\u70b9\u3002 </p><pre><code>(\n    x_train,\n    y_train,\n    z_train,\n    t_train,\n    x0_train,\n    y0_train,\n    z0_train,\n    t0_train,\n    u0_train,\n    v0_train,\n    w0_train,\n    xb_train,\n    yb_train,\n    zb_train,\n    tb_train,\n    ub_train,\n    vb_train,\n    wb_train,\n    x_star,\n    y_star,\n    z_star,\n    t_star,\n    u_star,\n    v_star,\n    w_star,\n    p_star,\n) = generate_data(N_TRAIN)\n</code></pre>"},{"location":"zh/examples/nsfnet/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u7531\u4e8e\u6211\u4eec\u8fb9\u754c\u70b9\u548c\u521d\u503c\u70b9\u5177\u6709\u89e3\u6790\u89e3,\u56e0\u6b64\u6211\u4eec\u4f7f\u7528\u76d1\u7763\u7ea6\u675f </p><pre><code>sup_constraint_b = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg_b,\n    ppsci.loss.MSELoss(\"mean\", ALPHA),\n    name=\"Sup_b\",\n)\n\n# supervised constraint s.t ||u-u_0||\nsup_constraint_0 = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg_0,\n    ppsci.loss.MSELoss(\"mean\", BETA),\n    name=\"Sup_0\",\n)\n</code></pre> <p>\u5176\u4e2dalpha\u548cbeta\u4e3a\u8be5\u635f\u5931\u51fd\u6570\u7684\u6743\u91cd,\u5728\u672c\u4ee3\u7801\u4e2d\u4e0e\u8bba\u6587\u4e2d\u63cf\u8ff0\u4e00\u81f4,\u90fd\u53d6\u4e3a100</p> <p>\u4f7f\u7528\u5185\u90e8\u70b9\u6784\u9020\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u7684\u6b8b\u5dee\u7ea6\u675f </p><pre><code>equation = {\n    \"NavierStokes\": ppsci.equation.NavierStokes(\n        nu=1.0 / cfg.re, rho=1.0, dim=3, time=True\n    ),\n}\n\npde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"NavierStokes\"].equations,\n    {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0, \"momentum_z\": 0},\n    geom,\n    {\n        \"dataset\": {\"name\": \"IterableNamedArrayDataset\"},\n        \"batch_size\": N_TRAIN,\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"EQ\",\n)\n</code></pre>"},{"location":"zh/examples/nsfnet/#35","title":"3.5 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u4f7f\u7528\u5728\u6570\u636e\u751f\u6210\u65f6\u751f\u6210\u7684\u6d4b\u8bd5\u70b9\u6784\u9020\u7684\u6d4b\u8bd5\u96c6\u7528\u4e8e\u6a21\u578b\u8bc4\u4f30\uff1a </p><pre><code>residual_validator = ppsci.validate.SupervisedValidator(\n    valida_dataloader_cfg,\n    ppsci.loss.L2RelLoss(),\n    output_expr={\n        \"u\": lambda d: d[\"u\"],\n        \"v\": lambda d: d[\"v\"],\n        \"p\": lambda d: d[\"p\"] - d[\"p\"].min() + p_star.min(),\n    },\n    metric={\"L2R\": ppsci.metric.L2Rel()},\n    name=\"Residual\",\n)\n\n# wrap validator\nvalidator = {residual_validator.name: residual_validator}\n</code></pre>"},{"location":"zh/examples/nsfnet/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u4e0e\u8bba\u6587\u4e2d\u63cf\u8ff0\u76f8\u540c,\u6211\u4eec\u4f7f\u7528\u5206\u6bb5\u5b66\u4e60\u7387\u6784\u9020Adam\u4f18\u5316\u5668,\u5176\u4e2d\u53ef\u4ee5\u901a\u8fc7\u8c03\u8282_epoch_list_\u6765\u8c03\u8282\u8bad\u7ec3\u8f6e\u6570\u3002 </p><pre><code># set optimizer\nepoch_list = [5000, 5000, 50000, 50000]\nnew_epoch_list = []\nfor i, _ in enumerate(epoch_list):\n    new_epoch_list.append(sum(epoch_list[: i + 1]))\nEPOCHS = new_epoch_list[-1]\nlr_list = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\nlr_scheduler = ppsci.optimizer.lr_scheduler.Piecewise(\n    EPOCHS, ITERS_PER_EPOCH, new_epoch_list, lr_list\n)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>"},{"location":"zh/examples/nsfnet/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e,\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model=model,\n    constraint=constraint,\n    optimizer=optimizer,\n    epochs=EPOCHS,\n    lr_scheduler=lr_scheduler,\n    iters_per_epoch=ITERS_PER_EPOCH,\n    eval_during_train=True,\n    log_freq=cfg.log_freq,\n    eval_freq=cfg.eval_freq,\n    seed=SEED,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    visualizer=None,\n    eval_with_no_grad=False,\n)\n</code></pre> <p>\u6700\u540e\u542f\u52a8\u8bad\u7ec3\u5373\u53ef\uff1a</p> <pre><code># train model\nsolver.train()\n</code></pre>"},{"location":"zh/examples/nsfnet/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"<p>NSFNet1: </p>NSFNet1.py<pre><code>import hydra\nimport numpy as np\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef analytic_solution_generate(x, y, lam):\n    u = 1 - np.exp(lam * x) * np.cos(2 * np.pi * y)\n    v = lam / (2 * np.pi) * np.exp(lam * x) * np.sin(2 * np.pi * y)\n    p = 0.5 * (1 - np.exp(2 * lam * x))\n    return u, v, p\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"VP_NSFNet1.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\ndef generate_data(N_TRAIN, lam, seed):\n    x = np.linspace(-0.5, 1.0, 101)\n    y = np.linspace(-0.5, 1.5, 101)\n\n    yb1 = np.array([-0.5] * 100)\n    yb2 = np.array([1] * 100)\n    xb1 = np.array([-0.5] * 100)\n    xb2 = np.array([1.5] * 100)\n\n    y_train1 = np.concatenate([y[1:101], y[0:100], xb1, xb2], 0).astype(\"float32\")\n    x_train1 = np.concatenate([yb1, yb2, x[0:100], x[1:101]], 0).astype(\"float32\")\n\n    xb_train = x_train1.reshape(x_train1.shape[0], 1).astype(\"float32\")\n    yb_train = y_train1.reshape(y_train1.shape[0], 1).astype(\"float32\")\n    ub_train, vb_train, _ = analytic_solution_generate(xb_train, yb_train, lam)\n\n    x_train = (np.random.rand(N_TRAIN, 1) - 1 / 3) * 3 / 2\n    y_train = (np.random.rand(N_TRAIN, 1) - 1 / 4) * 2\n\n    # generate test data\n    np.random.seed(seed)\n    x_star = ((np.random.rand(1000, 1) - 1 / 3) * 3 / 2).astype(\"float32\")\n    y_star = ((np.random.rand(1000, 1) - 1 / 4) * 2).astype(\"float32\")\n\n    u_star, v_star, p_star = analytic_solution_generate(x_star, y_star, lam)\n\n    return (\n        x_train,\n        y_train,\n        xb_train,\n        yb_train,\n        ub_train,\n        vb_train,\n        x_star,\n        y_star,\n        u_star,\n        v_star,\n        p_star,\n    )\n\n\ndef train(cfg: DictConfig):\n    OUTPUT_DIR = cfg.output_dir\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n\n    # set random seed for reproducibility\n    SEED = cfg.seed\n    ppsci.utils.misc.set_random_seed(SEED)\n\n    ITERS_PER_EPOCH = cfg.iters_per_epoch\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set the number of residual samples\n    N_TRAIN = cfg.ntrain\n\n    # set the number of boundary samples\n    NB_TRAIN = cfg.nb_train\n\n    # generate data\n\n    # set the Reynolds number and the corresponding lambda which is the parameter in the exact solution.\n    Re = cfg.re\n    lam = 0.5 * Re - np.sqrt(0.25 * (Re**2) + 4 * (np.pi**2))\n\n    (\n        x_train,\n        y_train,\n        xb_train,\n        yb_train,\n        ub_train,\n        vb_train,\n        x_star,\n        y_star,\n        u_star,\n        v_star,\n        p_star,\n    ) = generate_data(N_TRAIN, lam, SEED)\n\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": xb_train, \"y\": yb_train},\n            \"label\": {\"u\": ub_train, \"v\": vb_train},\n        },\n        \"batch_size\": NB_TRAIN,\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    valida_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x_star, \"y\": y_star},\n            \"label\": {\"u\": u_star, \"v\": v_star, \"p\": p_star},\n        },\n        \"total_size\": u_star.shape[0],\n        \"batch_size\": u_star.shape[0],\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    geom = ppsci.geometry.PointCloud({\"x\": x_train, \"y\": y_train}, (\"x\", \"y\"))\n\n    # supervised constraint s.t ||u-u_0||\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"Sup\",\n    )\n\n    # set equation constarint s.t. ||F(u)||\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(\n            nu=1.0 / Re, rho=1.0, dim=2, time=False\n        ),\n    }\n\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n        geom,\n        {\n            \"dataset\": {\"name\": \"IterableNamedArrayDataset\"},\n            \"batch_size\": N_TRAIN,\n            \"iters_per_epoch\": ITERS_PER_EPOCH,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"EQ\",\n    )\n\n    constraint = {\n        sup_constraint.name: sup_constraint,\n        pde_constraint.name: pde_constraint,\n    }\n\n    residual_validator = ppsci.validate.SupervisedValidator(\n        valida_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        metric={\"L2R\": ppsci.metric.L2Rel()},\n        name=\"Residual\",\n    )\n\n    # wrap validator\n    validator = {residual_validator.name: residual_validator}\n\n    # set learning rate scheduler\n    epoch_list = [5000, 5000, 50000, 50000]\n    new_epoch_list = []\n    for i, _ in enumerate(epoch_list):\n        new_epoch_list.append(sum(epoch_list[: i + 1]))\n    EPOCHS = new_epoch_list[-1]\n    lr_list = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Piecewise(\n        EPOCHS, ITERS_PER_EPOCH, new_epoch_list, lr_list\n    )()\n\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/eval.log\", \"info\")\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model=model,\n        constraint=constraint,\n        optimizer=optimizer,\n        epochs=EPOCHS,\n        lr_scheduler=lr_scheduler,\n        iters_per_epoch=ITERS_PER_EPOCH,\n        eval_during_train=False,\n        log_freq=cfg.log_freq,\n        eval_freq=cfg.eval_freq,\n        seed=SEED,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=None,\n        eval_with_no_grad=False,\n        output_dir=OUTPUT_DIR,\n    )\n\n    # train model\n    solver.train()\n\n    solver.eval()\n\n    # plot the loss\n    solver.plot_loss_history()\n\n    # set LBFGS optimizer\n    EPOCHS = 5000\n    optimizer = ppsci.optimizer.LBFGS(\n        max_iter=50000, tolerance_change=np.finfo(float).eps, history_size=50\n    )(model)\n\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/eval.log\", \"info\")\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model=model,\n        constraint=constraint,\n        optimizer=optimizer,\n        epochs=EPOCHS,\n        iters_per_epoch=ITERS_PER_EPOCH,\n        eval_during_train=False,\n        log_freq=2000,\n        eval_freq=2000,\n        seed=SEED,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=None,\n        eval_with_no_grad=False,\n        output_dir=OUTPUT_DIR,\n    )\n    # train model\n    solver.train()\n\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    OUTPUT_DIR = cfg.output_dir\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n\n    # set random seed for reproducibility\n    SEED = cfg.seed\n    ppsci.utils.misc.set_random_seed(SEED)\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n    ppsci.utils.load_pretrain(model, cfg.pretrained_model_path)\n\n    # set the number of residual samples\n    N_TRAIN = cfg.ntrain\n\n    # set the Reynolds number and the corresponding lambda which is the parameter in the exact solution.\n    Re = cfg.re\n    lam = 0.5 * Re - np.sqrt(0.25 * (Re**2) + 4 * (np.pi**2))\n\n    x_train = (np.random.rand(N_TRAIN, 1) - 1 / 3) * 3 / 2\n    y_train = (np.random.rand(N_TRAIN, 1) - 1 / 4) * 2\n\n    # generate test data\n    np.random.seed(SEED)\n    x_star = ((np.random.rand(1000, 1) - 1 / 3) * 3 / 2).astype(\"float32\")\n    y_star = ((np.random.rand(1000, 1) - 1 / 4) * 2).astype(\"float32\")\n    u_star = 1 - np.exp(lam * x_star) * np.cos(2 * np.pi * y_star)\n    v_star = (lam / (2 * np.pi)) * np.exp(lam * x_star) * np.sin(2 * np.pi * y_star)\n    p_star = 0.5 * (1 - np.exp(2 * lam * x_star))\n\n    valida_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x_star, \"y\": y_star},\n            \"label\": {\"u\": u_star, \"v\": v_star, \"p\": p_star},\n        },\n        \"total_size\": u_star.shape[0],\n        \"batch_size\": u_star.shape[0],\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    geom = ppsci.geometry.PointCloud({\"x\": x_train, \"y\": y_train}, (\"x\", \"y\"))\n\n    # set equation constarint s.t. ||F(u)||\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(\n            nu=1.0 / Re, rho=1.0, dim=2, time=False\n        ),\n    }\n\n    residual_validator = ppsci.validate.SupervisedValidator(\n        valida_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        output_expr={\n            \"u\": lambda d: d[\"u\"],\n            \"v\": lambda d: d[\"v\"],\n            \"p\": lambda d: d[\"p\"] - d[\"p\"].min() + p_star.min(),\n        },\n        metric={\"L2R\": ppsci.metric.L2Rel()},\n        name=\"Residual\",\n    )\n\n    # wrap validator\n    validator = {residual_validator.name: residual_validator}\n\n    # load solver\n    solver = ppsci.solver.Solver(\n        model,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n    )\n\n    # eval model\n    solver.eval()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> NSFNet2: NSFNet2.py<pre><code>import hydra\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport paddle\nimport scipy\nfrom omegaconf import DictConfig\nfrom scipy.interpolate import griddata\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"VP_NSFNet2.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\ndef load_data(path, N_TRAIN, NB_TRAIN, N0_TRAIN):\n    data = scipy.io.loadmat(path)\n\n    U_star = data[\"U_star\"].astype(\"float32\")  # N x 2 x T\n    P_star = data[\"p_star\"].astype(\"float32\")  # N x T\n    t_star = data[\"t\"].astype(\"float32\")  # T x 1\n    X_star = data[\"X_star\"].astype(\"float32\")  # N x 2\n\n    N = X_star.shape[0]\n    T = t_star.shape[0]\n\n    # rearrange data\n    XX = np.tile(X_star[:, 0:1], (1, T))  # N x T\n    YY = np.tile(X_star[:, 1:2], (1, T))  # N x T\n    TT = np.tile(t_star, (1, N)).T  # N x T\n\n    UU = U_star[:, 0, :]  # N x T\n    VV = U_star[:, 1, :]  # N x T\n    PP = P_star  # N x T\n\n    x = XX.flatten()[:, None]  # NT x 1\n    y = YY.flatten()[:, None]  # NT x 1\n    t = TT.flatten()[:, None]  # NT x 1\n\n    u = UU.flatten()[:, None]  # NT x 1\n    v = VV.flatten()[:, None]  # NT x 1\n    p = PP.flatten()[:, None]  # NT x 1\n\n    data1 = np.concatenate([x, y, t, u, v, p], 1)\n    data2 = data1[:, :][data1[:, 2] &lt;= 7]\n    data3 = data2[:, :][data2[:, 0] &gt;= 1]\n    data4 = data3[:, :][data3[:, 0] &lt;= 8]\n    data5 = data4[:, :][data4[:, 1] &gt;= -2]\n    data_domain = data5[:, :][data5[:, 1] &lt;= 2]\n    data_t0 = data_domain[:, :][data_domain[:, 2] == 0]\n    data_y1 = data_domain[:, :][data_domain[:, 0] == 1]\n    data_y8 = data_domain[:, :][data_domain[:, 0] == 8]\n    data_x = data_domain[:, :][data_domain[:, 1] == -2]\n    data_x2 = data_domain[:, :][data_domain[:, 1] == 2]\n    data_sup_b_train = np.concatenate([data_y1, data_y8, data_x, data_x2], 0)\n    idx = np.random.choice(data_domain.shape[0], N_TRAIN, replace=False)\n\n    x_train = data_domain[idx, 0].reshape(data_domain[idx, 0].shape[0], 1)\n    y_train = data_domain[idx, 1].reshape(data_domain[idx, 1].shape[0], 1)\n    t_train = data_domain[idx, 2].reshape(data_domain[idx, 2].shape[0], 1)\n\n    x0_train = data_t0[:, 0].reshape(data_t0[:, 0].shape[0], 1)\n    y0_train = data_t0[:, 1].reshape(data_t0[:, 1].shape[0], 1)\n    t0_train = data_t0[:, 2].reshape(data_t0[:, 2].shape[0], 1)\n    u0_train = data_t0[:, 3].reshape(data_t0[:, 3].shape[0], 1)\n    v0_train = data_t0[:, 4].reshape(data_t0[:, 4].shape[0], 1)\n\n    xb_train = data_sup_b_train[:, 0].reshape(data_sup_b_train[:, 0].shape[0], 1)\n    yb_train = data_sup_b_train[:, 1].reshape(data_sup_b_train[:, 1].shape[0], 1)\n    tb_train = data_sup_b_train[:, 2].reshape(data_sup_b_train[:, 2].shape[0], 1)\n    ub_train = data_sup_b_train[:, 3].reshape(data_sup_b_train[:, 3].shape[0], 1)\n    vb_train = data_sup_b_train[:, 4].reshape(data_sup_b_train[:, 4].shape[0], 1)\n\n    # set test set\n    snap = np.array([0])\n    x_star = X_star[:, 0:1]\n    y_star = X_star[:, 1:2]\n    t_star = TT[:, snap]\n\n    u_star = U_star[:, 0, snap]\n    v_star = U_star[:, 1, snap]\n    p_star = P_star[:, snap]\n\n    return (\n        x_train,\n        y_train,\n        t_train,\n        x0_train,\n        y0_train,\n        t0_train,\n        u0_train,\n        v0_train,\n        xb_train,\n        yb_train,\n        tb_train,\n        ub_train,\n        vb_train,\n        x_star,\n        y_star,\n        t_star,\n        u_star,\n        v_star,\n        p_star,\n    )\n\n\ndef train(cfg: DictConfig):\n    OUTPUT_DIR = cfg.output_dir\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n\n    # set random seed for reproducibility\n    SEED = cfg.seed\n    ppsci.utils.misc.set_random_seed(SEED)\n    ITERS_PER_EPOCH = cfg.iters_per_epoch\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set the number of residual samples\n    N_TRAIN = cfg.ntrain\n\n    # set the number of boundary samples\n    NB_TRAIN = cfg.nb_train\n\n    # set the number of initial samples\n    N0_TRAIN = cfg.n0_train\n\n    (\n        x_train,\n        y_train,\n        t_train,\n        x0_train,\n        y0_train,\n        t0_train,\n        u0_train,\n        v0_train,\n        xb_train,\n        yb_train,\n        tb_train,\n        ub_train,\n        vb_train,\n        x_star,\n        y_star,\n        t_star,\n        u_star,\n        v_star,\n        p_star,\n    ) = load_data(cfg.data_dir, N_TRAIN, NB_TRAIN, N0_TRAIN)\n    # set dataloader config\n    train_dataloader_cfg_b = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": xb_train, \"y\": yb_train, \"t\": tb_train},\n            \"label\": {\"u\": ub_train, \"v\": vb_train},\n        },\n        \"batch_size\": NB_TRAIN,\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    train_dataloader_cfg_0 = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x0_train, \"y\": y0_train, \"t\": t0_train},\n            \"label\": {\"u\": u0_train, \"v\": v0_train},\n        },\n        \"batch_size\": N0_TRAIN,\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    valida_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x_star, \"y\": y_star, \"t\": t_star},\n            \"label\": {\"u\": u_star, \"v\": v_star, \"p\": p_star},\n        },\n        \"total_size\": u_star.shape[0],\n        \"batch_size\": u_star.shape[0],\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    geom = ppsci.geometry.PointCloud(\n        {\"x\": x_train, \"y\": y_train, \"t\": t_train}, (\"x\", \"y\", \"t\")\n    )\n\n    # supervised constraint s.t ||u-u_b||\n    sup_constraint_b = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_b,\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"Sup_b\",\n    )\n\n    # supervised constraint s.t ||u-u_0||\n    sup_constraint_0 = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_0,\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"Sup_0\",\n    )\n\n    # set equation constarint s.t. ||F(u)||\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(\n            nu=1.0 / cfg.re, rho=1.0, dim=2, time=True\n        ),\n    }\n\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0},\n        geom,\n        {\n            \"dataset\": {\"name\": \"IterableNamedArrayDataset\"},\n            \"batch_size\": N_TRAIN,\n            \"iters_per_epoch\": ITERS_PER_EPOCH,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"EQ\",\n    )\n\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        sup_constraint_b.name: sup_constraint_b,\n        sup_constraint_0.name: sup_constraint_0,\n    }\n\n    residual_validator = ppsci.validate.SupervisedValidator(\n        valida_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        output_expr={\n            \"u\": lambda d: d[\"u\"],\n            \"v\": lambda d: d[\"v\"],\n            \"p\": lambda d: d[\"p\"] - d[\"p\"].min() + p_star.min(),\n        },\n        metric={\"L2R\": ppsci.metric.L2Rel()},\n        name=\"Residual\",\n    )\n\n    # wrap validator\n    validator = {residual_validator.name: residual_validator}\n\n    # set optimizer\n    epoch_list = [5000, 5000, 50000, 50000]\n    new_epoch_list = []\n    for i, _ in enumerate(epoch_list):\n        new_epoch_list.append(sum(epoch_list[: i + 1]))\n    EPOCHS = new_epoch_list[-1]\n    lr_list = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Piecewise(\n        EPOCHS, ITERS_PER_EPOCH, new_epoch_list, lr_list\n    )()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/eval.log\", \"info\")\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model=model,\n        constraint=constraint,\n        optimizer=optimizer,\n        epochs=EPOCHS,\n        lr_scheduler=lr_scheduler,\n        iters_per_epoch=ITERS_PER_EPOCH,\n        eval_during_train=True,\n        log_freq=cfg.log_freq,\n        eval_freq=cfg.eval_freq,\n        seed=SEED,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=None,\n        eval_with_no_grad=False,\n    )\n    # train model\n    solver.train()\n\n    # evaluate after finished training\n    solver.eval()\n\n    solver.plot_loss_history()\n\n\ndef evaluate(cfg: DictConfig):\n    OUTPUT_DIR = cfg.output_dir\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n\n    # set random seed for reproducibility\n    SEED = cfg.seed\n    ppsci.utils.misc.set_random_seed(SEED)\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n    ppsci.utils.load_pretrain(model, cfg.pretrained_model_path)\n\n    # set the number of residual samples\n    N_TRAIN = cfg.ntrain\n\n    data = scipy.io.loadmat(cfg.data_dir)\n\n    U_star = data[\"U_star\"].astype(\"float32\")  # N x 2 x T\n    P_star = data[\"p_star\"].astype(\"float32\")  # N x T\n    t_star = data[\"t\"].astype(\"float32\")  # T x 1\n    X_star = data[\"X_star\"].astype(\"float32\")  # N x 2\n\n    N = X_star.shape[0]\n    T = t_star.shape[0]\n\n    # rearrange data\n    XX = np.tile(X_star[:, 0:1], (1, T))  # N x T\n    YY = np.tile(X_star[:, 1:2], (1, T))  # N x T\n    TT = np.tile(t_star, (1, N)).T  # N x T\n\n    UU = U_star[:, 0, :]  # N x T\n    VV = U_star[:, 1, :]  # N x T\n    PP = P_star  # N x T\n\n    x = XX.flatten()[:, None]  # NT x 1\n    y = YY.flatten()[:, None]  # NT x 1\n    t = TT.flatten()[:, None]  # NT x 1\n\n    u = UU.flatten()[:, None]  # NT x 1\n    v = VV.flatten()[:, None]  # NT x 1\n    p = PP.flatten()[:, None]  # NT x 1\n\n    data1 = np.concatenate([x, y, t, u, v, p], 1)\n    data2 = data1[:, :][data1[:, 2] &lt;= 7]\n    data3 = data2[:, :][data2[:, 0] &gt;= 1]\n    data4 = data3[:, :][data3[:, 0] &lt;= 8]\n    data5 = data4[:, :][data4[:, 1] &gt;= -2]\n    data_domain = data5[:, :][data5[:, 1] &lt;= 2]\n\n    idx = np.random.choice(data_domain.shape[0], N_TRAIN, replace=False)\n\n    x_train = data_domain[idx, 0].reshape(data_domain[idx, 0].shape[0], 1)\n    y_train = data_domain[idx, 1].reshape(data_domain[idx, 1].shape[0], 1)\n    t_train = data_domain[idx, 2].reshape(data_domain[idx, 2].shape[0], 1)\n\n    snap = np.array([0])\n    x_star = X_star[:, 0:1]\n    y_star = X_star[:, 1:2]\n    t_star = TT[:, snap]\n\n    u_star = U_star[:, 0, snap]\n    v_star = U_star[:, 1, snap]\n    p_star = P_star[:, snap]\n\n    valida_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x_star, \"y\": y_star, \"t\": t_star},\n            \"label\": {\"u\": u_star, \"v\": v_star, \"p\": p_star},\n        },\n        \"total_size\": u_star.shape[0],\n        \"batch_size\": u_star.shape[0],\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    geom = ppsci.geometry.PointCloud(\n        {\"x\": x_train, \"y\": y_train, \"t\": t_train}, (\"x\", \"y\", \"t\")\n    )\n\n    # set equation constarint s.t. ||F(u)||\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(nu=0.01, rho=1.0, dim=2, time=True),\n    }\n\n    residual_validator = ppsci.validate.SupervisedValidator(\n        valida_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        output_expr={\n            \"u\": lambda d: d[\"u\"],\n            \"v\": lambda d: d[\"v\"],\n            \"p\": lambda d: d[\"p\"] - d[\"p\"].min() + p_star.min(),\n        },\n        metric={\"L2R\": ppsci.metric.L2Rel()},\n        name=\"Residual\",\n    )\n\n    # wrap validator\n    validator = {residual_validator.name: residual_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n    )\n\n    # eval\n    ## eval validate set\n    solver.eval()\n\n    ## eval every time\n    us = []\n    vs = []\n    for i in range(0, 70):\n        snap = np.array([i])\n        x_star = X_star[:, 0:1]\n        y_star = X_star[:, 1:2]\n        t_star = TT[:, snap]\n        u_star = paddle.to_tensor(U_star[:, 0, snap])\n        v_star = paddle.to_tensor(U_star[:, 1, snap])\n        p_star = paddle.to_tensor(P_star[:, snap])\n\n        solution = solver.predict({\"x\": x_star, \"y\": y_star, \"t\": t_star})\n        u_pred = solution[\"u\"]\n        v_pred = solution[\"v\"]\n        p_pred = solution[\"p\"]\n        p_pred = p_pred - p_pred.mean() + p_star.mean()\n        error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n        error_v = np.linalg.norm(v_star - v_pred, 2) / np.linalg.norm(v_star, 2)\n        error_p = np.linalg.norm(p_star - p_pred, 2) / np.linalg.norm(p_star, 2)\n        us.append(error_u)\n        vs.append(error_v)\n        print(\"t={:.2f},relative error of u: {:.3e}\".format(t_star[0].item(), error_u))\n        print(\"t={:.2f},relative error of v: {:.3e}\".format(t_star[0].item(), error_v))\n        print(\"t={:.2f},relative error of p: {:.3e}\".format(t_star[0].item(), error_p))\n\n    # plot\n    ## vorticity\n    grid_x, grid_y = np.mgrid[1.0:8.0:1000j, -2.0:2.0:1000j]\n    x_star = paddle.to_tensor(grid_x.reshape(-1, 1).astype(\"float32\"))\n    y_star = paddle.to_tensor(grid_y.reshape(-1, 1).astype(\"float32\"))\n    t_star = paddle.to_tensor((4.0) * np.ones(x_star.shape).astype(\"float32\"))\n    x_star.stop_gradient = False\n    y_star.stop_gradient = False\n    t_star.stop_gradient = False\n    sol = model.forward({\"x\": x_star, \"y\": y_star, \"t\": t_star})\n    u_y = paddle.grad(sol[\"u\"], y_star)\n    v_x = paddle.grad(sol[\"v\"], x_star)\n    w = np.array(v_x) - np.array(u_y)\n    w = w.reshape(1000, 1000)\n    l1 = np.arange(-4, 0, 0.25)\n    l2 = np.arange(0.25, 4, 0.25)\n    fig = plt.figure(figsize=(16, 8), dpi=80)\n    plt.contour(grid_x, grid_y, w, levels=np.concatenate([l1, l2]), cmap=\"jet\")\n    plt.savefig(f\"{OUTPUT_DIR}/vorticity_t=4.png\")\n\n    ## relative error\n    t_snap = []\n    for i in range(70):\n        t_snap.append(i / 10)\n    fig, ax = plt.subplots(1, 2, figsize=(12, 3))\n    ax[0].plot(t_snap, us)\n    ax[1].plot(t_snap, vs)\n    ax[0].set_title(\"u\")\n    ax[1].set_title(\"v\")\n    fig.savefig(f\"{OUTPUT_DIR}/l2_error.png\")\n\n    ## velocity\n    grid_x, grid_y = np.mgrid[0.0:8.0:1000j, -2.0:2.0:1000j]\n    for i in range(70):\n        snap = np.array([i])\n        x_star = X_star[:, 0:1]\n        y_star = X_star[:, 1:2]\n        t_star = TT[:, snap]\n        points = np.concatenate([x_star, y_star], -1)\n        u_star = U_star[:, 0, snap]\n        v_star = U_star[:, 1, snap]\n\n        solution = solver.predict({\"x\": x_star, \"y\": y_star, \"t\": t_star})\n        u_pred = solution[\"u\"]\n        v_pred = solution[\"v\"]\n        u_star_ = griddata(points, u_star, (grid_x, grid_y), method=\"cubic\")\n        u_pred_ = griddata(points, u_pred, (grid_x, grid_y), method=\"cubic\")\n        v_star_ = griddata(points, v_star, (grid_x, grid_y), method=\"cubic\")\n        v_pred_ = griddata(points, v_pred, (grid_x, grid_y), method=\"cubic\")\n        fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n        ax[0, 0].contourf(grid_x, grid_y, u_star_[:, :, 0])\n        ax[0, 1].contourf(grid_x, grid_y, u_pred_[:, :, 0])\n        ax[1, 0].contourf(grid_x, grid_y, v_star_[:, :, 0])\n        ax[1, 1].contourf(grid_x, grid_y, v_pred_[:, :, 0])\n        ax[0, 0].set_title(\"u_exact\")\n        ax[0, 1].set_title(\"u_pred\")\n        ax[1, 0].set_title(\"v_exact\")\n        ax[1, 1].set_title(\"v_pred\")\n        fig.savefig(OUTPUT_DIR + f\"/velocity_t={t_star[i]}.png\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> NSFNet3: NSFNet3.py<pre><code>import hydra\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef analytic_solution_generate(x, y, z, t):\n    a, d = 1, 1\n    u = (\n        -a\n        * (\n            np.exp(a * x) * np.sin(a * y + d * z)\n            + np.exp(a * z) * np.cos(a * x + d * y)\n        )\n        * np.exp(-d * d * t)\n    )\n    v = (\n        -a\n        * (\n            np.exp(a * y) * np.sin(a * z + d * x)\n            + np.exp(a * x) * np.cos(a * y + d * z)\n        )\n        * np.exp(-d * d * t)\n    )\n    w = (\n        -a\n        * (\n            np.exp(a * z) * np.sin(a * x + d * y)\n            + np.exp(a * y) * np.cos(a * z + d * x)\n        )\n        * np.exp(-d * d * t)\n    )\n    p = (\n        -0.5\n        * a\n        * a\n        * (\n            np.exp(2 * a * x)\n            + np.exp(2 * a * y)\n            + np.exp(2 * a * z)\n            + 2 * np.sin(a * x + d * y) * np.cos(a * z + d * x) * np.exp(a * (y + z))\n            + 2 * np.sin(a * y + d * z) * np.cos(a * x + d * y) * np.exp(a * (z + x))\n            + 2 * np.sin(a * z + d * x) * np.cos(a * y + d * z) * np.exp(a * (x + y))\n        )\n        * np.exp(-2 * d * d * t)\n    )\n\n    return u, v, w, p\n\n\ndef generate_data(N_TRAIN):\n    # generate boundary data\n    x1 = np.linspace(-1, 1, 31)\n    y1 = np.linspace(-1, 1, 31)\n    z1 = np.linspace(-1, 1, 31)\n    t1 = np.linspace(0, 1, 11)\n    b0 = np.array([-1] * 900)\n    b1 = np.array([1] * 900)\n\n    xt = np.tile(x1[0:30], 30)\n    yt = np.tile(y1[0:30], 30)\n    xt1 = np.tile(x1[1:31], 30)\n    yt1 = np.tile(y1[1:31], 30)\n\n    yr = y1[0:30].repeat(30)\n    zr = z1[0:30].repeat(30)\n    yr1 = y1[1:31].repeat(30)\n    zr1 = z1[1:31].repeat(30)\n\n    train1x = np.concatenate([b1, b0, xt1, xt, xt1, xt], 0).repeat(t1.shape[0])\n    train1y = np.concatenate([yt, yt1, b1, b0, yr1, yr], 0).repeat(t1.shape[0])\n    train1z = np.concatenate([zr, zr1, zr, zr1, b1, b0], 0).repeat(t1.shape[0])\n    train1t = np.tile(t1, 5400)\n\n    train1ub, train1vb, train1wb, train1pb = analytic_solution_generate(\n        train1x, train1y, train1z, train1t\n    )\n\n    xb_train = train1x.reshape(train1x.shape[0], 1).astype(\"float32\")\n    yb_train = train1y.reshape(train1y.shape[0], 1).astype(\"float32\")\n    zb_train = train1z.reshape(train1z.shape[0], 1).astype(\"float32\")\n    tb_train = train1t.reshape(train1t.shape[0], 1).astype(\"float32\")\n    ub_train = train1ub.reshape(train1ub.shape[0], 1).astype(\"float32\")\n    vb_train = train1vb.reshape(train1vb.shape[0], 1).astype(\"float32\")\n    wb_train = train1wb.reshape(train1wb.shape[0], 1).astype(\"float32\")\n\n    # generate initial data\n    x_0 = np.tile(x1, 31 * 31)\n    y_0 = np.tile(y1.repeat(31), 31)\n    z_0 = z1.repeat(31 * 31)\n    t_0 = np.array([0] * x_0.shape[0])\n    u_0, v_0, w_0, p_0 = analytic_solution_generate(x_0, y_0, z_0, t_0)\n    u0_train = u_0.reshape(u_0.shape[0], 1).astype(\"float32\")\n    v0_train = v_0.reshape(v_0.shape[0], 1).astype(\"float32\")\n    w0_train = w_0.reshape(w_0.shape[0], 1).astype(\"float32\")\n    x0_train = x_0.reshape(x_0.shape[0], 1).astype(\"float32\")\n    y0_train = y_0.reshape(y_0.shape[0], 1).astype(\"float32\")\n    z0_train = z_0.reshape(z_0.shape[0], 1).astype(\"float32\")\n    t0_train = t_0.reshape(t_0.shape[0], 1).astype(\"float32\")\n\n    # unsupervised part\n    xx = np.random.randint(31, size=N_TRAIN) / 15 - 1\n    yy = np.random.randint(31, size=N_TRAIN) / 15 - 1\n    zz = np.random.randint(31, size=N_TRAIN) / 15 - 1\n    tt = np.random.randint(11, size=N_TRAIN) / 10\n\n    x_train = xx.reshape(xx.shape[0], 1).astype(\"float32\")\n    y_train = yy.reshape(yy.shape[0], 1).astype(\"float32\")\n    z_train = zz.reshape(zz.shape[0], 1).astype(\"float32\")\n    t_train = tt.reshape(tt.shape[0], 1).astype(\"float32\")\n\n    # test data\n    x_star = ((np.random.rand(1000, 1) - 1 / 2) * 2).astype(\"float32\")\n    y_star = ((np.random.rand(1000, 1) - 1 / 2) * 2).astype(\"float32\")\n    z_star = ((np.random.rand(1000, 1) - 1 / 2) * 2).astype(\"float32\")\n    t_star = (np.random.randint(11, size=(1000, 1)) / 10).astype(\"float32\")\n\n    u_star, v_star, w_star, p_star = analytic_solution_generate(\n        x_star, y_star, z_star, t_star\n    )\n\n    return (\n        x_train,\n        y_train,\n        z_train,\n        t_train,\n        x0_train,\n        y0_train,\n        z0_train,\n        t0_train,\n        u0_train,\n        v0_train,\n        w0_train,\n        xb_train,\n        yb_train,\n        zb_train,\n        tb_train,\n        ub_train,\n        vb_train,\n        wb_train,\n        x_star,\n        y_star,\n        z_star,\n        t_star,\n        u_star,\n        v_star,\n        w_star,\n        p_star,\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"VP_NSFNet3.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\ndef train(cfg: DictConfig):\n    OUTPUT_DIR = cfg.output_dir\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n\n    # set random seed for reproducibility\n    SEED = cfg.seed\n    ppsci.utils.misc.set_random_seed(SEED)\n    ITERS_PER_EPOCH = cfg.iters_per_epoch\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set the number of residual samples\n    N_TRAIN = cfg.ntrain\n\n    # set the number of boundary samples\n    NB_TRAIN = cfg.nb_train\n\n    # set the number of initial samples\n    N0_TRAIN = cfg.n0_train\n    ALPHA = cfg.alpha\n    BETA = cfg.beta\n    (\n        x_train,\n        y_train,\n        z_train,\n        t_train,\n        x0_train,\n        y0_train,\n        z0_train,\n        t0_train,\n        u0_train,\n        v0_train,\n        w0_train,\n        xb_train,\n        yb_train,\n        zb_train,\n        tb_train,\n        ub_train,\n        vb_train,\n        wb_train,\n        x_star,\n        y_star,\n        z_star,\n        t_star,\n        u_star,\n        v_star,\n        w_star,\n        p_star,\n    ) = generate_data(N_TRAIN)\n\n    # set dataloader config\n    train_dataloader_cfg_b = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": xb_train, \"y\": yb_train, \"z\": zb_train, \"t\": tb_train},\n            \"label\": {\"u\": ub_train, \"v\": vb_train, \"w\": wb_train},\n        },\n        \"batch_size\": NB_TRAIN,\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    train_dataloader_cfg_0 = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x0_train, \"y\": y0_train, \"z\": z0_train, \"t\": t0_train},\n            \"label\": {\"u\": u0_train, \"v\": v0_train, \"w\": w0_train},\n        },\n        \"batch_size\": N0_TRAIN,\n        \"iters_per_epoch\": ITERS_PER_EPOCH,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n\n    valida_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x_star, \"y\": y_star, \"z\": z_star, \"t\": t_star},\n            \"label\": {\"u\": u_star, \"v\": v_star, \"w\": w_star, \"p\": p_star},\n        },\n        \"total_size\": u_star.shape[0],\n        \"batch_size\": u_star.shape[0],\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    geom = ppsci.geometry.PointCloud(\n        {\"x\": x_train, \"y\": y_train, \"z\": z_train, \"t\": t_train}, (\"x\", \"y\", \"z\", \"t\")\n    )\n\n    # supervised constraint s.t ||u-u_b||\n    sup_constraint_b = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_b,\n        ppsci.loss.MSELoss(\"mean\", ALPHA),\n        name=\"Sup_b\",\n    )\n\n    # supervised constraint s.t ||u-u_0||\n    sup_constraint_0 = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_0,\n        ppsci.loss.MSELoss(\"mean\", BETA),\n        name=\"Sup_0\",\n    )\n\n    # set equation constarint s.t. ||F(u)||\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(\n            nu=1.0 / cfg.re, rho=1.0, dim=3, time=True\n        ),\n    }\n\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0, \"momentum_z\": 0},\n        geom,\n        {\n            \"dataset\": {\"name\": \"IterableNamedArrayDataset\"},\n            \"batch_size\": N_TRAIN,\n            \"iters_per_epoch\": ITERS_PER_EPOCH,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"EQ\",\n    )\n\n    # wrap constraint\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        sup_constraint_b.name: sup_constraint_b,\n        sup_constraint_0.name: sup_constraint_0,\n    }\n\n    residual_validator = ppsci.validate.SupervisedValidator(\n        valida_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        output_expr={\n            \"u\": lambda d: d[\"u\"],\n            \"v\": lambda d: d[\"v\"],\n            \"p\": lambda d: d[\"p\"] - d[\"p\"].min() + p_star.min(),\n        },\n        metric={\"L2R\": ppsci.metric.L2Rel()},\n        name=\"Residual\",\n    )\n\n    # wrap validator\n    validator = {residual_validator.name: residual_validator}\n\n    # set optimizer\n    epoch_list = [5000, 5000, 50000, 50000]\n    new_epoch_list = []\n    for i, _ in enumerate(epoch_list):\n        new_epoch_list.append(sum(epoch_list[: i + 1]))\n    EPOCHS = new_epoch_list[-1]\n    lr_list = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Piecewise(\n        EPOCHS, ITERS_PER_EPOCH, new_epoch_list, lr_list\n    )()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/eval.log\", \"info\")\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model=model,\n        constraint=constraint,\n        optimizer=optimizer,\n        epochs=EPOCHS,\n        lr_scheduler=lr_scheduler,\n        iters_per_epoch=ITERS_PER_EPOCH,\n        eval_during_train=True,\n        log_freq=cfg.log_freq,\n        eval_freq=cfg.eval_freq,\n        seed=SEED,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        visualizer=None,\n        eval_with_no_grad=False,\n    )\n    # train model\n    solver.train()\n\n    # evaluate after finished training\n    solver.eval()\n    solver.plot_loss_history()\n\n\ndef evaluate(cfg: DictConfig):\n    OUTPUT_DIR = cfg.output_dir\n    logger.init_logger(\"ppsci\", f\"{OUTPUT_DIR}/train.log\", \"info\")\n\n    # set random seed for reproducibility\n    SEED = cfg.seed\n    ppsci.utils.misc.set_random_seed(SEED)\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n    ppsci.utils.load_pretrain(model, cfg.pretrained_model_path)\n\n    # set the number of residual samples\n    N_TRAIN = cfg.ntrain\n\n    # unsupervised part\n    xx = np.random.randint(31, size=N_TRAIN) / 15 - 1\n    yy = np.random.randint(31, size=N_TRAIN) / 15 - 1\n    zz = np.random.randint(31, size=N_TRAIN) / 15 - 1\n    tt = np.random.randint(11, size=N_TRAIN) / 10\n\n    x_train = xx.reshape(xx.shape[0], 1).astype(\"float32\")\n    y_train = yy.reshape(yy.shape[0], 1).astype(\"float32\")\n    z_train = zz.reshape(zz.shape[0], 1).astype(\"float32\")\n    t_train = tt.reshape(tt.shape[0], 1).astype(\"float32\")\n\n    # test data\n    x_star = ((np.random.rand(1000, 1) - 1 / 2) * 2).astype(\"float32\")\n    y_star = ((np.random.rand(1000, 1) - 1 / 2) * 2).astype(\"float32\")\n    z_star = ((np.random.rand(1000, 1) - 1 / 2) * 2).astype(\"float32\")\n    t_star = (np.random.randint(11, size=(1000, 1)) / 10).astype(\"float32\")\n\n    u_star, v_star, w_star, p_star = analytic_solution_generate(\n        x_star, y_star, z_star, t_star\n    )\n\n    valida_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x_star, \"y\": y_star, \"z\": z_star, \"t\": t_star},\n            \"label\": {\"u\": u_star, \"v\": v_star, \"w\": w_star, \"p\": p_star},\n        },\n        \"total_size\": u_star.shape[0],\n        \"batch_size\": u_star.shape[0],\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    }\n    geom = ppsci.geometry.PointCloud(\n        {\"x\": x_train, \"y\": y_train, \"z\": z_train, \"t\": t_train}, (\"x\", \"y\", \"z\", \"t\")\n    )\n\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(\n            nu=1.0 / cfg.re, rho=1.0, dim=3, time=True\n        ),\n    }\n    residual_validator = ppsci.validate.SupervisedValidator(\n        valida_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        output_expr={\n            \"u\": lambda d: d[\"u\"],\n            \"v\": lambda d: d[\"v\"],\n            \"p\": lambda d: d[\"p\"] - d[\"p\"].min() + p_star.min(),\n        },\n        metric={\"L2R\": ppsci.metric.L2Rel()},\n        name=\"Residual\",\n    )\n\n    # wrap validator\n    validator = {residual_validator.name: residual_validator}\n\n    # load solver\n    solver = ppsci.solver.Solver(\n        model,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n    )\n\n    # print the relative error\n    us = []\n    vs = []\n    ws = []\n    for i in [0, 0.25, 0.5, 0.75, 1.0]:\n        x_star, y_star, z_star = np.mgrid[-1.0:1.0:100j, -1.0:1.0:100j, -1.0:1.0:100j]\n        x_star, y_star, z_star = (\n            x_star.reshape(-1, 1),\n            y_star.reshape(-1, 1),\n            z_star.reshape(-1, 1),\n        )\n        t_star = i * np.ones(x_star.shape)\n        u_star, v_star, w_star, p_star = analytic_solution_generate(\n            x_star, y_star, z_star, t_star\n        )\n\n        solution = solver.predict({\"x\": x_star, \"y\": y_star, \"z\": z_star, \"t\": t_star})\n        u_pred = solution[\"u\"]\n        v_pred = solution[\"v\"]\n        w_pred = solution[\"w\"]\n        p_pred = solution[\"p\"]\n        p_pred = p_pred - p_pred.mean() + p_star.mean()\n        error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n        error_v = np.linalg.norm(v_star - v_pred, 2) / np.linalg.norm(v_star, 2)\n        error_w = np.linalg.norm(w_star - w_pred, 2) / np.linalg.norm(w_star, 2)\n        error_p = np.linalg.norm(p_star - p_pred, 2) / np.linalg.norm(p_star, 2)\n        us.append(error_u)\n        vs.append(error_v)\n        ws.append(error_w)\n        print(\"t={:.2f},relative error of u: {:.3e}\".format(t_star[0].item(), error_u))\n        print(\"t={:.2f},relative error of v: {:.3e}\".format(t_star[0].item(), error_v))\n        print(\"t={:.2f},relative error of w: {:.3e}\".format(t_star[0].item(), error_w))\n        print(\"t={:.2f},relative error of p: {:.3e}\".format(t_star[0].item(), error_p))\n\n    ## plot vorticity\n    grid_x, grid_y = np.mgrid[-1.0:1.0:1000j, -1.0:1.0:1000j]\n    grid_x = grid_x.reshape(-1, 1)\n    grid_y = grid_y.reshape(-1, 1)\n    grid_z = np.zeros(grid_x.shape)\n    T = np.linspace(0, 1, 101)\n    for i in T:\n        t_star = i * np.ones(x_star.shape)\n        u_star, v_star, w_star, p_star = analytic_solution_generate(\n            grid_x, grid_y, grid_z, t_star\n        )\n\n        solution = solver.predict({\"x\": grid_x, \"y\": grid_y, \"z\": grid_z, \"t\": t_star})\n        u_pred = np.array(solution[\"u\"])\n        v_pred = np.array(solution[\"v\"])\n        w_pred = np.array(solution[\"w\"])\n        p_pred = p_pred - p_pred.mean() + p_star.mean()\n        fig, ax = plt.subplots(3, 2, figsize=(12, 12))\n        ax[0, 0].contourf(\n            grid_x.reshape(1000, 1000),\n            grid_y.reshape(1000, 1000),\n            u_star.reshape(1000, 1000),\n            cmap=plt.get_cmap(\"RdYlBu\"),\n        )\n        ax[0, 1].contourf(\n            grid_x.reshape(1000, 1000),\n            grid_y.reshape(1000, 1000),\n            u_pred.reshape(1000, 1000),\n            cmap=plt.get_cmap(\"RdYlBu\"),\n        )\n        ax[1, 0].contourf(\n            grid_x.reshape(1000, 1000),\n            grid_y.reshape(1000, 1000),\n            v_star.reshape(1000, 1000),\n            cmap=plt.get_cmap(\"RdYlBu\"),\n        )\n        ax[1, 1].contourf(\n            grid_x.reshape(1000, 1000),\n            grid_y.reshape(1000, 1000),\n            v_pred.reshape(1000, 1000),\n            cmap=plt.get_cmap(\"RdYlBu\"),\n        )\n        ax[2, 0].contourf(\n            grid_x.reshape(1000, 1000),\n            grid_y.reshape(1000, 1000),\n            w_star.reshape(1000, 1000),\n            cmap=plt.get_cmap(\"RdYlBu\"),\n        )\n        ax[2, 1].contourf(\n            grid_x.reshape(1000, 1000),\n            grid_y.reshape(1000, 1000),\n            w_pred.reshape(1000, 1000),\n            cmap=plt.get_cmap(\"RdYlBu\"),\n        )\n        ax[0, 0].set_title(\"u_exact\")\n        ax[0, 1].set_title(\"u_pred\")\n        ax[1, 0].set_title(\"v_exact\")\n        ax[1, 1].set_title(\"v_pred\")\n        ax[2, 0].set_title(\"w_exact\")\n        ax[2, 1].set_title(\"w_pred\")\n        time = \"%.3f\" % i\n        fig.savefig(OUTPUT_DIR + f\"/velocity_t={str(time)}.png\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/nsfnet/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e3b\u8981\u53c2\u8003\u8bba\u6587\u6570\u636e\uff0c\u548c\u53c2\u8003\u4ee3\u7801\u7684\u6570\u636e\u3002</p>"},{"location":"zh/examples/nsfnet/#51-nsfnet1kovasznay-flow","title":"5.1 NSFNet1(Kovasznay flow)","text":"velocity paper code PaddleScience NN size u 0.072% 0.080% 0.056% 4 \u00d7 50 v 0.058% 0.539% 0.399% 4 \u00d7 50 p 0.027% 0.722% 1.123% 4 \u00d7 50 <p>\u5982\u8868\u683c\u6240\u793a,\u7b2c2,3,4\u5217\u5206\u522b\u4e3a\u8bba\u6587,\u5176\u4ed6\u5f00\u53d1\u8005\u548cPaddleScience\u590d\u73b0\u7684\\(L_{2}\\)\u8bef\u5deeKovasznay flow\u5728\\(x\\), \\(y\\)\u65b9\u5411\u7684\u901f\u5ea6\\(u\\), \\(v\\)\u7684\\(L_{2}\\)\u8bef\u5dee\u4e3a0.055%\u548c0.399%, \u6307\u6807\u5747\u4f18\u4e8e\u8bba\u6587(Table 2)\u548c\u53c2\u8003\u4ee3\u7801\u3002</p>"},{"location":"zh/examples/nsfnet/#52-nsfnet2cylinder-wake","title":"5.2 NSFNet2(Cylinder wake)","text":"<p>Cylinder wake\u5728\\(t=0\\)\u65f6\u523b\u9884\u6d4b\u7684\\(L_{2}\\)\u8bef\u5dee, \u5982\u8868\u683c\u6240\u793a, Cylinder flow\u5728\\(x\\), \\(y\\)\u65b9\u5411\u7684\u901f\u5ea6\\(u\\), \\(v\\)\u7684\\(L_{2}\\)\u8bef\u5dee\u4e3a0.138%\u548c0.488%, \u6307\u6807\u63a5\u8fd1\u8bba\u6587(Figure 9)\u548c\u4ee3\u7801\u3002</p> velocity paper (VP-NSFnet, \\(\\alpha=\\beta=1\\)) paper (VP-NSFnet, dynamic weights) code PaddleScience NN size u 0.09% 0.01% 0.403% 0.138% 4 \u00d7 50 v 0.25% 0.05% 1.5% 0.488% 4 \u00d7 50 p 1.9% 0.8% / / 4 \u00d7 50 <p>NSFNet2(2D Cylinder Flow)\u6848\u4f8b\u7684\u901f\u5ea6\u573a\u5982\u4e0b\u56fe\u6240\u793a, \u7b2c\u4e00\u884c\u7684\u4e24\u5f20\u56fe\u7247\u4e3a\u5706\u67f1\u5c3e\u90e8\u7ed5\u6d41\u533a\u57df, \u7b2c\u4e00\u884c\u7684\u56fe\u7247\u8868\u793a\u5728\\(x\\)\u6d41\u7ebf\u65b9\u5411\u4e0a\u7684\u6d41\u901f\\(u\\)\u7684\u6570\u503c\u5206\u5e03, \u5de6\u4fa7\u4e3aDNS\u9ad8\u4fdd\u771f\u6570\u636e\u4f5c\u4e3a\u53c2\u8003, \u53f3\u4fa7\u4e3a\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u503c, \u84dd\u8272\u4e3a\u8f83\u5c0f\u503c, \u7eff\u8272\u4e3a\u8f83\u5927\u503c, \u5206\u5e03\u533a\u57df\u4e3a \\(x=[1,8]\\), \\(y=[-2, 2]\\), \u7b2c\u4e8c\u884c\u7684\u56fe\u7247\u8868\u793a\u5728\\(y\\)\u5c55\u5411\u65b9\u5411\u4e0a\u7684\u6d41\u901f\\(v\\)\u7684\u5206\u5e03,\u5de6\u4fa7\u4e3aDNS\u9ad8\u4fdd\u771f\u6570\u636e\u53c2\u8003\u503c, \u53f3\u4fa7\u4e3a\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u503c, \u5206\u5e03\u533a\u57df\u4e3a \\(x=[1,8]\\), \\(y=[-2, 2]\\)\u3002</p> <p></p> <p>\u6839\u636e\u901f\u5ea6\u573a\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u6da1\u6d41\u573a, \u5982\u56fe\u6240\u793a, \u4e3aNSFNet2(2D Cylinder Flow)\u6848\u4f8b\u5728\\(t=4.0\\)\u65f6\u523b\u7684\u6da1\u6d41\u573a\u7684\u7b49\u503c\u7ebf\u56fe, \u6211\u4eec\u6839\u636e\\(x\\), \\(y\\)\u65b9\u5411\u7684\u6d41\u901f\\(u\\), \\(v\\),\u901a\u8fc7\u6da1\u91cf\u8ba1\u7b97\u516c\u5f0f, \u8ba1\u7b97\u5f97\u5230\u5982\u56fe\u6240\u793a\u6da1\u91cf\u56fe, \u6da1\u7ed3\u6784\u8fde\u7eed\u6027\u597d, \u548c\u8bba\u6587\u4e00\u81f4, \u8ba1\u7b97\u5206\u5e03\u533a\u57df\u4e3a\\(x=[1, 8]\\), \\(y=[-2, 2]\\)\u3002</p> <p></p>"},{"location":"zh/examples/nsfnet/#53-nsfnet3beltrami-flow","title":"5.3 NSFNet3(Beltrami flow)","text":"<p>\u6d4b\u8bd5\u6570\u636e\u96c6(\u89e3\u6790\u89e3)\u76f8\u5bf9\u8bef\u5dee\u5982\u8868\u683c\u6240\u793a, Beltrami flow\u5728\\(x\\), \\(y\\), \\(z\\)\u65b9\u5411\u7684\u901f\u5ea6\\(u\\), \\(v\\), \\(w\\)\u7684\\(L_{2}\\)\u8bef\u5dee\u4e3a0.059%, 0.082%\u548c0.0732%, \u4f18\u4e8e\u4ee3\u7801\u6570\u636e\u3002</p> velocity code(NN size:10\u00d7100) PaddleScience (NN size:10\u00d7100) u 0.0766% 0.059% v 0.0689% 0.082% w 0.1090% 0.073% p / / <p>Beltrami flow\u5728 $ t=1 $ \u65f6\u523b, $ z=0 \\(\u5e73\u9762\u4e0a\u7684\u9884\u6d4b\u76f8\u5bf9\u8bef\u5dee, \u5982\u8868\u683c\u6240\u793a, Beltrami flow\u5728\\)x, y, z\\(\u65b9\u5411\u7684\u901f\u5ea6\\)u, v, w\\(\u7684\\)L_{2}\\(\u8bef\u5dee\u4e3a0.115%, 0.199%\u548c0.217%, \u538b\u529b\\)p\\(\u7684\\)L_{2}$\u8bef\u5dee\u4e3a0.1.986%, \u5747\u4f18\u4e8e\u8bba\u6587\u6570\u636e(Table 4. VP)\u3002</p> velocity paper(NN size:7\u00d750) PaddleScience(NN size:10\u00d7100) u 0.1634\u00b10.0418% 0.115% v 0.2185\u00b10.0530% 0.199% w 0.1783\u00b10.0300% 0.217% p 8.9335\u00b12.4350% 1.986% <p>Beltrami flow\u901f\u5ea6\u573a,\u5982\u56fe\u6240\u793a,\u5de6\u4fa7\u4e3a\u89e3\u6790\u89e3\u53c2\u8003\u503c,\u53f3\u4fa7\u4e3a\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u503c,\u84dd\u8272\u4e3a\u8f83\u5c0f\u503c,\u7ea2\u8272\u4e3a\u8f83\u5927\u503c,\u5206\u5e03\u533a\u57df\u4e3a\\(x=[-1,1]\\), \\(y=[-1, 1]\\), \u7b2c\u4e00\u884c\u4e3a\u5728\\(x\\)\u65b9\u5411\u4e0a\u7684\u6d41\u901f\\(u\\)\u7684\u5206\u5e03,\u7b2c\u4e8c\u884c\u4e3a\u5728\\(y\\)\u65b9\u5411\u4e0a\u7684\u6d41\u901f\\(v\\)\u7684\u5206\u5e03,\u7b2c\u4e09\u884c\u4e3a\u5728\\(z\\)\u65b9\u5411\u4e0a\u6d41\u901f\\(w\\)\u7684\u5206\u5e03\u3002</p> <p></p>"},{"location":"zh/examples/nsfnet/#6","title":"6. \u7ed3\u679c\u8bf4\u660e","text":"<p>\u6211\u4eec\u4f7f\u7528PINN\u5bf9\u4e0d\u53ef\u538b\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u8fdb\u884c\u6570\u503c\u6c42\u89e3\u3002\u5728PINN\u4e2d,\u968f\u673a\u9009\u53d6\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u7684\u5750\u6807\u88ab\u5f53\u4f5c\u8f93\u5165\u503c,\u6240\u5bf9\u5e94\u7684\u901f\u5ea6\u573a\u4ee5\u53ca\u538b\u5f3a\u573a\u88ab\u5f53\u4f5c\u8f93\u51fa\u503c,\u4f7f\u7528\u521d\u503c\u3001\u8fb9\u754c\u6761\u4ef6\u5f53\u4f5c\u76d1\u7763\u7ea6\u675f\u4ee5\u53ca\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u672c\u8eab\u7684\u5f53\u4f5c\u65e0\u76d1\u7763\u7ea6\u675f\u6761\u4ef6\u52a0\u5165\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\u3002\u6211\u4eec\u9488\u5bf9\u4e09\u4e2a\u4e0d\u540c\u7c7b\u578b\u7684PINN\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b, \u8bbe\u8ba1\u4e86\u4e09\u4e2a\u4e0d\u540c\u7684\u6d41\u4f53\u6848\u4f8b, \u5373NSFNet1\u3001NSFNet2\u3001NSFNet3\u3002\u901a\u8fc7\u635f\u5931\u51fd\u6570\u7684\u4e0b\u964d\u3001\u7f51\u7edc\u9884\u6d4b\u7ed3\u679c\u4e0e\u9ad8\u4fdd\u771fDNS\u6570\u636e\uff0c\u4ee5\u53ca\u89e3\u6790\u89e3\u7684\\(L_{2}\\)\u8bef\u5dee\u7684\u964d\u4f4e\uff0c\u53ef\u4ee5\u8bc1\u660e\u795e\u7ecf\u7f51\u7edc\u5728\u6c42\u89e3\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u4e2d\u7684\u6536\u655b\u6027, \u8868\u660eNSFNets\u7684\u67b6\u6784\u62e5\u6709\u5bf9\u4e0d\u53ef\u538b\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u7684\u6c42\u89e3\u80fd\u529b\u3002\u800c\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, \u4e09\u4e2a\u4f7f\u7528NSFNet\u7684\u6b63\u95ee\u9898\u6848\u4f8b\uff0c\u90fd\u53ef\u4ee5\u5f88\u597d\u7684\u903c\u8fd1\u53c2\u8003\u89e3, \u5e76\u4e14\u6211\u4eec\u53d1\u73b0\u589e\u52a0\u8fb9\u754c\u7ea6\u675f, \u4ee5\u53ca\u521d\u503c\u7ea6\u675f\u7684\u6743\u91cd\u53ef\u4ee5\u4f7f\u5f97\u795e\u7ecf\u7f51\u7edc\u62e5\u6709\u66f4\u597d\u7684\u903c\u8fd1\u6548\u679c\u3002</p>"},{"location":"zh/examples/nsfnet/#7","title":"7. \u53c2\u8003\u8d44\u6599","text":"<p>NSFnets (Navier-Stokes Flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations</p> <p>Github NSFnets</p>"},{"location":"zh/examples/nsfnet4/","title":"NSFNet4","text":""},{"location":"zh/examples/nsfnet4/#nsfnet4","title":"NSFNet4","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># VP_NSFNet4\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/NSF4_data.zip -P ./data/\nunzip ./data/NSF4_data.zip\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/NSF4_data.zip --create-dirs -o ./data/NSF4_data.zip\n# unzip ./data/NSF4_data.zip\npython VP_NSFNet4.py    mode=eval  data_dir=./data/  EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/nsfnet/nsfnet4.pdparams\n</code></pre> <pre><code># VP_NSFNet4\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/NSF4_data.zip -P ./data/\nunzip ./data/NSF4_data.zip\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/NSF4_data.zip --create-dirs -o ./data/NSF4_data.zip\n# unzip ./data/NSF4_data.zip\npython VP_NSFNet4.py data_dir=./data/\n</code></pre> <pre><code>python VP_NSFNet4.py mode=export\n</code></pre> <pre><code># VP_NSFNet4\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/NSF4_data.zip -P ./data/\nunzip ./data/NSF4_data.zip\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/NSFNet/NSF4_data.zip --create-dirs -o ./data/NSF4_data.zip\n# unzip ./data/NSF4_data.zip\npython VP_NSFNet4.py mode=infer\n</code></pre>"},{"location":"zh/examples/nsfnet4/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6700\u8fd1\u51e0\u5e74, \u6df1\u5ea6\u5b66\u4e60\u5728\u5f88\u591a\u9886\u57df\u53d6\u5f97\u4e86\u975e\u51e1\u7684\u6210\u5c31, \u5c24\u5176\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762, \u800c\u53d7\u542f\u53d1\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55, \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u5f3a\u5927\u7684\u51fd\u6570\u903c\u8fd1\u80fd\u529b, \u795e\u7ecf\u7f51\u7edc\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u4e5f\u53d6\u5f97\u4e86\u6210\u529f, \u73b0\u9636\u6bb5\u7684\u7814\u7a76\u4e3b\u8981\u5206\u4e3a\u4e24\u5927\u7c7b, \u4e00\u7c7b\u662f\u5c06\u7269\u7406\u4fe1\u606f\u4ee5\u53ca\u7269\u7406\u9650\u5236\u52a0\u5165\u635f\u5931\u51fd\u6570\u6765\u5bf9\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3,  \u5176\u4ee3\u8868\u6709 PINN \u4ee5\u53ca Deep Ritz Net, \u53e6\u4e00\u7c7b\u662f\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50, \u5176\u4ee3\u8868\u6709 FNO \u4ee5\u53ca DeepONet\u3002\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u5728\u79d1\u5b66\u5b9e\u8df5\u4e2d\u83b7\u5f97\u4e86\u5e7f\u6cdb\u5e94\u7528, \u6bd4\u5982\u5929\u6c14\u9884\u6d4b, \u91cf\u5b50\u5316\u5b66, \u751f\u7269\u5de5\u7a0b, \u4ee5\u53ca\u8ba1\u7b97\u6d41\u4f53\u7b49\u9886\u57df\u3002\u800c\u4e3a\u5145\u5206\u63a2\u7d22PINN\u5bf9\u6d41\u4f53\u65b9\u7a0b\u7684\u6c42\u89e3\u80fd\u529b, \u672c\u6b21\u590d\u73b0\u8bba\u6587\u4f5c\u8005\u8bbe\u8ba1\u4e86NSFNets, \u5e76\u4e14\u5148\u540e\u4f7f\u7528\u5177\u6709\u89e3\u6790\u89e3\u6216\u6570\u503c\u89e3\u7684\u4e8c\u7ef4\u3001\u4e09\u7ef4\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u4ee5\u53ca\u4f7f\u7528DNS\u65b9\u6cd5\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u6c42\u89e3\u7684\u6570\u636e\u96c6\u4f5c\u4e3a\u53c2\u8003,  \u8fdb\u884c\u6b63\u95ee\u9898\u6c42\u89e3\u8bad\u7ec3\u3002\u8bba\u6587\u5b9e\u9a8c\u8868\u660ePINN\u5bf9\u4e0d\u53ef\u538b\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u5177\u6709\u4f18\u79c0\u7684\u6570\u503c\u6c42\u89e3\u80fd\u529b,  \u672c\u9879\u76ee\u4e3b\u8981\u76ee\u6807\u662f\u4f7f\u7528PaddleScience\u590d\u73b0\u8bba\u6587\u6240\u5b9e\u73b0\u7684\u9ad8\u7cbe\u5ea6\u6c42\u89e3\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u7684\u4ee3\u7801\u3002</p>"},{"location":"zh/examples/nsfnet4/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u95ee\u9898\u6240\u4f7f\u7528\u7684\u4e3a\u6700\u7ecf\u5178\u7684PINN\u6a21\u578b, \u5bf9\u6b64\u4e0d\u518d\u8d58\u8ff0\u3002</p> <p>\u4e3b\u8981\u4ecb\u7ecd\u6240\u6c42\u89e3\u7684\u51e0\u7c7b\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\uff1a</p> <p>\u4e0d\u53ef\u538b\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> \\[\\frac{\\partial \\mathbf{u}}{\\partial t}+(\\mathbf{u} \\cdot \\nabla) \\mathbf{u} =-\\nabla p+\\frac{1}{Re} \\nabla^2 \\mathbf{u} \\quad \\text { in } \\Omega, \\] \\[\\nabla \\cdot \\mathbf{u} =0 \\quad  \\text { in } \\Omega, \\] \\[\\mathbf{u} =\\mathbf{u}_{\\Gamma} \\quad \\text { on } \\Gamma_D, \\] \\[\\frac{\\partial \\mathbf{u}}{\\partial n} =0 \\quad \\text { on } \\Gamma_N.\\]"},{"location":"zh/examples/nsfnet4/#21-jhtdb","title":"2.1 JHTDB \u6570\u636e\u96c6","text":"<p>\u6570\u636e\u96c6\u4e3a\u4f7f\u7528DNS\u6c42\u89e3Re=999.35\u7684\u4e09\u7ef4\u4e0d\u53ef\u538b\u5f3a\u8feb\u5404\u5411\u540c\u6027\u6e4d\u6d41\u7684\u9ad8\u7cbe\u5ea6\u6570\u636e\u96c6, \u8be6\u7ec6\u53c2\u6570\u53ef\u89c1readme.</p>"},{"location":"zh/examples/nsfnet4/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":""},{"location":"zh/examples/nsfnet4/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u672c\u6587\u4f7f\u7528PINN\u7ecf\u5178\u7684MLP\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002</p> <pre><code>model = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre>"},{"location":"zh/examples/nsfnet4/#32","title":"3.2 \u6570\u636e\u751f\u6210","text":"<p>\u5148\u540e\u53d6\u8fb9\u754c\u70b9\u3001\u521d\u503c\u70b9\u3001\u4ee5\u53ca\u7528\u4e8e\u8ba1\u7b97\u6b8b\u5dee\u7684\u5185\u90e8\u70b9\uff08\u5177\u4f53\u53d6\u6cd5\u89c1\u8bba\u6587\u82823.3\uff09\u4ee5\u53ca\u751f\u6210\u6d4b\u8bd5\u70b9\u3002</p> <pre><code># load data\n(\n    x_train,\n    y_train,\n    z_train,\n    t_train,\n    x0_train,\n    y0_train,\n    z0_train,\n    t0_train,\n    u0_train,\n    v0_train,\n    w0_train,\n    xb_train,\n    yb_train,\n    zb_train,\n    tb_train,\n    ub_train,\n    vb_train,\n    wb_train,\n    x_star,\n    y_star,\n    z_star,\n    t_star,\n    u_star,\n    v_star,\n    w_star,\n    p_star,\n) = generate_data(cfg.data_dir)\n</code></pre>"},{"location":"zh/examples/nsfnet4/#33","title":"3.3 \u5f52\u4e00\u5316\u5904\u7406","text":"<p>\u4e3a\u5c06\u6240\u53d6\u8f83\u5c0f\u957f\u65b9\u4f53\u533a\u57df\u6539\u4e3a\u6b63\u65b9\u4f53\u533a\u57df, \u6211\u4eec\u5c06\u5f52\u4e00\u5316\u51fd\u6570\u5d4c\u5165\u7f51\u7edc\u8bad\u7ec3\u524d\u3002</p> <pre><code># normalization\nXb = np.concatenate([xb_train, yb_train, zb_train, tb_train], 1)\nlowb = Xb.min(0)  # minimal number in each column\nupb = Xb.max(0)\ntrans = Transform(paddle.to_tensor(lowb), paddle.to_tensor(upb))\nmodel.register_input_transform(trans.input_trans)\n</code></pre>"},{"location":"zh/examples/nsfnet4/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u7531\u4e8e\u6211\u4eec\u8fb9\u754c\u70b9\u548c\u521d\u503c\u70b9\u5177\u6709\u89e3\u6790\u89e3, \u56e0\u6b64\u6211\u4eec\u4f7f\u7528\u76d1\u7763\u7ea6\u675f, \u5176\u4e2dalpha\u548cbeta\u4e3a\u8be5\u635f\u5931\u51fd\u6570\u7684\u6743\u91cd, \u5728\u672c\u4ee3\u7801\u4e2d\u4e0e\u8bba\u6587\u4e2d\u63cf\u8ff0\u4e00\u81f4, \u90fd\u53d6\u4e3a100\u3002</p> <pre><code>sup_constraint_b = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg_b,\n    ppsci.loss.MSELoss(\"mean\", cfg.alpha),\n    name=\"Sup_b\",\n)\n\n# supervised constraint s.t ||u-u_0||\nsup_constraint_0 = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg_ic,\n    ppsci.loss.MSELoss(\"mean\", cfg.beta),\n    name=\"Sup_ic\",\n)\n</code></pre> <p>\u4f7f\u7528\u5185\u90e8\u70b9\u6784\u9020\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u7684\u6b8b\u5dee\u7ea6\u675f</p> <pre><code># set equation constarint s.t. ||F(u)||\nequation = {\n    \"NavierStokes\": ppsci.equation.NavierStokes(\n        nu=1.0 / cfg.re, rho=1.0, dim=3, time=True\n    ),\n}\n\npde_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"NavierStokes\"].equations,\n    {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0, \"momentum_z\": 0},\n    geom,\n    {\n        \"dataset\": {\"name\": \"NamedArrayDataset\"},\n        \"batch_size\": cfg.ntrain,\n        \"iters_per_epoch\": cfg.TRAIN.lr_scheduler.iters_per_epoch,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    name=\"EQ\",\n)\n</code></pre>"},{"location":"zh/examples/nsfnet4/#35","title":"3.5 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u4f7f\u7528\u5728\u6570\u636e\u751f\u6210\u65f6\u751f\u6210\u7684\u6d4b\u8bd5\u70b9\u6784\u9020\u7684\u6d4b\u8bd5\u96c6\u7528\u4e8e\u6a21\u578b\u8bc4\u4f30\uff1a</p> <pre><code>residual_validator = ppsci.validate.SupervisedValidator(\n    valid_dataloader_cfg,\n    ppsci.loss.L2RelLoss(),\n    metric={\"L2R\": ppsci.metric.L2Rel()},\n    name=\"Residual\",\n)\n</code></pre>"},{"location":"zh/examples/nsfnet4/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u4e0e\u8bba\u6587\u4e2d\u63cf\u8ff0\u76f8\u540c, \u6211\u4eec\u4f7f\u7528\u5206\u6bb5\u5b66\u4e60\u7387\u6784\u9020Adam\u4f18\u5316\u5668, \u5176\u4e2d\u53ef\u4ee5\u901a\u8fc7\u8c03\u8282epoch_list\u6765\u8c03\u8282\u8bad\u7ec3\u8f6e\u6570\u3002</p> <pre><code># set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.Piecewise(**cfg.TRAIN.lr_scheduler)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n</code></pre>"},{"location":"zh/examples/nsfnet4/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e, \u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model=model,\n    constraint=constraint,\n    output_dir=cfg.output_dir,\n    optimizer=optimizer,\n    lr_scheduler=lr_scheduler,\n    epochs=cfg.epochs,\n    iters_per_epoch=cfg.TRAIN.lr_scheduler.iters_per_epoch,\n    log_freq=cfg.TRAIN.log_freq,\n    save_freq=cfg.TRAIN.save_freq,\n    eval_freq=cfg.TRAIN.eval_freq,\n    eval_during_train=True,\n    seed=cfg.seed,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    eval_with_no_grad=cfg.TRAIN.eval_with_no_grad,\n)\n</code></pre> <p>\u6700\u540e\u542f\u52a8\u8bad\u7ec3\u5373\u53ef\uff1a</p> <pre><code># train model\nsolver.train()\n</code></pre>"},{"location":"zh/examples/nsfnet4/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"NSFNet.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os.path as osp\n\nimport hydra\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef generate_data(data_dir):\n    train_ini1 = np.load(osp.join(data_dir, \"train_ini2.npy\")).astype(\n        paddle.get_default_dtype()\n    )\n    train_iniv1 = np.load(osp.join(data_dir, \"train_iniv2.npy\")).astype(\n        paddle.get_default_dtype()\n    )\n    train_xb1 = np.load(osp.join(data_dir, \"train_xb2.npy\")).astype(\n        paddle.get_default_dtype()\n    )\n    train_vb1 = np.load(osp.join(data_dir, \"train_vb2.npy\")).astype(\n        paddle.get_default_dtype()\n    )\n\n    xnode = np.linspace(12.47, 12.66, 191).astype(paddle.get_default_dtype())\n    ynode = np.linspace(-1, -0.0031, 998).astype(paddle.get_default_dtype())\n    znode = np.linspace(4.61, 4.82, 211).astype(paddle.get_default_dtype())\n\n    x0_train = train_ini1[:, 0:1]\n    y0_train = train_ini1[:, 1:2]\n    z0_train = train_ini1[:, 2:3]\n    t0_train = np.zeros_like(train_ini1[:, 0:1]).astype(paddle.get_default_dtype())\n    u0_train = train_iniv1[:, 0:1]\n    v0_train = train_iniv1[:, 1:2]\n    w0_train = train_iniv1[:, 2:3]\n\n    xb_train = train_xb1[:, 0:1]\n    yb_train = train_xb1[:, 1:2]\n    zb_train = train_xb1[:, 2:3]\n    tb_train = train_xb1[:, 3:4]\n    ub_train = train_vb1[:, 0:1]\n    vb_train = train_vb1[:, 1:2]\n    wb_train = train_vb1[:, 2:3]\n\n    x_train1 = xnode.reshape(-1, 1)[np.random.choice(191, 100000, replace=True), :]\n    y_train1 = ynode.reshape(-1, 1)[np.random.choice(998, 100000, replace=True), :]\n    z_train1 = znode.reshape(-1, 1)[np.random.choice(211, 100000, replace=True), :]\n    x_train = np.tile(x_train1, (17, 1))\n    y_train = np.tile(y_train1, (17, 1))\n    z_train = np.tile(z_train1, (17, 1))\n\n    total_times1 = (np.array(list(range(17))) * 0.0065).astype(\n        paddle.get_default_dtype()\n    )\n    t_train1 = total_times1.repeat(100000)\n    t_train = t_train1.reshape(-1, 1)\n    # test data\n    test_x = np.load(osp.join(data_dir, \"test43_l.npy\")).astype(\n        paddle.get_default_dtype()\n    )\n    test_v = np.load(osp.join(data_dir, \"test43_vp.npy\")).astype(\n        paddle.get_default_dtype()\n    )\n    t = np.array([0.0065, 4 * 0.0065, 7 * 0.0065, 10 * 0.0065, 13 * 0.0065]).astype(\n        paddle.get_default_dtype()\n    )\n    t_star = np.tile(t.reshape(5, 1), (1, 3000)).reshape(-1, 1)\n    x_star = np.tile(test_x[:, 0:1], (5, 1))\n    y_star = np.tile(test_x[:, 1:2], (5, 1))\n    z_star = np.tile(test_x[:, 2:3], (5, 1))\n    u_star = test_v[:, 0:1]\n    v_star = test_v[:, 1:2]\n    w_star = test_v[:, 2:3]\n    p_star = test_v[:, 3:4]\n\n    return (\n        x_train,\n        y_train,\n        z_train,\n        t_train,\n        x0_train,\n        y0_train,\n        z0_train,\n        t0_train,\n        u0_train,\n        v0_train,\n        w0_train,\n        xb_train,\n        yb_train,\n        zb_train,\n        tb_train,\n        ub_train,\n        vb_train,\n        wb_train,\n        x_star,\n        y_star,\n        z_star,\n        t_star,\n        u_star,\n        v_star,\n        w_star,\n        p_star,\n    )\n\n\nclass Transform:\n    def __init__(self, lowb, upb) -&gt; None:\n        self.lowb = {\"x\": lowb[0], \"y\": lowb[1], \"z\": lowb[2], \"t\": lowb[3]}\n        self.upb = {\"x\": upb[0], \"y\": upb[1], \"z\": upb[2], \"t\": upb[3]}\n\n    def input_trans(self, input_dict):\n        for key, v in input_dict.items():\n            v = 2.0 * (v - self.lowb[key]) / (self.upb[key] - self.lowb[key]) - 1.0\n            input_dict[key] = v\n        return input_dict\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # load data\n    (\n        x_train,\n        y_train,\n        z_train,\n        t_train,\n        x0_train,\n        y0_train,\n        z0_train,\n        t0_train,\n        u0_train,\n        v0_train,\n        w0_train,\n        xb_train,\n        yb_train,\n        zb_train,\n        tb_train,\n        ub_train,\n        vb_train,\n        wb_train,\n        x_star,\n        y_star,\n        z_star,\n        t_star,\n        u_star,\n        v_star,\n        w_star,\n        p_star,\n    ) = generate_data(cfg.data_dir)\n\n    # normalization\n    Xb = np.concatenate([xb_train, yb_train, zb_train, tb_train], 1)\n    lowb = Xb.min(0)  # minimal number in each column\n    upb = Xb.max(0)\n    trans = Transform(paddle.to_tensor(lowb), paddle.to_tensor(upb))\n    model.register_input_transform(trans.input_trans)\n\n    # set dataloader config\n    train_dataloader_cfg_b = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": xb_train, \"y\": yb_train, \"z\": zb_train, \"t\": tb_train},\n            \"label\": {\"u\": ub_train, \"v\": vb_train, \"w\": wb_train},\n        },\n        \"batch_size\": cfg.nb_train,\n        \"iters_per_epoch\": cfg.TRAIN.lr_scheduler.iters_per_epoch,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    }\n\n    train_dataloader_cfg_ic = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x0_train, \"y\": y0_train, \"z\": z0_train, \"t\": t0_train},\n            \"label\": {\"u\": u0_train, \"v\": v0_train, \"w\": w0_train},\n        },\n        \"batch_size\": cfg.n0_train,\n        \"iters_per_epoch\": cfg.TRAIN.lr_scheduler.iters_per_epoch,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    }\n\n    valid_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"x\": x_star, \"y\": y_star, \"z\": z_star, \"t\": t_star},\n            \"label\": {\"u\": u_star, \"v\": v_star, \"w\": w_star, \"p\": p_star},\n        },\n        \"total_size\": u_star.shape[0],\n        \"batch_size\": u_star.shape[0],\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    }\n\n    geom = ppsci.geometry.PointCloud(\n        {\"x\": x_train, \"y\": y_train, \"z\": z_train, \"t\": t_train}, (\"x\", \"y\", \"z\", \"t\")\n    )\n    # supervised constraint s.t ||u-u_b||\n    sup_constraint_b = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_b,\n        ppsci.loss.MSELoss(\"mean\", cfg.alpha),\n        name=\"Sup_b\",\n    )\n\n    # supervised constraint s.t ||u-u_0||\n    sup_constraint_0 = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg_ic,\n        ppsci.loss.MSELoss(\"mean\", cfg.beta),\n        name=\"Sup_ic\",\n    )\n\n    # set equation constarint s.t. ||F(u)||\n    equation = {\n        \"NavierStokes\": ppsci.equation.NavierStokes(\n            nu=1.0 / cfg.re, rho=1.0, dim=3, time=True\n        ),\n    }\n\n    pde_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"NavierStokes\"].equations,\n        {\"continuity\": 0, \"momentum_x\": 0, \"momentum_y\": 0, \"momentum_z\": 0},\n        geom,\n        {\n            \"dataset\": {\"name\": \"NamedArrayDataset\"},\n            \"batch_size\": cfg.ntrain,\n            \"iters_per_epoch\": cfg.TRAIN.lr_scheduler.iters_per_epoch,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        name=\"EQ\",\n    )\n\n    # wrap constraints\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        sup_constraint_b.name: sup_constraint_b,\n        sup_constraint_0.name: sup_constraint_0,\n    }\n\n    residual_validator = ppsci.validate.SupervisedValidator(\n        valid_dataloader_cfg,\n        ppsci.loss.L2RelLoss(),\n        metric={\"L2R\": ppsci.metric.L2Rel()},\n        name=\"Residual\",\n    )\n\n    # wrap validator\n    validator = {residual_validator.name: residual_validator}\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Piecewise(**cfg.TRAIN.lr_scheduler)()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)(model)\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model=model,\n        constraint=constraint,\n        output_dir=cfg.output_dir,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n        epochs=cfg.epochs,\n        iters_per_epoch=cfg.TRAIN.lr_scheduler.iters_per_epoch,\n        log_freq=cfg.TRAIN.log_freq,\n        save_freq=cfg.TRAIN.save_freq,\n        eval_freq=cfg.TRAIN.eval_freq,\n        eval_during_train=True,\n        seed=cfg.seed,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        eval_with_no_grad=cfg.TRAIN.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n\n    # evaluate after finished training\n    solver.eval()\n\n    solver.plot_loss_history()\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # test Data\n    test_x = np.load(osp.join(cfg.data_dir, \"test43_l.npy\")).astype(\n        paddle.get_default_dtype()\n    )\n    test_v = np.load(osp.join(cfg.data_dir, \"test43_vp.npy\")).astype(\n        paddle.get_default_dtype()\n    )\n    t = np.array([0.0065, 4 * 0.0065, 7 * 0.0065, 10 * 0.0065, 13 * 0.0065]).astype(\n        paddle.get_default_dtype()\n    )\n    t_star = paddle.to_tensor(np.tile(t.reshape(5, 1), (1, 3000)).reshape(-1, 1))\n    x_star = paddle.to_tensor(np.tile(test_x[:, 0:1], (5, 1)).reshape(-1, 1))\n    y_star = paddle.to_tensor(np.tile(test_x[:, 1:2], (5, 1)).reshape(-1, 1))\n    z_star = paddle.to_tensor(np.tile(test_x[:, 2:3], (5, 1)).reshape(-1, 1))\n    u_star = paddle.to_tensor(test_v[:, 0:1])\n    v_star = paddle.to_tensor(test_v[:, 1:2])\n    w_star = paddle.to_tensor(test_v[:, 2:3])\n    p_star = paddle.to_tensor(test_v[:, 3:4])\n\n    # wrap validator\n    ppsci.utils.load_pretrain(model, cfg.EVAL.pretrained_model_path)\n\n    # print the relative error\n    solution = model(\n        {\n            \"x\": x_star,\n            \"y\": y_star,\n            \"z\": z_star,\n            \"t\": t_star,\n        }\n    )\n    u_pred = solution[\"u\"].reshape((5, -1))\n    v_pred = solution[\"v\"].reshape((5, -1))\n    w_pred = solution[\"w\"].reshape((5, -1))\n    p_pred = solution[\"p\"].reshape((5, -1))\n    u_star = u_star.reshape((5, -1))\n    v_star = v_star.reshape((5, -1))\n    w_star = w_star.reshape((5, -1))\n    p_star = p_star.reshape((5, -1))\n\n    # NS equation can figure out pressure drop, need background pressure p_star.mean()\n    p_pred = p_pred - p_pred.mean() + p_star.mean()\n\n    u_error = paddle.linalg.norm(u_pred - u_star, axis=1) / np.linalg.norm(\n        u_star, axis=1\n    )\n    v_error = paddle.linalg.norm(v_pred - v_star, axis=1) / np.linalg.norm(\n        v_star, axis=1\n    )\n    w_error = paddle.linalg.norm(w_pred - w_star, axis=1) / np.linalg.norm(\n        w_star, axis=1\n    )\n    p_error = paddle.linalg.norm(p_pred - p_star, axis=1) / np.linalg.norm(\n        w_star, axis=1\n    )\n    t = np.array([0.0065, 4 * 0.0065, 7 * 0.0065, 10 * 0.0065, 13 * 0.0065])\n    plt.plot(t, np.array(u_error))\n    plt.plot(t, np.array(v_error))\n    plt.plot(t, np.array(w_error))\n    plt.plot(t, np.array(p_error))\n    plt.legend([\"u_error\", \"v_error\", \"w_error\", \"p_error\"])\n    plt.xlabel(\"t\")\n    plt.ylabel(\"Relative l2 Error\")\n    plt.title(\"Relative l2 Error, on test dataset\")\n    plt.savefig(osp.join(cfg.output_dir, \"error.jpg\"))\n    logger.info(\"L2 error picture is saved\")\n\n    grid_x, grid_y = np.mgrid[\n        x_star.min() : x_star.max() : 100j, y_star.min() : y_star.max() : 100j\n    ].astype(paddle.get_default_dtype())\n    x_plot = paddle.to_tensor(grid_x.reshape(-1, 1))\n    y_plot = paddle.to_tensor(grid_y.reshape(-1, 1))\n    z_plot = paddle.to_tensor(z_star.min() * paddle.ones(y_plot.shape))\n    t_plot = paddle.to_tensor((t[-1]) * np.ones(x_plot.shape), paddle.float32)\n    sol = model({\"x\": x_plot, \"y\": y_plot, \"z\": z_plot, \"t\": t_plot})\n    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n    cmap = matplotlib.colormaps.get_cmap(\"jet\")\n\n    ax[0].contourf(grid_x, grid_y, sol[\"u\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[0].set_title(\"u prediction\")\n    ax[1].contourf(grid_x, grid_y, sol[\"v\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[1].set_title(\"v prediction\")\n    ax[2].contourf(grid_x, grid_y, sol[\"w\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[2].set_title(\"w prediction\")\n    ax[3].contourf(grid_x, grid_y, sol[\"p\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[3].set_title(\"p prediction\")\n    norm = matplotlib.colors.Normalize(\n        vmin=sol[\"u\"].min(), vmax=sol[\"u\"].max()\n    )  # set maximum and minimum\n    im = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n    ax13 = fig.add_axes([0.125, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.325, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.525, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.725, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    plt.savefig(osp.join(cfg.output_dir, \"z=0 plane\"))\n\n    grid_y, grid_z = np.mgrid[\n        y_star.min() : y_star.max() : 100j, z_star.min() : z_star.max() : 100j\n    ].astype(paddle.get_default_dtype())\n    z_plot = paddle.to_tensor(grid_z.reshape(-1, 1))\n    y_plot = paddle.to_tensor(grid_y.reshape(-1, 1))\n    x_plot = paddle.to_tensor(x_star.min() * paddle.ones(y_plot.shape))\n    t_plot = paddle.to_tensor((t[-1]) * np.ones(x_plot.shape), paddle.float32)\n    sol = model({\"x\": x_plot, \"y\": y_plot, \"z\": z_plot, \"t\": t_plot})\n    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n    cmap = matplotlib.colormaps.get_cmap(\"jet\")\n\n    ax[0].contourf(grid_y, grid_z, sol[\"u\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[0].set_title(\"u prediction\")\n    ax[1].contourf(grid_y, grid_z, sol[\"v\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[1].set_title(\"v prediction\")\n    ax[2].contourf(grid_y, grid_z, sol[\"w\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[2].set_title(\"w prediction\")\n    ax[3].contourf(grid_y, grid_z, sol[\"p\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[3].set_title(\"p prediction\")\n    norm = matplotlib.colors.Normalize(\n        vmin=sol[\"u\"].min(), vmax=sol[\"u\"].max()\n    )  # set maximum and minimum\n    im = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n    ax13 = fig.add_axes([0.125, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.325, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.525, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.725, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    plt.savefig(osp.join(cfg.output_dir, \"x=0 plane\"))\n\n\ndef export(cfg: DictConfig):\n    from paddle.static import InputSpec\n\n    # set models\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # load pretrained model\n    solver = ppsci.solver.Solver(\n        model=model, pretrained_model_path=cfg.INFER.pretrained_model_path\n    )\n\n    # export models\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    # set model predictor\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # infer Data\n    test_x = np.load(osp.join(cfg.data_dir, \"test43_l.npy\")).astype(np.float32)\n    test_v = np.load(osp.join(cfg.data_dir, \"test43_vp.npy\")).astype(np.float32)\n    t = np.array([0.0065, 4 * 0.0065, 7 * 0.0065, 10 * 0.0065, 13 * 0.0065]).astype(\n        np.float32\n    )\n    t_star = np.tile(t.reshape(5, 1), (1, 3000)).reshape(-1, 1)\n    x_star = np.tile(test_x[:, 0:1], (5, 1)).reshape(-1, 1)\n    y_star = np.tile(test_x[:, 1:2], (5, 1)).reshape(-1, 1)\n    z_star = np.tile(test_x[:, 2:3], (5, 1)).reshape(-1, 1)\n    u_star = test_v[:, 0:1]\n    v_star = test_v[:, 1:2]\n    w_star = test_v[:, 2:3]\n    p_star = test_v[:, 3:4]\n\n    pred = predictor.predict(\n        {\n            \"x\": x_star,\n            \"y\": y_star,\n            \"z\": z_star,\n            \"t\": t_star,\n        },\n        cfg.INFER.batch_size,\n    )\n\n    pred = {\n        store_key: pred[infer_key]\n        for store_key, infer_key in zip(cfg.INFER.output_keys, pred.keys())\n    }\n\n    u_pred = pred[\"u\"].reshape((5, -1))\n    v_pred = pred[\"v\"].reshape((5, -1))\n    w_pred = pred[\"w\"].reshape((5, -1))\n    p_pred = pred[\"p\"].reshape((5, -1))\n    u_star = u_star.reshape((5, -1))\n    v_star = v_star.reshape((5, -1))\n    w_star = w_star.reshape((5, -1))\n    p_star = p_star.reshape((5, -1))\n\n    # NS equation can figure out pressure drop, need background pressure p_star.mean()\n    p_pred = p_pred - p_pred.mean() + p_star.mean()\n\n    u_error = np.linalg.norm(u_pred - u_star, axis=1) / np.linalg.norm(u_star, axis=1)\n    v_error = np.linalg.norm(v_pred - v_star, axis=1) / np.linalg.norm(v_star, axis=1)\n    w_error = np.linalg.norm(w_pred - w_star, axis=1) / np.linalg.norm(w_star, axis=1)\n    p_error = np.linalg.norm(p_pred - p_star, axis=1) / np.linalg.norm(w_star, axis=1)\n    t = np.array([0.0065, 4 * 0.0065, 7 * 0.0065, 10 * 0.0065, 13 * 0.0065])\n    plt.plot(t, np.array(u_error))\n    plt.plot(t, np.array(v_error))\n    plt.plot(t, np.array(w_error))\n    plt.plot(t, np.array(p_error))\n    plt.legend([\"u_error\", \"v_error\", \"w_error\", \"p_error\"])\n    plt.xlabel(\"t\")\n    plt.ylabel(\"Relative l2 Error\")\n    plt.title(\"Relative l2 Error, on test dataset\")\n    plt.savefig(osp.join(cfg.output_dir, \"error.jpg\"))\n\n    grid_x, grid_y = np.mgrid[\n        x_star.min() : x_star.max() : 100j, y_star.min() : y_star.max() : 100j\n    ].astype(np.float32)\n    x_plot = grid_x.reshape(-1, 1)\n    y_plot = grid_y.reshape(-1, 1)\n    z_plot = (z_star.min() * np.ones(y_plot.shape)).astype(np.float32)\n    t_plot = ((t[-1]) * np.ones(x_plot.shape)).astype(np.float32)\n    sol = predictor.predict(\n        {\"x\": x_plot, \"y\": y_plot, \"z\": z_plot, \"t\": t_plot}, cfg.INFER.batch_size\n    )\n    sol = {\n        store_key: sol[infer_key]\n        for store_key, infer_key in zip(cfg.INFER.output_keys, sol.keys())\n    }\n    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n    cmap = matplotlib.colormaps.get_cmap(\"jet\")\n\n    ax[0].contourf(grid_x, grid_y, sol[\"u\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[0].set_title(\"u prediction\")\n    ax[1].contourf(grid_x, grid_y, sol[\"v\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[1].set_title(\"v prediction\")\n    ax[2].contourf(grid_x, grid_y, sol[\"w\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[2].set_title(\"w prediction\")\n    ax[3].contourf(grid_x, grid_y, sol[\"p\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[3].set_title(\"p prediction\")\n    norm = matplotlib.colors.Normalize(\n        vmin=sol[\"u\"].min(), vmax=sol[\"u\"].max()\n    )  # set maximum and minimum\n    im = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n    ax13 = fig.add_axes([0.125, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.325, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.525, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.725, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    plt.savefig(osp.join(cfg.output_dir, \"z=0 plane\"))\n\n    grid_y, grid_z = np.mgrid[\n        y_star.min() : y_star.max() : 100j, z_star.min() : z_star.max() : 100j\n    ].astype(np.float32)\n    z_plot = grid_z.reshape(-1, 1)\n    y_plot = grid_y.reshape(-1, 1)\n    x_plot = (x_star.min() * np.ones(y_plot.shape)).astype(np.float32)\n    t_plot = ((t[-1]) * np.ones(x_plot.shape)).astype(np.float32)\n    sol = predictor.predict(\n        {\"x\": x_plot, \"y\": y_plot, \"z\": z_plot, \"t\": t_plot}, cfg.INFER.batch_size\n    )\n    sol = {\n        store_key: sol[infer_key]\n        for store_key, infer_key in zip(cfg.INFER.output_keys, sol.keys())\n    }\n    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n    cmap = matplotlib.colormaps.get_cmap(\"jet\")\n\n    ax[0].contourf(grid_y, grid_z, sol[\"u\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[0].set_title(\"u prediction\")\n    ax[1].contourf(grid_y, grid_z, sol[\"v\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[1].set_title(\"v prediction\")\n    ax[2].contourf(grid_y, grid_z, sol[\"w\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[2].set_title(\"w prediction\")\n    ax[3].contourf(grid_y, grid_z, sol[\"p\"].reshape(grid_x.shape), levels=50, cmap=cmap)\n    ax[3].set_title(\"p prediction\")\n    norm = matplotlib.colors.Normalize(\n        vmin=sol[\"u\"].min(), vmax=sol[\"u\"].max()\n    )  # set maximum and minimum\n    im = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n    ax13 = fig.add_axes([0.125, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.325, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.525, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    ax13 = fig.add_axes([0.725, 0.0, 0.175, 0.02])\n    plt.colorbar(im, cax=ax13, orientation=\"horizontal\")\n    plt.savefig(osp.join(cfg.output_dir, \"x=0 plane\"))\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"VP_NSFNet4.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/nsfnet4/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":""},{"location":"zh/examples/nsfnet4/#nsfnet4_1","title":"NSFNet4","text":"<p>\u5982\u56fe\u6240\u793a, NSFNet\u7684\u7ed3\u679c\u5728\u65f6\u95f4\u4e0a\u7684\u8bef\u5dee\u76f8\u5bf9\u5e73\u7a33, \u5e76\u672a\u51fa\u73b0\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7ecf\u5e38\u51fa\u73b0\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002\u5176\u4e2d, \u867d\u7136\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e09\u4e2a\u65b9\u5411\u7684\u901f\u5ea6\u5e76\u672a\u88ab\u8bbe\u7f6e\u6743\u91cd, \u4f46\u662f\u8bad\u7ec3\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa, \u795e\u7ecf\u7f51\u7edc\u5728\u7b2c\u4e00\u4e2a\u901f\u5ea6\u65b9\u5411u\u4e0a\u9762\u903c\u8fd1\u6548\u679c\u6700\u597d, \u5728\u7b2c\u4e09\u4e2a\u901f\u5ea6\u65b9\u5411w\u4e0a\u9762\u903c\u8fd1\u6548\u679c\u6b21\u4e4b, \u5728\u7b2c\u4e8c\u4e2a\u901f\u5ea6v\u4e0a\u9762\u903c\u8fd1\u6548\u679c\u6700\u5dee\u4e14\u51fa\u73b0\u8f83\u4e3a\u660e\u663e\u7684\u8bef\u5dee\u7d2f\u79ef\u73b0\u8c61\u3002 </p> <p>\u5982\u56fe\u6240\u793a, \u5728x=12.47\u7684y-z\u5e73\u9762\u7684\u8f6e\u5ed3\u56fe, \u7b2c\u4e00\u4e2a\u4e3a\u901f\u5ea6u\u7684\u8f6e\u5ed3\u56fe, \u7b2c\u4e8c\u4e2a\u4e3a\u901f\u5ea6v\u7684\u8f6e\u5ed3\u56fe, \u7b2c\u4e09\u4e2a\u4e3a\u901f\u5ea6w\u7684\u8f6e\u5ed3\u56fe, \u7b2c\u56db\u4e2a\u4e3a\u901f\u5ea6p\u7684\u8f6e\u5ed3\u56fe\u3002\u53ef\u4ee5\u770b\u51fa, \u901f\u5ea6u\u7684\u8f6e\u5ed3\u56fe\u76f8\u5bf9\u4e8ev, w, p\u6765\u8bf4\u8f83\u4e3a\u5149\u6ed1\u3002 </p> <p>\u5982\u56fe\u6240\u793a, \u5728z=4.61\u7684x-y\u5e73\u9762\u7684\u8f6e\u5ed3\u56fe, \u7b2c\u4e00\u4e2a\u4e3a\u901f\u5ea6u\u7684\u8f6e\u5ed3\u56fe, \u7b2c\u4e8c\u4e2a\u4e3a\u901f\u5ea6v\u7684\u8f6e\u5ed3\u56fe, \u7b2c\u4e09\u4e2a\u4e3a\u901f\u5ea6w\u7684\u8f6e\u5ed3\u56fe, \u7b2c\u56db\u4e2a\u4e3a\u901f\u5ea6p\u7684\u8f6e\u5ed3\u56fe\u3002\u53ef\u4ee5\u770b\u51fa, \u901f\u5ea6u\u7684\u8f6e\u5ed3\u56fe\u76f8\u5bf9\u4e8ev, w, p\u6765\u8bf4\u8f83\u4e3a\u5149\u6ed1\u3002 </p> <p>\u7efc\u4e0a\u6240\u8ff0, \u867d\u7136u, v, w\u4e09\u4e2a\u901f\u5ea6\u65b9\u5411\u90fd\u662f\u9700\u8981\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3, \u4f46\u662f\u5bf9\u4e8eJHTDB\u6570\u636e\u96c6\u6765\u8bf4, u\u65b9\u5411\u6570\u636e\u8f83\u4e3a\u5149\u6ed1, \u66f4\u5bb9\u6613\u88ab\u795e\u7ecf\u7f51\u7edc\u6240\u5b66\u4e60\u3002\u56e0\u6b64\u5728\u540e\u7eed\u7814\u7a76\u4e2d, \u53ef\u4ee5\u5c1d\u8bd5\u5bf9\u4e09\u4e2a\u4e0d\u540c\u65b9\u5411\u7684\u5206\u91cf\u5206\u800c\u6cbb\u4e4b, \u52a0\u5927\u590d\u6742\u5206\u91cf\u65b9\u5411\u7684\u8bad\u7ec3\u5f3a\u5ea6, \u51cf\u5c11\u7b80\u5355\u5206\u91cf\u65b9\u5411\u7684\u8bad\u7ec3\u5f3a\u5ea6\u3002</p>"},{"location":"zh/examples/nsfnet4/#6","title":"6. \u7ed3\u679c\u8bf4\u660e","text":"<p>\u6211\u4eec\u4f7f\u7528PINN\u5bf9\u4e0d\u53ef\u538b\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u8fdb\u884c\u6570\u503c\u6c42\u89e3\u3002\u5728PINN\u4e2d, \u968f\u673a\u9009\u53d6\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u7684\u5750\u6807\u88ab\u5f53\u4f5c\u8f93\u5165\u503c, \u6240\u5bf9\u5e94\u7684\u901f\u5ea6\u573a\u4ee5\u53ca\u538b\u5f3a\u573a\u88ab\u5f53\u4f5c\u8f93\u51fa\u503c, \u4f7f\u7528\u521d\u503c\u3001\u8fb9\u754c\u6761\u4ef6\u5f53\u4f5c\u76d1\u7763\u7ea6\u675f\u4ee5\u53ca\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u672c\u8eab\u7684\u5f53\u4f5c\u65e0\u76d1\u7763\u7ea6\u675f\u6761\u4ef6\u52a0\u5165\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\u3002\u6211\u4eec\u4f7f\u7528\u9ad8\u7cbe\u5ea6JHTDB\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u635f\u5931\u51fd\u6570\u7684\u4e0b\u964d\u53ef\u4ee5\u8bc1\u660e\u795e\u7ecf\u7f51\u7edc\u5728\u6c42\u89e3\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u4e2d\u7684\u6536\u655b\u6027, \u8868\u660ePINN\u62e5\u6709\u5bf9\u4e0d\u53ef\u538b\u5f3a\u8feb\u5404\u9879\u540c\u6027\u6e4d\u6d41\u7684\u6c42\u89e3\u80fd\u529b\u3002\u800c\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, PINN\u53ef\u4ee5\u5f88\u597d\u7684\u903c\u8fd1\u5bf9\u5e94\u7684\u9ad8\u7cbe\u5ea6\u4e0d\u53ef\u538b\u5f3a\u8feb\u5404\u9879\u540c\u6027\u6e4d\u6d41\u6570\u636e\u96c6, \u5e76\u4e14, \u6211\u4eec\u53d1\u73b0\u589e\u52a0\u8fb9\u754c\u7ea6\u675f\u4ee5\u53ca\u521d\u503c\u7ea6\u675f\u7684\u6743\u91cd\u53ef\u4ee5\u4f7f\u5f97\u795e\u7ecf\u7f51\u7edc\u62e5\u6709\u66f4\u597d\u7684\u903c\u8fd1\u6548\u679c\u3002\u76f8\u6bd4\u4e4b\u4e0b, \u5728\u8bef\u5dee\u5141\u8bb8\u8303\u56f4\u5185, \u4f7f\u7528PINN\u6c42\u89e3\u8be5\u7eb3\u97e6\u65af\u6258\u514b\u65b9\u7a0b\u6bd4\u539f\u672c\u4f7f\u7528DNS\u65b9\u6cd5\u7684\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\u3002</p>"},{"location":"zh/examples/nsfnet4/#7","title":"7. \u53c2\u8003\u8d44\u6599","text":"<ul> <li> <p>NSFnets (Navier-Stokes Flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations</p> </li> <li> <p>Github NSFnets</p> </li> </ul>"},{"location":"zh/examples/phycrnet/","title":"PhyCRNet","text":""},{"location":"zh/examples/phycrnet/#phycrnet","title":"PhyCRNet","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyCRNet/burgers_1501x2x128x128.mat -P ./data/\n\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyCRNet/burgers_1501x2x128x128.mat --create-dirs -o ./data/burgers_1501x2x128x128.mat\n\npython main.py DATA_PATH=./data/burgers_1501x2x128x128.mat\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyCRNet/burgers_1501x2x128x128.mat -P ./data/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyCRNet/burgers_1501x2x128x128.mat --create-dirs -o ./data/burgers_1501x2x128x128.mat\n\npython main.py mode=eval DATA_PATH=./data/burgers_1501x2x128x128.mat EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/phycrnet/phycrnet_burgers.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 phycrnet_burgers_pretrained.pdparams a-RMSE: 3.20e-3"},{"location":"zh/examples/phycrnet/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u590d\u6742\u65f6\u7a7a\u7cfb\u7edf\u901a\u5e38\u53ef\u4ee5\u901a\u8fc7\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6765\u5efa\u6a21\uff0c\u5b83\u4eec\u5728\u8bb8\u591a\u9886\u57df\u90fd\u5341\u5206\u5e38\u89c1\uff0c\u5982\u5e94\u7528\u6570\u5b66\u3001\u7269\u7406\u5b66\u3001\u751f\u7269\u5b66\u3001\u5316\u5b66\u548c\u5de5\u7a0b\u5b66\u3002\u6c42\u89e3PDE\u7cfb\u7edf\u4e00\u76f4\u662f\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u7684\u4e00\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002 \u672c\u6587\u7684\u5177\u4f53\u76ee\u6807\u662f\u4e3a\u4e86\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u3001\u8003\u8651\u7269\u7406\u4fe1\u606f\u7684\u5377\u79ef-\u9012\u5f52\u5b66\u4e60\u67b6\u6784\uff08PhyCRNet\uff09\u53ca\u5176\u8f7b\u91cf\u7ea7\u53d8\u4f53\uff08PhyCRNet-s\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u6ca1\u6709\u4efb\u4f55\u6807\u7b7e\u6570\u636e\u7684\u591a\u7ef4\u65f6\u95f4\u7a7a\u95f4PDEs\u3002\u672c\u9879\u76ee\u4e3b\u8981\u76ee\u6807\u662f\u4f7f\u7528PaddleScience\u590d\u73b0\u8bba\u6587\u6240\u63d0\u4f9b\u7684\u4ee3\u7801\uff0c\u5e76\u4e0e\u4ee3\u7801\u7684\u7cbe\u5ea6\u5bf9\u9f50\u3002</p> <p>\u8be5\u7f51\u7edc\u6709\u4ee5\u4e0b\u4f18\u52bf\uff1a</p> <p>1\u3001 \u4f7f\u7528ConvLSTM(enconder-decoder Convolutional Long Short-Term Memory network) \u53ef\u4ee5\u5145\u5206\u63d0\u53d6\u4f4e\u7ef4\u7a7a\u95f4\u4e0a\u7684\u7279\u5f81\u4ee5\u53ca\u5b66\u4e60\u5176\u65f6\u95f4\u4e0a\u7684\u53d8\u5316\u3002</p> <p>2\u3001\u4f7f\u7528\u4e00\u4e2a\u5168\u5c40\u7684\u6b8b\u5dee\u8fed\u4ee3\u4ece\u800c\u53ef\u4ee5\u4e25\u683c\u5730\u6267\u884c\u65f6\u95f4\u4e0a\u7684\u8fed\u4ee3\u8fc7\u7a0b\u3002</p> <p>3\u3001\u4f7f\u7528\u57fa\u4e8e\u9ad8\u9636\u6709\u9650\u5dee\u5206\u683c\u5f0f\u7684\u6ee4\u6ce2\u4ece\u800c\u80fd\u591f\u7cbe\u786e\u6c42\u89e3\u91cd\u8981\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u5bfc\u6570\u503c\u3002</p> <p>4\u3001\u4f7f\u7528\u5f3a\u5236\u8fb9\u754c\u6761\u4ef6\u662f\u7684\u6240\u6c42\u89e3\u7684\u6570\u503c\u89e3\u53ef\u4ee5\u6ee1\u8db3\u539f\u65b9\u7a0b\u6240\u8981\u6c42\u7684\u521d\u503c\u4ee5\u53ca\u8fb9\u754c\u6761\u4ef6\u3002</p>"},{"location":"zh/examples/phycrnet/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u5728\u672c\u6a21\u578b\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u7684\u662f\u542b\u6709\u65f6\u95f4\u548c\u7a7a\u95f4\u7684PDE\u6a21\u578b\uff0c\u6b64\u7c7b\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f1a\u5b58\u5728\u65f6\u95f4\u4e0a\u7684\u8bef\u5dee\u7d2f\u79ef\u7684\u95ee\u9898\uff0c\u56e0\u6b64\uff0c\u672c\u6587\u901a\u8fc7\u8bbe\u8ba1\u5faa\u73af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bd5\u56fe\u51cf\u8f7b\u6bcf\u4e00\u6b65\u65f6\u95f4\u8fed\u4ee3\u7684\u8bef\u5dee\u7d2f\u79ef\u3002\u800c\u6211\u4eec\u6240\u6c42\u89e3\u7684\u95ee\u9898\u4e3a\u4ee5\u9ad8\u65af\u5206\u5e03\u968f\u673a\u5f97\u5230\u7684\u503c\u4e3a\u521d\u503c\u7684\u4e8c\u7ef4Burgers' Equation\uff1a</p> \\[u_t+u\\cdot \\nabla u -\\nu u =0\\] <p>\u4e8c\u7ef4Burgers' Equation \u523b\u753b\u4e86\u590d\u6742\u7684\u975e\u7ebf\u6027\u7684\u53cd\u5e94\u6269\u6563\u76f8\u4e92\u4f5c\u7528\u7684\u95ee\u9898\uff0c\u56e0\u6b64\uff0c\u7ecf\u5e38\u88ab\u7528\u6765\u5f53\u4f5cbenchmark\u6765\u6bd4\u8f83\u5404\u79cd\u79d1\u5b66\u8ba1\u7b97\u7b97\u6cd5\u3002</p>"},{"location":"zh/examples/phycrnet/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":""},{"location":"zh/examples/phycrnet/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8fd9\u4e00\u90e8\u5206\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd PhyCRNet \u7684\u67b6\u6784\uff0c\u5305\u62ec\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u5757\u3001\u6b8b\u5dee\u8fde\u63a5\u3001\u81ea\u56de\u5f52\uff08AR\uff09\u8fc7\u7a0b\u548c\u57fa\u4e8e\u8fc7\u6ee4\u7684\u5fae\u5206\u3002\u7f51\u7edc\u67b6\u6784\u5982\u56fe\u6240\u793a\u3002\u7f16\u7801\u5668(\u9ec4\u8272Encoder\uff0c\u5305\u542b3\u4e2a\u5377\u79ef\u5c42)\uff0c\u7528\u4e8e\u4ece\u8f93\u5165\u72b6\u6001\u53d8\u91cf \\(u(t=i)\uff0ci = 0,1,2,..,T-1\\) \u5b66\u4e60\u4f4e\u7ef4\u6f5c\u5728\u7279\u5f81\uff0c\u5176\u4e2d \\(T\\) \u8868\u793a\u603b\u65f6\u95f4\u6b65\u3002\u6211\u4eec\u5e94\u7528 ReLU \u4f5c\u4e3a\u5377\u79ef\u5c42\u7684\u6fc0\u6d3b\u51fd\u6570\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06ConvLSTM\u5c42\u7684\u8f93\u51fa(Encoder\u5f97\u5230\u7684\u4f4e\u5206\u8fa8\u7387)\uff0c\u6f5c\u5728\u7279\u5f81\u7684\u65f6\u95f4\u4f20\u64ad\u5668(\u7eff\u8272\u90e8\u5206)\uff0c\u5176\u4e2d\uff0c\u8f93\u51fa\u7684LSTM\u7684\u8bb0\u5fc6\u5355\u5143 \\(C_i\\) \u548cLSTM\u7684\u9690\u85cf\u53d8\u91cf\u5355\u5143 \\(h_i\\) \u4f1a\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u3002\u8fd9\u6837\u505a\u7684\u597d\u5904\u662f\u5bf9\u4f4e\u7ef4\u53d8\u91cf\u7684\u57fa\u672c\u52a8\u6001\u8fdb\u884c\u5efa\u6a21,\u80fd\u591f\u51c6\u786e\u5730\u6355\u83b7\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u540c\u65f6\u6709\u52a9\u4e8e\u51cf\u8f7b\u8bb0\u5fc6\u8d1f\u62c5\u3002 \u4f7f\u7528 LSTM \u7684\u53e6\u4e00\u4e2a\u4f18\u52bf\u6765\u81ea\u8f93\u51fa\u72b6\u6001\u7684\u53cc\u66f2\u6b63\u5207\u51fd\u6570\uff0c\u5b83\u53ef\u4ee5\u4fdd\u6301\u5e73\u6ed1\u7684\u68af\u5ea6\u66f2\u7ebf\uff0c\u5e76\u5c06\u503c\u63a7\u5236\u5728 -1 \u548c 1 \u4e4b\u95f4\u3002\u5728\u5efa\u7acb\u4f4e\u5206\u8fa8\u7387LSTM\u5377\u79ef\u5faa\u73af\u65b9\u6848\u540e\uff0c\u6211\u4eec\u57fa\u4e8e\u4e0a\u91c7\u6837\u64cd\u4f5cDecoder(\u84dd\u8272\u90e8\u5206)\u76f4\u63a5\u5c06\u4f4e\u5206\u8fa8\u7387\u6f5c\u5728\u7a7a\u95f4\u91cd\u5efa\u4e3a\u9ad8\u5206\u8fa8\u7387\u91cf\u3002\u7279\u522b\u6ce8\u660e\uff0c\u5e94\u7528\u4e86\u5b50\u50cf\u7d20\u5377\u79ef\u5c42\uff08\u5373\u50cf\u7d20shuffle\uff09\uff0c\u56e0\u4e3a\u4e0e\u53cd\u5377\u79ef\u76f8\u6bd4\uff0c\u5b83\u5177\u6709\u66f4\u597d\u7684\u6548\u7387\u548c\u91cd\u5efa\u7cbe\u5ea6\uff0c\u4e14\u4f2a\u50cf\u66f4\u5c11\u3002 \u6700\u540e\uff0c\u6211\u4eec\u6dfb\u52a0\u53e6\u4e00\u4e2a\u5377\u79ef\u5c42\uff0c\u7528\u4e8e\u5c06\u6709\u754c\u6f5c\u53d8\u91cf\u7a7a\u95f4\u8f93\u51fa\uff0c\u7f29\u653e\u56de\u539f\u59cb\u7684\u7269\u7406\u7a7a\u95f4\u3002\u8be5Decoder\u540e\u9762\u6ca1\u6709\u6fc0\u6d3b\u51fd\u6570\u3002 \u6b64\u5916\uff0c\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u9274\u4e8e\u8f93\u5165\u53d8\u91cf\u6570\u91cf\u6709\u9650\u53ca\u5176\u5bf9\u8d85\u5206\u8fa8\u7387\u7684\u7f3a\u9677\uff0c\u6211\u4eec\u5728 PhyCRNet \u4e2d\u6ca1\u6709\u8003\u8651 batch normalization\u3002 \u4f5c\u4e3a\u66ff\u4ee3\uff0c\u6211\u4eec\u4f7f\u7528 batch normalization \u6765\u8bad\u7ec3\u7f51\u7edc\uff0c\u4ee5\u5b9e\u73b0\u8bad\u7ec3\u52a0\u901f\u548c\u66f4\u597d\u7684\u6536\u655b\u6027\u3002\u53d7\u5230\u52a8\u529b\u5b66\u4e2d\uff0cForward Eular Scheme \u7684\u542f\u53d1\uff0c\u6211\u4eec\u5728\u8f93\u5165\u72b6\u6001\u53d8\u91cf \\(u_i\\) \u548c\u8f93\u51fa\u53d8\u91cf \\(u_{i+1}\\) \u4e4b\u95f4\u9644\u52a0\u5168\u5c40\u6b8b\u5dee\u8fde\u63a5\u3002\u5177\u4f53\u7f51\u7edc\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <p></p> <p>\u63a5\u4e0b\u6765\uff0c\u5269\u4e0b\u7684\u6311\u6218\u662f\uff0c\u5982\u4f55\u8fdb\u884c\u7269\u7406\u5d4c\u5165\uff0c\u6765\u878d\u5408N-S\u65b9\u7a0b\u5e26\u6765\u7684\u7cbe\u5ea6\u63d0\u5347\u3002\u6211\u4eec\u5e94\u7528\u65e0\u68af\u5ea6\u5377\u79ef\u6ee4\u6ce2\u5668\uff0c\u6765\u8868\u793a\u79bb\u6563\u6570\u503c\u5fae\u5206\uff0c\u4ee5\u8fd1\u4f3c\u611f\u5174\u8da3\u7684\u5bfc\u6570\u9879\u3002 \u4f8b\u5982\uff0c\u6211\u4eec\u5728\u672c\u6587\u4e2d\u8003\u8651\u7684\u57fa\u4e8e Finite Difference \u6709\u9650\u5dee\u5206\u7684\u6ee4\u6ce2\u5668\u662f2\u9636\u548c4\u9636\u4e2d\u5fc3\u5dee\u5206\u683c\u5f0f\uff0c\u6765\u8ba1\u7b97\u65f6\u95f4\u548c\u7a7a\u95f4\u5bfc\u6570\u3002</p> <p>\u65f6\u95f4\u5dee\u5206:</p> \\[K_t = [-1,0,1] \\times \\frac{1}{2 \\delta t},\\] <p>\u7a7a\u95f4\u5dee\u5206:</p> \\[K_s = \\begin{bmatrix}    0  &amp; 0  &amp; -1  &amp; 0  &amp; 0  \\\\    0  &amp; 0  &amp; 16  &amp; 0  &amp; 0  \\\\    -1 &amp; 16 &amp; -60 &amp; 16 &amp; -1 \\\\    0  &amp; 0  &amp; 16  &amp; 0  &amp; 0  \\\\    0  &amp; 0  &amp; -1  &amp; 0  &amp; 0  \\\\ \\end{bmatrix} \\times \\frac{1}{12 (\\delta x)^2},\\] <p>\u5176\u4e2d \\(\\delta t\\) \u548c \\(\\delta x\\) \u8868\u793a\u65f6\u95f4\u6b65\u957f\u548c\u7a7a\u95f4\u6b65\u957f\u3002</p> <p>\u6b64\u5916\u9700\u8981\u6ce8\u610f\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u8fb9\u754c\u4e0a\u7684\u5bfc\u6570\uff0c\u4e22\u5931\u8fb9\u754c\u5dee\u5f02\u4fe1\u606f\u7684\u98ce\u9669\u53ef\u4ee5\u901a\u8fc7\u63a5\u4e0b\u6765\u5f15\u5165\u7684\u5728\u4f20\u7edf\u6709\u9650\u5dee\u5206\u4e2d\u7ecf\u5e38\u4f7f\u7528\u7684\u9b3c\u70b9\u586b\u5145\u673a\u5236\u6765\u51cf\u8f7b\uff0c\u5176\u4e3b\u8981\u6838\u5fc3\u662f\u5728\u77e9\u9635\u5916\u56f4\u586b\u5145\u4e00\u5c42\u6216\u591a\u5c42\u9b3c\u70b9\uff08\u5c42\u6570\u53d6\u51b3\u4e8e\u5dee\u5206\u683c\u5f0f\uff0c\u5373\uff0c\u8fc7\u6ee4\u5668\u7684\u5927\u5c0f\uff09\uff0c\u4ee5\u4e0b\u56fe\u4e3a\u4f8b\uff0c\u5728\u8fea\u5229\u514b\u96f7\u8fb9\u754c\u6761\u4ef6\uff08Dirichlet BCs\uff09\u4e0b\uff0c\u6211\u4eec\u53ea\u9700\u8981\u628a\u5e38\u503c\u9b3c\u70b9\u5728\u539f\u77e9\u9635\u5916\u56f4\u586b\u5145\u5373\u53ef\uff1b\u5728\u8bfa\u4f0a\u66fc\u8fb9\u754c\u6761\u4ef6(Neumann BCs)\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u6839\u636e\u5176\u8fb9\u754c\u6761\u4ef6\u5bfc\u6570\u503c\u786e\u5b9a\u9b3c\u70b9\u7684\u503c\u3002</p> <p></p> <pre><code>    dt=cfg.DT, step=steps, effective_step=effective_step, **cfg.MODEL\n)\n</code></pre> <pre><code># model settings\nMODEL:\n  input_channels: 2\n  hidden_channels: [8, 32, 128, 128]\n  input_kernel_size: [4, 4, 4, 3]\n  input_stride: [2, 2, 2, 1]\n  input_padding: [1, 1, 1, 1]\n  num_layers: [3, 1]\n</code></pre>"},{"location":"zh/examples/phycrnet/#32","title":"3.2 \u6570\u636e\u8f7d\u5165","text":"<p>\u6211\u4eec\u4f7f\u7528RK4\u6216\u8005\u8c31\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\uff08\u521d\u503c\u4e3a\u4f7f\u7528\u6b63\u6001\u5206\u5e03\u751f\u6210\uff09\uff0c\u9700\u8981\u4ece.mat\u6587\u4ef6\u4e2d\u5c06\u5176\u8bfb\u5165\uff0c\uff1a</p> <pre><code>uv = data[\"uv\"]  # [t,c,h,w]\nfunctions.uv = uv\n\n# generate input data\n(\n    input_dict_train,\n    label_dict_train,\n    input_dict_val,\n    label_dict_val,\n) = functions.Dataset(\n    paddle.to_tensor(initial_state),\n    paddle.to_tensor(\n        uv[\n            0:1,\n        ],\n        dtype=paddle.get_default_dtype(),\n    ),\n).get()\n</code></pre>"},{"location":"zh/examples/phycrnet/#33","title":"3.3 \u7ea6\u675f\u6784\u5efa","text":"<p>\u8bbe\u7f6e\u7ea6\u675f\u4ee5\u53ca\u76f8\u5173\u635f\u5931\u51fd\u6570\uff1a</p> <pre><code>    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict_train,\n            \"label\": label_dict_train,\n        },\n        \"batch_size\": 1,\n        \"num_workers\": 0,\n    },\n    ppsci.loss.FunctionalLoss(functions.train_loss_func),\n    {\n        \"loss\": lambda out: out[\"loss\"],\n    },\n    name=\"sup_train\",\n)\nconstraint_pde = {sup_constraint_pde.name: sup_constraint_pde}\n</code></pre>"},{"location":"zh/examples/phycrnet/#34","title":"3.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u8bbe\u7f6e\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u76f8\u5173\u635f\u5931\u51fd\u6570\uff1a</p> <pre><code>    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict_val,\n            \"label\": label_dict_val,\n        },\n        \"batch_size\": 1,\n        \"num_workers\": 0,\n    },\n    ppsci.loss.FunctionalLoss(functions.val_loss_func),\n    {\n        \"loss\": lambda out: out[\"loss\"],\n    },\n    metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n    name=\"sup_valid\",\n)\nvalidator_pde = {sup_validator_pde.name: sup_validator_pde}\n</code></pre>"},{"location":"zh/examples/phycrnet/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9 <code>Adam</code> \u4f18\u5316\u5668\u5e76\u8bbe\u5b9a <code>learning_rate</code></p> <pre><code>optimizer = ppsci.optimizer.Adam(scheduler)(model)\nsolver = ppsci.solver.Solver(\n    model,\n    constraint_pde,\n</code></pre>"},{"location":"zh/examples/phycrnet/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u4e3a\u4e86\u8bc4\u4f30\u6240\u6709\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6c42\u89e3\u5668\u4ea7\u751f\u7684\u89e3\u51b3\u65b9\u6848\u7cbe\u5ea6\uff0c\u6211\u4eec\u5206\u4e24\u4e2a\u9636\u6bb5\u8bc4\u4f30\u4e86\u5168\u573a\u8bef\u5dee\u4f20\u64ad\uff1a\u8bad\u7ec3\u548c\u5916\u63a8\u3002\u5728\u65f6\u523b \u03c4 \u7684\u5168\u573a\u8bef\u5dee \\(\\epsilon_\\tau\\) \u7684\u5b9a\u4e49\u4e3a\u7ed9\u5b9a b \u7684\u7d2f\u79ef\u5747\u65b9\u6839\u8bef\u5dee (a-RMSE)\u3002</p> \\[ \\epsilon_\\tau=\\sqrt{\\frac{1}{N_\\tau} \\sum_{k=1}^{N_\\tau} \\frac{\\left\\|\\mathbf{u}^*\\left(\\mathbf{x}, t_k\\right)-\\mathbf{u}^\\theta\\left(\\mathbf{x}, t_k\\right)\\right\\|_2^2}{m n}} \\] <p>\u8fd9\u4e00\u6b65\u9700\u8981\u901a\u8fc7\u8bbe\u7f6e\u5916\u754c\u51fd\u6570\u6765\u8fdb\u884c\uff0c\u56e0\u6b64\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528<code>function.transform_out</code>\u6765\u8fdb\u884c\u8bad\u7ec3</p> <pre><code>    return functions.transform_out(_in, _out, model)\n\nmodel.register_input_transform(functions.transform_in)\nmodel.register_output_transform(_transform_out)\n</code></pre> <p>\u800c\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528<code>function.tranform_output_val</code>\u6765\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u751f\u6210\u7d2f\u8ba1\u5747\u65b9\u6839\u8bef\u5dee\u3002</p> <pre><code>(h0, c0) = (paddle.randn((1, 128, 16, 16)), paddle.randn((1, 128, 16, 16)))\n</code></pre> <p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\u3002</p> <pre><code>    cfg.output_dir,\n    optimizer,\n    scheduler,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    validator=validator_pde,\n    eval_with_no_grad=cfg.TRAIN.eval_with_no_grad,\n    pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n)\n\n# train model\nsolver.train()\n# evaluate after finished training\n</code></pre> <p>\u6700\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u5373\u53ef\uff1a</p> <pre><code>def evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set initial states for convlstm\n</code></pre>"},{"location":"zh/examples/phycrnet/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"phycrnet<pre><code>\"\"\"\nPhyCRNet for solving spatiotemporal PDEs\nReference: https://github.com/isds-neu/PhyCRNet/\n\"\"\"\nfrom os import path as osp\n\nimport functions\nimport hydra\nimport paddle\nimport scipy.io as scio\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set initial states for convlstm\n    NUM_CONVLSTM = cfg.num_convlstm\n    (h0, c0) = (paddle.randn((1, 128, 16, 16)), paddle.randn((1, 128, 16, 16)))\n    initial_state = []\n    for _ in range(NUM_CONVLSTM):\n        initial_state.append((h0, c0))\n\n    # grid parameters\n    time_steps = cfg.TIME_STEPS\n    dx = cfg.DX[0] / cfg.DX[1]\n\n    steps = cfg.TIME_BATCH_SIZE + 1\n    effective_step = list(range(0, steps))\n    num_time_batch = int(time_steps / cfg.TIME_BATCH_SIZE)\n\n    functions.dt = cfg.DT\n    functions.dx = dx\n    functions.time_steps = cfg.TIME_STEPS\n    functions.num_time_batch = num_time_batch\n    model = ppsci.arch.PhyCRNet(\n        dt=cfg.DT, step=steps, effective_step=effective_step, **cfg.MODEL\n    )\n\n    def _transform_out(_in, _out):\n        return functions.transform_out(_in, _out, model)\n\n    model.register_input_transform(functions.transform_in)\n    model.register_output_transform(_transform_out)\n\n    # use Burgers_2d_solver_HighOrder.py to generate data\n    data = scio.loadmat(cfg.DATA_PATH)\n    uv = data[\"uv\"]  # [t,c,h,w]\n    functions.uv = uv\n\n    # generate input data\n    (\n        input_dict_train,\n        label_dict_train,\n        input_dict_val,\n        label_dict_val,\n    ) = functions.Dataset(\n        paddle.to_tensor(initial_state),\n        paddle.to_tensor(\n            uv[\n                0:1,\n            ],\n            dtype=paddle.get_default_dtype(),\n        ),\n    ).get()\n\n    sup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_train,\n                \"label\": label_dict_train,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.train_loss_func),\n        {\n            \"loss\": lambda out: out[\"loss\"],\n        },\n        name=\"sup_train\",\n    )\n    constraint_pde = {sup_constraint_pde.name: sup_constraint_pde}\n\n    sup_validator_pde = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_val,\n                \"label\": label_dict_val,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.val_loss_func),\n        {\n            \"loss\": lambda out: out[\"loss\"],\n        },\n        metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n        name=\"sup_valid\",\n    )\n    validator_pde = {sup_validator_pde.name: sup_validator_pde}\n\n    # initialize solver\n    scheduler = ppsci.optimizer.lr_scheduler.Step(**cfg.TRAIN.lr_scheduler)()\n\n    optimizer = ppsci.optimizer.Adam(scheduler)(model)\n    solver = ppsci.solver.Solver(\n        model,\n        constraint_pde,\n        cfg.output_dir,\n        optimizer,\n        scheduler,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        validator=validator_pde,\n        eval_with_no_grad=cfg.TRAIN.eval_with_no_grad,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n    )\n\n    # train model\n    solver.train()\n    # evaluate after finished training\n    model.register_output_transform(functions.tranform_output_val)\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # set initial states for convlstm\n    NUM_CONVLSTM = cfg.num_convlstm\n    (h0, c0) = (paddle.randn((1, 128, 16, 16)), paddle.randn((1, 128, 16, 16)))\n    initial_state = []\n    for _ in range(NUM_CONVLSTM):\n        initial_state.append((h0, c0))\n\n    # grid parameters\n    time_steps = cfg.TIME_STEPS\n    dx = cfg.DX[0] / cfg.DX[1]\n\n    steps = cfg.EVAL.TIME_BATCH_SIZE + 1\n    effective_step = list(range(0, steps))\n    num_time_batch = int(time_steps / cfg.EVAL.TIME_BATCH_SIZE)\n\n    functions.dt = cfg.DT\n    functions.dx = dx\n    functions.num_time_batch = num_time_batch\n    model = ppsci.arch.PhyCRNet(\n        dt=cfg.DT, step=steps, effective_step=effective_step, **cfg.MODEL\n    )\n\n    def _transform_out(_in, _out):\n        return functions.transform_out(_in, _out, model)\n\n    model.register_input_transform(functions.transform_in)\n    model.register_output_transform(_transform_out)\n\n    # use the generated data\n    data = scio.loadmat(cfg.DATA_PATH)\n    uv = data[\"uv\"]  # [t,c,h,w]\n    functions.uv = uv\n    _, _, input_dict_val, _ = functions.Dataset(\n        paddle.to_tensor(initial_state),\n        paddle.to_tensor(uv[0:1, ...], dtype=paddle.get_default_dtype()),\n    ).get()\n    ppsci.utils.load_pretrain(model, cfg.EVAL.pretrained_model_path)\n    model.register_output_transform(None)\n    functions.output_graph(model, input_dict_val, cfg.output_dir, cfg.case_name)\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"burgers_equations.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/phycrnet/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u672c\u6587\u901a\u8fc7\u5bf9Burgers' Equation\u8fdb\u884c\u8bad\u7ec3\uff0c\u6240\u5f97\u7ed3\u679c\u5982\u4e0b\uff0c\u6839\u636e\u7cbe\u5ea6\u548c\u6269\u5c55\u80fd\u529b\u7684\u5bf9\u6bd4\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\uff08t=1.0,2.0\uff09\u4ee5\u53ca\u62d3\u5c55\u96c6\uff08t=3.0,4.0\uff09\u4e0a\u5747\u6709\u826f\u597d\u7684\u8868\u73b0\u6548\u679c\u3002pred\u4e3a\u4f7f\u7528\u7f51\u7edc\u9884\u6d4b\u7684\u901f\u5ea6\u7684\u7b2c\u4e00\u5206\u91cfu\u5728\u5b9a\u4e49\u57df\u4e0a\u7684contour\u56fe\uff0ctruth\u4e3a\u771f\u5b9e\u7684\u901f\u5ea6\u7b2c\u4e00\u5206\u91cfu\u5728\u5b9a\u4e49\u57df\u4e0a\u7684contour\u56fe\uff0cError\u4e3a\u9884\u6d4b\u503c\u4e0e\u771f\u5b9e\u503c\u4e4b\u95f4\u5728\u6574\u4e2a\u5b9a\u4e49\u57df\u5dee\u503c\u3002</p> <p></p>"},{"location":"zh/examples/phycrnet/#6","title":"6. \u7ed3\u679c\u8bf4\u660e","text":"<p>\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u662f\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u800c\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u5728\u6c42\u89e3\u9006\u95ee\u9898\u4ee5\u53ca\u6570\u636e\u540c\u5316\u95ee\u9898\u7b49\u5728\u4f20\u7edf\u65b9\u6cd5\u4e0a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u4e0a\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4f46\u662f\uff0c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u65b9\u6cd5\u53d7\u9650\u5236\u4e8e\u53ef\u6269\u5c55\u6027\uff0c\u8bef\u5dee\u4f20\u5bfc\u4ee5\u53ca\u6cdb\u5316\u80fd\u529b\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u672c\u8bba\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u795e\u7ecf\u7f51\u7edcPhyCRNet,\u901a\u8fc7\u5c06\u4f20\u7edf\u6709\u9650\u5dee\u5206\u7684\u601d\u8def\u5d4c\u5165\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u9488\u5bf9\u6027\u5730\u89e3\u51b3\u539f\u795e\u7ecf\u7f51\u7edc\u7f3a\u5c11\u5bf9\u957f\u65f6\u95f4\u6570\u636e\u7684\u63a8\u7406\u80fd\u529b\u3001\u8bef\u5dee\u7d2f\u79ef\u4ee5\u53ca\u7f3a\u5c11\u6cdb\u5316\u80fd\u529b\u7684\u95ee\u9898\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u672c\u6587\u901a\u8fc7\u7c7b\u4f3c\u4e8e\u6709\u9650\u5dee\u5206\u7684\u8fb9\u754c\u5904\u7406\u65b9\u5f0f\uff0c\u5c06\u539f\u672c\u8fb9\u754c\u6761\u4ef6\u7684\u8f6f\u9650\u5236\u8f6c\u4e3a\u786c\u9650\u5236\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u51c6\u786e\u6027\u3002\u65b0\u63d0\u51fa\u7684\u7f51\u7edc\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u4e0a\u8ff0\u63d0\u5230\u7684\u6570\u636e\u540c\u5316\u95ee\u9898\u4ee5\u53ca\u9006\u95ee\u9898\u3002</p>"},{"location":"zh/examples/phycrnet/#7","title":"7. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs</li> <li>https://github.com/isds-neu/PhyCRNet</li> </ul>"},{"location":"zh/examples/phygeonet/","title":"PhyGeoNet","text":""},{"location":"zh/examples/phygeonet/#phygeonet","title":"PhyGeoNet","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <pre><code># heat_equation\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation.npz -P ./data/\n\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation.npz --create-dirs -o ./data/heat_equation.npz\n\npython heat_equation.py\n\n# heat_equation_bc\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation_bc.npz -P ./data/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation_bc_test.npz -P ./data/\n\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation_bc.npz --create-dirs -o ./data/heat_equation.npz\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation_bc_test.npz --create-dirs -o ./data/heat_equation.npz\n\npython heat_equation_with_bc.py\n</code></pre> <pre><code># heat_equation\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation.npz -P ./data/\n\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation.npz --create-dirs -o ./data/heat_equation.npz\n\npython heat_equation.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/PhyGeoNet/heat_equation_pretrain.pdparams\n\n# heat_equation_bc\n# linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation_bc.npz -P ./data/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation_bc_test.npz -P ./data/\n\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation_bc.npz --create-dirs -o ./data/heat_equation.npz\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyGeoNet/heat_equation_bc_test.npz --create-dirs -o ./data/heat_equation.npz\n\npython heat_equation_with_bc.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/PhyGeoNet/heat_equation_bc_pretrain.pdparams\n</code></pre> \u6a21\u578b mRes ev heat_equation_pretrain.pdparams 0.815 0.095 heat_equation_bc_pretrain.pdparams 992 0.31"},{"location":"zh/examples/phygeonet/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6700\u8fd1\u51e0\u5e74\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u5f88\u591a\u9886\u57df\u53d6\u5f97\u4e86\u975e\u51e1\u7684\u6210\u5c31\uff0c\u5c24\u5176\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\uff0c\u53d7\u542f\u53d1\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u5f3a\u5927\u7684\u51fd\u6570\u903c\u8fd1\u80fd\u529b\uff0c\u795e\u7ecf\u7f51\u7edc\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u4e5f\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u73b0\u9636\u6bb5\u7684\u7814\u7a76\u4e3b\u8981\u5206\u4e3a\u4e24\u5927\u7c7b\uff0c\u4e00\u7c7b\u662f\u5c06\u7269\u7406\u4fe1\u606f\u4ee5\u53ca\u7269\u7406\u9650\u5236\u52a0\u5165\u635f\u5931\u51fd\u6570\u6765\u5bf9\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3, \u5176\u4ee3\u8868\u6709 PINN \u4ee5\u53ca Deep Ritz Net\uff0c\u53e6\u4e00\u7c7b\u662f\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff0c\u5176\u4ee3\u8868\u6709 FNO \u4ee5\u53ca DeepONet\u3002\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u5728\u79d1\u5b66\u5b9e\u8df5\u4e2d\u83b7\u5f97\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u6bd4\u5982\u5929\u6c14\u9884\u6d4b\uff0c\u91cf\u5b50\u5316\u5b66\uff0c\u751f\u7269\u5de5\u7a0b\uff0c\u4ee5\u53ca\u8ba1\u7b97\u6d41\u4f53\u7b49\u9886\u57df\u3002\u7531\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u53c2\u6570\u5171\u4eab\u7684\u6027\u8d28\uff0c\u53ef\u4ee5\u5b66\u4e60\u5927\u5c3a\u5ea6\u7684\u65f6\u7a7a\u57df\uff0c\u56e0\u6b64\u83b7\u5f97\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002</p>"},{"location":"zh/examples/phygeonet/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u800c\u5728\u5b9e\u9645\u79d1\u5b66\u8ba1\u7b97\u95ee\u9898\u4e2d\uff0c\u5f88\u591a\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6c42\u89e3\u57df\u662f\u590d\u6742\u8fb9\u754c\u4e14\u975e\u5747\u5300\u7684\u3002\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u5f80\u5f80\u9488\u5bf9\u5177\u6709\u89c4\u5219\u8fb9\u754c\u4ee5\u53ca\u5747\u5300\u7f51\u683c\u7684\u6c42\u89e3\u57df\uff0c\u6240\u4ee5\u5e76\u6ca1\u6709\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002</p> <p>\u672c\u6587\u9488\u5bf9\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u5728\u590d\u6742\u8fb9\u754c\u975e\u5747\u5300\u7f51\u683c\u6c42\u89e3\u57df\u4e0a\u6548\u679c\u8f83\u5dee\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u5750\u6807\u53d8\u5316\u5c06\u4e0d\u89c4\u5219\u8fb9\u754c\u975e\u5747\u5300\u7f51\u683c\u53d8\u6210\u89c4\u5219\u8fb9\u754c\u5747\u5300\u7f51\u683c\u7684\u65b9\u6cd5\uff0c\u9664\u6b64\u4e4b\u5916\uff0c\u672c\u6587\u5229\u7528\u53d8\u6210\u5747\u5300\u7f51\u683c\u540e\uff0c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u4e0a\u8ff0\u4f18\u52bf\uff0c\u63d0\u51fa\u76f8\u5bf9\u5e94\u7684\u7684\u7269\u7406\u4fe1\u606f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002</p>"},{"location":"zh/examples/phygeonet/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u4e3a\u8282\u7ea6\u7bc7\u5e45\uff0c\u63a5\u4e0b\u6765\u5c06\u4ee5 <code>heat equation</code> \u4e3a\u4f8b\u8bb2\u89e3\u5982\u4f55\u4f7f\u7528 PaddleScience \u8fdb\u884c\u5b9e\u73b0\u3002</p>"},{"location":"zh/examples/phygeonet/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4f7f\u7528\u63d0\u51fa\u7684 USCNN \u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u6a21\u578b\u7684\u6784\u5efa\u5165\u65b9\u5f0f\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>model = ppsci.arch.USCNN(**cfg.MODEL)\n</code></pre> <p>\u5176\u4e2d\uff0c\u6784\u5efa\u6a21\u578b\u6240\u9700\u7684\u53c2\u6570\u53ef\u4ee5\u4ece\u5bf9\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\u83b7\u53d6\u3002</p> <pre><code>MODEL:\n  input_keys: [ 'coords' ]\n  output_keys: [ 'output_v' ]\n  hidden_size: [16, 32, 16]\n  h: 0.01\n  ny: 19\n  nx: 84\n  nvar_in: 2\n  nvar_out: 1\n  pad_singleside: 1\n</code></pre>"},{"location":"zh/examples/phygeonet/#32","title":"3.2 \u6570\u636e\u8bfb\u53d6","text":"<p>\u672c\u6848\u4f8b\u4f7f\u7528\u7684\u6570\u636e\u96c6\u5b58\u50a8\u5728 <code>.npz</code> \u6587\u4ef6\u4e2d\uff0c\u4f7f\u7528\u5982\u4e0b\u7684\u4ee3\u7801\u8fdb\u884c\u8bfb\u53d6\u3002</p> <pre><code>data = np.load(cfg.data_dir)\ncoords = data[\"coords\"]\njinvs = data[\"jinvs\"]\ndxdxis = data[\"dxdxis\"]\ndydxis = data[\"dydxis\"]\ndxdetas = data[\"dxdetas\"]\ndydetas = data[\"dydetas\"]\n</code></pre>"},{"location":"zh/examples/phygeonet/#33","title":"3.3 \u8f93\u51fa\u8f6c\u5316\u51fd\u6570\u6784\u5efa","text":"<p>\u672c\u6587\u4e3a\u5f3a\u5236\u8fb9\u754c\u7ea6\u675f\uff0c\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u76f8\u5bf9\u5e94\u7684\u8f93\u51fa\u8f6c\u5316\u51fd\u6570\u5bf9\u6a21\u578b\u7684\u8f93\u51fa\u7ed3\u679c\u8ba1\u7b97\u5fae\u5206\u3002</p> <pre><code>sup_constraint = {sup_constraint_res.name: sup_constraint_res}\n\ndef _transform_out(\n    _input: Dict[str, paddle.Tensor],\n    _output: Dict[str, paddle.Tensor],\n    pad_singleside: int = cfg.MODEL.pad_singleside,\n):\n    \"\"\"Calculation residual.\n\n    Args:\n        _input (Dict[str, paddle.Tensor]): The input of the model.\n        _output (Dict[str, paddle.Tensor]): The output of the model.\n        pad_singleside (int, optional): Pad size. Defaults to cfg.MODEL.pad_singleside.\n    \"\"\"\n    output_v = _output[\"output_v\"]\n    jinv = _input[\"jinvs\"]\n    dxdxi = _input[\"dxdxis\"]\n    dydxi = _input[\"dydxis\"]\n    dxdeta = _input[\"dxdetas\"]\n    dydeta = _input[\"dydetas\"]\n    output_v[:, 0, -pad_singleside:, pad_singleside:-pad_singleside] = 0\n    output_v[:, 0, :pad_singleside, pad_singleside:-pad_singleside] = 1\n    output_v[:, 0, pad_singleside:-pad_singleside, -pad_singleside:] = 1\n    output_v[:, 0, pad_singleside:-pad_singleside, 0:pad_singleside] = 1\n    output_v[:, 0, 0, 0] = 0.5 * (output_v[:, 0, 0, 1] + output_v[:, 0, 1, 0])\n    output_v[:, 0, 0, -1] = 0.5 * (output_v[:, 0, 0, -2] + output_v[:, 0, 1, -1])\n    dvdx = utils.dfdx(output_v, dydeta, dydxi, jinv)\n    d2vdx2 = utils.dfdx(dvdx, dydeta, dydxi, jinv)\n    dvdy = utils.dfdy(output_v, dxdxi, dxdeta, jinv)\n    d2vdy2 = utils.dfdy(dvdy, dxdxi, dxdeta, jinv)\n</code></pre>"},{"location":"zh/examples/phygeonet/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u6784\u5efa\u76f8\u5bf9\u5e94\u7ea6\u675f\u6761\u4ef6\uff0c\u7531\u4e8e\u8fb9\u754c\u7ea6\u675f\u4e3a\u5f3a\u5236\u7ea6\u675f\uff0c\u7ea6\u675f\u6761\u4ef6\u4e3b\u8981\u4e3a\u6b8b\u5dee\u7ea6\u675f\u3002</p> <pre><code>sup_constraint_res = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\n                \"coords\": coords,\n                \"jinvs\": jinvs,\n                \"dxdxis\": dxdxis,\n                \"dydxis\": dydxis,\n                \"dxdetas\": dxdetas,\n                \"dydetas\": dydetas,\n            },\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"iters_per_epoch\": iters_per_epoch,\n        \"num_workers\": 0,\n    },\n    ppsci.loss.FunctionalLoss(\n        lambda out, label, weight: {\"residual\": out[\"residual\"]}\n    ),\n    name=\"residual\",\n</code></pre>"},{"location":"zh/examples/phygeonet/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u4e0e\u8bba\u6587\u4e2d\u63cf\u8ff0\u76f8\u540c\uff0c\u6211\u4eec\u4f7f\u7528\u6052\u5b9a\u5b66\u4e60\u7387 0.001 \u6784\u9020 Adam \u4f18\u5316\u5668\u3002</p> <pre><code>optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>"},{"location":"zh/examples/phygeonet/#36","title":"3.6 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\u3002</p> <pre><code>model.register_output_transform(_transform_out)\nsolver = ppsci.solver.Solver(\n    model,\n    sup_constraint,\n    cfg.output_dir,\n    optimizer,\n    epochs=cfg.epochs,\n</code></pre> <p>\u6700\u540e\u542f\u52a8\u8bad\u7ec3\u5373\u53ef\uff1a</p> <pre><code>iters_per_epoch=iters_per_epoch,\n</code></pre>"},{"location":"zh/examples/phygeonet/#37","title":"3.7 \u6a21\u578b\u8bc4\u4f30","text":"<p>\u5728\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\u4e4b\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 evaluate() \u51fd\u6570\u5bf9\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u53ef\u89c6\u5316\u3002</p> <pre><code>def evaluate(cfg: DictConfig):\n    data = np.load(cfg.data_dir)\n    coords = data[\"coords\"]\n\n    ofv_sb = paddle.to_tensor(data[\"OFV_sb\"])\n\n    ## create model\n    pad_singleside = cfg.MODEL.pad_singleside\n    model = ppsci.arch.USCNN(**cfg.MODEL)\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,  ### the path of the model\n    )\n    output_v = solver.predict({\"coords\": paddle.to_tensor(coords)})\n    output_v = output_v[\"output_v\"]\n\n    output_v[0, 0, -pad_singleside:, pad_singleside:-pad_singleside] = 0\n    output_v[0, 0, :pad_singleside, pad_singleside:-pad_singleside] = 1\n    output_v[0, 0, pad_singleside:-pad_singleside, -pad_singleside:] = 1\n    output_v[0, 0, pad_singleside:-pad_singleside, 0:pad_singleside] = 1\n    output_v[0, 0, 0, 0] = 0.5 * (output_v[0, 0, 0, 1] + output_v[0, 0, 1, 0])\n    output_v[0, 0, 0, -1] = 0.5 * (output_v[0, 0, 0, -2] + output_v[0, 0, 1, -1])\n\n    ev = paddle.sqrt(\n        paddle.mean((ofv_sb - output_v[0, 0]) ** 2) / paddle.mean(ofv_sb**2)\n    ).item()\n    logger.info(f\"ev: {ev}\")\n\n    output_v = output_v.numpy()\n    ofv_sb = ofv_sb.numpy()\n    fig = plt.figure()\n    ax = plt.subplot(1, 2, 1)\n    utils.visualize(\n        ax,\n        coords[0, 0, 1:-1, 1:-1],\n        coords[0, 1, 1:-1, 1:-1],\n        output_v[0, 0, 1:-1, 1:-1],\n        \"horizontal\",\n        [0, 1],\n    )\n    utils.set_axis_label(ax, \"p\")\n    ax.set_title(\"CNN \" + r\"$T$\")\n    ax.set_aspect(\"equal\")\n    ax = plt.subplot(1, 2, 2)\n    utils.visualize(\n        ax,\n        coords[0, 0, 1:-1, 1:-1],\n        coords[0, 1, 1:-1, 1:-1],\n        ofv_sb[1:-1, 1:-1],\n        \"horizontal\",\n        [0, 1],\n    )\n    utils.set_axis_label(ax, \"p\")\n    ax.set_aspect(\"equal\")\n    ax.set_title(\"FV \" + r\"$T$\")\n    fig.tight_layout(pad=1)\n</code></pre>"},{"location":"zh/examples/phygeonet/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"heat_equation.py<pre><code>from typing import Dict\n\nimport hydra\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport paddle\nimport utils\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    data = np.load(cfg.data_dir)\n    coords = data[\"coords\"]\n    jinvs = data[\"jinvs\"]\n    dxdxis = data[\"dxdxis\"]\n    dydxis = data[\"dydxis\"]\n    dxdetas = data[\"dxdetas\"]\n    dydetas = data[\"dydetas\"]\n\n    model = ppsci.arch.USCNN(**cfg.MODEL)\n\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n\n    iters_per_epoch = coords.shape[0]\n    sup_constraint_res = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": {\n                    \"coords\": coords,\n                    \"jinvs\": jinvs,\n                    \"dxdxis\": dxdxis,\n                    \"dydxis\": dydxis,\n                    \"dxdetas\": dxdetas,\n                    \"dydetas\": dydetas,\n                },\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"iters_per_epoch\": iters_per_epoch,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(\n            lambda out, label, weight: {\"residual\": out[\"residual\"]}\n        ),\n        name=\"residual\",\n    )\n    sup_constraint = {sup_constraint_res.name: sup_constraint_res}\n\n    def _transform_out(\n        _input: Dict[str, paddle.Tensor],\n        _output: Dict[str, paddle.Tensor],\n        pad_singleside: int = cfg.MODEL.pad_singleside,\n    ):\n        \"\"\"Calculation residual.\n\n        Args:\n            _input (Dict[str, paddle.Tensor]): The input of the model.\n            _output (Dict[str, paddle.Tensor]): The output of the model.\n            pad_singleside (int, optional): Pad size. Defaults to cfg.MODEL.pad_singleside.\n        \"\"\"\n        output_v = _output[\"output_v\"]\n        jinv = _input[\"jinvs\"]\n        dxdxi = _input[\"dxdxis\"]\n        dydxi = _input[\"dydxis\"]\n        dxdeta = _input[\"dxdetas\"]\n        dydeta = _input[\"dydetas\"]\n        output_v[:, 0, -pad_singleside:, pad_singleside:-pad_singleside] = 0\n        output_v[:, 0, :pad_singleside, pad_singleside:-pad_singleside] = 1\n        output_v[:, 0, pad_singleside:-pad_singleside, -pad_singleside:] = 1\n        output_v[:, 0, pad_singleside:-pad_singleside, 0:pad_singleside] = 1\n        output_v[:, 0, 0, 0] = 0.5 * (output_v[:, 0, 0, 1] + output_v[:, 0, 1, 0])\n        output_v[:, 0, 0, -1] = 0.5 * (output_v[:, 0, 0, -2] + output_v[:, 0, 1, -1])\n        dvdx = utils.dfdx(output_v, dydeta, dydxi, jinv)\n        d2vdx2 = utils.dfdx(dvdx, dydeta, dydxi, jinv)\n        dvdy = utils.dfdy(output_v, dxdxi, dxdeta, jinv)\n        d2vdy2 = utils.dfdy(dvdy, dxdxi, dxdeta, jinv)\n        continuity = d2vdy2 + d2vdx2\n        return {\"residual\": paddle.mean(continuity**2)}\n\n    model.register_output_transform(_transform_out)\n    solver = ppsci.solver.Solver(\n        model,\n        sup_constraint,\n        cfg.output_dir,\n        optimizer,\n        epochs=cfg.epochs,\n        iters_per_epoch=iters_per_epoch,\n    )\n    solver.train()\n    solver.plot_loss_history()\n\n\ndef evaluate(cfg: DictConfig):\n    data = np.load(cfg.data_dir)\n    coords = data[\"coords\"]\n\n    ofv_sb = paddle.to_tensor(data[\"OFV_sb\"])\n\n    ## create model\n    pad_singleside = cfg.MODEL.pad_singleside\n    model = ppsci.arch.USCNN(**cfg.MODEL)\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,  ### the path of the model\n    )\n    output_v = solver.predict({\"coords\": paddle.to_tensor(coords)})\n    output_v = output_v[\"output_v\"]\n\n    output_v[0, 0, -pad_singleside:, pad_singleside:-pad_singleside] = 0\n    output_v[0, 0, :pad_singleside, pad_singleside:-pad_singleside] = 1\n    output_v[0, 0, pad_singleside:-pad_singleside, -pad_singleside:] = 1\n    output_v[0, 0, pad_singleside:-pad_singleside, 0:pad_singleside] = 1\n    output_v[0, 0, 0, 0] = 0.5 * (output_v[0, 0, 0, 1] + output_v[0, 0, 1, 0])\n    output_v[0, 0, 0, -1] = 0.5 * (output_v[0, 0, 0, -2] + output_v[0, 0, 1, -1])\n\n    ev = paddle.sqrt(\n        paddle.mean((ofv_sb - output_v[0, 0]) ** 2) / paddle.mean(ofv_sb**2)\n    ).item()\n    logger.info(f\"ev: {ev}\")\n\n    output_v = output_v.numpy()\n    ofv_sb = ofv_sb.numpy()\n    fig = plt.figure()\n    ax = plt.subplot(1, 2, 1)\n    utils.visualize(\n        ax,\n        coords[0, 0, 1:-1, 1:-1],\n        coords[0, 1, 1:-1, 1:-1],\n        output_v[0, 0, 1:-1, 1:-1],\n        \"horizontal\",\n        [0, 1],\n    )\n    utils.set_axis_label(ax, \"p\")\n    ax.set_title(\"CNN \" + r\"$T$\")\n    ax.set_aspect(\"equal\")\n    ax = plt.subplot(1, 2, 2)\n    utils.visualize(\n        ax,\n        coords[0, 0, 1:-1, 1:-1],\n        coords[0, 1, 1:-1, 1:-1],\n        ofv_sb[1:-1, 1:-1],\n        \"horizontal\",\n        [0, 1],\n    )\n    utils.set_axis_label(ax, \"p\")\n    ax.set_aspect(\"equal\")\n    ax.set_title(\"FV \" + r\"$T$\")\n    fig.tight_layout(pad=1)\n    fig.savefig(f\"{cfg.output_dir}/result.png\", bbox_inches=\"tight\")\n    plt.close(fig)\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"heat_equation.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/phygeonet/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>Heat equation\u7ed3\u679c\u5c55\u793a: </p> <p>Heat equation with boundary \u7ed3\u679c\u5c55\u793a\uff1a</p> <p>T=0 </p> <p>T=3 </p> <p>T=6 </p>"},{"location":"zh/examples/phygeonet/#6","title":"6. \u603b\u7ed3","text":"<p>\u672c\u6587\u901a\u8fc7\u4f7f\u7528\u8c03\u548c\u6620\u5c04\u6784\u9020\u5750\u6807\u53d8\u6362\u51fd\u6570\uff0c\u4f7f\u5f97\u7269\u7406\u4fe1\u606f\u7f51\u7edc\u53ef\u4ee5\u5728\u4e0d\u89c4\u5219\u975e\u5747\u5300\u7f51\u683c\u4e0a\u9762\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\uff0c\u56e0\u4e3a\u8be5\u6620\u5c04\u4e3a\u4f7f\u7528\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\uff0c\u6240\u4ee5\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5728\u7f51\u7edc\u524d\u540e\u5d4c\u5165\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7f51\u7edc\u53ef\u4ee5\u5728\u5404\u79cd\u4e0d\u89c4\u5219\u7f51\u683c\u95ee\u9898\u4e0a\u8868\u73b0\u6bd4SOAT\u7f51\u7edc\u7a81\u51fa\u3002</p>"},{"location":"zh/examples/phygeonet/#7","title":"7. \u53c2\u8003\u8d44\u6599","text":"<p>PhyGeoNet: Physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state PDEs on irregular domain</p> <p>Github PhyGeoNet</p>"},{"location":"zh/examples/phylstm/","title":"Phy-LSTM","text":""},{"location":"zh/examples/phylstm/#phylstm","title":"PhyLSTM","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 phylstm2phylstm3 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyLSTM/data_boucwen.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyLSTM/data_boucwen.mat -o data_boucwen.mat\npython phylstm2.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyLSTM/data_boucwen.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyLSTM/data_boucwen.mat -o data_boucwen.mat\npython phylstm3.py\n</code></pre> phylstm2phylstm3 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyLSTM/data_boucwen.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyLSTM/data_boucwen.mat -o data_boucwen.mat\npython phylstm2.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/phylstm/phylstm2_pretrained.pdparams\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyLSTM/data_boucwen.mat\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyLSTM/data_boucwen.mat -o data_boucwen.mat\npython phylstm3.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/phylstm/phylstm3_pretrained.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 phylstm2_pretrained.pdparams loss(sup_valid): 0.00799 phylstm3_pretrained.pdparams loss(sup_valid): 0.03098"},{"location":"zh/examples/phylstm/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u7269\u7406\u77e5\u8bc6LSTM\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u7f3a\u4e4f\u6570\u636e\u7684\u975e\u7ebf\u6027\u7ed3\u6784\u7cfb\u7edf\u8fdb\u884c\u5143\u5efa\u6a21\u3002\u57fa\u672c\u6982\u5ff5\u662f\u5c06\u53ef\u7528\u4f46\u5c1a\u4e0d\u5b8c\u6574\u7684\u7269\u7406\u77e5\u8bc6\uff08\u5982\u7269\u7406\u5b9a\u5f8b\u3001\u79d1\u5b66\u539f\u7406\uff09\u6574\u5408\u5230\u6df1\u5ea6\u957f\u77ed\u65f6\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\u4e2d\uff0c\u8be5\u7f51\u7edc\u5728\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u5185\u9650\u5236\u548c\u4fc3\u8fdb\u5b66\u4e60\u3002\u7269\u7406\u7ea6\u675f\u5d4c\u5165\u5728\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u4ee5\u5f3a\u5236\u6267\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u5373\u4f7f\u5728\u53ef\u7528\u8bad\u7ec3\u6570\u636e\u96c6\u975e\u5e38\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u51c6\u786e\u5730\u6355\u6349\u6f5c\u5728\u7684\u7cfb\u7edf\u975e\u7ebf\u6027\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u52a8\u6001\u7ed3\u6784\uff0c\u8003\u8651\u8fd0\u52a8\u65b9\u7a0b\u7684\u7269\u7406\u5b9a\u5f8b\u3001\u72b6\u6001\u4f9d\u8d56\u6027\u548c\u6ede\u540e\u672c\u6784\u5173\u7cfb\u6765\u6784\u5efa\u7269\u7406\u635f\u5931\u3002\u5d4c\u5165\u5f0f\u7269\u7406\u53ef\u4ee5\u7f13\u89e3\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u5927\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u9700\u6c42\uff0c\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4f7f\u5176\u5177\u6709\u5916\u63a8\u80fd\u529b\uff0c\u4ece\u800c\u8fdb\u884c\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u7269\u7406\u77e5\u8bc6\u6307\u5bfc\u7684\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\u4f18\u4e8e\u4f20\u7edf\u7684\u975e\u7269\u7406\u6307\u5bfc\u7684\u6570\u636e\u9a71\u52a8\u795e\u7ecf\u7f51\u7edc\u3002</p>"},{"location":"zh/examples/phylstm/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u7ed3\u6784\u7cfb\u7edf\u7684\u5143\u5efa\u6a21\u65e8\u5728\u5f00\u53d1\u4f4e\u4fdd\u771f\u5ea6\uff08\u6216\u4f4e\u9636\uff09\u6a21\u578b\uff0c\u4ee5\u6709\u6548\u5730\u6355\u6349\u6f5c\u5728\u7684\u975e\u7ebf\u6027\u8f93\u5165-\u8f93\u51fa\u884c\u4e3a\u3002\u5143\u6a21\u578b\u53ef\u4ee5\u5728\u4ece\u9ad8\u4fdd\u771f\u5ea6\u6a21\u62df\u6216\u5b9e\u9645\u7cfb\u7edf\u611f\u77e5\u83b7\u5f97\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u8bf4\u660e\uff0c\u6211\u4eec\u8003\u8651\u4e00\u4e2a\u5efa\u7b51\u7c7b\u578b\u7ed3\u6784\u5e76\u5047\u8bbe\u5730\u9707\u52a8\u529b\u5b66\u7531\u4f4e\u4fdd\u771f\u5ea6\u975e\u7ebf\u6027\u8fd0\u52a8\u65b9\u7a0b\uff08EOM\uff09\u652f\u914d\uff1a</p> \\[ \\mathbf{M} \\ddot{\\mathbf{u}}+\\underbrace{\\mathbf{C} \\dot{\\mathbf{u}}+\\lambda \\mathbf{K u}+(1-\\lambda) \\mathbf{K r}}_{\\mathbf{h}}=-\\mathbf{M} \\Gamma a_g \\] <p>\u5176\u4e2dM\u662f\u8d28\u91cf\u77e9\u9635\uff1bC\u4e3a\u963b\u5c3c\u77e9\u9635\uff1bK\u4e3a\u521a\u5ea6\u77e9\u9635\u3002</p> <p>\u63a7\u5236\u65b9\u7a0b\u53ef\u4ee5\u6539\u5199\u6210\u4e00\u4e2a\u66f4\u4e00\u822c\u7684\u5f62\u5f0f\uff1a</p> \\[ \\ddot{\\mathbf{u}}+\\mathrm{g}=-\\Gamma a_g \\]"},{"location":"zh/examples/phylstm/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/phylstm/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 PhyLSTM \u95ee\u9898\u4e2d\uff0c\u5efa\u7acb LSTM \u7f51\u7edc Deep LSTM network\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code>model = ppsci.arch.DeepPhyLSTM(\n    cfg.MODEL.input_size,\n    eta.shape[2],\n    cfg.MODEL.hidden_size,\n    cfg.MODEL.model_type,\n)\n</code></pre> <p>DeepPhyLSTM \u53c2\u6570 input_size \u662f\u8f93\u5165\u5927\u5c0f\uff0coutput_size \u662f\u8f93\u51fa\u5927\u5c0f\uff0chidden_size \u662f\u9690\u85cf\u5c42\u5927\u5c0f\uff0cmodel_type\u662f\u6a21\u578b\u7c7b\u578b\u3002</p>"},{"location":"zh/examples/phylstm/#32","title":"3.2 \u6570\u636e\u6784\u5efa","text":"<p>\u8fd0\u884c\u672c\u95ee\u9898\u4ee3\u7801\u524d\u8bf7\u6309\u7167\u4e0b\u65b9\u547d\u4ee4\u4e0b\u8f7d data_boucwen.mat</p> <pre><code>wget -nc -P ./ https://paddle-org.bj.bcebos.com/paddlescience/datasets/PhyLSTM/data_boucwen.mat\n</code></pre> <p>\u672c\u6848\u4f8b\u6d89\u53ca\u8bfb\u53d6\u6570\u636e\u6784\u5efa\uff0c\u5982\u4e0b\u6240\u793a</p> <pre><code>mat = scipy.io.loadmat(cfg.DATA_FILE_PATH)\nag_data = mat[\"input_tf\"]  # ag, ad, av\nu_data = mat[\"target_X_tf\"]\nut_data = mat[\"target_Xd_tf\"]\nutt_data = mat[\"target_Xdd_tf\"]\nag_data = ag_data.reshape([ag_data.shape[0], ag_data.shape[1], 1])\nu_data = u_data.reshape([u_data.shape[0], u_data.shape[1], 1])\nut_data = ut_data.reshape([ut_data.shape[0], ut_data.shape[1], 1])\nutt_data = utt_data.reshape([utt_data.shape[0], utt_data.shape[1], 1])\n\nt = mat[\"time\"]\ndt = t[0, 1] - t[0, 0]\n\nag_all = ag_data\nu_all = u_data\nu_t_all = ut_data\nu_tt_all = utt_data\n\n# finite difference\nN = u_data.shape[1]\nphi1 = np.concatenate(\n    [\n        np.array([-3 / 2, 2, -1 / 2]),\n        np.zeros([N - 3]),\n    ]\n)\ntemp1 = np.concatenate([-1 / 2 * np.identity(N - 2), np.zeros([N - 2, 2])], axis=1)\ntemp2 = np.concatenate([np.zeros([N - 2, 2]), 1 / 2 * np.identity(N - 2)], axis=1)\nphi2 = temp1 + temp2\nphi3 = np.concatenate(\n    [\n        np.zeros([N - 3]),\n        np.array([1 / 2, -2, 3 / 2]),\n    ]\n)\nphi_t0 = (\n    1\n    / dt\n    * np.concatenate(\n        [\n            np.reshape(phi1, [1, phi1.shape[0]]),\n            phi2,\n            np.reshape(phi3, [1, phi3.shape[0]]),\n        ],\n        axis=0,\n    )\n)\nphi_t0 = np.reshape(phi_t0, [1, N, N])\n\nag_star = ag_all[0:10]\neta_star = u_all[0:10]\neta_t_star = u_t_all[0:10]\neta_tt_star = u_tt_all[0:10]\nag_c_star = ag_all[0:50]\nlift_star = -ag_c_star\n\neta = eta_star\nag = ag_star\nlift = lift_star\neta_t = eta_t_star\neta_tt = eta_tt_star\nag_c = ag_c_star\ng = -eta_tt - ag\nphi_t = np.repeat(phi_t0, ag_c_star.shape[0], axis=0)\n</code></pre>"},{"location":"zh/examples/phylstm/#33","title":"3.3 \u7ea6\u675f\u6784\u5efa","text":"<p>\u8bbe\u7f6e\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u635f\u5931\u8ba1\u7b97\u51fd\u6570\uff0c\u8fd4\u56de\u5b57\u6bb5\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>sup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict_train,\n            \"label\": label_dict_train,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": 1,\n        \"num_workers\": 0,\n    },\n    ppsci.loss.FunctionalLoss(functions.train_loss_func2),\n    {\n        \"eta_pred\": lambda out: out[\"eta_pred\"],\n        \"eta_dot_pred\": lambda out: out[\"eta_dot_pred\"],\n        \"g_pred\": lambda out: out[\"g_pred\"],\n        \"eta_t_pred_c\": lambda out: out[\"eta_t_pred_c\"],\n        \"eta_dot_pred_c\": lambda out: out[\"eta_dot_pred_c\"],\n        \"lift_pred_c\": lambda out: out[\"lift_pred_c\"],\n    },\n    name=\"sup_train\",\n)\nconstraint_pde = {sup_constraint_pde.name: sup_constraint_pde}\n</code></pre>"},{"location":"zh/examples/phylstm/#34","title":"3.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u8bbe\u7f6e\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u635f\u5931\u8ba1\u7b97\u51fd\u6570\uff0c\u8fd4\u56de\u5b57\u6bb5\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>sup_validator_pde = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": input_dict_val,\n            \"label\": label_dict_val,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": 1,\n        \"num_workers\": 0,\n    },\n    ppsci.loss.FunctionalLoss(functions.train_loss_func2),\n    {\n        \"eta_pred\": lambda out: out[\"eta_pred\"],\n        \"eta_dot_pred\": lambda out: out[\"eta_dot_pred\"],\n        \"g_pred\": lambda out: out[\"g_pred\"],\n        \"eta_t_pred_c\": lambda out: out[\"eta_t_pred_c\"],\n        \"eta_dot_pred_c\": lambda out: out[\"eta_dot_pred_c\"],\n        \"lift_pred_c\": lambda out: out[\"lift_pred_c\"],\n    },\n    metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n    name=\"sup_valid\",\n)\nvalidator_pde = {sup_validator_pde.name: sup_validator_pde}\n</code></pre>"},{"location":"zh/examples/phylstm/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 100 \u8f6e\u8bad\u7ec3\u8f6e\u6570\u3002</p> <pre><code>epochs: 100\n</code></pre>"},{"location":"zh/examples/phylstm/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9 <code>Adam</code> \u4f18\u5316\u5668\u5e76\u8bbe\u5b9a <code>learning_rate</code> \u4e3a 1e-3\u3002</p> <pre><code>optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n</code></pre>"},{"location":"zh/examples/phylstm/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\u3002</p> <pre><code>solver = ppsci.solver.Solver(\n    model,\n    constraint_pde,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    log_freq=cfg.log_freq,\n    seed=cfg.seed,\n    validator=validator_pde,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n</code></pre> <p>\u6700\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u5373\u53ef\uff1a</p> <pre><code># train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/phylstm/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"phylstm2phylstm3 phylstm2.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nReference: https://github.com/zhry10/PhyLSTM.git\n\"\"\"\n\nfrom os import path as osp\n\nimport functions\nimport hydra\nimport numpy as np\nimport scipy.io\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    mat = scipy.io.loadmat(cfg.DATA_FILE_PATH)\n    ag_data = mat[\"input_tf\"]  # ag, ad, av\n    u_data = mat[\"target_X_tf\"]\n    ut_data = mat[\"target_Xd_tf\"]\n    utt_data = mat[\"target_Xdd_tf\"]\n    ag_data = ag_data.reshape([ag_data.shape[0], ag_data.shape[1], 1])\n    u_data = u_data.reshape([u_data.shape[0], u_data.shape[1], 1])\n    ut_data = ut_data.reshape([ut_data.shape[0], ut_data.shape[1], 1])\n    utt_data = utt_data.reshape([utt_data.shape[0], utt_data.shape[1], 1])\n\n    t = mat[\"time\"]\n    dt = t[0, 1] - t[0, 0]\n\n    ag_all = ag_data\n    u_all = u_data\n    u_t_all = ut_data\n    u_tt_all = utt_data\n\n    # finite difference\n    N = u_data.shape[1]\n    phi1 = np.concatenate(\n        [\n            np.array([-3 / 2, 2, -1 / 2]),\n            np.zeros([N - 3]),\n        ]\n    )\n    temp1 = np.concatenate([-1 / 2 * np.identity(N - 2), np.zeros([N - 2, 2])], axis=1)\n    temp2 = np.concatenate([np.zeros([N - 2, 2]), 1 / 2 * np.identity(N - 2)], axis=1)\n    phi2 = temp1 + temp2\n    phi3 = np.concatenate(\n        [\n            np.zeros([N - 3]),\n            np.array([1 / 2, -2, 3 / 2]),\n        ]\n    )\n    phi_t0 = (\n        1\n        / dt\n        * np.concatenate(\n            [\n                np.reshape(phi1, [1, phi1.shape[0]]),\n                phi2,\n                np.reshape(phi3, [1, phi3.shape[0]]),\n            ],\n            axis=0,\n        )\n    )\n    phi_t0 = np.reshape(phi_t0, [1, N, N])\n\n    ag_star = ag_all[0:10]\n    eta_star = u_all[0:10]\n    eta_t_star = u_t_all[0:10]\n    eta_tt_star = u_tt_all[0:10]\n    ag_c_star = ag_all[0:50]\n    lift_star = -ag_c_star\n\n    eta = eta_star\n    ag = ag_star\n    lift = lift_star\n    eta_t = eta_t_star\n    eta_tt = eta_tt_star\n    ag_c = ag_c_star\n    g = -eta_tt - ag\n    phi_t = np.repeat(phi_t0, ag_c_star.shape[0], axis=0)\n\n    model = ppsci.arch.DeepPhyLSTM(\n        cfg.MODEL.input_size,\n        eta.shape[2],\n        cfg.MODEL.hidden_size,\n        cfg.MODEL.model_type,\n    )\n    model.register_input_transform(functions.transform_in)\n    model.register_output_transform(functions.transform_out)\n\n    dataset_obj = functions.Dataset(eta, eta_t, g, ag, ag_c, lift, phi_t)\n    (\n        input_dict_train,\n        label_dict_train,\n        input_dict_val,\n        label_dict_val,\n    ) = dataset_obj.get(cfg.TRAIN.epochs)\n\n    sup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_train,\n                \"label\": label_dict_train,\n            },\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": True,\n                \"shuffle\": True,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.train_loss_func2),\n        {\n            \"eta_pred\": lambda out: out[\"eta_pred\"],\n            \"eta_dot_pred\": lambda out: out[\"eta_dot_pred\"],\n            \"g_pred\": lambda out: out[\"g_pred\"],\n            \"eta_t_pred_c\": lambda out: out[\"eta_t_pred_c\"],\n            \"eta_dot_pred_c\": lambda out: out[\"eta_dot_pred_c\"],\n            \"lift_pred_c\": lambda out: out[\"lift_pred_c\"],\n        },\n        name=\"sup_train\",\n    )\n    constraint_pde = {sup_constraint_pde.name: sup_constraint_pde}\n\n    sup_validator_pde = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_val,\n                \"label\": label_dict_val,\n            },\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.train_loss_func2),\n        {\n            \"eta_pred\": lambda out: out[\"eta_pred\"],\n            \"eta_dot_pred\": lambda out: out[\"eta_dot_pred\"],\n            \"g_pred\": lambda out: out[\"g_pred\"],\n            \"eta_t_pred_c\": lambda out: out[\"eta_t_pred_c\"],\n            \"eta_dot_pred_c\": lambda out: out[\"eta_dot_pred_c\"],\n            \"lift_pred_c\": lambda out: out[\"lift_pred_c\"],\n        },\n        metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n        name=\"sup_valid\",\n    )\n    validator_pde = {sup_validator_pde.name: sup_validator_pde}\n\n    # initialize solver\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n    solver = ppsci.solver.Solver(\n        model,\n        constraint_pde,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator_pde,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    mat = scipy.io.loadmat(cfg.DATA_FILE_PATH)\n    ag_data = mat[\"input_tf\"]  # ag, ad, av\n    u_data = mat[\"target_X_tf\"]\n    ut_data = mat[\"target_Xd_tf\"]\n    utt_data = mat[\"target_Xdd_tf\"]\n    ag_data = ag_data.reshape([ag_data.shape[0], ag_data.shape[1], 1])\n    u_data = u_data.reshape([u_data.shape[0], u_data.shape[1], 1])\n    ut_data = ut_data.reshape([ut_data.shape[0], ut_data.shape[1], 1])\n    utt_data = utt_data.reshape([utt_data.shape[0], utt_data.shape[1], 1])\n\n    t = mat[\"time\"]\n    dt = t[0, 1] - t[0, 0]\n\n    ag_all = ag_data\n    u_all = u_data\n    u_t_all = ut_data\n    u_tt_all = utt_data\n\n    # finite difference\n    N = u_data.shape[1]\n    phi1 = np.concatenate(\n        [\n            np.array([-3 / 2, 2, -1 / 2]),\n            np.zeros([N - 3]),\n        ]\n    )\n    temp1 = np.concatenate([-1 / 2 * np.identity(N - 2), np.zeros([N - 2, 2])], axis=1)\n    temp2 = np.concatenate([np.zeros([N - 2, 2]), 1 / 2 * np.identity(N - 2)], axis=1)\n    phi2 = temp1 + temp2\n    phi3 = np.concatenate(\n        [\n            np.zeros([N - 3]),\n            np.array([1 / 2, -2, 3 / 2]),\n        ]\n    )\n    phi_t0 = (\n        1\n        / dt\n        * np.concatenate(\n            [\n                np.reshape(phi1, [1, phi1.shape[0]]),\n                phi2,\n                np.reshape(phi3, [1, phi3.shape[0]]),\n            ],\n            axis=0,\n        )\n    )\n    phi_t0 = np.reshape(phi_t0, [1, N, N])\n\n    ag_star = ag_all[0:10]\n    eta_star = u_all[0:10]\n    eta_t_star = u_t_all[0:10]\n    eta_tt_star = u_tt_all[0:10]\n    ag_c_star = ag_all[0:50]\n    lift_star = -ag_c_star\n\n    eta = eta_star\n    ag = ag_star\n    lift = lift_star\n    eta_t = eta_t_star\n    eta_tt = eta_tt_star\n    ag_c = ag_c_star\n    g = -eta_tt - ag\n    phi_t = np.repeat(phi_t0, ag_c_star.shape[0], axis=0)\n\n    model = ppsci.arch.DeepPhyLSTM(\n        cfg.MODEL.input_size,\n        eta.shape[2],\n        cfg.MODEL.hidden_size,\n        cfg.MODEL.model_type,\n    )\n    model.register_input_transform(functions.transform_in)\n    model.register_output_transform(functions.transform_out)\n\n    dataset_obj = functions.Dataset(eta, eta_t, g, ag, ag_c, lift, phi_t)\n    (\n        _,\n        _,\n        input_dict_val,\n        label_dict_val,\n    ) = dataset_obj.get(1)\n\n    sup_validator_pde = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_val,\n                \"label\": label_dict_val,\n            },\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.train_loss_func2),\n        {\n            \"eta_pred\": lambda out: out[\"eta_pred\"],\n            \"eta_dot_pred\": lambda out: out[\"eta_dot_pred\"],\n            \"g_pred\": lambda out: out[\"g_pred\"],\n            \"eta_t_pred_c\": lambda out: out[\"eta_t_pred_c\"],\n            \"eta_dot_pred_c\": lambda out: out[\"eta_dot_pred_c\"],\n            \"lift_pred_c\": lambda out: out[\"lift_pred_c\"],\n        },\n        metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n        name=\"sup_valid\",\n    )\n    validator_pde = {sup_validator_pde.name: sup_validator_pde}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        validator=validator_pde,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate\n    solver.eval()\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"phylstm2.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> phylstm3.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nReference: https://github.com/zhry10/PhyLSTM.git\n\"\"\"\n\nfrom os import path as osp\n\nimport functions\nimport hydra\nimport numpy as np\nimport scipy.io\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    mat = scipy.io.loadmat(cfg.DATA_FILE_PATH)\n    t = mat[\"time\"]\n    dt = 0.02\n    n1 = int(dt / 0.005)\n    t = t[::n1]\n\n    ag_data = mat[\"input_tf\"][:, ::n1]  # ag, ad, av\n    u_data = mat[\"target_X_tf\"][:, ::n1]\n    ut_data = mat[\"target_Xd_tf\"][:, ::n1]\n    utt_data = mat[\"target_Xdd_tf\"][:, ::n1]\n    ag_data = ag_data.reshape([ag_data.shape[0], ag_data.shape[1], 1])\n    u_data = u_data.reshape([u_data.shape[0], u_data.shape[1], 1])\n    ut_data = ut_data.reshape([ut_data.shape[0], ut_data.shape[1], 1])\n    utt_data = utt_data.reshape([utt_data.shape[0], utt_data.shape[1], 1])\n\n    ag_pred = mat[\"input_pred_tf\"][:, ::n1]  # ag, ad, av\n    u_pred = mat[\"target_pred_X_tf\"][:, ::n1]\n    ut_pred = mat[\"target_pred_Xd_tf\"][:, ::n1]\n    utt_pred = mat[\"target_pred_Xdd_tf\"][:, ::n1]\n    ag_pred = ag_pred.reshape([ag_pred.shape[0], ag_pred.shape[1], 1])\n    u_pred = u_pred.reshape([u_pred.shape[0], u_pred.shape[1], 1])\n    ut_pred = ut_pred.reshape([ut_pred.shape[0], ut_pred.shape[1], 1])\n    utt_pred = utt_pred.reshape([utt_pred.shape[0], utt_pred.shape[1], 1])\n\n    N = u_data.shape[1]\n    phi1 = np.concatenate(\n        [\n            np.array([-3 / 2, 2, -1 / 2]),\n            np.zeros([N - 3]),\n        ]\n    )\n    temp1 = np.concatenate([-1 / 2 * np.identity(N - 2), np.zeros([N - 2, 2])], axis=1)\n    temp2 = np.concatenate([np.zeros([N - 2, 2]), 1 / 2 * np.identity(N - 2)], axis=1)\n    phi2 = temp1 + temp2\n    phi3 = np.concatenate(\n        [\n            np.zeros([N - 3]),\n            np.array([1 / 2, -2, 3 / 2]),\n        ]\n    )\n    phi_t0 = (\n        1\n        / dt\n        * np.concatenate(\n            [\n                np.reshape(phi1, [1, phi1.shape[0]]),\n                phi2,\n                np.reshape(phi3, [1, phi3.shape[0]]),\n            ],\n            axis=0,\n        )\n    )\n    phi_t0 = np.reshape(phi_t0, [1, N, N])\n\n    ag_star = ag_data\n    eta_star = u_data\n    eta_t_star = ut_data\n    eta_tt_star = utt_data\n    ag_c_star = np.concatenate([ag_data, ag_pred[0:53]])\n    lift_star = -ag_c_star\n\n    eta = eta_star\n    ag = ag_star\n    lift = lift_star\n    eta_t = eta_t_star\n    eta_tt = eta_tt_star\n    g = -eta_tt - ag\n    ag_c = ag_c_star\n\n    phi_t = np.repeat(phi_t0, ag_c_star.shape[0], axis=0)\n\n    model = ppsci.arch.DeepPhyLSTM(\n        cfg.MODEL.input_size,\n        eta.shape[2],\n        cfg.MODEL.hidden_size,\n        cfg.MODEL.model_type,\n    )\n    model.register_input_transform(functions.transform_in)\n    model.register_output_transform(functions.transform_out)\n\n    dataset_obj = functions.Dataset(eta, eta_t, g, ag, ag_c, lift, phi_t)\n    (\n        input_dict_train,\n        label_dict_train,\n        input_dict_val,\n        label_dict_val,\n    ) = dataset_obj.get(cfg.TRAIN.epochs)\n\n    sup_constraint_pde = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_train,\n                \"label\": label_dict_train,\n            },\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.train_loss_func3),\n        {\n            \"eta_pred\": lambda out: out[\"eta_pred\"],\n            \"eta_dot_pred\": lambda out: out[\"eta_dot_pred\"],\n            \"g_pred\": lambda out: out[\"g_pred\"],\n            \"eta_t_pred_c\": lambda out: out[\"eta_t_pred_c\"],\n            \"eta_dot_pred_c\": lambda out: out[\"eta_dot_pred_c\"],\n            \"lift_pred_c\": lambda out: out[\"lift_pred_c\"],\n            \"g_t_pred_c\": lambda out: out[\"g_t_pred_c\"],\n            \"g_dot_pred_c\": lambda out: out[\"g_dot_pred_c\"],\n        },\n        name=\"sup_train\",\n    )\n    constraint_pde = {sup_constraint_pde.name: sup_constraint_pde}\n\n    sup_validator_pde = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_val,\n                \"label\": label_dict_val,\n            },\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.train_loss_func3),\n        {\n            \"eta_pred\": lambda out: out[\"eta_pred\"],\n            \"eta_dot_pred\": lambda out: out[\"eta_dot_pred\"],\n            \"g_pred\": lambda out: out[\"g_pred\"],\n            \"eta_t_pred_c\": lambda out: out[\"eta_t_pred_c\"],\n            \"eta_dot_pred_c\": lambda out: out[\"eta_dot_pred_c\"],\n            \"lift_pred_c\": lambda out: out[\"lift_pred_c\"],\n            \"g_t_pred_c\": lambda out: out[\"g_t_pred_c\"],\n            \"g_dot_pred_c\": lambda out: out[\"g_dot_pred_c\"],\n        },\n        metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n        name=\"sup_valid\",\n    )\n    validator_pde = {sup_validator_pde.name: sup_validator_pde}\n\n    # initialize solver\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(model)\n    solver = ppsci.solver.Solver(\n        model,\n        constraint_pde,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        validator=validator_pde,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    mat = scipy.io.loadmat(cfg.DATA_FILE_PATH)\n    t = mat[\"time\"]\n    dt = 0.02\n    n1 = int(dt / 0.005)\n    t = t[::n1]\n\n    ag_data = mat[\"input_tf\"][:, ::n1]  # ag, ad, av\n    u_data = mat[\"target_X_tf\"][:, ::n1]\n    ut_data = mat[\"target_Xd_tf\"][:, ::n1]\n    utt_data = mat[\"target_Xdd_tf\"][:, ::n1]\n    ag_data = ag_data.reshape([ag_data.shape[0], ag_data.shape[1], 1])\n    u_data = u_data.reshape([u_data.shape[0], u_data.shape[1], 1])\n    ut_data = ut_data.reshape([ut_data.shape[0], ut_data.shape[1], 1])\n    utt_data = utt_data.reshape([utt_data.shape[0], utt_data.shape[1], 1])\n\n    ag_pred = mat[\"input_pred_tf\"][:, ::n1]  # ag, ad, av\n    u_pred = mat[\"target_pred_X_tf\"][:, ::n1]\n    ut_pred = mat[\"target_pred_Xd_tf\"][:, ::n1]\n    utt_pred = mat[\"target_pred_Xdd_tf\"][:, ::n1]\n    ag_pred = ag_pred.reshape([ag_pred.shape[0], ag_pred.shape[1], 1])\n    u_pred = u_pred.reshape([u_pred.shape[0], u_pred.shape[1], 1])\n    ut_pred = ut_pred.reshape([ut_pred.shape[0], ut_pred.shape[1], 1])\n    utt_pred = utt_pred.reshape([utt_pred.shape[0], utt_pred.shape[1], 1])\n\n    N = u_data.shape[1]\n    phi1 = np.concatenate(\n        [\n            np.array([-3 / 2, 2, -1 / 2]),\n            np.zeros([N - 3]),\n        ]\n    )\n    temp1 = np.concatenate([-1 / 2 * np.identity(N - 2), np.zeros([N - 2, 2])], axis=1)\n    temp2 = np.concatenate([np.zeros([N - 2, 2]), 1 / 2 * np.identity(N - 2)], axis=1)\n    phi2 = temp1 + temp2\n    phi3 = np.concatenate(\n        [\n            np.zeros([N - 3]),\n            np.array([1 / 2, -2, 3 / 2]),\n        ]\n    )\n    phi_t0 = (\n        1\n        / dt\n        * np.concatenate(\n            [\n                np.reshape(phi1, [1, phi1.shape[0]]),\n                phi2,\n                np.reshape(phi3, [1, phi3.shape[0]]),\n            ],\n            axis=0,\n        )\n    )\n    phi_t0 = np.reshape(phi_t0, [1, N, N])\n\n    ag_star = ag_data\n    eta_star = u_data\n    eta_t_star = ut_data\n    eta_tt_star = utt_data\n    ag_c_star = np.concatenate([ag_data, ag_pred[0:53]])\n    lift_star = -ag_c_star\n\n    eta = eta_star\n    ag = ag_star\n    lift = lift_star\n    eta_t = eta_t_star\n    eta_tt = eta_tt_star\n    g = -eta_tt - ag\n    ag_c = ag_c_star\n\n    phi_t = np.repeat(phi_t0, ag_c_star.shape[0], axis=0)\n\n    model = ppsci.arch.DeepPhyLSTM(\n        cfg.MODEL.input_size,\n        eta.shape[2],\n        cfg.MODEL.hidden_size,\n        cfg.MODEL.model_type,\n    )\n    model.register_input_transform(functions.transform_in)\n    model.register_output_transform(functions.transform_out)\n\n    dataset_obj = functions.Dataset(eta, eta_t, g, ag, ag_c, lift, phi_t)\n    (\n        _,\n        _,\n        input_dict_val,\n        label_dict_val,\n    ) = dataset_obj.get(1)\n\n    sup_validator_pde = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": input_dict_val,\n                \"label\": label_dict_val,\n            },\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n            \"batch_size\": 1,\n            \"num_workers\": 0,\n        },\n        ppsci.loss.FunctionalLoss(functions.train_loss_func3),\n        {\n            \"eta_pred\": lambda out: out[\"eta_pred\"],\n            \"eta_dot_pred\": lambda out: out[\"eta_dot_pred\"],\n            \"g_pred\": lambda out: out[\"g_pred\"],\n            \"eta_t_pred_c\": lambda out: out[\"eta_t_pred_c\"],\n            \"eta_dot_pred_c\": lambda out: out[\"eta_dot_pred_c\"],\n            \"lift_pred_c\": lambda out: out[\"lift_pred_c\"],\n            \"g_t_pred_c\": lambda out: out[\"g_t_pred_c\"],\n            \"g_dot_pred_c\": lambda out: out[\"g_dot_pred_c\"],\n        },\n        metric={\"metric\": ppsci.metric.FunctionalMetric(functions.metric_expr)},\n        name=\"sup_valid\",\n    )\n    validator_pde = {sup_validator_pde.name: sup_validator_pde}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        validator=validator_pde,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n\n    # evaluate\n    solver.eval()\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"phylstm3.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/phylstm/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>PhyLSTM2 \u6848\u4f8b\u9488\u5bf9 epoch=100 \u548c learning_rate=1e-3 \u7684\u53c2\u6570\u914d\u7f6e\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8fd4\u56deLoss\u4e3a 0.00799\u3002</p> <p>PhyLSTM3 \u6848\u4f8b\u9488\u5bf9 epoch=200 \u548c learning_rate=1e-3 \u7684\u53c2\u6570\u914d\u7f6e\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8fd4\u56deLoss\u4e3a 0.03098\u3002</p>"},{"location":"zh/examples/phylstm/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>Physics-informed multi-LSTM networks for metamodeling of nonlinear structures</li> <li>https://github.com/zhry10/PhyLSTM.git</li> </ul>"},{"location":"zh/examples/pirbn/","title":"PIRBN","text":""},{"location":"zh/examples/pirbn/#pirbn","title":"PIRBN","text":"\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u547d\u4ee4 <pre><code>cd PaddleScience/jointContribution/PIRBN\npython main.py\n</code></pre>"},{"location":"zh/examples/pirbn/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6211\u4eec\u6700\u8fd1\u53d1\u73b0\u7ecf\u8fc7\u8bad\u7ec3\uff0c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u5f80\u5f80\u4f1a\u6210\u4e3a\u5c40\u90e8\u8fd1\u4f3c\u51fd\u6570\u3002\u8fd9\u4e00\u89c2\u5bdf\u7ed3\u679c\u4fc3\u4f7f\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u7269\u7406-\u4fe1\u606f\u5f84\u5411\u57fa\u7f51\u7edc\uff08PIRBN\uff09\uff0c\u8be5\u7f51\u7edc\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u90fd\u80fd\u591f\u7ef4\u6301\u5c40\u90e8\u8fd1\u4f3c\u6027\u8d28\u3002\u4e0e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0d\u540c\uff0cPIRBN \u4ec5\u5305\u542b\u4e00\u4e2a\u9690\u85cf\u5c42\u548c\u4e00\u4e2a\u5f84\u5411\u57fa\u201c\u6fc0\u6d3b\u201d\u51fd\u6570\u3002\u5728\u9002\u5f53\u7684\u6761\u4ef6\u4e0b\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u8bad\u7ec3 PIRBN \u53ef\u4ee5\u6536\u655b\u5230\u9ad8\u65af\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u901a\u8fc7\u795e\u7ecf\u90bb\u8fd1\u6838\uff08NTK\uff09\u7406\u8bba\u7814\u7a76\u4e86 PIRBN \u7684\u8bad\u7ec3\u52a8\u6001\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5bf9 PIRBN \u7684\u521d\u59cb\u5316\u7b56\u7565\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\u3002\u57fa\u4e8e\u6570\u503c\u793a\u4f8b\uff0c\u6211\u4eec\u53d1\u73b0 PIRBN \u5728\u89e3\u51b3\u5177\u6709\u9ad8\u9891\u7279\u5f81\u548c\u75c5\u6001\u8ba1\u7b97\u57df\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u65b9\u9762\u6bd4PINN\u66f4\u6709\u6548\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684 PINN \u6570\u503c\u6280\u672f\uff0c\u5982\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5206\u89e3\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u635f\u5931\u51fd\u6570\uff0c\u4e5f\u9002\u7528\u4e8e PIRBN\u3002</p> <p> </p> \u7f51\u7edc\u7684\u7ed3\u6784 <p>\u56fe\u7247\u5de6\u4fa7\u4e3a\u5e38\u89c1\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u7684\u8f93\u5165\u5c42\uff0c\u9690\u85cf\u5c42\uff0c\u8f93\u51fa\u5c42\uff0c\u9690\u85cf\u5c42\u5305\u542b\u6fc0\u6d3b\u5c42\uff0ca \u4e2d\u4e3a\u5355\u5c42\u9690\u85cf\u5c42\uff0cb \u4e2d\u4e3a\u591a\u5c42\u9690\u85cf\u5c42\uff0c\u56fe\u7247\u53f3\u4fa7\u4e3a PIRBN \u7f51\u7edc\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u8ba1\u7b97\u7f51\u7edc\u7684\u635f\u5931 Loss \u5e76\u53cd\u5411\u4f20\u9012\u3002\u56fe\u7247\u8bf4\u660e\u5f53\u4f7f\u7528 PIRBN \u65f6\uff0c\u6bcf\u4e2a RBF \u795e\u7ecf\u5143\u4ec5\u5728\u8f93\u5165\u63a5\u8fd1\u795e\u7ecf\u5143\u4e2d\u5fc3\u65f6\u88ab\u6fc0\u6d3b\u3002\u76f4\u89c2\u5730\u8bf4\uff0cPIRBN \u5177\u6709\u5c40\u90e8\u903c\u8fd1\u7279\u6027\u3002\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u8bad\u7ec3\u4e00\u4e2a PIRBN \u4e5f\u53ef\u4ee5\u901a\u8fc7 NTK \u7406\u8bba\u8fdb\u884c\u5206\u6790\u3002</p> <p> </p> \u4e0d\u540c\u9636\u6570\u7684\u9ad8\u65af\u6fc0\u6d3b\u51fd\u6570 <p>(a) 0, 1, 2 \u9636\u9ad8\u65af\u6fc0\u6d3b\u51fd\u6570 (b) \u8bbe\u7f6e\u4e0d\u540c b \u503c (c) \u8bbe\u7f6e\u4e0d\u540c c \u503c</p> <p>\u5f53\u4f7f\u7528\u9ad8\u65af\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u65f6\uff0c\u8f93\u5165\u4e0e\u8f93\u51fa\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u53ef\u4ee5\u6570\u5b66\u4e0a\u8868\u793a\u4e3a\u9ad8\u65af\u51fd\u6570\u7684\u67d0\u79cd\u5f62\u5f0f\u3002RBF \u7f51\u7edc\u662f\u4e00\u79cd\u5e38\u7528\u4e8e\u6a21\u5f0f\u8bc6\u522b\u3001\u6570\u636e\u63d2\u503c\u548c\u51fd\u6570\u903c\u8fd1\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5176\u5173\u952e\u7279\u5f81\u662f\u4f7f\u7528\u5f84\u5411\u57fa\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u4f7f\u5f97\u7f51\u7edc\u5177\u6709\u66f4\u597d\u7684\u5168\u5c40\u903c\u8fd1\u80fd\u529b\u548c\u7075\u6d3b\u6027\u3002</p>"},{"location":"zh/examples/pirbn/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u5728 NTK \u548c\u57fa\u4e8e NTK \u7684\u9002\u5e94\u6027\u8bad\u7ec3\u65b9\u6cd5\u7684\u5e2e\u52a9\u4e0b\uff0cPINN \u5728\u5904\u7406\u5177\u6709\u9ad8\u9891\u7279\u5f81\u7684\u95ee\u9898\u65f6\u7684\u6027\u80fd\u53ef\u4ee5\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002\u4f8b\u5982\uff0c\u8003\u8651\u4e00\u4e2a\u504f\u5fae\u5206\u65b9\u7a0b\u53ca\u5176\u8fb9\u754c\u6761\u4ef6\uff1a</p> \\[ \\begin{aligned} &amp; \\frac{\\mathrm{d}^2}{\\mathrm{~d} x^2} u(x)-4 \\mu^2 \\pi^2 \\sin (2 \\mu \\pi x)=0, \\text { for } x \\in[0,1] \\\\ &amp; u(0)=u(1)=0 \\end{aligned} \\] <p>\u5176\u4e2d\\(\\mu\\)\u662f\u4e00\u4e2a\u63a7\u5236PDE\u89e3\u7684\u9891\u7387\u7279\u5f81\u7684\u5e38\u6570\u3002</p>"},{"location":"zh/examples/pirbn/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddlePaddle \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddlePaddle\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/pirbn/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 PIRBN \u95ee\u9898\u4e2d\uff0c\u5efa\u7acb\u7f51\u7edc\uff0c\u7528 PaddlePaddle \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># Set up PIRBN\nrbn = rbn_net.RBN_Net(n_in, n_out, n_neu, b, c, activation_function)\nrbn_loss = pirbn.PIRBN(rbn, activation_function)\n</code></pre>"},{"location":"zh/examples/pirbn/#32","title":"3.2 \u6570\u636e\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u6d89\u53ca\u8bfb\u53d6\u6570\u636e\u6784\u5efa\uff0c\u5982\u4e0b\u6240\u793a</p> <pre><code># Define the number of sample points\nns = 50\n\n# Define the sample points' interval\ndx = 1.0 / (ns - 1)\n\n# Initialise sample points' coordinates\nx_eq = np.linspace(0.0, 1.0, ns)[:, None]\n\nfor i in range(0, ns):\n    x_eq[i, 0] = i * dx + right_by\nx_bc = np.array([[right_by + 0.0], [right_by + 1.0]])\nx = [x_eq, x_bc]\ny = -4 * mu**2 * np.pi**2 * np.sin(2 * mu * np.pi * x_eq)\n\n# Set up radial basis network\nn_in = 1\nn_out = 1\nn_neu = 61\nb = 10.0\nc = [right_by - 0.1, right_by + 1.1]\n</code></pre>"},{"location":"zh/examples/pirbn/#33","title":"3.3 \u8bad\u7ec3\u548c\u8bc4\u4f30\u6784\u5efa","text":"<p>\u8bad\u7ec3\u548c\u8bc4\u4f30\u6784\u5efa\uff0c\u8bbe\u7f6e\u635f\u5931\u8ba1\u7b97\u51fd\u6570\uff0c\u8fd4\u56de\u5b57\u6bb5\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>def evaluate(self):\n    # compute loss\n    loss, loss_g, loss_b = self.Loss(self.x_train, self.y_train, self.a_g, self.a_b)\n    loss_g_numpy = float(loss_g)\n    loss_b_numpy = float(loss_b)\n    # eq loss\n    self.loss_g.append(loss_g_numpy)\n    # boundary loss\n    self.loss_b.append(loss_b_numpy)\n    if self.iter % 100 == 0:\n        if self.adaptive_weights:\n            self.a_g, self.a_b, _ = self.pirbn.cal_ntk(self.x_train)\n            print(\n                \"Iter : \",\n                self.iter,\n                \"\\tloss : \",\n                float(loss),\n                \"\\tboundary loss : \",\n                float(loss_b),\n                \"\\teq loss : \",\n                float(loss_g),\n            )\n            print(\"\\ta_g =\", float(self.a_g), \"\\ta_b =\", float(self.a_b))\n        else:\n            print(\n                \"Iter : \",\n                self.iter,\n                \"\\tloss : \",\n                float(loss),\n                \"\\tboundary loss : \",\n                float(loss_b),\n                \"\\teq loss : \",\n                float(loss_g),\n            )\n    self.his_a_g.append(self.a_g)\n    self.his_a_b.append(self.a_b)\n\n    self.iter = self.iter + 1\n    return loss\n</code></pre>"},{"location":"zh/examples/pirbn/#34","title":"3.4 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 20001 \u8f6e\u8bad\u7ec3\u8f6e\u6570\u3002</p> <pre><code>maxiter = 20001\n</code></pre>"},{"location":"zh/examples/pirbn/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9 <code>Adam</code> \u4f18\u5316\u5668\u5e76\u8bbe\u5b9a <code>learning_rate</code> \u4e3a 1e-3\u3002</p> <pre><code>self.optimizer = paddle.optimizer.Adam(\n    learning_rate=0.001, parameters=self.pirbn.parameters()\n)\n</code></pre>"},{"location":"zh/examples/pirbn/#36","title":"3.6 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30</p> <pre><code>def fit(self, output_Kgg):\n    for i in range(0, self.maxiter):\n        loss = self.evaluate()\n        loss.backward()\n        if i in output_Kgg:\n            self.ntk_list[f\"{i}\"] = self.pirbn.cal_K(self.x_train)\n        self.optimizer.step()\n        self.optimizer.clear_grad()\n</code></pre>"},{"location":"zh/examples/pirbn/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"main.py<pre><code>import analytical_solution\nimport numpy as np\nimport pirbn\nimport rbn_net\nimport train\n\nimport ppsci\n\n# set random seed for reproducibility\nSEED = 2023\nppsci.utils.misc.set_random_seed(SEED)\n\n# mu, Fig.1, Page5\n# right_by, Formula (15) Page5\ndef sine_function_main(\n    mu, adaptive_weights=True, right_by=0, activation_function=\"gaussian\"\n):\n    # Define the number of sample points\n    ns = 50\n\n    # Define the sample points' interval\n    dx = 1.0 / (ns - 1)\n\n    # Initialise sample points' coordinates\n    x_eq = np.linspace(0.0, 1.0, ns)[:, None]\n\n    for i in range(0, ns):\n        x_eq[i, 0] = i * dx + right_by\n    x_bc = np.array([[right_by + 0.0], [right_by + 1.0]])\n    x = [x_eq, x_bc]\n    y = -4 * mu**2 * np.pi**2 * np.sin(2 * mu * np.pi * x_eq)\n\n    # Set up radial basis network\n    n_in = 1\n    n_out = 1\n    n_neu = 61\n    b = 10.0\n    c = [right_by - 0.1, right_by + 1.1]\n\n    # Set up PIRBN\n    rbn = rbn_net.RBN_Net(n_in, n_out, n_neu, b, c, activation_function)\n    rbn_loss = pirbn.PIRBN(rbn, activation_function)\n    maxiter = 20001\n    output_Kgg = [0, int(0.1 * maxiter), maxiter - 1]\n    train_obj = train.Trainer(\n        rbn_loss,\n        x,\n        y,\n        learning_rate=0.001,\n        maxiter=maxiter,\n        adaptive_weights=adaptive_weights,\n    )\n    train_obj.fit(output_Kgg)\n\n    # Visualise results\n    analytical_solution.output_fig(\n        train_obj, mu, b, right_by, activation_function, output_Kgg\n    )\n\n\n# Fig.1\nsine_function_main(mu=4, right_by=0, activation_function=\"tanh\")\n# Fig.2\nsine_function_main(mu=8, right_by=0, activation_function=\"tanh\")\n# Fig.3\nsine_function_main(mu=4, right_by=100, activation_function=\"tanh\")\n# Fig.6\nsine_function_main(mu=8, right_by=100, activation_function=\"gaussian\")\n</code></pre>"},{"location":"zh/examples/pirbn/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>PINN \u6848\u4f8b\u9488\u5bf9 epoch=20001 \u548c learning_rate=1e-3 \u7684\u53c2\u6570\u914d\u7f6e\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8fd4\u56deLoss\u4e3a 0.13567\u3002</p> <p>PIRBN \u6848\u4f8b\u9488\u5bf9 epoch=20001 \u548c learning_rate=1e-3 \u7684\u53c2\u6570\u914d\u7f6e\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8fd4\u56deLoss\u4e3a 0.59471\u3002</p> <p> </p> PINN \u7ed3\u679c\u56fe <p>\u56fe\u4e3a\u4f7f\u7528\u53cc\u66f2\u6b63\u5207\u51fd\u6570\uff08tanh\uff09\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff08activation function\uff09\uff0c\u5e76\u4e14\u4f7f\u7528 LuCun \u521d\u59cb\u5316\u65b9\u6cd5\u6765\u521d\u59cb\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u6240\u6709\u53c2\u6570\u3002</p> <ul> <li>\u56fe\u4e2d\u5b50\u56fe 1 \u4e3a\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u503c\u7684\u66f2\u7ebf\u6bd4\u8f83</li> <li>\u56fe\u4e2d\u5b50\u56fe 2 \u4e3a\u8bef\u5dee\u503c</li> <li>\u56fe\u4e2d\u5b50\u56fe 3 \u4e3a\u635f\u5931\u503c</li> <li>\u56fe\u4e2d\u5b50\u56fe 4 \u4e3a\u8bad\u7ec3 1 \u6b21\u7684 Kg \u56fe</li> <li>\u56fe\u4e2d\u5b50\u56fe 5 \u4e3a\u8bad\u7ec3 2000 \u6b21\u7684 Kg \u56fe</li> <li>\u56fe\u4e2d\u5b50\u56fe 6 \u4e3a\u8bad\u7ec3 20000 \u6b21\u7684 Kg \u56fe</li> </ul> <p>\u53ef\u4ee5\u770b\u5230\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u503c\u53ef\u4ee5\u5339\u914d\uff0c\u8bef\u5dee\u503c\u9010\u6e10\u5347\u9ad8\u7136\u540e\u9010\u6e10\u51cf\u5c11\uff0cLoss \u5386\u53f2\u964d\u4f4e\u540e\u6ce2\u52a8\uff0cKg \u56fe\u968f\u8bad\u7ec3\u6b21\u6570\u589e\u52a0\u800c\u9010\u6e10\u6536\u655b\u3002</p> <p> </p> PIRBN \u7ed3\u679c\u56fe <p>\u56fe\u4e3a\u4f7f\u7528\u9ad8\u65af\u51fd\u6570\uff08gaussian function\uff09\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff08activation function\uff09\u751f\u6210\u7684\u6570\u636e\uff0c\u5e76\u4e14\u4f7f\u7528 LuCun \u521d\u59cb\u5316\u65b9\u6cd5\u6765\u521d\u59cb\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u6240\u6709\u53c2\u6570\u3002</p> <ul> <li>\u56fe\u4e2d\u5b50\u56fe 1 \u4e3a\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u503c\u7684\u66f2\u7ebf\u6bd4\u8f83</li> <li>\u56fe\u4e2d\u5b50\u56fe 2 \u4e3a\u8bef\u5dee\u503c</li> <li>\u56fe\u4e2d\u5b50\u56fe 3 \u4e3a\u635f\u5931\u503c</li> <li>\u56fe\u4e2d\u5b50\u56fe 4 \u4e3a\u8bad\u7ec3 1 \u6b21\u7684 Kg \u56fe</li> <li>\u56fe\u4e2d\u5b50\u56fe 5 \u4e3a\u8bad\u7ec3 2000 \u6b21\u7684 Kg \u56fe</li> <li>\u56fe\u4e2d\u5b50\u56fe 6 \u4e3a\u8bad\u7ec3 20000 \u6b21\u7684 Kg \u56fe</li> </ul> <p>\u53ef\u4ee5\u770b\u5230\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u503c\u53ef\u4ee5\u5339\u914d\uff0c\u8bef\u5dee\u503c\u9010\u6e10\u5347\u9ad8\u7136\u540e\u9010\u6e10\u51cf\u5c11\u518d\u5347\u9ad8\uff0cLoss \u5386\u53f2\u964d\u4f4e\u540e\u6ce2\u52a8\uff0cKg \u56fe\u968f\u8bad\u7ec3\u6b21\u6570\u589e\u52a0\u800c\u9010\u6e10\u6536\u655b\u3002</p>"},{"location":"zh/examples/pirbn/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>Physics-informed radial basis network (PIRBN): A local approximating neural network for solving nonlinear PDEs</li> <li>https://github.com/JinshuaiBai/PIRBN</li> </ul>"},{"location":"zh/examples/rossler/","title":"Rossler_transform_physx","text":""},{"location":"zh/examples/rossler/#rossler-system","title":"Rossler System","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_training.hdf5 -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_valid.hdf5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_training.hdf5 --create-dirs -o ./datasets/rossler_training.hdf5\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_valid.hdf5 --create-dirs -o ./datasets/rossler_valid.hdf5\npython train_enn.py\npython train_transformer.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_training.hdf5 -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_valid.hdf5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_training.hdf5 --create-dirs -o ./datasets/rossler_training.hdf5\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_valid.hdf5 --create-dirs -o ./datasets/rossler_valid.hdf5\npython train_enn.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/rossler/rossler_pretrained.pdparams\npython train_transformer.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/rossler/rossler_transformer_pretrained.pdparams EMBEDDING_MODEL_PATH=https://paddle-org.bj.bcebos.com/paddlescience/models/rossler/rossler_pretrained.pdparams\n</code></pre> <pre><code>python train_transformer.py mode=export EMBEDDING_MODEL_PATH=https://paddle-org.bj.bcebos.com/paddlescience/models/rossler/rossler_pretrained.pdparams\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_training.hdf5 -P ./datasets/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_valid.hdf5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_training.hdf5 --create-dirs -o ./datasets/rossler_training.hdf5\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/transformer_physx/rossler_valid.hdf5 --create-dirs -o ./datasets/rossler_valid.hdf5\npython train_transformer.py mode=infer\n</code></pre> \u6a21\u578b MSE rossler_transformer_pretrained.pdparams 0.022"},{"location":"zh/examples/rossler/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>Rossler System\uff0c\u6700\u65e9\u7531\u5fb7\u56fd\u79d1\u5b66\u5bb6 Rossler \u63d0\u51fa\uff0c\u4e5f\u662f\u5e38\u89c1\u7684\u6df7\u6c8c\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5728\u6df7\u6c8c\u7406\u8bba\u7684\u7814\u7a76\u4e2d\u5177\u6709\u91cd\u8981\u5730\u4f4d\uff0c\u4e3a\u6df7\u6c8c\u73b0\u8c61\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u5b66\u63cf\u8ff0\u548c\u7406\u89e3\u65b9\u6cd5\u3002\u540c\u65f6\u7531\u4e8e\u8be5\u7cfb\u7edf\u5bf9\u6570\u503c\u6270\u52a8\u6781\u4e3a\u654f\u611f\uff0c\u56e0\u6b64\u4e5f\u662f\u662f\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\uff08\u6df1\u5ea6\u5b66\u4e60\uff09\u6a21\u578b\u51c6\u786e\u6027\u7684\u826f\u597d\u57fa\u51c6\u3002</p>"},{"location":"zh/examples/rossler/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>Rossler \u7cfb\u7edf\u7684\u72b6\u6001\u65b9\u7a0b\uff1a</p> \\[ \\begin{cases}   \\dfrac{\\partial x}{\\partial t} = -\\omega y - z, &amp; \\\\   \\dfrac{\\partial y}{\\partial t} = \\omega x + \\alpha y, &amp; \\\\   \\dfrac{\\partial z}{\\partial t} = \\beta + z(x - \\gamma) \\end{cases} \\] <p>\u5f53\u53c2\u6570\u53d6\u4ee5\u4e0b\u503c\u65f6\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u7ecf\u5178\u7684\u6df7\u6c8c\u7279\u6027\uff1a</p> \\[\\omega = 1.0, \\alpha = 0.165, \\beta = 0.2, \\gamma = 10\\] <p>\u5728\u8fd9\u4e2a\u6848\u4f8b\u4e2d\uff0c\u8981\u6c42\u7ed9\u5b9a\u521d\u59cb\u65f6\u523b\u70b9\u7684\u5750\u6807\uff0c\u9884\u6d4b\u672a\u6765\u4e00\u6bb5\u65f6\u95f4\u5185\u70b9\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002</p>"},{"location":"zh/examples/rossler/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u57fa\u4e8e PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002\u672c\u6848\u4f8b\u57fa\u4e8e\u8bba\u6587 Transformers for Modeling Physical Systems \u65b9\u6cd5\u8fdb\u884c\u6c42\u89e3\uff0c\u5173\u4e8e\u8be5\u65b9\u6cd5\u7684\u7406\u8bba\u90e8\u5206\u8bf7\u53c2\u8003\u6b64\u6587\u6863\u6216\u539f\u8bba\u6587\u3002\u63a5\u4e0b\u6765\u9996\u5148\u4f1a\u5bf9\u4f7f\u7528\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4ecb\u7ecd\uff0c\u7136\u540e\u5bf9\u8be5\u65b9\u6cd5\u4e24\u4e2a\u8bad\u7ec3\u6b65\u9aa4\uff08Embedding \u6a21\u578b\u8bad\u7ec3\u3001Transformer \u6a21\u578b\u8bad\u7ec3\uff09\u7684\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u3001\u6a21\u578b\u6784\u5efa\u7b49\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/rossler/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u6570\u636e\u96c6\u91c7\u7528\u4e86 Transformer-Physx \u4e2d\u63d0\u4f9b\u7684\u6570\u636e\u3002\u8be5\u6570\u636e\u96c6\u4f7f\u7528\u9f99\u683c\uff0d\u5e93\u5854\uff08Runge-Kutta\uff09\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u65b9\u6cd5\u5f97\u5230\uff0c\u6570\u636e\u96c6\u7684\u5212\u5206\u5982\u4e0b\uff1a</p> \u6570\u636e\u96c6 \u65f6\u95f4\u5e8f\u5217\u7684\u6570\u91cf \u65f6\u95f4\u6b65\u7684\u6570\u91cf \u4e0b\u8f7d\u5730\u5740 \u8bad\u7ec3\u96c6 256 1025 rossler_training.hdf5 \u9a8c\u8bc1\u96c6 32 1025 rossler_valid.hdf5 <p>\u6570\u636e\u96c6\u5b98\u7f51\u4e3a\uff1ahttps://zenodo.org/record/5148524#.ZDe77-xByrc</p>"},{"location":"zh/examples/rossler/#32-embedding","title":"3.2 Embedding \u6a21\u578b","text":"<p>\u9996\u5148\u5c55\u793a\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u5404\u4e2a\u53c2\u6570\u53d8\u91cf\uff0c\u6bcf\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u4f1a\u5728\u4e0b\u9762\u4f7f\u7528\u5230\u65f6\u8fdb\u884c\u89e3\u91ca\u3002</p> examples/rossler/conf/enn.yaml<pre><code># general settings\nmode: train # running mode: train/eval\nseed: 6\noutput_dir: ${hydra:run.dir}\nTRAIN_BLOCK_SIZE: 16\nVALID_BLOCK_SIZE: 32\nTRAIN_FILE_PATH: ./datasets/rossler_training.hdf5\nVALID_FILE_PATH: ./datasets/rossler_valid.hdf5\n\n# model settings\nMODEL:\n  input_keys: [\"states\"]\n  output_keys: [\"pred_states\", \"recover_states\"]\n</code></pre>"},{"location":"zh/examples/rossler/#321","title":"3.2.1 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/rossler/train_enn.py<pre><code>train_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"RosslerDataset\",\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n        \"stride\": 16,\n        \"weight_dict\": {\n            key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n        },\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 4,\n}\n</code></pre> <p>\u5176\u4e2d\uff0c\"dataset\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Dataset</code> \u7c7b\u540d\u4e3a <code>RosslerDataset</code>\uff0c\u53e6\u5916\u8fd8\u6307\u5b9a\u4e86\u8be5\u7c7b\u521d\u59cb\u5316\u65f6\u53c2\u6570\u7684\u53d6\u503c\uff1a</p> <ol> <li><code>file_path</code>\uff1a\u4ee3\u8868\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6587\u4ef6\u8def\u5f84\uff0c\u6307\u5b9a\u4e3a\u53d8\u91cf <code>train_file_path</code> \u7684\u503c\uff1b</li> <li><code>input_keys</code>\uff1a\u4ee3\u8868\u6a21\u578b\u8f93\u5165\u6570\u636e\u7684\u53d8\u91cf\u540d\u79f0\uff0c\u6b64\u5904\u586b\u5165\u53d8\u91cf <code>input_keys</code>\uff1b</li> <li><code>label_keys</code>\uff1a\u4ee3\u8868\u771f\u5b9e\u6807\u7b7e\u7684\u53d8\u91cf\u540d\u79f0\uff0c\u6b64\u5904\u586b\u5165\u53d8\u91cf <code>output_keys</code>\uff1b</li> <li><code>block_size</code>\uff1a\u4ee3\u8868\u4f7f\u7528\u591a\u957f\u7684\u65f6\u95f4\u6b65\u8fdb\u884c\u8bad\u7ec3\uff0c\u6307\u5b9a\u4e3a\u53d8\u91cf <code>train_block_size</code> \u7684\u503c\uff1b</li> <li><code>stride</code>\uff1a\u4ee3\u8868\u8fde\u7eed\u7684\u4e24\u4e2a\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u7684\u65f6\u95f4\u6b65\u95f4\u9694\uff0c\u6307\u5b9a\u4e3a16\uff1b</li> <li><code>weight_dict</code>\uff1a\u4ee3\u8868\u6a21\u578b\u8f93\u51fa\u5404\u4e2a\u53d8\u91cf\u4e0e\u771f\u5b9e\u6807\u7b7e\u635f\u5931\u51fd\u6570\u7684\u6743\u91cd\uff0c\u6b64\u5904\u4f7f\u7528 <code>output_keys</code>\u3001<code>weights</code> \u751f\u6210\u3002</li> </ol> <p>\"sampler\" \u5b57\u6bb5\u5b9a\u4e49\u4e86\u4f7f\u7528\u7684 <code>Sampler</code> \u7c7b\u540d\u4e3a <code>BatchSampler</code>\uff0c\u53e6\u5916\u8fd8\u6307\u5b9a\u4e86\u8be5\u7c7b\u521d\u59cb\u5316\u65f6\u53c2\u6570 <code>drop_last</code>\u3001<code>shuffle</code> \u5747\u4e3a <code>True</code>\u3002</p> <p><code>train_dataloader_cfg</code> \u8fd8\u5b9a\u4e49\u4e86 <code>batch_size</code>\u3001<code>num_workers</code> \u7684\u503c\u3002</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/rossler/train_enn.py<pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.MSELossWithL2Decay(\n        regularization_dict={regularization_key: 1e-1 * (cfg.TRAIN_BLOCK_SIZE - 1)}\n    ),\n    {\n        key: lambda out, k=key: out[k]\n        for key in cfg.MODEL.output_keys + (regularization_key,)\n    },\n    name=\"Sup\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u6570\u636e\u7684\u52a0\u8f7d\u65b9\u5f0f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e0a\u6587\u4e2d\u5b9a\u4e49\u7684 <code>train_dataloader_cfg</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u4f7f\u7528\u5e26\u6709 L2Decay \u7684 MSELoss\uff0c\u7c7b\u540d\u4e3a <code>MSELossWithL2Decay</code>\uff0c<code>regularization_dict</code> \u8bbe\u7f6e\u4e86\u6b63\u5219\u5316\u7684\u53d8\u91cf\u540d\u79f0\u548c\u5bf9\u5e94\u7684\u6743\u91cd\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u8868\u793a\u5728\u8bad\u7ec3\u65f6\u5982\u4f55\u8ba1\u7b97\u9700\u8981\u88ab\u7ea6\u675f\u7684\u4e2d\u95f4\u53d8\u91cf\uff0c\u6b64\u5904\u6211\u4eec\u7ea6\u675f\u7684\u53d8\u91cf\u5c31\u662f\u7f51\u7edc\u7684\u8f93\u51fa\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u547d\u540d\u4e3a \"Sup\"\u3002</p>"},{"location":"zh/examples/rossler/#322","title":"3.2.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0cEmbedding \u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u90fd\u662f\u7269\u7406\u7a7a\u95f4\u4e2d\u70b9\u7684\u4f4d\u7f6e\u5750\u6807 \\((x, y, z)\\) \uff0c\u4f7f\u7528\u4e86\u5168\u8fde\u63a5\u5c42\u5b9e\u73b0 Embedding \u6a21\u578b\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002</p> <p> </p> Embedding \u7f51\u7edc\u6a21\u578b <p>\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/rossler/train_enn.py<pre><code># manually init model\ndata_mean, data_std = get_mean_std(sup_constraint.data_loader.dataset.data)\nmodel = ppsci.arch.RosslerEmbedding(\n    cfg.MODEL.input_keys,\n    cfg.MODEL.output_keys + (regularization_key,),\n    data_mean,\n    data_std,\n)\n</code></pre> <p>\u5176\u4e2d\uff0c<code>RosslerEmbedding</code> \u7684\u524d\u4e24\u4e2a\u53c2\u6570\u5728\u524d\u6587\u4e2d\u5df2\u6709\u63cf\u8ff0\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\uff0c\u7f51\u7edc\u6a21\u578b\u7684\u7b2c\u4e09\u3001\u56db\u4e2a\u53c2\u6570\u662f\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5747\u503c\u548c\u65b9\u5dee\uff0c\u7528\u4e8e\u5f52\u4e00\u5316\u8f93\u5165\u6570\u636e\u3002\u8ba1\u7b97\u5747\u503c\u3001\u65b9\u5dee\u7684\u7684\u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/rossler/train_enn.py<pre><code>def get_mean_std(data: np.ndarray):\n    mean = np.asarray(\n        [np.mean(data[:, :, 0]), np.mean(data[:, :, 1]), np.min(data[:, :, 2])]\n    ).reshape(1, 3)\n    std = np.asarray(\n        [\n            np.std(data[:, :, 0]),\n            np.std(data[:, :, 1]),\n            np.max(data[:, :, 2]) - np.min(data[:, :, 2]),\n        ]\n    ).reshape(1, 3)\n    return mean, std\n</code></pre>"},{"location":"zh/examples/rossler/#323","title":"3.2.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>ExponentialDecay</code> \uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a0.001\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>Adam</code>\uff0c\u68af\u5ea6\u88c1\u526a\u4f7f\u7528\u4e86 Paddle \u5185\u7f6e\u7684 <code>ClipGradByGlobalNorm</code> \u65b9\u6cd5\u3002\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> examples/rossler/train_enn.py<pre><code># init optimizer and lr scheduler\nclip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\nlr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n    iters_per_epoch=ITERS_PER_EPOCH,\n    decay_steps=ITERS_PER_EPOCH,\n    **cfg.TRAIN.lr_scheduler,\n)()\noptimizer = ppsci.optimizer.Adam(\n    lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n)(model)\n</code></pre>"},{"location":"zh/examples/rossler/#324","title":"3.2.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/rossler/train_enn.py<pre><code>eval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"RosslerDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 32,\n        \"weight_dict\": {\n            key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n        },\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"num_workers\": 4,\n}\n\nmse_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"MSE_Validator\",\n)\nvalidator = {mse_validator.name: mse_validator}\n</code></pre> <p><code>SupervisedValidator</code> \u8bc4\u4f30\u5668\u4e0e <code>SupervisedConstraint</code> \u6bd4\u8f83\u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u8bc4\u4f30\u5668\u9700\u8981\u8bbe\u7f6e\u8bc4\u4ef7\u6307\u6807 <code>metric</code>\uff0c\u5728\u8fd9\u91cc\u4f7f\u7528 <code>ppsci.metric.MSE</code> \u3002</p>"},{"location":"zh/examples/rossler/#325","title":"3.2.5 \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/rossler/train_enn.py<pre><code>solver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=True,\n    validator=validator,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/rossler/#33-transformer","title":"3.3 Transformer \u6a21\u578b","text":"<p>\u4e0a\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u6784\u5efa Embedding \u6a21\u578b\u7684\u8bad\u7ec3\u3001\u8bc4\u4f30\uff0c\u5728\u672c\u8282\u4e2d\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528\u8bad\u7ec3\u597d\u7684 Embedding \u6a21\u578b\u8bad\u7ec3 Transformer \u6a21\u578b\u3002\u56e0\u4e3a\u8bad\u7ec3 Transformer \u6a21\u578b\u7684\u6b65\u9aa4\u4e0e\u8bad\u7ec3 Embedding \u6a21\u578b\u7684\u6b65\u9aa4\u57fa\u672c\u76f8\u4f3c\uff0c\u56e0\u6b64\u672c\u8282\u5728\u4e24\u8005\u7684\u91cd\u590d\u90e8\u5206\u7684\u5404\u4e2a\u53c2\u6570\u4e0d\u518d\u8be6\u7ec6\u4ecb\u7ecd\u3002\u9996\u5148\u5c06\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u5404\u4e2a\u53c2\u6570\u53d8\u91cf\u5c55\u793a\u5982\u4e0b\uff0c\u6bcf\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u4f1a\u5728\u4e0b\u9762\u4f7f\u7528\u5230\u65f6\u8fdb\u884c\u89e3\u91ca\u3002</p> examples/rossler/conf/transformer.yaml<pre><code># general settings\nmode: train # running mode: train/eval\nseed: 42\noutput_dir: ${hydra:run.dir}\nlog_freq: 20\nTRAIN_BLOCK_SIZE: 32\nVALID_BLOCK_SIZE: 256\nTRAIN_FILE_PATH: ./datasets/rossler_training.hdf5\nVALID_FILE_PATH: ./datasets/rossler_valid.hdf5\n\n# set working condition\nEMBEDDING_MODEL_PATH: ./outputs_rossler_enn/checkpoints/latest\n</code></pre>"},{"location":"zh/examples/rossler/#331","title":"3.3.1 \u7ea6\u675f\u6784\u5efa","text":"<p>Transformer \u6a21\u578b\u540c\u6837\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6c42\u89e3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>SupervisedConstraint</code> \u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u9996\u5148\u6307\u5b9a\u76d1\u7763\u7ea6\u675f\u4e2d\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/rossler/train_transformer.py<pre><code># manually build constraint(s)\ntrain_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"RosslerDataset\",\n        \"file_path\": cfg.TRAIN_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n        \"stride\": 16,\n        \"embedding_model\": embedding_model,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": True,\n        \"shuffle\": True,\n    },\n    \"batch_size\": cfg.TRAIN.batch_size,\n    \"num_workers\": 4,\n}\n</code></pre> <p>\u6570\u636e\u52a0\u8f7d\u7684\u5404\u4e2a\u53c2\u6570\u4e0e Embedding \u6a21\u578b\u4e2d\u7684\u57fa\u672c\u4e00\u81f4\uff0c\u4e0d\u518d\u8d58\u8ff0\u3002\u9700\u8981\u8bf4\u660e\u7684\u662f\u7531\u4e8e Transformer \u6a21\u578b\u8bad\u7ec3\u7684\u8f93\u5165\u6570\u636e\u662f Embedding \u6a21\u578b Encoder \u6a21\u5757\u7684\u8f93\u51fa\u6570\u636e\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u8bad\u7ec3\u597d\u7684 Embedding \u6a21\u578b\u4f5c\u4e3a <code>RosslerDataset</code> \u7684\u4e00\u4e2a\u53c2\u6570\uff0c\u5728\u521d\u59cb\u5316\u65f6\u9996\u5148\u5c06\u8bad\u7ec3\u6570\u636e\u6620\u5c04\u5230\u7f16\u7801\u7a7a\u95f4\u3002</p> <p>\u5b9a\u4e49\u76d1\u7763\u7ea6\u675f\u7684\u4ee3\u7801\u5982\u4e0b\uff1a</p> examples/rossler/train_transformer.py<pre><code>sup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    name=\"Sup\",\n)\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre>"},{"location":"zh/examples/rossler/#332","title":"3.3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u8be5\u6848\u4f8b\u4e2d\uff0cTransformer \u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u90fd\u662f\u7f16\u7801\u7a7a\u95f4\u4e2d\u7684\u5411\u91cf\uff0c\u4f7f\u7528\u7684 Transformer \u7ed3\u6784\u5982\u4e0b\uff1a</p> <p> </p> Transformer \u7f51\u7edc\u6a21\u578b <p>\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/rossler/train_transformer.py<pre><code>model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n</code></pre> <p>\u7c7b <code>PhysformerGPT2</code> \u9664\u4e86\u9700\u8981\u586b\u5165 <code>input_keys</code>\u3001<code>output_keys</code> \u5916\uff0c\u8fd8\u9700\u8981\u8bbe\u7f6e Transformer \u6a21\u578b\u7684\u5c42\u6570 <code>num_layers</code>\u3001\u4e0a\u4e0b\u6587\u7684\u5927\u5c0f <code>num_ctx</code>\u3001\u8f93\u5165\u7684 Embedding \u5411\u91cf\u7684\u957f\u5ea6 <code>embed_size</code>\u3001\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u53c2\u6570 <code>num_heads</code>\uff0c\u5728\u8fd9\u91cc\u586b\u5165\u7684\u6570\u503c\u4e3a4\u300164\u300132\u30014\u3002</p>"},{"location":"zh/examples/rossler/#333","title":"3.3.3 \u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u65b9\u6cd5\u4e3a <code>CosineWarmRestarts</code>\uff0c\u5b66\u4e60\u7387\u5927\u5c0f\u8bbe\u7f6e\u4e3a0.001\u3002\u4f18\u5316\u5668\u4f7f\u7528 <code>Adam</code>\uff0c\u68af\u5ea6\u88c1\u526a\u4f7f\u7528\u4e86 Paddle \u5185\u7f6e\u7684 <code>ClipGradByGlobalNorm</code> \u65b9\u6cd5\u3002\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/rossler/train_transformer.py<pre><code># init optimizer and lr scheduler\nclip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\nlr_scheduler = ppsci.optimizer.lr_scheduler.CosineWarmRestarts(\n    iters_per_epoch=ITERS_PER_EPOCH, **cfg.TRAIN.lr_scheduler\n)()\noptimizer = ppsci.optimizer.Adam(\n    lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n)(model)\n</code></pre>"},{"location":"zh/examples/rossler/#334","title":"3.3.4 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6309\u7167\u4e00\u5b9a\u7684\u8bad\u7ec3\u8f6e\u6570\u95f4\u9694\uff0c\u4f7f\u7528\u9a8c\u8bc1\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u9700\u8981\u4f7f\u7528 <code>SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> examples/rossler/train_transformer.py<pre><code>eval_dataloader_cfg = {\n    \"dataset\": {\n        \"name\": \"RosslerDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 1024,\n        \"embedding_model\": embedding_model,\n    },\n    \"sampler\": {\n        \"name\": \"BatchSampler\",\n        \"drop_last\": False,\n        \"shuffle\": False,\n    },\n    \"batch_size\": cfg.EVAL.batch_size,\n    \"num_workers\": 4,\n}\n\nmse_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    ppsci.loss.MSELoss(),\n    metric={\"MSE\": ppsci.metric.MSE()},\n    name=\"MSE_Validator\",\n)\nvalidator = {mse_validator.name: mse_validator}\n</code></pre>"},{"location":"zh/examples/rossler/#335","title":"3.3.5 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u4e2d\u53ef\u4ee5\u901a\u8fc7\u6784\u5efa\u53ef\u89c6\u5316\u5668\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\u5c06\u8bc4\u4f30\u7ed3\u679c\u53ef\u89c6\u5316\u51fa\u6765\uff0c\u7531\u4e8e Transformer \u6a21\u578b\u7684\u8f93\u51fa\u6570\u636e\u662f\u9884\u6d4b\u7684\u7f16\u7801\u7a7a\u95f4\u7684\u6570\u636e\u65e0\u6cd5\u76f4\u63a5\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u56e0\u6b64\u9700\u8981\u989d\u5916\u5c06\u8f93\u51fa\u6570\u636e\u4f7f\u7528 Embedding \u7f51\u7edc\u7684 Decoder \u6a21\u5757\u53d8\u6362\u5230\u7269\u7406\u72b6\u6001\u7a7a\u95f4\u3002</p> <p>\u5728\u672c\u6587\u4e2d\u9996\u5148\u5b9a\u4e49\u4e86\u5bf9 Transformer \u6a21\u578b\u8f93\u51fa\u6570\u636e\u53d8\u6362\u5230\u7269\u7406\u72b6\u6001\u7a7a\u95f4\u7684\u4ee3\u7801\uff1a</p> examples/rossler/train_transformer.py<pre><code>def build_embedding_model(embedding_model_path: str) -&gt; ppsci.arch.RosslerEmbedding:\n    input_keys = (\"states\",)\n    output_keys = (\"pred_states\", \"recover_states\")\n    regularization_key = \"k_matrix\"\n    model = ppsci.arch.RosslerEmbedding(input_keys, output_keys + (regularization_key,))\n    save_load.load_pretrain(model, embedding_model_path)\n    return model\n\n\nclass OutputTransform(object):\n    def __init__(self, model: base.Arch):\n        self.model = model\n        self.model.eval()\n\n    def __call__(self, x: Dict[str, paddle.Tensor]):\n        pred_embeds = x[\"pred_embeds\"]\n        pred_states = self.model.decoder(pred_embeds)\n\n        return pred_states\n</code></pre> examples/rossler/train_transformer.py<pre><code># manually build constraint(s)\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230\uff0c\u7a0b\u5e8f\u9996\u5148\u8f7d\u5165\u4e86\u8bad\u7ec3\u597d\u7684 Embedding \u6a21\u578b\uff0c\u7136\u540e\u5728 <code>OutputTransform</code> \u7684 <code>__call__</code> \u51fd\u6570\u5185\u5b9e\u73b0\u4e86\u7f16\u7801\u5411\u91cf\u5230\u7269\u7406\u72b6\u6001\u7a7a\u95f4\u7684\u53d8\u6362\u3002</p> <p>\u5728\u5b9a\u4e49\u597d\u4e86\u4ee5\u4e0a\u4ee3\u7801\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u5b9e\u73b0\u53ef\u89c6\u5316\u5668\u4ee3\u7801\u7684\u6784\u5efa\u4e86\uff1a</p> examples/rossler/train_transformer.py<pre><code># set visualizer(optional)\nstates = mse_validator.data_loader.dataset.data\nembedding_data = mse_validator.data_loader.dataset.embedding_data\nvis_data = {\n    \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1, :],\n    \"states\": states[: cfg.VIS_DATA_NUMS, 1:, :],\n}\n\nvisualizer = {\n    \"visualize_states\": ppsci.visualize.VisualizerScatter3D(\n        vis_data,\n        {\n            \"pred_states\": lambda d: output_transform(d),\n            \"states\": lambda d: d[\"states\"],\n        },\n        num_timestamps=1,\n        prefix=\"result_states\",\n    )\n}\n</code></pre> <p>\u9996\u5148\u4f7f\u7528\u4e0a\u6587\u4e2d\u7684 <code>mse_validator</code> \u4e2d\u7684\u6570\u636e\u96c6\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u53e6\u5916\u8fd8\u5f15\u5165\u4e86 <code>vis_data_nums</code> \u53d8\u91cf\u7528\u4e8e\u63a7\u5236\u9700\u8981\u53ef\u89c6\u5316\u6837\u672c\u7684\u6570\u91cf\u3002\u6700\u540e\u901a\u8fc7 <code>VisualizerScatter3D</code> \u6784\u5efa\u53ef\u89c6\u5316\u5668\u3002</p>"},{"location":"zh/examples/rossler/#336","title":"3.3.6 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> examples/rossler/train_transformer.py<pre><code>solver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    lr_scheduler,\n    cfg.TRAIN.epochs,\n    ITERS_PER_EPOCH,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    validator=validator,\n    visualizer=visualizer,\n)\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>"},{"location":"zh/examples/rossler/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"rossler/train_enn.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Two-stage training\n# 1. Train a embedding model by running train_enn.py.\n# 2. Load pretrained embedding model and freeze it, then train a transformer model by running train_transformer.py.\n\n# This file is for step1: training a embedding model.\n# This file is based on PaddleScience/ppsci API.\nfrom os import path as osp\n\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef get_mean_std(data: np.ndarray):\n    mean = np.asarray(\n        [np.mean(data[:, :, 0]), np.mean(data[:, :, 1]), np.min(data[:, :, 2])]\n    ).reshape(1, 3)\n    std = np.asarray(\n        [\n            np.std(data[:, :, 0]),\n            np.std(data[:, :, 1]),\n            np.max(data[:, :, 2]) - np.min(data[:, :, 2]),\n        ]\n    ).reshape(1, 3)\n    return mean, std\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    weights = (1.0 * (cfg.TRAIN_BLOCK_SIZE - 1), 1.0e3 * cfg.TRAIN_BLOCK_SIZE)\n    regularization_key = \"k_matrix\"\n    # manually build constraint(s)\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"RosslerDataset\",\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n            \"stride\": 16,\n            \"weight_dict\": {\n                key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n            },\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 4,\n    }\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELossWithL2Decay(\n            regularization_dict={regularization_key: 1e-1 * (cfg.TRAIN_BLOCK_SIZE - 1)}\n        ),\n        {\n            key: lambda out, k=key: out[k]\n            for key in cfg.MODEL.output_keys + (regularization_key,)\n        },\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(sup_constraint.data_loader)\n\n    # manually init model\n    data_mean, data_std = get_mean_std(sup_constraint.data_loader.dataset.data)\n    model = ppsci.arch.RosslerEmbedding(\n        cfg.MODEL.input_keys,\n        cfg.MODEL.output_keys + (regularization_key,),\n        data_mean,\n        data_std,\n    )\n\n    # init optimizer and lr scheduler\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\n    lr_scheduler = ppsci.optimizer.lr_scheduler.ExponentialDecay(\n        iters_per_epoch=ITERS_PER_EPOCH,\n        decay_steps=ITERS_PER_EPOCH,\n        **cfg.TRAIN.lr_scheduler,\n    )()\n    optimizer = ppsci.optimizer.Adam(\n        lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n    )(model)\n\n    # manually build validator\n    weights = (1.0 * (cfg.VALID_BLOCK_SIZE - 1), 1.0e4 * cfg.VALID_BLOCK_SIZE)\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"RosslerDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 32,\n            \"weight_dict\": {\n                key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n            },\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=True,\n        validator=validator,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    weights = (1.0 * (cfg.TRAIN_BLOCK_SIZE - 1), 1.0e3 * cfg.TRAIN_BLOCK_SIZE)\n    regularization_key = \"k_matrix\"\n    # manually build constraint(s)\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"RosslerDataset\",\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n            \"stride\": 16,\n            \"weight_dict\": {\n                key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n            },\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 4,\n    }\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELossWithL2Decay(\n            regularization_dict={regularization_key: 1e-1 * (cfg.TRAIN_BLOCK_SIZE - 1)}\n        ),\n        {\n            key: lambda out, k=key: out[k]\n            for key in cfg.MODEL.output_keys + (regularization_key,)\n        },\n        name=\"Sup\",\n    )\n\n    # manually init model\n    data_mean, data_std = get_mean_std(sup_constraint.data_loader.dataset.data)\n    model = ppsci.arch.RosslerEmbedding(\n        cfg.MODEL.input_keys,\n        cfg.MODEL.output_keys + (regularization_key,),\n        data_mean,\n        data_std,\n    )\n\n    # manually build validator\n    weights = (1.0 * (cfg.VALID_BLOCK_SIZE - 1), 1.0e4 * cfg.VALID_BLOCK_SIZE)\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"RosslerDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 32,\n            \"weight_dict\": {\n                key: value for key, value in zip(cfg.MODEL.output_keys, weights)\n            },\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"enn.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> rossler/train_transformer.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Two-stage training\n# 1. Train a embedding model by running train_enn.py.\n# 2. Load pretrained embedding model and freeze it, then train a transformer model by running train_transformer.py.\n\n# This file is for step2: training a transformer model, based on frozen pretrained embedding model.\n# This file is based on PaddleScience/ppsci API.\nfrom os import path as osp\nfrom typing import Dict\n\nimport hydra\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.arch import base\nfrom ppsci.utils import logger\nfrom ppsci.utils import save_load\n\n\ndef build_embedding_model(embedding_model_path: str) -&gt; ppsci.arch.RosslerEmbedding:\n    input_keys = (\"states\",)\n    output_keys = (\"pred_states\", \"recover_states\")\n    regularization_key = \"k_matrix\"\n    model = ppsci.arch.RosslerEmbedding(input_keys, output_keys + (regularization_key,))\n    save_load.load_pretrain(model, embedding_model_path)\n    return model\n\n\nclass OutputTransform(object):\n    def __init__(self, model: base.Arch):\n        self.model = model\n        self.model.eval()\n\n    def __call__(self, x: Dict[str, paddle.Tensor]):\n        pred_embeds = x[\"pred_embeds\"]\n        pred_states = self.model.decoder(pred_embeds)\n\n        return pred_states\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    output_transform = OutputTransform(embedding_model)\n\n    # manually build constraint(s)\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"RosslerDataset\",\n            \"file_path\": cfg.TRAIN_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.TRAIN_BLOCK_SIZE,\n            \"stride\": 16,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": True,\n            \"shuffle\": True,\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"num_workers\": 4,\n    }\n\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        name=\"Sup\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set iters_per_epoch by dataloader length\n    ITERS_PER_EPOCH = len(constraint[\"Sup\"].data_loader)\n\n    # manually init model\n    model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n\n    # init optimizer and lr scheduler\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.1)\n    lr_scheduler = ppsci.optimizer.lr_scheduler.CosineWarmRestarts(\n        iters_per_epoch=ITERS_PER_EPOCH, **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer = ppsci.optimizer.Adam(\n        lr_scheduler, grad_clip=clip, **cfg.TRAIN.optimizer\n    )(model)\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"RosslerDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 1024,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    # set visualizer(optional)\n    states = mse_validator.data_loader.dataset.data\n    embedding_data = mse_validator.data_loader.dataset.embedding_data\n    vis_data = {\n        \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1, :],\n        \"states\": states[: cfg.VIS_DATA_NUMS, 1:, :],\n    }\n\n    visualizer = {\n        \"visualize_states\": ppsci.visualize.VisualizerScatter3D(\n            vis_data,\n            {\n                \"pred_states\": lambda d: output_transform(d),\n                \"states\": lambda d: d[\"states\"],\n            },\n            num_timestamps=1,\n            prefix=\"result_states\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        lr_scheduler,\n        cfg.TRAIN.epochs,\n        ITERS_PER_EPOCH,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        validator=validator,\n        visualizer=visualizer,\n    )\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    output_transform = OutputTransform(embedding_model)\n\n    # manually init model\n    model = ppsci.arch.PhysformerGPT2(**cfg.MODEL)\n\n    # manually build validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"RosslerDataset\",\n            \"file_path\": cfg.VALID_FILE_PATH,\n            \"input_keys\": cfg.MODEL.input_keys,\n            \"label_keys\": cfg.MODEL.output_keys,\n            \"block_size\": cfg.VALID_BLOCK_SIZE,\n            \"stride\": 1024,\n            \"embedding_model\": embedding_model,\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"num_workers\": 4,\n    }\n\n    mse_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(),\n        metric={\"MSE\": ppsci.metric.MSE()},\n        name=\"MSE_Validator\",\n    )\n    validator = {mse_validator.name: mse_validator}\n\n    # set visualizer(optional)\n    states = mse_validator.data_loader.dataset.data\n    embedding_data = mse_validator.data_loader.dataset.embedding_data\n    vis_datas = {\n        \"embeds\": embedding_data[: cfg.VIS_DATA_NUMS, :-1, :],\n        \"states\": states[: cfg.VIS_DATA_NUMS, 1:, :],\n    }\n\n    visualizer = {\n        \"visulzie_states\": ppsci.visualize.VisualizerScatter3D(\n            vis_datas,\n            {\n                \"pred_states\": lambda d: output_transform(d),\n                \"states\": lambda d: d[\"states\"],\n            },\n            num_timestamps=1,\n            prefix=\"result_states\",\n        )\n    }\n\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        validator=validator,\n        visualizer=visualizer,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n    solver.eval()\n    # visualize prediction for pretrained model(optional)\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    # set model\n    embedding_model = build_embedding_model(cfg.EMBEDDING_MODEL_PATH)\n    model_cfg = {\n        **cfg.MODEL,\n        \"embedding_model\": embedding_model,\n        \"input_keys\": [\"states\"],\n        \"output_keys\": [\"pred_states\"],\n    }\n    model = ppsci.arch.PhysformerGPT2(**model_cfg)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {\n            key: InputSpec([None, 255, 3], \"float32\", name=key)\n            for key in model.input_keys\n        },\n    ]\n\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    dataset_cfg = {\n        \"name\": \"RosslerDataset\",\n        \"file_path\": cfg.VALID_FILE_PATH,\n        \"input_keys\": cfg.MODEL.input_keys,\n        \"label_keys\": cfg.MODEL.output_keys,\n        \"block_size\": cfg.VALID_BLOCK_SIZE,\n        \"stride\": 1024,\n    }\n\n    dataset = ppsci.data.dataset.build_dataset(dataset_cfg)\n\n    input_dict = {\n        \"states\": dataset.data[: cfg.VIS_DATA_NUMS, :-1, :],\n    }\n\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_keys = [\"pred_states\"]\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(output_keys, output_dict.keys())\n    }\n\n    input_dict = {\n        \"states\": dataset.data[: cfg.VIS_DATA_NUMS, 1:, :],\n    }\n\n    data_dict = {**input_dict, **output_dict}\n    for i in range(cfg.VIS_DATA_NUMS):\n        ppsci.visualize.save_plot_from_3d_dict(\n            f\"./rossler_transformer_pred_{i}\",\n            {key: value[i] for key, value in data_dict.items()},\n            (\"states\", \"pred_states\"),\n        )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"transformer.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/rossler/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u56fe\u4e2d\u5c55\u793a\u4e86\u4e24\u4e2a\u4e0d\u540c\u521d\u59cb\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u548c\u4f20\u7edf\u6570\u503c\u5fae\u5206\u7684\u9884\u6d4b\u7ed3\u679c\u3002</p> <p> </p> \u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff08\"pred_states\"\uff09\u4e0e\u4f20\u7edf\u6570\u503c\u5fae\u5206\u7ed3\u679c\uff08\"states\"\uff09 <p> </p> \u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff08\"pred_states\"\uff09\u4e0e\u4f20\u7edf\u6570\u503c\u5fae\u5206\u7ed3\u679c\uff08\"states\"\uff09"},{"location":"zh/examples/shock_wave/","title":"ShockWave","text":""},{"location":"zh/examples/shock_wave/#shock-wave","title":"Shock Wave","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u9884\u6d4b\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 Ma=2.0Ma=0.728 <pre><code>python shock_wave.py\n</code></pre> <pre><code>python shock_wave.py -cn=shock_wave_Ma0.728\n</code></pre> Ma=2.0Ma=0.728 <pre><code>python shock_wave.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/shockwave/shock_wave_Ma2_pretrained.pdparams\n</code></pre> <pre><code>python shock_wave.py -cn=shock_wave_Ma0.728 mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/shockwave/shock_wave_Ma0728_pretrained.pdparams\n</code></pre> Ma=2.0Ma=0.728 <pre><code>python shock_wave.py mode=export\n</code></pre> <pre><code>python shock_wave.py -cn=shock_wave_Ma0.728 mode=export\n</code></pre> Ma=2.0Ma=0.728 <pre><code>python shock_wave.py mode=infer\n</code></pre> <pre><code>python shock_wave.py -cn=shock_wave_Ma0.728 mode=infer\n</code></pre>"},{"location":"zh/examples/shock_wave/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6fc0\u6ce2\u662f\u81ea\u7136\u754c\u4ee5\u53ca\u5de5\u7a0b\u5e94\u7528\u4e2d\u7ecf\u5e38\u53d1\u73b0\u7684\u73b0\u8c61\u3002\u5b83\u4eec\u4e0d\u4ec5\u5e7f\u6cdb\u5730\u5b58\u5728\u4e8e\u822a\u7a7a\u822a\u5929\u9886\u57df\u7684\u53ef\u538b\u7f29\u6d41\u52a8\u4e2d\uff0c\u800c\u4e14\u4e5f\u8868\u73b0\u5728\u7406\u8bba\u4e0e\u5e94\u7528\u7269\u7406\u4ee5\u53ca\u5de5\u7a0b\u5e94\u7528\u7b49\u5176\u5b83\u9886\u57df\u3002\u5728\u8d85\u58f0\u901f\u4e0e\u9ad8\u8d85\u58f0\u901f\u6d41\u52a8\u4e2d\uff0c\u6fc0\u6ce2\u7684\u51fa\u73b0\u5bf9\u6d41\u4f53\u6d41\u52a8\u7684\u6574\u4f53\u7279\u5f81\u4f1a\u4ea7\u751f\u91cd\u8981\u5f71\u54cd\u3002\u6fc0\u6ce2\u6355\u6349\u95ee\u9898\u5df2\u5728CFD\u9886\u57df\u53d1\u5c55\u4e86\u6570\u5341\u5e74\uff0c\u4ee5\u5f31\u89e3\u7684\u6570\u5b66\u7406\u8bba\u4e3a\u57fa\u7840\u7684\u6fc0\u6ce2\u6355\u6349\u65b9\u6cd5\u4ee5\u5176\u7b80\u5355\u6613\u5b9e\u73b0\u7684\u7279\u70b9\u53d1\u5c55\u8fc5\u901f\uff0c\u5e76\u5728\u590d\u6742\u8d85\u58f0\u901f\u3001\u9ad8\u8d85\u58f0\u901f\u6d41\u52a8\u6570\u503c\u6a21\u62df\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002</p> <p>\u672c\u6848\u4f8b\u9488\u5bf9 PINN-WE \u6a21\u578b\u8fdb\u884c\u4f18\u5316\uff0c\u4f7f\u5f97\u8be5\u6a21\u578b\u53ef\u9002\u7528\u4e8e\u8d85\u97f3\u901f\u3001\u9ad8\u8d85\u97f3\u901f\u7b49\u5177\u6709\u5f3a\u6fc0\u6ce2\u7684\u6d41\u573a\u6a21\u62df\u4e2d\u3002</p> <p>PINN-WE \u6a21\u578b\u901a\u8fc7\u635f\u5931\u51fd\u6570\u52a0\u6743\uff0c\u5728 PINN \u4f18\u5316\u8fc7\u7a0b\u4e2d\u51cf\u5f31\u5f3a\u68af\u5ea6\u533a\u57df\u7684\u62df\u5408\uff0c\u907f\u514d\u4e86\u56e0\u6fc0\u6ce2\u533a\u57df\u5f3a\u68af\u5ea6\u5f15\u8d77\u7684\u6fc0\u6ce2\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5176\u5728\u4e00\u7ef4 Euler \u95ee\u9898\u3001\u5f31\u6fc0\u6ce2\u60c5\u51b5\u4e0b\u7684\u4e8c\u7ef4\u95ee\u9898\u4e2d\u53d6\u5f97\u4e86\u4e0d\u9519\u7684\u7ed3\u679c\u3002\u4f46\u662f\u5728\u8d85\u97f3\u901f\u4e8c\u7ef4\u6d41\u573a\u4e2d\uff0c\u8be5\u6a21\u578b\u5e76\u6ca1\u6709\u53d6\u5f97\u5f88\u597d\u7684\u6548\u679c\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8fd8\u53d1\u73b0\u8be5\u6a21\u578b\u7ecf\u5e38\u51fa\u73b0\u6fc0\u6ce2\u4f4d\u7f6e\u504f\u79fb\uff0c\u6fc0\u6ce2\u5f62\u72b6\u4e0d\u5bf9\u79f0\u7b49\u975e\u7269\u7406\u89e3\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u56e0\u6b64\u672c\u6848\u4f8b\u9488\u5bf9\u4e0a\u8ff0 PINN-WE \u6a21\u578b\u7684\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u6e10\u8fdb\u52a0\u6743\u7684\u601d\u60f3\uff0c\u629b\u5f03\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5f3a\u8c03\u68af\u5ea6\u601d\u60f3\uff0c\u800c\u662f\u521b\u65b0\u6027\u5730\u901a\u8fc7\u9010\u6b65\u5f3a\u5316\u68af\u5ea6\u6743\u91cd\u5bf9\u6a21\u578b\u4f18\u5316\u7684\u5f71\u54cd\uff0c\u4f7f\u5f97\u6a21\u578b\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u80fd\u591f\u5f97\u5230\u8f83\u597d\u7684\u3001\u7b26\u5408\u7269\u7406\u7684\u6fc0\u6ce2\u4f4d\u7f6e\u3002</p>"},{"location":"zh/examples/shock_wave/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u95ee\u9898\u9488\u5bf9\u4e8c\u7ef4\u8d85\u58f0\u901f\u6d41\u573a\u5706\u67f1\u5f13\u5f62\u6fc0\u6ce2\u8fdb\u884c\u6a21\u62df\uff0c\u6d89\u53ca\u4e8c\u7ef4Euler\u65b9\u7a0b\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> \\[ \\begin{array}{cc}   \\dfrac{\\partial \\hat{U}}{\\partial t}+\\dfrac{\\partial \\hat{F}}{\\partial \\xi}+\\dfrac{\\partial \\hat{G}}{\\partial \\eta}=0 \\\\   \\text { \u5176\u4e2d, } \\quad   \\begin{cases}     \\hat{U}=J U \\\\     \\hat{F}=J\\left(F \\xi_x+G \\xi_y\\right) \\\\     \\hat{G}=J\\left(F \\eta_x+G \\eta_y\\right)   \\end{cases} \\\\   U=\\left(\\begin{array}{l}   \\rho \\\\   \\rho u \\\\   \\rho v \\\\   E   \\end{array}\\right), \\quad F=\\left(\\begin{array}{l}   \\rho u \\\\   \\rho u^2+p \\\\   \\rho u v \\\\   (E+p) u   \\end{array}\\right), \\quad G=\\left(\\begin{array}{l}   \\rho v \\\\   \\rho v u \\\\   \\rho v^2+p \\\\   (E+p) v   \\end{array}\\right) \\end{array} \\] <p>\u81ea\u7531\u6765\u6d41\u6761\u4ef6 \\(\\rho_{\\infty}=1.225 \\mathrm{~kg} / \\mathrm{m}^3\\) ; \\(P_{\\infty}=1 \\mathrm{~atm}\\)</p> <p>\u6574\u4f53\u6d41\u7a0b\u5982\u4e0b\u6240\u793a\uff1a</p> <p></p>"},{"location":"zh/examples/shock_wave/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/shock_wave/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 ShockWave \u95ee\u9898\u4e2d\uff0c\u7ed9\u5b9a\u65f6\u95f4 \\(t\\) \u548c\u4f4d\u7f6e\u5750\u6807 \\((x,y)\\)\uff0c\u6a21\u578b\u8d1f\u8d23\u9884\u6d4b\u51fa\u5bf9\u5e94\u7684 \\(x\\) \u65b9\u5411\u901f\u5ea6\u3001 \\(y\\) \u9632\u7ebf\u901f\u5ea6\u3001\u538b\u529b\u3001\u5bc6\u5ea6\u56db\u4e2a\u7269\u7406\u91cf \\((u,v,p,\\rho)\\)\uff0c\u56e0\u6b64\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\((t,x,y)\\) \u5230 \\((u,v,p,\\rho)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(g: \\mathbb{R}^3 \\to \\mathbb{R}^4\\) \uff0c\u5373\uff1a</p> \\[ u,v,p,\\rho = g(t,x,y) \\] <p>\u4e0a\u5f0f\u4e2d \\(g\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"t\", \"x\", \"y\")</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>(\"u\", \"v\", \"p\", \"rho\")</code>\uff0c\u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\u4ee5\u53ca\u6fc0\u6d3b\u51fd\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 9 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 90\uff0c\u4f7f\u7528 \"tanh\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>"},{"location":"zh/examples/shock_wave/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u6d89\u53ca\u4e8c\u7ef4\u6b27\u62c9\u65b9\u7a0b\u548c\u8fb9\u754c\u4e0a\u7684\u65b9\u7a0b\uff0c\u5982\u4e0b\u6240\u793a</p> <pre><code>class Euler2D(equation.PDE):\n    def __init__(self):\n        super().__init__()\n        # HACK: solver will be added here for tracking run-time epoch to\n        # compute loss factor `relu` dynamically.\n        self.solver: ppsci.solver.Solver = None\n\n        def continuity_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            t, x, y = out[\"t\"], out[\"x\"], out[\"y\"]\n            u, v, rho = out[\"u\"], out[\"v\"], out[\"rho\"]\n            rho__t = jacobian(rho, t)\n            rho_u = rho * u\n            rho_v = rho * v\n            rho_u__x = jacobian(rho_u, x)\n            rho_v__y = jacobian(rho_v, y)\n\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n            nab = paddle.abs(delta_u) - delta_u\n            lam = (0.1 * nab) * relu + 1\n            continuity = (rho__t + rho_u__x + rho_v__y) / lam\n            return continuity\n\n        self.add_equation(\"continuity\", continuity_compute_func)\n\n        def x_momentum_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            t, x, y = out[\"t\"], out[\"x\"], out[\"y\"]\n            u, v, p, rho = out[\"u\"], out[\"v\"], out[\"p\"], out[\"rho\"]\n            rho_u = rho * u\n            rho_u__t = jacobian(rho_u, t)\n\n            u1 = rho * u**2 + p\n            u2 = rho * u * v\n            u1__x = jacobian(u1, x)\n            u2__y = jacobian(u2, y)\n\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n            nab = paddle.abs(delta_u) - delta_u\n            lam = (0.1 * nab) * relu + 1\n            x_momentum = (rho_u__t + u1__x + u2__y) / lam\n            return x_momentum\n\n        self.add_equation(\"x_momentum\", x_momentum_compute_func)\n\n        def y_momentum_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            t, x, y = out[\"t\"], out[\"x\"], out[\"y\"]\n            u, v, p, rho = out[\"u\"], out[\"v\"], out[\"p\"], out[\"rho\"]\n            rho_v = rho * v\n            rho_v__t = jacobian(rho_v, t)\n\n            u2 = rho * u * v\n            u3 = rho * v**2 + p\n            u2__x = jacobian(u2, x)\n            u3__y = jacobian(u3, y)\n\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n            nab = paddle.abs(delta_u) - delta_u\n            lam = (0.1 * nab) * relu + 1\n            y_momentum = (rho_v__t + u2__x + u3__y) / lam\n            return y_momentum\n\n        self.add_equation(\"y_momentum\", y_momentum_compute_func)\n\n        def energy_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            t, x, y = out[\"t\"], out[\"x\"], out[\"y\"]\n            u, v, p, rho = out[\"u\"], out[\"v\"], out[\"p\"], out[\"rho\"]\n            e1 = (rho * 0.5 * (u**2 + v**2) + 3.5 * p) * u\n            e2 = (rho * 0.5 * (u**2 + v**2) + 3.5 * p) * v\n            e = rho * 0.5 * (u**2 + v**2) + p / 0.4\n\n            e1__x = jacobian(e1, x)\n            e2__y = jacobian(e2, y)\n            e__t = jacobian(e, t)\n\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n            nab = paddle.abs(delta_u) - delta_u\n            lam = (0.1 * nab) * relu + 1\n            energy = (e__t + e1__x + e2__y) / lam\n            return energy\n\n        self.add_equation(\"energy\", energy_compute_func)\n\n\nclass BC_EQ(equation.PDE):\n    def __init__(self):\n        super().__init__()\n        # HACK: solver will be added here for tracking run-time epoch to\n        # compute loss factor `relu` dynamically.\n        self.solver: ppsci.solver.Solver = None\n\n        def item1_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            x, y = out[\"x\"], out[\"y\"]\n            u, v = out[\"u\"], out[\"v\"]\n            sin, cos = out[\"sin\"], out[\"cos\"]\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n\n            lam = 0.1 * (paddle.abs(delta_u) - delta_u) * relu + 1\n            item1 = (u * cos + v * sin) / lam\n\n            return item1\n\n        self.add_equation(\"item1\", item1_compute_func)\n\n        def item2_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            x, y = out[\"x\"], out[\"y\"]\n            u, v, p = out[\"u\"], out[\"v\"], out[\"p\"]\n            sin, cos = out[\"sin\"], out[\"cos\"]\n            p__x = jacobian(p, x)\n            p__y = jacobian(p, y)\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n\n            lam = 0.1 * (paddle.abs(delta_u) - delta_u) * relu + 1\n            item2 = (p__x * cos + p__y * sin) / lam\n\n            return item2\n\n        self.add_equation(\"item2\", item2_compute_func)\n\n        def item3_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            x, y = out[\"x\"], out[\"y\"]\n            u, v, rho = out[\"u\"], out[\"v\"], out[\"rho\"]\n            sin, cos = out[\"sin\"], out[\"cos\"]\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            rho__x = jacobian(rho, x)\n            rho__y = jacobian(rho, y)\n            delta_u = u__x + v__y\n\n            lam = 0.1 * (paddle.abs(delta_u) - delta_u) * relu + 1\n            item3 = (rho__x * cos + rho__y * sin) / lam\n\n            return item3\n\n        self.add_equation(\"item3\", item3_compute_func)\n</code></pre> <pre><code># set equation\nequation = {\"Euler2D\": Euler2D(), \"BC_EQ\": BC_EQ()}\n</code></pre>"},{"location":"zh/examples/shock_wave/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u7684\u8ba1\u7b97\u57df\u4e3a 0 ~ 0.4 \u5355\u4f4d\u65f6\u95f4\uff0c\u957f\u4e3a 1.5\uff0c\u5bbd\u4e3a 2.0 \u7684\u957f\u65b9\u5f62\u533a\u57df\uff0c\u5176\u5185\u542b\u6709\u4e00\u4e2a\u5706\u5fc3\u5750\u6807\u4e3a [1, 1]\uff0c\u534a\u5f84\u4e3a 0.25 \u7684\u5706\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a</p> <pre><code>MA: 2.0\n\n# set hyper-parameters\nLt: 0.4\nLx: 1.5\nLy: 2.0\nrx: 1.0\nry: 1.0\nrd: 0.25\nN_INTERIOR: 100000\nN_BOUNDARY: 10000\nRHO1: 2.112\nP1: 3.001\n</code></pre>"},{"location":"zh/examples/shock_wave/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":""},{"location":"zh/examples/shock_wave/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u6211\u4eec\u5c06\u6b27\u62c9\u65b9\u7a0b\u65bd\u52a0\u5728\u8ba1\u7b97\u57df\u7684\u5185\u90e8\u70b9\u4e0a\uff0c\u5e76\u4e14\u4f7f\u7528\u62c9\u4e01\u8d85\u7acb\u65b9(Latin HyperCube Sampling, LHS)\u65b9\u6cd5\u91c7\u6837\u5171 <code>N_INTERIOR</code> \u4e2a\u8bad\u7ec3\u70b9\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>ry: 1.0\n</code></pre> <pre><code># Latin HyperCube Sampling\n# generate PDE data\nxlimits = np.array([[0.0, 0.0, 0.0], [cfg.Lt, cfg.Lx, cfg.Ly]]).T\ndoe_lhs = lhs.LHS(cfg.N_INTERIOR, xlimits)\nx_int_train = doe_lhs.get_sample()\nx_int_train = x_int_train[\n    ~(\n        (x_int_train[:, 1] - cfg.rx) ** 2 + (x_int_train[:, 2] - cfg.ry) ** 2\n        &lt; cfg.rd**2\n    )\n]\nx_int_train_dict = misc.convert_to_dict(x_int_train, cfg.MODEL.input_keys)\n\ny_int_train = np.zeros([len(x_int_train), len(cfg.MODEL.output_keys)], dtype)\ny_int_train_dict = misc.convert_to_dict(\n    y_int_train, tuple(equation[\"Euler2D\"].equations.keys())\n)\n</code></pre> <pre><code># set constraints\npde_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableNamedArrayDataset\",\n            \"input\": x_int_train_dict,\n            \"label\": y_int_train_dict,\n        },\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    output_expr=equation[\"Euler2D\"].equations,\n    name=\"PDE\",\n)\n</code></pre>"},{"location":"zh/examples/shock_wave/#342","title":"3.4.2 \u8fb9\u754c\u7ea6\u675f","text":"<p>\u6211\u4eec\u5c06\u8fb9\u754c\u6761\u4ef6\u65bd\u52a0\u5728\u8ba1\u7b97\u57df\u7684\u8fb9\u754c\u70b9\u4e0a\uff0c\u540c\u6837\u4f7f\u7528\u62c9\u4e01\u8d85\u7acb\u65b9(Latin HyperCube Sampling, LHS)\u65b9\u6cd5\u5728\u8fb9\u754c\u4e0a\u91c7\u6837\u5171 <code>N_BOUNDARY</code> \u4e2a\u8bad\u7ec3\u70b9\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>rd: 0.25\n</code></pre> <pre><code># generate BC data(left, right side)\nxlimits = np.array([[0.0, 0.0, 0.0], [cfg.Lt, 0.0, cfg.Ly]]).T\ndoe_lhs = lhs.LHS(cfg.N_BOUNDARY, xlimits)\nx_bcL_train = doe_lhs.get_sample()\nx_bcL_train_dict = misc.convert_to_dict(x_bcL_train, cfg.MODEL.input_keys)\n\nu_bcL_train, v_bcL_train, p_bcL_train, rho_bcL_train = generate_bc_left_points(\n    x_bcL_train, cfg.MA, cfg.RHO1, cfg.P1, cfg.V1, cfg.GAMMA\n)\ny_bcL_train = np.concatenate(\n    [\n        u_bcL_train,\n        v_bcL_train,\n        p_bcL_train,\n        rho_bcL_train,\n    ],\n    axis=1,\n)\ny_bcL_train_dict = misc.convert_to_dict(\n    y_bcL_train,\n    tuple(model.output_keys),\n)\n\nx_bcI_train, sin_bcI_train, cos_bcI_train = generate_bc_down_circle_points(\n    cfg.Lt, cfg.rx, cfg.ry, cfg.rd, cfg.N_BOUNDARY\n)\nx_bcI_train_dict = misc.convert_to_dict(\n    np.concatenate([x_bcI_train, sin_bcI_train, cos_bcI_train], axis=1),\n    cfg.MODEL.input_keys + [\"sin\", \"cos\"],\n)\ny_bcI_train_dict = misc.convert_to_dict(\n    np.zeros((len(x_bcI_train), 3), dtype),\n    (\"item1\", \"item2\", \"item3\"),\n)\n</code></pre> <pre><code>bcI_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableNamedArrayDataset\",\n            \"input\": x_bcI_train_dict,\n            \"label\": y_bcI_train_dict,\n        },\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    },\n    ppsci.loss.MSELoss(\"mean\", weight=10),\n    output_expr=equation[\"BC_EQ\"].equations,\n    name=\"BCI\",\n)\nbcL_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableNamedArrayDataset\",\n            \"input\": x_bcL_train_dict,\n            \"label\": y_bcL_train_dict,\n        },\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    },\n    ppsci.loss.MSELoss(\"mean\", weight=10),\n    name=\"BCL\",\n)\n</code></pre>"},{"location":"zh/examples/shock_wave/#343","title":"3.4.3 \u521d\u503c\u7ea6\u675f","text":"<p>\u6211\u4eec\u5c06\u8fb9\u754c\u6761\u4ef6\u65bd\u52a0\u5728\u8ba1\u7b97\u57df\u7684\u521d\u59cb\u65f6\u523b\u7684\u70b9\u4e0a\uff0c\u540c\u6837\u4f7f\u7528\u62c9\u4e01\u8d85\u7acb\u65b9(Latin HyperCube Sampling, LHS)\u65b9\u6cd5\u5728\u521d\u59cb\u65f6\u523b\u7684\u8ba1\u7b97\u57df\u5185\u91c7\u6837\u5171 <code>N_BOUNDARY</code> \u4e2a\u8bad\u7ec3\u70b9\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code># generate IC data\nxlimits = np.array([[0.0, 0.0, 0.0], [0.0, cfg.Lx, cfg.Ly]]).T\ndoe_lhs = lhs.LHS(cfg.N_BOUNDARY, xlimits)\nx_ic_train = doe_lhs.get_sample()\nx_ic_train = x_ic_train[\n    ~(\n        (x_ic_train[:, 1] - cfg.rx) ** 2 + (x_ic_train[:, 2] - cfg.ry) ** 2\n        &lt; cfg.rd**2\n    )\n]\nx_ic_train_dict = misc.convert_to_dict(x_ic_train, cfg.MODEL.input_keys)\nU1 = np.sqrt(cfg.GAMMA * cfg.P1 / cfg.RHO1) * cfg.MA\ny_ic_train = np.concatenate(\n    [\n        np.full([len(x_ic_train), 1], U1, dtype),\n        np.full([len(x_ic_train), 1], 0, dtype),\n        np.full([len(x_ic_train), 1], cfg.P1, dtype),\n        np.full([len(x_ic_train), 1], cfg.RHO1, dtype),\n    ],\n    axis=1,\n)\ny_ic_train_dict = misc.convert_to_dict(\n    y_ic_train,\n    model.output_keys,\n)\n</code></pre> <pre><code>ic_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"IterableNamedArrayDataset\",\n            \"input\": x_ic_train_dict,\n            \"label\": y_ic_train_dict,\n        },\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    },\n    ppsci.loss.MSELoss(\"mean\", weight=10),\n    name=\"IC\",\n)\n</code></pre> <p>\u5728\u4ee5\u4e0a\u4e09\u4e2a\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u9700\u8981\u5c06\u4ed6\u4eec\u5305\u88c5\u6210\u4e00\u4e2a\u5b57\u5178\uff0c\u65b9\u4fbf\u540e\u7eed\u4f5c\u4e3a\u53c2\u6570\u4f20\u9012</p> <pre><code>constraint = {\n    pde_constraint.name: pde_constraint,\n    ic_constraint.name: ic_constraint,\n    bcI_constraint.name: bcI_constraint,\n    bcL_constraint.name: bcL_constraint,\n}\n</code></pre>"},{"location":"zh/examples/shock_wave/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 100 \u8f6e\u8bad\u7ec3\u8f6e\u6570\u3002</p> <pre><code># training settings\n</code></pre>"},{"location":"zh/examples/shock_wave/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9 <code>L-BFGS</code> \u4f18\u5316\u5668\u5e76\u8bbe\u5b9a <code>max_iter</code> \u4e3a 100\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.LBFGS(\n    cfg.TRAIN.learning_rate, max_iter=cfg.TRAIN.max_iter\n)(model)\n</code></pre>"},{"location":"zh/examples/shock_wave/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    None,\n    cfg.TRAIN.epochs,\n    cfg.TRAIN.iters_per_epoch,\n    save_freq=cfg.TRAIN.save_freq,\n    log_freq=cfg.log_freq,\n    seed=cfg.seed,\n    equation=equation,\n    pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n</code></pre> <p>\u672c\u6848\u4f8b\u9700\u8981\u6839\u636e\u6bcf\u4e00\u8f6e\u8bad\u7ec3\u7684 epoch \u503c\uff0c\u8ba1\u7b97PDE\u3001BC\u65b9\u7a0b\u5185\u7684\u6743\u91cd\u7cfb\u6570 <code>relu</code>\u3002\u56e0\u6b64\u5728 solver \u5b9e\u4f8b\u5316\u5b8c\u6bd5\u4e4b\u540e\uff0c\u9700\u989d\u5916\u5c06\u5176\u4f20\u9012\u7ed9\u65b9\u7a0b\u672c\u8eab\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># HACK: Given entire solver to euaqtion object for tracking run-time epoch\n# to compute factor `relu` dynamically.\nequation[\"Euler2D\"].solver = solver\nequation[\"BC_EQ\"].solver = solver\n</code></pre> <p>\u6700\u540e\u542f\u52a8\u8bad\u7ec3\u5373\u53ef\uff1a</p> <pre><code># train model\nsolver.train()\n</code></pre> <p>\u8bad\u7ec3\u5b8c\u6bd5\u540e\uff0c\u6211\u4eec\u53ef\u89c6\u5316\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u8ba1\u7b97\u57df\u5185\u8fa8\u7387\u4e3a 600x600 \u7684\u6fc0\u6ce2\uff0c\u5171 360000 \u4e2a\u70b9\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># visualize prediction\nt = np.linspace(cfg.T, cfg.T, 1)\nx = np.linspace(0.0, cfg.Lx, cfg.Nd)\ny = np.linspace(0.0, cfg.Ly, cfg.Nd)\n_, x_grid, y_grid = np.meshgrid(t, x, y)\n\nx_test = misc.cartesian_product(t, x, y)\nx_test_dict = misc.convert_to_dict(\n    x_test,\n    cfg.MODEL.input_keys,\n)\n\noutput_dict = solver.predict(x_test_dict, return_numpy=True)\nu, v, p, rho = (\n    output_dict[\"u\"],\n    output_dict[\"v\"],\n    output_dict[\"p\"],\n    output_dict[\"rho\"],\n)\n\nzero_mask = (\n    (x_test[:, 1] - cfg.rx) ** 2 + (x_test[:, 2] - cfg.ry) ** 2\n) &lt; cfg.rd**2\nu[zero_mask] = 0\nv[zero_mask] = 0\np[zero_mask] = 0\nrho[zero_mask] = 0\n\nu = u.reshape(cfg.Nd, cfg.Nd)\nv = v.reshape(cfg.Nd, cfg.Nd)\np = p.reshape(cfg.Nd, cfg.Nd)\nrho = rho.reshape(cfg.Nd, cfg.Nd)\n\nfig, ax = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(15, 15))\n\nplt.subplot(2, 2, 1)\nplt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], u * 241.315, 60)\nplt.title(\"U m/s\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\naxe = plt.gca()\naxe.set_aspect(1)\nplt.colorbar()\n\nplt.subplot(2, 2, 2)\nplt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], v * 241.315, 60)\nplt.title(\"V m/s\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\naxe = plt.gca()\naxe.set_aspect(1)\nplt.colorbar()\n\nplt.subplot(2, 2, 3)\nplt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], p * 33775, 60)\nplt.title(\"P Pa\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\naxe = plt.gca()\naxe.set_aspect(1)\nplt.colorbar()\n\nplt.subplot(2, 2, 4)\nplt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], rho * 0.58, 60)\nplt.title(\"Rho kg/m^3\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\naxe = plt.gca()\naxe.set_aspect(1)\nplt.colorbar()\n\nplt.savefig(osp.join(cfg.output_dir, f\"shock_wave(Ma_{cfg.MA:.3f}).png\"))\n</code></pre>"},{"location":"zh/examples/shock_wave/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"shock_wave.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\n\nimport hydra\nimport lhs\nimport numpy as np\nimport paddle\nfrom matplotlib import pyplot as plt\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci import equation\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\nfrom ppsci.utils import misc\n\n\nclass Euler2D(equation.PDE):\n    def __init__(self):\n        super().__init__()\n        # HACK: solver will be added here for tracking run-time epoch to\n        # compute loss factor `relu` dynamically.\n        self.solver: ppsci.solver.Solver = None\n\n        def continuity_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            t, x, y = out[\"t\"], out[\"x\"], out[\"y\"]\n            u, v, rho = out[\"u\"], out[\"v\"], out[\"rho\"]\n            rho__t = jacobian(rho, t)\n            rho_u = rho * u\n            rho_v = rho * v\n            rho_u__x = jacobian(rho_u, x)\n            rho_v__y = jacobian(rho_v, y)\n\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n            nab = paddle.abs(delta_u) - delta_u\n            lam = (0.1 * nab) * relu + 1\n            continuity = (rho__t + rho_u__x + rho_v__y) / lam\n            return continuity\n\n        self.add_equation(\"continuity\", continuity_compute_func)\n\n        def x_momentum_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            t, x, y = out[\"t\"], out[\"x\"], out[\"y\"]\n            u, v, p, rho = out[\"u\"], out[\"v\"], out[\"p\"], out[\"rho\"]\n            rho_u = rho * u\n            rho_u__t = jacobian(rho_u, t)\n\n            u1 = rho * u**2 + p\n            u2 = rho * u * v\n            u1__x = jacobian(u1, x)\n            u2__y = jacobian(u2, y)\n\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n            nab = paddle.abs(delta_u) - delta_u\n            lam = (0.1 * nab) * relu + 1\n            x_momentum = (rho_u__t + u1__x + u2__y) / lam\n            return x_momentum\n\n        self.add_equation(\"x_momentum\", x_momentum_compute_func)\n\n        def y_momentum_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            t, x, y = out[\"t\"], out[\"x\"], out[\"y\"]\n            u, v, p, rho = out[\"u\"], out[\"v\"], out[\"p\"], out[\"rho\"]\n            rho_v = rho * v\n            rho_v__t = jacobian(rho_v, t)\n\n            u2 = rho * u * v\n            u3 = rho * v**2 + p\n            u2__x = jacobian(u2, x)\n            u3__y = jacobian(u3, y)\n\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n            nab = paddle.abs(delta_u) - delta_u\n            lam = (0.1 * nab) * relu + 1\n            y_momentum = (rho_v__t + u2__x + u3__y) / lam\n            return y_momentum\n\n        self.add_equation(\"y_momentum\", y_momentum_compute_func)\n\n        def energy_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            t, x, y = out[\"t\"], out[\"x\"], out[\"y\"]\n            u, v, p, rho = out[\"u\"], out[\"v\"], out[\"p\"], out[\"rho\"]\n            e1 = (rho * 0.5 * (u**2 + v**2) + 3.5 * p) * u\n            e2 = (rho * 0.5 * (u**2 + v**2) + 3.5 * p) * v\n            e = rho * 0.5 * (u**2 + v**2) + p / 0.4\n\n            e1__x = jacobian(e1, x)\n            e2__y = jacobian(e2, y)\n            e__t = jacobian(e, t)\n\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n            nab = paddle.abs(delta_u) - delta_u\n            lam = (0.1 * nab) * relu + 1\n            energy = (e__t + e1__x + e2__y) / lam\n            return energy\n\n        self.add_equation(\"energy\", energy_compute_func)\n\n\nclass BC_EQ(equation.PDE):\n    def __init__(self):\n        super().__init__()\n        # HACK: solver will be added here for tracking run-time epoch to\n        # compute loss factor `relu` dynamically.\n        self.solver: ppsci.solver.Solver = None\n\n        def item1_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            x, y = out[\"x\"], out[\"y\"]\n            u, v = out[\"u\"], out[\"v\"]\n            sin, cos = out[\"sin\"], out[\"cos\"]\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n\n            lam = 0.1 * (paddle.abs(delta_u) - delta_u) * relu + 1\n            item1 = (u * cos + v * sin) / lam\n\n            return item1\n\n        self.add_equation(\"item1\", item1_compute_func)\n\n        def item2_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            x, y = out[\"x\"], out[\"y\"]\n            u, v, p = out[\"u\"], out[\"v\"], out[\"p\"]\n            sin, cos = out[\"sin\"], out[\"cos\"]\n            p__x = jacobian(p, x)\n            p__y = jacobian(p, y)\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            delta_u = u__x + v__y\n\n            lam = 0.1 * (paddle.abs(delta_u) - delta_u) * relu + 1\n            item2 = (p__x * cos + p__y * sin) / lam\n\n            return item2\n\n        self.add_equation(\"item2\", item2_compute_func)\n\n        def item3_compute_func(out):\n            relu = max(\n                0.0,\n                (self.solver.global_step // self.solver.iters_per_epoch + 1)\n                / self.solver.epochs\n                - 0.05,\n            )\n            x, y = out[\"x\"], out[\"y\"]\n            u, v, rho = out[\"u\"], out[\"v\"], out[\"rho\"]\n            sin, cos = out[\"sin\"], out[\"cos\"]\n            u__x = jacobian(u, x)\n            v__y = jacobian(v, y)\n            rho__x = jacobian(rho, x)\n            rho__y = jacobian(rho, y)\n            delta_u = u__x + v__y\n\n            lam = 0.1 * (paddle.abs(delta_u) - delta_u) * relu + 1\n            item3 = (rho__x * cos + rho__y * sin) / lam\n\n            return item3\n\n        self.add_equation(\"item3\", item3_compute_func)\n\n\ndtype = paddle.get_default_dtype()\n\n\ndef generate_bc_down_circle_points(t: float, xc: float, yc: float, r: float, n: int):\n    rand_arr1 = np.random.randn(n, 1).astype(dtype)\n    theta = 2 * np.pi * rand_arr1\n    cos = np.cos(np.pi / 2 + theta)\n    sin = np.sin(np.pi / 2 + theta)\n\n    rand_arr2 = np.random.randn(n, 1).astype(dtype)\n    x = np.concatenate([rand_arr2 * t, xc + cos * r, yc + sin * r], axis=1)\n\n    return x, sin, cos\n\n\ndef generate_bc_left_points(\n    x: np.ndarray, Ma: float, rho1: float, p1: float, v1: float, gamma: float\n):\n    u1: float = np.sqrt(gamma * p1 / rho1) * Ma\n    u_init = np.full((x.shape[0], 1), u1, dtype)\n    v_init = np.full((x.shape[0], 1), v1, dtype)\n    p_init = np.full((x.shape[0], 1), p1, dtype)\n    rho_init = np.full((x.shape[0], 1), rho1, dtype)\n\n    return u_init, v_init, p_init, rho_init\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"Euler2D\": Euler2D(), \"BC_EQ\": BC_EQ()}\n\n    # Latin HyperCube Sampling\n    # generate PDE data\n    xlimits = np.array([[0.0, 0.0, 0.0], [cfg.Lt, cfg.Lx, cfg.Ly]]).T\n    doe_lhs = lhs.LHS(cfg.N_INTERIOR, xlimits)\n    x_int_train = doe_lhs.get_sample()\n    x_int_train = x_int_train[\n        ~(\n            (x_int_train[:, 1] - cfg.rx) ** 2 + (x_int_train[:, 2] - cfg.ry) ** 2\n            &lt; cfg.rd**2\n        )\n    ]\n    x_int_train_dict = misc.convert_to_dict(x_int_train, cfg.MODEL.input_keys)\n\n    y_int_train = np.zeros([len(x_int_train), len(cfg.MODEL.output_keys)], dtype)\n    y_int_train_dict = misc.convert_to_dict(\n        y_int_train, tuple(equation[\"Euler2D\"].equations.keys())\n    )\n\n    # generate BC data(left, right side)\n    xlimits = np.array([[0.0, 0.0, 0.0], [cfg.Lt, 0.0, cfg.Ly]]).T\n    doe_lhs = lhs.LHS(cfg.N_BOUNDARY, xlimits)\n    x_bcL_train = doe_lhs.get_sample()\n    x_bcL_train_dict = misc.convert_to_dict(x_bcL_train, cfg.MODEL.input_keys)\n\n    u_bcL_train, v_bcL_train, p_bcL_train, rho_bcL_train = generate_bc_left_points(\n        x_bcL_train, cfg.MA, cfg.RHO1, cfg.P1, cfg.V1, cfg.GAMMA\n    )\n    y_bcL_train = np.concatenate(\n        [\n            u_bcL_train,\n            v_bcL_train,\n            p_bcL_train,\n            rho_bcL_train,\n        ],\n        axis=1,\n    )\n    y_bcL_train_dict = misc.convert_to_dict(\n        y_bcL_train,\n        tuple(model.output_keys),\n    )\n\n    x_bcI_train, sin_bcI_train, cos_bcI_train = generate_bc_down_circle_points(\n        cfg.Lt, cfg.rx, cfg.ry, cfg.rd, cfg.N_BOUNDARY\n    )\n    x_bcI_train_dict = misc.convert_to_dict(\n        np.concatenate([x_bcI_train, sin_bcI_train, cos_bcI_train], axis=1),\n        cfg.MODEL.input_keys + [\"sin\", \"cos\"],\n    )\n    y_bcI_train_dict = misc.convert_to_dict(\n        np.zeros((len(x_bcI_train), 3), dtype),\n        (\"item1\", \"item2\", \"item3\"),\n    )\n\n    # generate IC data\n    xlimits = np.array([[0.0, 0.0, 0.0], [0.0, cfg.Lx, cfg.Ly]]).T\n    doe_lhs = lhs.LHS(cfg.N_BOUNDARY, xlimits)\n    x_ic_train = doe_lhs.get_sample()\n    x_ic_train = x_ic_train[\n        ~(\n            (x_ic_train[:, 1] - cfg.rx) ** 2 + (x_ic_train[:, 2] - cfg.ry) ** 2\n            &lt; cfg.rd**2\n        )\n    ]\n    x_ic_train_dict = misc.convert_to_dict(x_ic_train, cfg.MODEL.input_keys)\n    U1 = np.sqrt(cfg.GAMMA * cfg.P1 / cfg.RHO1) * cfg.MA\n    y_ic_train = np.concatenate(\n        [\n            np.full([len(x_ic_train), 1], U1, dtype),\n            np.full([len(x_ic_train), 1], 0, dtype),\n            np.full([len(x_ic_train), 1], cfg.P1, dtype),\n            np.full([len(x_ic_train), 1], cfg.RHO1, dtype),\n        ],\n        axis=1,\n    )\n    y_ic_train_dict = misc.convert_to_dict(\n        y_ic_train,\n        model.output_keys,\n    )\n\n    # set constraints\n    pde_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableNamedArrayDataset\",\n                \"input\": x_int_train_dict,\n                \"label\": y_int_train_dict,\n            },\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        output_expr=equation[\"Euler2D\"].equations,\n        name=\"PDE\",\n    )\n    ic_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableNamedArrayDataset\",\n                \"input\": x_ic_train_dict,\n                \"label\": y_ic_train_dict,\n            },\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        },\n        ppsci.loss.MSELoss(\"mean\", weight=10),\n        name=\"IC\",\n    )\n    bcI_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableNamedArrayDataset\",\n                \"input\": x_bcI_train_dict,\n                \"label\": y_bcI_train_dict,\n            },\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        },\n        ppsci.loss.MSELoss(\"mean\", weight=10),\n        output_expr=equation[\"BC_EQ\"].equations,\n        name=\"BCI\",\n    )\n    bcL_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"IterableNamedArrayDataset\",\n                \"input\": x_bcL_train_dict,\n                \"label\": y_bcL_train_dict,\n            },\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        },\n        ppsci.loss.MSELoss(\"mean\", weight=10),\n        name=\"BCL\",\n    )\n    constraint = {\n        pde_constraint.name: pde_constraint,\n        ic_constraint.name: ic_constraint,\n        bcI_constraint.name: bcI_constraint,\n        bcL_constraint.name: bcL_constraint,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.LBFGS(\n        cfg.TRAIN.learning_rate, max_iter=cfg.TRAIN.max_iter\n    )(model)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        None,\n        cfg.TRAIN.epochs,\n        cfg.TRAIN.iters_per_epoch,\n        save_freq=cfg.TRAIN.save_freq,\n        log_freq=cfg.log_freq,\n        seed=cfg.seed,\n        equation=equation,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # HACK: Given entire solver to euaqtion object for tracking run-time epoch\n    # to compute factor `relu` dynamically.\n    equation[\"Euler2D\"].solver = solver\n    equation[\"BC_EQ\"].solver = solver\n\n    # train model\n    solver.train()\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        seed=cfg.seed,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n    )\n\n    # visualize prediction\n    t = np.linspace(cfg.T, cfg.T, 1)\n    x = np.linspace(0.0, cfg.Lx, cfg.Nd)\n    y = np.linspace(0.0, cfg.Ly, cfg.Nd)\n    _, x_grid, y_grid = np.meshgrid(t, x, y)\n\n    x_test = misc.cartesian_product(t, x, y)\n    x_test_dict = misc.convert_to_dict(\n        x_test,\n        cfg.MODEL.input_keys,\n    )\n\n    output_dict = solver.predict(x_test_dict, return_numpy=True)\n    u, v, p, rho = (\n        output_dict[\"u\"],\n        output_dict[\"v\"],\n        output_dict[\"p\"],\n        output_dict[\"rho\"],\n    )\n\n    zero_mask = (\n        (x_test[:, 1] - cfg.rx) ** 2 + (x_test[:, 2] - cfg.ry) ** 2\n    ) &lt; cfg.rd**2\n    u[zero_mask] = 0\n    v[zero_mask] = 0\n    p[zero_mask] = 0\n    rho[zero_mask] = 0\n\n    u = u.reshape(cfg.Nd, cfg.Nd)\n    v = v.reshape(cfg.Nd, cfg.Nd)\n    p = p.reshape(cfg.Nd, cfg.Nd)\n    rho = rho.reshape(cfg.Nd, cfg.Nd)\n\n    fig, ax = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(15, 15))\n\n    plt.subplot(2, 2, 1)\n    plt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], u * 241.315, 60)\n    plt.title(\"U m/s\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    axe = plt.gca()\n    axe.set_aspect(1)\n    plt.colorbar()\n\n    plt.subplot(2, 2, 2)\n    plt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], v * 241.315, 60)\n    plt.title(\"V m/s\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    axe = plt.gca()\n    axe.set_aspect(1)\n    plt.colorbar()\n\n    plt.subplot(2, 2, 3)\n    plt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], p * 33775, 60)\n    plt.title(\"P Pa\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    axe = plt.gca()\n    axe.set_aspect(1)\n    plt.colorbar()\n\n    plt.subplot(2, 2, 4)\n    plt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], rho * 0.58, 60)\n    plt.title(\"Rho kg/m^3\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    axe = plt.gca()\n    axe.set_aspect(1)\n    plt.colorbar()\n\n    plt.savefig(osp.join(cfg.output_dir, f\"shock_wave(Ma_{cfg.MA:.3f}).png\"))\n\n\ndef export(cfg: DictConfig):\n    from paddle.static import InputSpec\n\n    # set models\n    model = ppsci.arch.MLP(**cfg.MODEL)\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n\n    # export models\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    # set model predictor\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # visualize prediction\n    t = np.linspace(cfg.T, cfg.T, 1, dtype=np.float32)\n    x = np.linspace(0.0, cfg.Lx, cfg.Nd, dtype=np.float32)\n    y = np.linspace(0.0, cfg.Ly, cfg.Nd, dtype=np.float32)\n    _, x_grid, y_grid = np.meshgrid(t, x, y)\n\n    x_test = misc.cartesian_product(t, x, y)\n    x_test_dict = misc.convert_to_dict(\n        x_test,\n        cfg.MODEL.input_keys,\n    )\n    output_dict = predictor.predict(\n        x_test_dict,\n        cfg.INFER.batch_size,\n    )\n\n    # mapping data to cfg.MODEL.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n\n    u, v, p, rho = (\n        output_dict[\"u\"],\n        output_dict[\"v\"],\n        output_dict[\"p\"],\n        output_dict[\"rho\"],\n    )\n\n    zero_mask = (\n        (x_test[:, 1] - cfg.rx) ** 2 + (x_test[:, 2] - cfg.ry) ** 2\n    ) &lt; cfg.rd**2\n    u[zero_mask] = 0\n    v[zero_mask] = 0\n    p[zero_mask] = 0\n    rho[zero_mask] = 0\n\n    u = u.reshape(cfg.Nd, cfg.Nd)\n    v = v.reshape(cfg.Nd, cfg.Nd)\n    p = p.reshape(cfg.Nd, cfg.Nd)\n    rho = rho.reshape(cfg.Nd, cfg.Nd)\n\n    fig, ax = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(15, 15))\n\n    plt.subplot(2, 2, 1)\n    plt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], u * 241.315, 60)\n    plt.title(\"U m/s\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    axe = plt.gca()\n    axe.set_aspect(1)\n    plt.colorbar()\n\n    plt.subplot(2, 2, 2)\n    plt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], v * 241.315, 60)\n    plt.title(\"V m/s\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    axe = plt.gca()\n    axe.set_aspect(1)\n    plt.colorbar()\n\n    plt.subplot(2, 2, 3)\n    plt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], p * 33775, 60)\n    plt.title(\"P Pa\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    axe = plt.gca()\n    axe.set_aspect(1)\n    plt.colorbar()\n\n    plt.subplot(2, 2, 4)\n    plt.contourf(x_grid[:, 0, :], y_grid[:, 0, :], rho * 0.58, 60)\n    plt.title(\"Rho kg/m^3\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    axe = plt.gca()\n    axe.set_aspect(1)\n    plt.colorbar()\n\n    plt.savefig(osp.join(cfg.output_dir, f\"shock_wave(Ma_{cfg.MA:.3f}).png\"))\n\n\n@hydra.main(\n    version_base=None, config_path=\"./conf\", config_name=\"shock_wave_Ma2.0.yaml\"\n)\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/shock_wave/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u672c\u6848\u4f8b\u9488\u5bf9 \\(Ma=2.0\\) \u548c \\(Ma=0.728\\) \u4e24\u79cd\u4e0d\u540c\u7684\u53c2\u6570\u914d\u7f6e\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u5982\u4e0b\u6240\u793a</p> Ma=2.0Ma=0.728 <p>  Ma=2.0\u65f6\uff0cx\u65b9\u5411\u901f\u5ea6u\u3001y\u65b9\u5411\u901f\u5ea6v\u3001\u538b\u529bp\u3001\u5bc6\u5ea6rho\u7684\u9884\u6d4b\u7ed3\u679c </p> <p>  Ma=0.728\u65f6\uff0cx\u65b9\u5411\u901f\u5ea6u\u3001y\u65b9\u5411\u901f\u5ea6v\u3001\u538b\u529bp\u3001\u5bc6\u5ea6rho\u7684\u9884\u6d4b\u7ed3\u679c </p>"},{"location":"zh/examples/shock_wave/#6","title":"6. \u53c2\u8003\u8d44\u6599","text":"<ul> <li>Compressible PINN - AIStudio</li> <li>Discontinuity computing with physics-informed neural network</li> </ul>"},{"location":"zh/examples/tempoGAN/","title":"tempoGAN","text":""},{"location":"zh/examples/tempoGAN/#tempogantemporally-generative-adversarial-networks","title":"tempoGAN(temporally Generative Adversarial Networks)","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_train.mat -P datasets/tempoGAN/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_valid.mat -P datasets/tempoGAN/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_train.mat --create-dirs -o ./datasets/tempoGAN/2d_train.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_valid.mat --create-dirs -o ./datasets/tempoGAN/2d_valid.mat\npython tempoGAN.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_train.mat -P datasets/tempoGAN/\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_valid.mat -P datasets/tempoGAN/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_train.mat --create-dirs -o ./datasets/tempoGAN/2d_train.mat\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_valid.mat --create-dirs -o ./datasets/tempoGAN/2d_valid.mat\npython tempoGAN.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/tempoGAN/tempogan_pretrained.pdparams\n</code></pre> <pre><code>python tempoGAN.py mode=export\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_valid.mat -P datasets/tempoGAN/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/tempoGAN/2d_valid.mat --create-dirs -o ./datasets/tempoGAN/2d_valid.mat\npython tempoGAN.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 tempogan_pretrained.pdparams MSE: 4.21e-5PSNR: 47.19SSIM: 0.9974"},{"location":"zh/examples/tempoGAN/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6d41\u4f53\u6a21\u62df\u65b9\u9762\u7684\u95ee\u9898\uff0c\u6355\u6349\u6e4d\u6d41\u7684\u590d\u6742\u7ec6\u8282\u4e00\u76f4\u662f\u6570\u503c\u6a21\u62df\u7684\u957f\u671f\u6311\u6218\uff0c\u7528\u79bb\u6563\u6a21\u578b\u89e3\u51b3\u8fd9\u4e9b\u7ec6\u8282\u4f1a\u4ea7\u751f\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5bf9\u4e8e\u4eba\u7c7b\u7a7a\u95f4\u548c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u6d41\u52a8\u6765\u8bf4\uff0c\u5f88\u5feb\u5c31\u4f1a\u53d8\u5f97\u4e0d\u53ef\u884c\u3002\u56e0\u6b64\u6d41\u4f53\u8d85\u5206\u8fa8\u7387\u7684\u9700\u6c42\u5e94\u8fd0\u800c\u751f\uff0c\u5b83\u65e8\u5728\u901a\u8fc7\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u62df\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5c06\u4f4e\u5206\u8fa8\u7387\u6d41\u4f53\u6a21\u62df\u7ed3\u679c\u6062\u590d\u4e3a\u9ad8\u5206\u8fa8\u7387\u7ed3\u679c\uff0c\u4ee5\u51cf\u5c11\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6d41\u4f53\u8fc7\u7a0b\u4e2d\u7684\u5de8\u5927\u8ba1\u7b97\u6210\u672c\u3002\u8be5\u6280\u672f\u53ef\u4ee5\u5e94\u7528\u4e8e\u5404\u79cd\u6d41\u4f53\u6a21\u62df\uff0c\u4f8b\u5982\u6c34\u6d41\u3001\u7a7a\u6c14\u6d41\u52a8\u3001\u706b\u7130\u6a21\u62df\u7b49\u3002</p> <p>\u751f\u6210\u5f0f\u5bf9\u6297\u7f51\u7edc GAN(Generative Adversarial Networks) \u662f\u4e00\u79cd\u4f7f\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0cGAN \u7f51\u7edc\u4e2d\uff08\u81f3\u5c11\uff09\u5305\u542b\u4e24\u4e2a\u6a21\u578b\uff1a\u751f\u6210\u5668(Generator) \u548c\u5224\u522b\u5668(Discriminator)\uff0c\u751f\u6210\u5668\u7528\u4e8e\u751f\u6210\u95ee\u9898\u7684\u8f93\u51fa\uff0c\u5224\u522b\u5668\u7528\u4e8e\u5224\u65ad\u8f93\u51fa\u7684\u771f\u5047\uff0c\u4e24\u8005\u5728\u76f8\u4e92\u535a\u5f08\u4e2d\u5171\u540c\u4f18\u5316\uff0c\u6700\u7ec8\u4f7f\u5f97\u751f\u6210\u5668\u7684\u8f93\u51fa\u63a5\u8fd1\u771f\u5b9e\u503c\u3002</p> <p>tempoGAN \u5728 GAN \u7f51\u7edc\u7684\u57fa\u7840\u4e0a\u65b0\u589e\u4e86\u4e00\u4e2a\u4e0e\u65f6\u95f4\u76f8\u5173\u7684\u5224\u522b\u5668 Discriminator_tempo\uff0c\u8be5\u5224\u522b\u5668\u7684\u7f51\u7edc\u7ed3\u6784\u4e0e\u57fa\u7840\u5224\u522b\u5668\u76f8\u540c\uff0c\u4f46\u8f93\u5165\u4e3a\u65f6\u95f4\u8fde\u7eed\u7684\u51e0\u5e27\u6570\u636e\uff0c\u800c\u4e0d\u662f\u5355\u5e27\u6570\u636e\uff0c\u4ece\u800c\u5c06\u65f6\u5e8f\u7eb3\u5165\u8003\u8651\u8303\u56f4\u3002</p> <p>\u672c\u95ee\u9898\u4e3b\u8981\u4f7f\u7528\u8be5\u7f51\u7edc\uff0c\u901a\u8fc7\u8f93\u5165\u7684\u4f4e\u5bc6\u5ea6\u6d41\u4f53\u6570\u636e\uff0c\u5f97\u5230\u5bf9\u5e94\u7684\u9ad8\u5bc6\u5ea6\u6d41\u4f53\u6570\u636e\uff0c\u5927\u5927\u8282\u7701\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\u3002</p>"},{"location":"zh/examples/tempoGAN/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u95ee\u9898\u5305\u542b\u4e09\u4e2a\u6a21\u578b\uff1a\u751f\u6210\u5668(Generator)\u3001\u5224\u522b\u5668(Discriminator)\u548c\u4e0e\u65f6\u95f4\u76f8\u5173\u7684\u5224\u522b\u5668(Discriminator_tempo)\uff0c\u6839\u636e GAN \u7f51\u7edc\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8fd9\u4e09\u4e2a\u6a21\u578b\u4ea4\u66ff\u8bad\u7ec3\uff0c\u8bad\u7ec3\u987a\u5e8f\u4f9d\u6b21\u4e3a\uff1aDiscriminator\u3001Discriminator_tempo\u3001Generator\u3002 GAN \u7f51\u7edc\u4e3a\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u672c\u95ee\u9898\u7f51\u7edc\u8bbe\u8ba1\u4e2d\u5c06\u76ee\u6807\u503c\u4f5c\u4e3a\u4e00\u4e2a\u8f93\u5165\u503c\uff0c\u8f93\u5165\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\u3002</p>"},{"location":"zh/examples/tempoGAN/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002\u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u7ea6\u675f\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/tempoGAN/#31","title":"3.1 \u6570\u636e\u96c6\u4ecb\u7ecd","text":"<p>\u6570\u636e\u96c6\u4e3a\u4f7f\u7528\u5f00\u6e90\u4ee3\u7801\u5305 mantaflow \u751f\u6210\u7684 2d \u6d41\u4f53\u6570\u636e\u96c6\uff0c\u6570\u636e\u96c6\u4e2d\u5305\u62ec\u4e00\u5b9a\u6570\u91cf\u8fde\u7eed\u5e27\u7684\u4f4e\u3001\u9ad8\u5bc6\u5ea6\u6d41\u4f53\u56fe\u50cf\u8f6c\u5316\u6210\u7684\u6570\u503c\uff0c\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b58\u50a8\u5728 <code>.mat</code> \u6587\u4ef6\u4e2d\u3002</p> <p>\u8fd0\u884c\u672c\u95ee\u9898\u4ee3\u7801\u524d\u8bf7\u4e0b\u8f7d \u8bad\u7ec3\u6570\u636e\u96c6 \u548c \u9a8c\u8bc1\u6570\u636e\u96c6\uff0c \u4e0b\u8f7d\u540e\u5206\u522b\u5b58\u653e\u5728\u8def\u5f84\uff1a</p> <pre><code>output_dir: ${hydra:run.dir}\nlog_freq: 20\n</code></pre>"},{"location":"zh/examples/tempoGAN/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"tempoGAN \u7f51\u7edc\u6a21\u578b <p>\u4e0a\u56fe\u4e3atempoGAN \u5b8c\u6574\u7684\u6a21\u578b\u7ed3\u6784\u56fe\uff0c\u4f46\u672c\u95ee\u9898\u53ea\u9488\u5bf9\u8f83\u4e3a\u7b80\u5355\u7684\u60c5\u51b5\u8fdb\u884c\u5904\u7406\uff0c\u4e0d\u6d89\u53ca\u5305\u542b\u901f\u5ea6\u548c\u6da1\u5ea6\u7684\u8f93\u5165\u30013d\u3001\u6570\u636e\u589e\u5f3a\u3001advection operator \u7b49\u90e8\u5206\uff0c\u5982\u679c\u60a8\u5bf9\u8fd9\u4e9b\u6587\u6863\u4e2d\u672a\u5305\u542b\u7684\u5185\u5bb9\u611f\u5174\u8da3\uff0c\u53ef\u4ee5\u81ea\u884c\u4fee\u6539\u4ee3\u7801\u5e76\u8fdb\u884c\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u3002</p> <p>\u5982\u4e0a\u56fe\u6240\u793a\uff0cGenerator \u7684\u8f93\u5165\u4e3a\u4f4e\u5bc6\u5ea6\u6d41\u4f53\u6570\u636e\u7684\u63d2\u503c\uff0c\u8f93\u51fa\u4e3a\u751f\u6210\u7684\u9ad8\u5bc6\u5ea6\u6d41\u4f53\u6a21\u62df\u6570\u636e\uff0cDiscriminator \u7684\u8f93\u5165\u4e3a\u4f4e\u5bc6\u5ea6\u6d41\u4f53\u6570\u636e\u7684\u63d2\u503c\u5206\u522b\u4e0e Generator \u751f\u6210\u7684\u9ad8\u5bc6\u5ea6\u6d41\u4f53\u6a21\u62df\u6570\u636e\u3001\u76ee\u6807\u9ad8\u5bc6\u5ea6\u6d41\u4f53\u6570\u636e\u7684\u62fc\u63a5\uff0c Discriminator_tempo \u7684\u8f93\u5165\u4e3a\u591a\u5e27\u8fde\u7eed\u7684 Generator \u751f\u6210\u7684\u9ad8\u5bc6\u5ea6\u6d41\u4f53\u6a21\u62df\u6570\u636e\u4ee5\u53ca\u76ee\u6807\u9ad8\u5bc6\u5ea6\u6d41\u4f53\u6570\u636e\u3002</p> <p>\u867d\u7136\u8f93\u5165\u8f93\u51fa\u7684\u7ec4\u6210\u770b\u8d77\u6765\u8f83\u4e3a\u590d\u6742\uff0c\u4f46\u672c\u8d28\u90fd\u662f\u6d41\u4f53\u7684\u5bc6\u5ea6\u6570\u636e\uff0c\u56e0\u6b64 3 \u4e2a\u7f51\u7edc\u7684\u6620\u5c04\u51fd\u6570\u90fd\u662f \\(f: \\mathbb{R}^1 \\to \\mathbb{R}^1\\)\u3002</p> <p>\u4e0e\u7b80\u5355\u7684 MLP \u7f51\u7edc\u4e0d\u540c\uff0c\u6839\u636e\u8981\u89e3\u51b3\u7684\u95ee\u9898\u4e0d\u540c\uff0cGAN \u7684\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u6709\u591a\u79cd\u7f51\u7edc\u7ed3\u6784\u53ef\u4ee5\u9009\u62e9\uff0c\u5728\u6b64\u4e0d\u518d\u8d58\u8ff0\u3002\u7531\u4e8e\u8fd9\u79cd\u72ec\u7279\u6027\uff0c\u672c\u95ee\u9898\u4e2d\u7684 tempoGAN \u7f51\u7edc\u6ca1\u6709\u88ab\u5185\u7f6e\u5728 PaddleScience \u4e2d\uff0c\u9700\u8981\u989d\u5916\u5b9e\u73b0\u3002</p> <p>\u672c\u95ee\u9898\u4e2d\u7684 Generator \u662f\u4e00\u4e2a\u62e5\u6709 4 \u5c42\u6539\u826f Res Block \u7684\u6a21\u578b\uff0cDiscriminator \u548c Discriminator_tempo \u4e3a\u540c\u4e00\u4e2a\u62e5\u6709 4 \u5c42\u5377\u79ef\u7ed3\u679c\u7684\u6a21\u578b\uff0c\u4e24\u8005\u7f51\u7edc\u7ed3\u6784\u76f8\u540c\u4f46\u8f93\u5165\u4e0d\u540c\u3002Generator\u3001Discriminator \u548c Discriminator_tempo \u7684\u7f51\u7edc\u53c2\u6570\u4e5f\u9700\u8981\u989d\u5916\u5b9a\u4e49\u3002</p> <p>\u5177\u4f53\u4ee3\u7801\u8bf7\u53c2\u8003 \u5b8c\u6574\u4ee3\u7801 \u4e2d gan.py \u6587\u4ef6\u3002</p> <p>\u7531\u4e8e GAN \u7f51\u7edc\u4e2d\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7684\u4e2d\u95f4\u7ed3\u679c\u8981\u76f8\u4e92\u8c03\u7528\uff0c\u53c2\u4e0e\u5bf9\u65b9\u7684 loss \u8ba1\u7b97\uff0c\u56e0\u6b64\u4f7f\u7528 Model List \u5b9e\u73b0\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> <pre><code># define Generator model\nmodel_gen = ppsci.arch.Generator(**cfg.MODEL.gen_net)\nmodel_gen.register_input_transform(gen_funcs.transform_in)\ndisc_funcs.model_gen = model_gen\n\nmodel_tuple = (model_gen,)\n# define Discriminators\nif cfg.USE_SPATIALDISC:\n    model_disc = ppsci.arch.Discriminator(**cfg.MODEL.disc_net)\n    model_disc.register_input_transform(disc_funcs.transform_in)\n    model_tuple += (model_disc,)\n\n# define temporal Discriminators\nif cfg.USE_TEMPODISC:\n    model_disc_tempo = ppsci.arch.Discriminator(**cfg.MODEL.tempo_net)\n    model_disc_tempo.register_input_transform(disc_funcs.transform_in_tempo)\n    model_tuple += (model_disc_tempo,)\n\n# define model_list\nmodel_list = ppsci.arch.ModelList(model_tuple)\n</code></pre> <p>\u6ce8\u610f\u5230\u4e0a\u8ff0\u4ee3\u7801\u4e2d\u5b9a\u4e49\u7684\u7f51\u7edc\u8f93\u5165\u4e0e\u5b9e\u9645\u7f51\u7edc\u8f93\u5165\u4e0d\u5b8c\u5168\u4e00\u6837\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u8f93\u5165\u8fdb\u884ctransform\u3002</p>"},{"location":"zh/examples/tempoGAN/#33-transform","title":"3.3 transform\u6784\u5efa","text":"<p>Generator \u7684\u8f93\u5165\u4e3a\u4f4e\u5bc6\u5ea6\u6d41\u4f53\u6570\u636e\u7684\u63d2\u503c\uff0c\u800c\u6570\u636e\u96c6\u4e2d\u4fdd\u5b58\u7684\u4e3a\u539f\u59cb\u7684\u4f4e\u5bc6\u5ea6\u6d41\u4f53\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u884c\u4e00\u4e2a\u63d2\u503c\u7684 transform\u3002</p> <pre><code>def transform_in(self, _in):\n    ratio = 2\n    input_dict = reshape_input(_in)\n    density_low = input_dict[\"density_low\"]\n    density_low_inp = interpolate(density_low, ratio, \"nearest\")\n    return {\"input_gen\": density_low_inp}\n</code></pre> <p>Discriminator \u548c Discriminator_tempo \u5bf9\u8f93\u5165\u7684 transform \u66f4\u4e3a\u590d\u6742\uff0c\u5206\u522b\u4e3a\uff1a</p> <pre><code>def transform_in(self, _in):\n    ratio = 2\n    input_dict = reshape_input(_in)\n    density_low = input_dict[\"density_low\"]\n    density_high_from_target = input_dict[\"density_high\"]\n\n    density_low_inp = interpolate(density_low, ratio, \"nearest\")\n\n    density_high_from_gen = self.model_gen(input_dict)[\"output_gen\"]\n    density_high_from_gen.stop_gradient = True\n\n    density_input_from_target = paddle.concat(\n        [density_low_inp, density_high_from_target], axis=1\n    )\n    density_input_from_gen = paddle.concat(\n        [density_low_inp, density_high_from_gen], axis=1\n    )\n    return {\n        \"input_disc_from_target\": density_input_from_target,\n        \"input_disc_from_gen\": density_input_from_gen,\n    }\n\ndef transform_in_tempo(self, _in):\n    density_high_from_target = _in[\"density_high\"]\n\n    input_dict = reshape_input(_in)\n    density_high_from_gen = self.model_gen(input_dict)[\"output_gen\"]\n    density_high_from_gen.stop_gradient = True\n\n    input_trans = {\n        \"input_tempo_disc_from_target\": density_high_from_target,\n        \"input_tempo_disc_from_gen\": density_high_from_gen,\n    }\n\n    return dereshape_input(input_trans, 3)\n</code></pre> <p>\u5176\u4e2d\uff1a</p> <pre><code>density_high_from_gen.stop_gradient = True\n</code></pre> <p>\u8868\u793a\u505c\u6b62\u53c2\u6570\u7684\u8ba1\u7b97\u68af\u5ea6\uff0c\u8fd9\u6837\u8bbe\u7f6e\u662f\u56e0\u4e3a\u8fd9\u4e2a\u53d8\u91cf\u5728\u8fd9\u91cc\u4ec5\u4f5c\u4e3a Discriminator \u548c Discriminator_tempo \u7684\u8f93\u5165\uff0c\u5728\u53cd\u5411\u8ba1\u7b97\u65f6\u4e0d\u5e94\u8be5\u53c2\u4e0e\u68af\u5ea6\u56de\u4f20\uff0c\u5982\u679c\u4e0d\u8fdb\u884c\u8fd9\u6837\u7684\u8bbe\u7f6e\uff0c\u7531\u4e8e\u8fd9\u4e2a\u53d8\u91cf\u6765\u6e90\u4e8e Generator \u7684\u8f93\u51fa\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u68af\u5ea6\u4f1a\u6cbf\u7740\u8fd9\u4e2a\u53d8\u91cf\u4f20\u7ed9 Generator\uff0c\u4ece\u800c\u6539\u53d8 Generator \u4e2d\u7684\u53c2\u6570\uff0c\u8fd9\u663e\u7136\u4e0d\u662f\u6211\u4eec\u60f3\u8981\u7684\u3002</p> <p>\u8fd9\u6837\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 Generator\u3001Discriminator \u548c Discriminator_tempo \u5e76\u5305\u542b\u8f93\u5165 transform \u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model list</code>\u3002</p>"},{"location":"zh/examples/tempoGAN/#34","title":"3.4 \u53c2\u6570\u548c\u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u6211\u4eec\u9700\u8981\u6307\u5b9a\u95ee\u9898\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5982\u6570\u636e\u96c6\u8def\u5f84\u3001\u5404\u9879 loss \u7684\u6743\u91cd\u53c2\u6570\u7b49\u3002</p> <pre><code>output_dir: ${hydra:run.dir}\nlog_freq: 20\nDATASET_PATH: ./datasets/tempoGAN/2d_train.mat\nDATASET_PATH_VALID: ./datasets/tempoGAN/2d_valid.mat\n\n# set working condition\nUSE_AMP: true\nUSE_SPATIALDISC: true\nUSE_TEMPODISC: true\nWEIGHT_GEN: [5.0, 0.0, 1.0]  # lambda_l1, lambda_l2, lambda_t\nWEIGHT_GEN_LAYER: [-1.0e-5, -1.0e-5, -1.0e-5, -1.0e-5, -1.0e-5]\n</code></pre> <p>\u6ce8\u610f\u5230\u5176\u4e2d\u5305\u542b 3 \u4e2a bool \u7c7b\u578b\u7684\u53d8\u91cf <code>use_amp</code>\u3001<code>use_spatialdisc</code> \u548c <code>use_tempodisc</code>\uff0c\u5b83\u4eec\u5206\u522b\u8868\u793a\u662f\u5426\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3(AMP)\u3001\u662f\u5426\u4f7f\u7528 Discriminator \u548c\u662f\u5426\u4f7f\u7528 Discriminator_tempo\uff0c\u5f53 <code>use_spatialdisc</code> \u548c <code>use_tempodisc</code> \u90fd\u88ab\u8bbe\u7f6e\u4e3a <code>False</code> \u65f6\uff0c\u672c\u95ee\u9898\u7684\u7f51\u7edc\u7ed3\u6784\u5c06\u4f1a\u53d8\u4e3a\u4e00\u4e2a\u5355\u7eaf\u7684 Genrator \u6a21\u578b\uff0c\u4e0d\u518d\u662f GAN \u7f51\u7edc\u4e86\u3002</p> <p>\u540c\u65f6\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\u7b49\u8d85\u53c2\u6570\uff0c\u6ce8\u610f\u7531\u4e8e GAN \u7f51\u7edc\u8bad\u7ec3\u6d41\u7a0b\u4e0e\u4e00\u822c\u5355\u4e2a\u6a21\u578b\u7684\u7f51\u7edc\u4e0d\u540c\uff0c<code>EPOCHS</code> \u7684\u8bbe\u7f6e\u4e5f\u6709\u6240\u4e0d\u540c\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 40000\n  epochs_gen: 1\n</code></pre>"},{"location":"zh/examples/tempoGAN/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u4f7f\u7528 Adam \u4f18\u5316\u5668\uff0c\u5b66\u4e60\u7387\u5728 <code>Epoch</code> \u8fbe\u5230\u4e00\u534a\u65f6\u51cf\u5c0f\u5230\u539f\u6765\u7684 \\(1/20\\)\uff0c\u56e0\u6b64\u4f7f\u7528 <code>Step</code> \u65b9\u6cd5\u4f5c\u4e3a\u5b66\u4e60\u7387\u7b56\u7565\u3002\u5982\u679c\u5c06 <code>by_epoch</code> \u8bbe\u4e3a True\uff0c\u5b66\u4e60\u7387\u5c06\u6839\u636e\u8bad\u7ec3\u7684 <code>Epoch</code> \u6539\u53d8\uff0c\u5426\u5219\u5c06\u6839\u636e <code>Iteration</code> \u6539\u53d8\u3002</p> <pre><code># initialize Adam optimizer\nlr_scheduler_gen = ppsci.optimizer.lr_scheduler.Step(\n    step_size=cfg.TRAIN.epochs // 2, **cfg.TRAIN.lr_scheduler\n)()\noptimizer_gen = ppsci.optimizer.Adam(lr_scheduler_gen)(model_gen)\nif cfg.USE_SPATIALDISC:\n    lr_scheduler_disc = ppsci.optimizer.lr_scheduler.Step(\n        step_size=cfg.TRAIN.epochs // 2, **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer_disc = ppsci.optimizer.Adam(lr_scheduler_disc)(model_disc)\nif cfg.USE_TEMPODISC:\n    lr_scheduler_disc_tempo = ppsci.optimizer.lr_scheduler.Step(\n        step_size=cfg.TRAIN.epochs // 2, **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer_disc_tempo = ppsci.optimizer.Adam(lr_scheduler_disc_tempo)(\n        (model_disc_tempo,)\n    )\n</code></pre>"},{"location":"zh/examples/tempoGAN/#36","title":"3.6 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u95ee\u9898\u91c7\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u867d\u7136\u4e0d\u662f\u4ee5\u76d1\u7763\u5b66\u4e60\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u6b64\u5904\u4ecd\u7136\u53ef\u4ee5\u91c7\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code>\uff0c\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u9700\u8981\u7ed9\u76d1\u7763\u7ea6\u675f\u6307\u5b9a\u6587\u4ef6\u8def\u5f84\u7b49\u6570\u636e\u8bfb\u53d6\u914d\u7f6e\uff0c\u56e0\u4e3a tempoGAN \u5c5e\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u6570\u636e\u96c6\u4e2d\u6ca1\u6709\u6807\u7b7e\u6570\u636e\uff0c\u800c\u662f\u4f7f\u7528\u4e00\u90e8\u5206\u8f93\u5165\u6570\u636e\u4f5c\u4e3a <code>label</code>\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u7f6e\u7ea6\u675f\u7684 <code>output_expr</code>\u3002</p> <pre><code>{\n    \"output_gen\": lambda out: out[\"output_gen\"],\n    \"density_high\": lambda out: out[\"density_high\"],\n},\n</code></pre>"},{"location":"zh/examples/tempoGAN/#361-generator","title":"3.6.1 Generator \u7684\u7ea6\u675f","text":"<p>\u4e0b\u9762\u662f\u7ea6\u675f\u7684\u5177\u4f53\u5185\u5bb9\uff0c\u8981\u6ce8\u610f\u4e0a\u8ff0\u63d0\u5230\u7684 <code>output_expr</code>\uff1a</p> <pre><code>sup_constraint_gen = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\n                \"density_low\": dataset_train[\"density_low\"],\n                \"density_high\": dataset_train[\"density_high\"],\n            },\n            \"transforms\": (\n                {\n                    \"FunctionalTransform\": {\n                        \"transform_func\": data_funcs.transform,\n                    },\n                },\n            ),\n        },\n        \"batch_size\": cfg.TRAIN.batch_size.sup_constraint,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n    },\n    ppsci.loss.FunctionalLoss(gen_funcs.loss_func_gen),\n    {\n        \"output_gen\": lambda out: out[\"output_gen\"],\n        \"density_high\": lambda out: out[\"density_high\"],\n    },\n    name=\"sup_constraint_gen\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u5176\u4e2d <code>dataset</code> \u5b57\u6bb5\u8868\u793a\u4f7f\u7528\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4fe1\u606f\uff0c\u5404\u4e2a\u5b57\u6bb5\u5206\u522b\u8868\u793a\uff1a</p> <ol> <li><code>name</code>\uff1a \u6570\u636e\u96c6\u7c7b\u578b\uff0c\u6b64\u5904 <code>NamedArrayDataset</code> \u8868\u793a\u4ece Array \u4e2d\u8bfb\u53d6\u7684 <code>.mat</code> \u7c7b\u578b\u7684\u6570\u636e\u96c6\uff1b</li> <li><code>input</code>\uff1a Array \u7c7b\u578b\u7684\u8f93\u5165\u6570\u636e\uff1b</li> <li><code>label</code>\uff1a Array \u7c7b\u578b\u7684\u6807\u7b7e\u6570\u636e\uff1b</li> <li><code>transforms</code>\uff1a \u6240\u6709\u6570\u636e transform \u65b9\u6cd5\uff0c\u6b64\u5904 <code>FunctionalTransform</code> \u4e3aPaddleScience \u9884\u7559\u7684\u81ea\u5b9a\u4e49\u6570\u636e transform \u7c7b\uff0c\u8be5\u7c7b\u652f\u6301\u7f16\u5199\u4ee3\u7801\u65f6\u81ea\u5b9a\u4e49\u8f93\u5165\u6570\u636e\u7684 transform\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u53c2\u8003 \u81ea\u5b9a\u4e49 loss \u548c data transform\uff1b</li> </ol> <p><code>batch_size</code> \u5b57\u6bb5\u8868\u793a batch\u7684\u5927\u5c0f\uff1b</p> <p><code>sampler</code> \u5b57\u6bb5\u8868\u793a\u91c7\u6837\u65b9\u6cd5\uff0c\u5176\u4e2d\u5404\u4e2a\u5b57\u6bb5\u8868\u793a\uff1a</p> <ol> <li><code>name</code>\uff1a \u91c7\u6837\u5668\u7c7b\u578b\uff0c\u6b64\u5904 <code>BatchSampler</code> \u8868\u793a\u6279\u91c7\u6837\u5668\uff1b</li> <li><code>drop_last</code>\uff1a \u662f\u5426\u9700\u8981\u4e22\u5f03\u6700\u540e\u65e0\u6cd5\u51d1\u6574\u4e00\u4e2a mini-batch \u7684\u6837\u672c\uff0c\u9ed8\u8ba4\u503c\u4e3a False\uff1b</li> <li><code>shuffle</code>\uff1a \u662f\u5426\u9700\u8981\u5728\u751f\u6210\u6837\u672c\u4e0b\u6807\u65f6\u6253\u4e71\u987a\u5e8f\uff0c\u9ed8\u8ba4\u503c\u4e3a False\uff1b</li> </ol> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u7684 <code>FunctionalLoss</code> \u4e3a PaddleScience \u9884\u7559\u7684\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u7c7b\uff0c\u8be5\u7c7b\u652f\u6301\u7f16\u5199\u4ee3\u7801\u65f6\u81ea\u5b9a\u4e49 loss \u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u8bf8\u5982 <code>MSE</code> \u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u53c2\u8003 \u81ea\u5b9a\u4e49 loss \u548c data transform\u3002</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684 <code>output_expr</code>\uff0c\u5982\u4e0a\u6240\u8ff0\uff0c\u662f\u4e3a\u4e86\u8ba9\u7a0b\u5e8f\u53ef\u4ee5\u5c06\u8f93\u5165\u6570\u636e\u4f5c\u4e3a <code>label</code>\u3002</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002</p> <p>\u5728\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\uff0c\u7531\u4e8e\u672c\u95ee\u9898\u8bbe\u7f6e\u4e86<code>use_spatialdisc</code> \u548c <code>use_tempodisc</code>\uff0c\u5bfc\u81f4 Generator \u7684\u90e8\u5206\u7ea6\u675f\u4e0d\u4e00\u5b9a\u5b58\u5728\uff0c\u56e0\u6b64\u5148\u5c01\u88c5\u4e00\u5b9a\u5b58\u5728\u7684\u7ea6\u675f\u5230\u5b57\u5178\u4e2d\uff0c\u5f53\u5176\u4f59\u7ea6\u675f\u5b58\u5728\u65f6\uff0c\u5728\u5411\u5b57\u5178\u4e2d\u6dfb\u52a0\u7ea6\u675f\u5143\u7d20\u3002</p> <pre><code>if cfg.USE_TEMPODISC:\n    sup_constraint_gen_tempo = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": {\n                    \"density_low\": dataset_train[\"density_low_tempo\"],\n                    \"density_high\": dataset_train[\"density_high_tempo\"],\n                },\n                \"transforms\": (\n                    {\n                        \"FunctionalTransform\": {\n                            \"transform_func\": data_funcs.transform,\n                        },\n                    },\n                ),\n            },\n            \"batch_size\": int(cfg.TRAIN.batch_size.sup_constraint // 3),\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.FunctionalLoss(gen_funcs.loss_func_gen_tempo),\n        {\n            \"output_gen\": lambda out: out[\"output_gen\"],\n            \"density_high\": lambda out: out[\"density_high\"],\n        },\n        name=\"sup_constraint_gen_tempo\",\n    )\n    constraint_gen[sup_constraint_gen_tempo.name] = sup_constraint_gen_tempo\n</code></pre>"},{"location":"zh/examples/tempoGAN/#362-discriminator","title":"3.6.2 Discriminator \u7684\u7ea6\u675f","text":"<pre><code>if cfg.USE_SPATIALDISC:\n    sup_constraint_disc = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": {\n                    \"density_low\": dataset_train[\"density_low\"],\n                    \"density_high\": dataset_train[\"density_high\"],\n                },\n                \"label\": {\n                    \"out_disc_from_target\": np.ones(\n                        (np.shape(dataset_train[\"density_high\"])[0], 1),\n                        dtype=paddle.get_default_dtype(),\n                    ),\n                    \"out_disc_from_gen\": np.ones(\n                        (np.shape(dataset_train[\"density_high\"])[0], 1),\n                        dtype=paddle.get_default_dtype(),\n                    ),\n                },\n                \"transforms\": (\n                    {\n                        \"FunctionalTransform\": {\n                            \"transform_func\": data_funcs.transform,\n                        },\n                    },\n                ),\n            },\n            \"batch_size\": cfg.TRAIN.batch_size.sup_constraint,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.FunctionalLoss(disc_funcs.loss_func),\n        name=\"sup_constraint_disc\",\n    )\n    constraint_disc = {sup_constraint_disc.name: sup_constraint_disc}\n</code></pre> <p>\u5404\u4e2a\u53c2\u6570\u542b\u4e49\u4e0eGenerator \u7684\u7ea6\u675f\u76f8\u540c\u3002</p>"},{"location":"zh/examples/tempoGAN/#363-discriminator_tempo","title":"3.6.3 Discriminator_tempo \u7684\u7ea6\u675f","text":"<pre><code>if cfg.USE_TEMPODISC:\n    sup_constraint_disc_tempo = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": {\n                    \"density_low\": dataset_train[\"density_low_tempo\"],\n                    \"density_high\": dataset_train[\"density_high_tempo\"],\n                },\n                \"label\": {\n                    \"out_disc_tempo_from_target\": np.ones(\n                        (np.shape(dataset_train[\"density_high_tempo\"])[0], 1),\n                        dtype=paddle.get_default_dtype(),\n                    ),\n                    \"out_disc_tempo_from_gen\": np.ones(\n                        (np.shape(dataset_train[\"density_high_tempo\"])[0], 1),\n                        dtype=paddle.get_default_dtype(),\n                    ),\n                },\n                \"transforms\": (\n                    {\n                        \"FunctionalTransform\": {\n                            \"transform_func\": data_funcs.transform,\n                        },\n                    },\n                ),\n            },\n            \"batch_size\": int(cfg.TRAIN.batch_size.sup_constraint // 3),\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.FunctionalLoss(disc_funcs.loss_func_tempo),\n        name=\"sup_constraint_disc_tempo\",\n    )\n    constraint_disc_tempo = {\n        sup_constraint_disc_tempo.name: sup_constraint_disc_tempo\n    }\n</code></pre> <p>\u5404\u4e2a\u53c2\u6570\u542b\u4e49\u4e0eGenerator \u7684\u7ea6\u675f\u76f8\u540c\u3002</p>"},{"location":"zh/examples/tempoGAN/#37","title":"3.7 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u56e0\u4e3a GAN \u7f51\u7edc\u8bad\u7ec3\u7684\u7279\u6027\uff0c\u672c\u95ee\u9898\u4e0d\u4f7f\u7528 PaddleScience \u4e2d\u5185\u7f6e\u7684\u53ef\u89c6\u5316\u5668\uff0c\u800c\u662f\u81ea\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7528\u4e8e\u5b9e\u73b0\u63a8\u7406\u7684\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u8bfb\u53d6\u9a8c\u8bc1\u96c6\u6570\u636e\uff0c\u5f97\u5230\u63a8\u7406\u7ed3\u679c\u5e76\u5c06\u7ed3\u679c\u4ee5\u56fe\u7247\u5f62\u5f0f\u4fdd\u5b58\u4e0b\u6765\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6309\u7167\u4e00\u5b9a\u95f4\u9694\u8c03\u7528\u8be5\u51fd\u6570\u5373\u53ef\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u76d1\u63a7\u8bad\u7ec3\u6548\u679c\u3002</p> <pre><code>def predict_and_save_plot(\n    output_dir: str,\n    epoch_id: int,\n    solver_gen: ppsci.solver.Solver,\n    dataset_valid: np.ndarray,\n    tile_ratio: int = 1,\n):\n    \"\"\"Predicting and plotting.\n\n    Args:\n        output_dir (str): Output dir path.\n        epoch_id (int): Which epoch it is.\n        solver_gen (ppsci.solver.Solver): Solver for predicting.\n        dataset_valid (np.ndarray): Valid dataset.\n        tile_ratio (int, optional): How many tiles of one dim. Defaults to 1.\n    \"\"\"\n    dir_pred = \"predict/\"\n    os.makedirs(os.path.join(output_dir, dir_pred), exist_ok=True)\n\n    start_idx = 190\n    density_low = dataset_valid[\"density_low\"][start_idx : start_idx + 3]\n    density_high = dataset_valid[\"density_high\"][start_idx : start_idx + 3]\n\n    # tile\n    density_low = (\n        split_data(density_low, tile_ratio) if tile_ratio != 1 else density_low\n    )\n    density_high = (\n        split_data(density_high, tile_ratio) if tile_ratio != 1 else density_high\n    )\n\n    pred_dict = solver_gen.predict(\n        {\n            \"density_low\": density_low,\n            \"density_high\": density_high,\n        },\n        {\"density_high\": lambda out: out[\"output_gen\"]},\n        batch_size=tile_ratio * tile_ratio if tile_ratio != 1 else 3,\n        no_grad=False,\n    )\n    if epoch_id == 1:\n        # plot interpolated input image\n        input_img = np.expand_dims(dataset_valid[\"density_low\"][start_idx], axis=0)\n        input_img = paddle.to_tensor(input_img, dtype=paddle.get_default_dtype())\n        input_img = F.interpolate(\n            input_img,\n            [input_img.shape[-2] * 4, input_img.shape[-1] * 4],\n            mode=\"nearest\",\n        ).numpy()\n        Img.imsave(\n            os.path.join(output_dir, dir_pred, \"input.png\"),\n            np.squeeze(input_img),\n            vmin=0.0,\n            vmax=1.0,\n            cmap=\"gray\",\n        )\n        # plot target image\n        Img.imsave(\n            os.path.join(output_dir, dir_pred, \"target.png\"),\n            np.squeeze(dataset_valid[\"density_high\"][start_idx]),\n            vmin=0.0,\n            vmax=1.0,\n            cmap=\"gray\",\n        )\n    # plot pred image\n    pred_img = (\n        concat_data(pred_dict[\"density_high\"].numpy(), tile_ratio)\n        if tile_ratio != 1\n        else np.squeeze(pred_dict[\"density_high\"][0].numpy())\n    )\n    Img.imsave(\n        os.path.join(output_dir, dir_pred, f\"pred_epoch_{str(epoch_id)}.png\"),\n        pred_img,\n        vmin=0.0,\n        vmax=1.0,\n        cmap=\"gray\",\n    )\n</code></pre>"},{"location":"zh/examples/tempoGAN/#38-loss-data-transform","title":"3.8 \u81ea\u5b9a\u4e49 loss \u548c data transform","text":"<p>\u7531\u4e8e\u672c\u95ee\u9898\u91c7\u7528\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u6807\u7b7e\u6570\u636e\uff0closs \u4e3a\u8ba1\u7b97\u5f97\u5230\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u5b9a\u4e49 loss \u3002\u65b9\u6cd5\u4e3a\u5148\u5b9a\u4e49\u76f8\u5173\u51fd\u6570\uff0c\u518d\u5c06\u51fd\u6570\u540d\u4f5c\u4e3a\u53c2\u6570\u4f20\u7ed9 <code>FunctionalLoss</code>\u3002\u9700\u8981\u6ce8\u610f\u81ea\u5b9a\u4e49 loss \u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u53c2\u6570\u9700\u8981\u4e0e PaddleScience \u4e2d\u5982 <code>MSE</code> \u7b49\u5176\u4ed6\u51fd\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u5373\u8f93\u5165\u4e3a\u6a21\u578b\u8f93\u51fa <code>output_dict</code> \u7b49\u5b57\u5178\u53d8\u91cf\uff0c\u8f93\u51fa\u4e3a loss \u503c <code>paddle.Tensor</code>\u3002</p>"},{"location":"zh/examples/tempoGAN/#381-generator-loss","title":"3.8.1 Generator \u7684 loss","text":"<p>Generator \u7684 loss \u63d0\u4f9b\u4e86 l1 loss\u3001l2 loss\u3001\u8f93\u51fa\u7ecf\u8fc7 Discriminator \u5224\u65ad\u7684 loss \u548c \u8f93\u51fa\u7ecf\u8fc7 Discriminator_tempo \u5224\u65ad\u7684 loss\u3002\u8fd9\u4e9b loss \u662f\u5426\u5b58\u5728\u6839\u636e\u6743\u91cd\u53c2\u6570\u63a7\u5236\uff0c\u82e5\u67d0\u4e00\u9879 loss \u7684\u6743\u91cd\u53c2\u6570\u4e3a 0\uff0c\u5219\u8868\u793a\u8bad\u7ec3\u4e2d\u4e0d\u6dfb\u52a0\u8be5 loss \u9879\u3002</p> <pre><code>def loss_func_gen(self, output_dict: Dict, *args) -&gt; paddle.Tensor:\n    \"\"\"Calculate loss of generator when use spatial discriminator.\n        The loss consists of l1 loss, l2 loss and layer loss when use spatial discriminator.\n        Notice that all item of loss is optional because weight of them might be 0.\n\n    Args:\n        output_dict (Dict): output dict of model.\n\n    Returns:\n        paddle.Tensor: Loss of generator.\n    \"\"\"\n    # l1 loss\n    loss_l1 = F.l1_loss(\n        output_dict[\"output_gen\"], output_dict[\"density_high\"], \"mean\"\n    )\n    losses = loss_l1 * self.weight_gen[0]\n\n    # l2 loss\n    loss_l2 = F.mse_loss(\n        output_dict[\"output_gen\"], output_dict[\"density_high\"], \"mean\"\n    )\n    losses += loss_l2 * self.weight_gen[1]\n\n    if self.weight_gen_layer is not None:\n        # disc(generator_out) loss\n        out_disc_from_gen = output_dict[\"out_disc_from_gen\"][-1]\n        label_ones = paddle.ones_like(out_disc_from_gen)\n        loss_gen = F.binary_cross_entropy_with_logits(\n            out_disc_from_gen, label_ones, reduction=\"mean\"\n        )\n        losses += loss_gen\n\n        # layer loss\n        key_list = list(output_dict.keys())\n        # [\"out0_layer0\",\"out0_layer1\",\"out0_layer2\",\"out0_layer3\",\"out_disc_from_target\",\n        # \"out1_layer0\",\"out1_layer1\",\"out1_layer2\",\"out1_layer3\",\"out_disc_from_gen\"]\n        loss_layer = 0\n        for i in range(1, len(self.weight_gen_layer)):\n            # i = 0,1,2,3\n            loss_layer += (\n                self.weight_gen_layer[i]\n                * F.mse_loss(\n                    output_dict[key_list[i]],\n                    output_dict[key_list[5 + i]],\n                    reduction=\"sum\",\n                )\n                / 2\n            )\n        losses += loss_layer * self.weight_gen_layer[0]\n\n    return {\"output_gen\": losses}\n\ndef loss_func_gen_tempo(self, output_dict: Dict, *args) -&gt; paddle.Tensor:\n    \"\"\"Calculate loss of generator when use temporal discriminator.\n        The loss is cross entropy loss when use temporal discriminator.\n\n    Args:\n        output_dict (Dict): output dict of model.\n\n    Returns:\n        paddle.Tensor: Loss of generator.\n    \"\"\"\n    out_disc_tempo_from_gen = output_dict[\"out_disc_tempo_from_gen\"][-1]\n    label_t_ones = paddle.ones_like(out_disc_tempo_from_gen)\n\n    loss_gen_t = F.binary_cross_entropy_with_logits(\n        out_disc_tempo_from_gen, label_t_ones, reduction=\"mean\"\n    )\n    losses = loss_gen_t * self.weight_gen[2]\n    return {\"out_disc_tempo_from_gen\": losses}\n</code></pre>"},{"location":"zh/examples/tempoGAN/#382-discriminator-loss","title":"3.8.2 Discriminator \u7684 loss","text":"<p>Discriminator \u4e3a\u5224\u522b\u5668\uff0c\u5b83\u7684\u4f5c\u7528\u662f\u5224\u65ad\u6570\u636e\u4e3a\u771f\u6570\u636e\u8fd8\u662f\u5047\u6570\u636e\uff0c\u56e0\u6b64\u5b83\u7684 loss \u4e3a Generator \u4ea7\u751f\u7684\u6570\u636e\u5e94\u5f53\u5224\u65ad\u4e3a\u5047\u800c\u4ea7\u751f\u7684 loss \u548c \u76ee\u6807\u503c\u6570\u636e\u5e94\u5f53\u5224\u65ad\u4e3a\u771f\u800c\u4ea7\u751f\u7684 loss\u3002</p> <pre><code>def loss_func(self, output_dict, *args):\n    out_disc_from_target = output_dict[\"out_disc_from_target\"]\n    out_disc_from_gen = output_dict[\"out_disc_from_gen\"]\n\n    label_ones = paddle.ones_like(out_disc_from_target)\n    label_zeros = paddle.zeros_like(out_disc_from_gen)\n\n    loss_disc_from_target = F.binary_cross_entropy_with_logits(\n        out_disc_from_target, label_ones, reduction=\"mean\"\n    )\n    loss_disc_from_gen = F.binary_cross_entropy_with_logits(\n        out_disc_from_gen, label_zeros, reduction=\"mean\"\n    )\n    losses = loss_disc_from_target * self.weight_disc + loss_disc_from_gen\n    return losses\n</code></pre>"},{"location":"zh/examples/tempoGAN/#383-discriminator_tempo-loss","title":"3.8.3 Discriminator_tempo \u7684 loss","text":"<p>Discriminator_tempo \u7684 loss \u6784\u6210 \u4e0e Discriminator \u76f8\u540c\uff0c\u53ea\u662f\u6240\u9700\u6570\u636e\u4e0d\u540c\u3002</p> <pre><code>def loss_func_tempo(self, output_dict, *args):\n    out_disc_tempo_from_target = output_dict[\"out_disc_tempo_from_target\"]\n    out_disc_tempo_from_gen = output_dict[\"out_disc_tempo_from_gen\"]\n\n    label_ones = paddle.ones_like(out_disc_tempo_from_target)\n    label_zeros = paddle.zeros_like(out_disc_tempo_from_gen)\n\n    loss_disc_tempo_from_target = F.binary_cross_entropy_with_logits(\n        out_disc_tempo_from_target, label_ones, reduction=\"mean\"\n    )\n    loss_disc_tempo_from_gen = F.binary_cross_entropy_with_logits(\n        out_disc_tempo_from_gen, label_zeros, reduction=\"mean\"\n    )\n    losses = (\n        loss_disc_tempo_from_target * self.weight_disc + loss_disc_tempo_from_gen\n    )\n    return losses\n</code></pre>"},{"location":"zh/examples/tempoGAN/#384-data-transform","title":"3.8.4 \u81ea\u5b9a\u4e49 data transform","text":"<p>\u672c\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f93\u5165\u6570\u636e\u5904\u7406\u65b9\u6cd5\uff0c\u5c06\u8f93\u5165\u7684\u6d41\u4f53\u5bc6\u5ea6\u6570\u636e\u968f\u673a\u88c1\u526a\u4e00\u5757\uff0c\u7136\u540e\u8fdb\u884c\u5bc6\u5ea6\u503c\u5224\u65ad\uff0c\u82e5\u88c1\u526a\u4e0b\u6765\u7684\u5757\u5bc6\u5ea6\u503c\u4f4e\u4e8e\u9608\u503c\u5219\u91cd\u65b0\u88c1\u526a\uff0c\u76f4\u5230\u5bc6\u5ea6\u6ee1\u8db3\u6761\u4ef6\u6216\u88c1\u526a\u6b21\u6570\u8fbe\u5230\u9608\u503c\u3002\u8fd9\u6837\u505a\u4e3b\u8981\u662f\u4e3a\u4e86\u51cf\u5c11\u8bad\u7ec3\u6240\u9700\u7684\u663e\u5b58\uff0c\u540c\u65f6\u5bf9\u88c1\u526a\u4e0b\u6765\u7684\u5757\u5bc6\u5ea6\u503c\u7684\u5224\u65ad\u4fdd\u8bc1\u4e86\u5757\u4e2d\u4fe1\u606f\u7684\u4e30\u5bcc\u7a0b\u5ea6\u3002\u53c2\u6570\u548c\u8d85\u53c2\u6570\u8bbe\u5b9a\u4e2d <code>tile_ratio</code> \u8868\u793a\u539f\u59cb\u5c3a\u5bf8\u662f\u5757\u7684\u5c3a\u5bf8\u7684\u51e0\u500d\uff0c\u5373\u82e5<code>tile_ratio</code> \u4e3a 2\uff0c\u88c1\u526a\u4e0b\u6765\u7684\u5757\u7684\u5927\u5c0f\u4e3a\u6574\u5f20\u539f\u59cb\u56fe\u7247\u7684\u56db\u5206\u4e4b\u4e00\u3002</p> <pre><code>class DataFuncs:\n    \"\"\"All functions used for data transform.\n\n    Args:\n        tile_ratio (int, optional): How many tiles of one dim. Defaults to 1.\n        density_min (float, optional): Minimize density of one tile. Defaults to 0.02.\n        max_turn (int, optional): Maximize turn of taking a tile from one image. Defaults to 20.\n    \"\"\"\n\n    def __init__(\n        self, tile_ratio: int = 1, density_min: float = 0.02, max_turn: int = 20\n    ) -&gt; None:\n        self.tile_ratio = tile_ratio\n        self.density_min = density_min\n        self.max_turn = max_turn\n\n    def transform(\n        self,\n        input_item: Dict[str, np.ndarray],\n        label_item: Dict[str, np.ndarray],\n        weight_item: Dict[str, np.ndarray],\n    ) -&gt; Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n        if self.tile_ratio == 1:\n            return input_item, label_item, weight_item\n        for _ in range(self.max_turn):\n            rand_ratio = np.random.rand()\n            density_low = self.cut_data(input_item[\"density_low\"], rand_ratio)\n            density_high = self.cut_data(input_item[\"density_high\"], rand_ratio)\n            if self.is_valid_tile(density_low):\n                break\n\n        input_item[\"density_low\"] = density_low\n        input_item[\"density_high\"] = density_high\n        return input_item, label_item, weight_item\n\n    def cut_data(self, data: np.ndarray, rand_ratio: float) -&gt; paddle.Tensor:\n        # data: C,H,W\n        _, H, W = data.shape\n        if H % self.tile_ratio != 0 or W % self.tile_ratio != 0:\n            exit(\n                f\"ERROR: input images cannot be divided into {self.tile_ratio} parts evenly!\"\n            )\n        tile_shape = [H // self.tile_ratio, W // self.tile_ratio]\n        rand_shape = np.floor(rand_ratio * (np.array([H, W]) - np.array(tile_shape)))\n        start = [int(rand_shape[0]), int(rand_shape[1])]\n        end = [int(rand_shape[0] + tile_shape[0]), int(rand_shape[1] + tile_shape[1])]\n        data = paddle.slice(\n            paddle.to_tensor(data), axes=[-2, -1], starts=start, ends=end\n        )\n\n        return data\n\n    def is_valid_tile(self, tile: paddle.Tensor):\n        img_density = tile[0].sum()\n        return img_density &gt;= (\n            self.density_min * tile.shape[0] * tile.shape[1] * tile.shape[2]\n        )\n</code></pre> <p>\u6ce8\u610f\uff0c\u6b64\u5904\u4ee3\u7801\u4ec5\u63d0\u4f9b data transform \u7684\u601d\u8def\u3002\u5f53\u524d\u4ee3\u7801\u4e2d\u7b80\u5355\u7684\u5206\u5757\u65b9\u6cd5\u7531\u4e8e\u8f93\u5165\u5305\u542b\u7684\u4fe1\u606f\u53d8\u5c11\uff0c\u663e\u7136\u4f1a\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\uff0c\u56e0\u6b64\u672c\u95ee\u9898\u4e2d\u5f53\u663e\u5b58\u5145\u8db3\u65f6\uff0c\u5e94\u5f53\u5c06 <code>tile_ratio</code> \u8bbe\u7f6e\u4e3a 1\uff0c\u5f53\u663e\u5b58\u4e0d\u8db3\u65f6\uff0c\u4e5f\u5efa\u8bae\u4f18\u5148\u8003\u8651\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6765\u51cf\u5c11\u73b0\u5b58\u5360\u7528\u3002</p>"},{"location":"zh/examples/tempoGAN/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u9996\u5148\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3002</p> <pre><code>solver_gen = ppsci.solver.Solver(\n    model_list,\n    constraint_gen,\n    cfg.output_dir,\n    optimizer_gen,\n    lr_scheduler_gen,\n    cfg.TRAIN.epochs_gen,\n    cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    use_amp=cfg.USE_AMP,\n    amp_level=cfg.TRAIN.amp_level,\n)\n</code></pre> <p>\u6ce8\u610f GAN \u7c7b\u578b\u7684\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u591a\u4e2a\u6a21\u578b\u4ea4\u66ff\u8bad\u7ec3\uff0c\u4e0e\u5355\u4e00\u6a21\u578b\u6216\u591a\u6a21\u578b\u5206\u9636\u6bb5\u8bad\u7ec3\u4e0d\u540c\uff0c\u4e0d\u80fd\u7b80\u5355\u7684\u4f7f\u7528 <code>solver.train</code> API\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u53c2\u8003 \u5b8c\u6574\u4ee3\u7801 \u4e2d tempoGAN.py \u6587\u4ef6\u3002</p>"},{"location":"zh/examples/tempoGAN/#310","title":"3.10 \u6a21\u578b\u8bc4\u4f30","text":""},{"location":"zh/examples/tempoGAN/#3101","title":"3.10.1 \u8bad\u7ec3\u4e2d\u8bc4\u4f30","text":"<p>\u8bad\u7ec3\u4e2d\u4ec5\u5728\u7279\u5b9a <code>Epoch</code> \u4fdd\u5b58\u7279\u5b9a\u56fe\u7247\u7684\u76ee\u6807\u7ed3\u679c\u548c\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\uff0c\u8bad\u7ec3\u7ed3\u675f\u540e\u9488\u5bf9\u6700\u540e\u4e00\u4e2a <code>Epoch</code> \u7684\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u4e00\u6b21\u8bc4\u4f30\uff0c\u4ee5\u4fbf\u76f4\u89c2\u8bc4\u4ef7\u6a21\u578b\u4f18\u5316\u6548\u679c\u3002\u4e0d\u4f7f\u7528 PaddleScience \u4e2d\u5185\u7f6e\u7684\u8bc4\u4f30\u5668\uff0c\u4e5f\u4e0d\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u8bc4\u4f30:</p> <pre><code>for i in range(1, cfg.TRAIN.epochs + 1):\n    logger.message(f\"\\nEpoch: {i}\\n\")\n    # plotting during training\n    if i == 1 or i % PRED_INTERVAL == 0 or i == cfg.TRAIN.epochs:\n        func_module.predict_and_save_plot(\n            cfg.output_dir, i, solver_gen, dataset_valid, cfg.TILE_RATIO\n        )\n</code></pre> <pre><code>############### evaluation for training ###############\nimg_target = (\n    func_module.get_image_array(\n        os.path.join(cfg.output_dir, \"predict\", \"target.png\")\n    )\n    / 255.0\n)\nimg_pred = (\n    func_module.get_image_array(\n        os.path.join(\n            cfg.output_dir, \"predict\", f\"pred_epoch_{cfg.TRAIN.epochs}.png\"\n        )\n    )\n    / 255.0\n)\neval_mse, eval_psnr, eval_ssim = func_module.evaluate_img(img_target, img_pred)\nlogger.message(f\"MSE: {eval_mse}, PSNR: {eval_psnr}, SSIM: {eval_ssim}\")\n</code></pre> <p>\u5177\u4f53\u4ee3\u7801\u8bf7\u53c2\u8003 \u5b8c\u6574\u4ee3\u7801 \u4e2d tempoGAN.py \u6587\u4ef6\u3002</p>"},{"location":"zh/examples/tempoGAN/#3102-eval","title":"3.10.2 eval \u4e2d\u8bc4\u4f30","text":"<p>\u672c\u95ee\u9898\u7684\u8bc4\u4f30\u6307\u6807\u4e3a\uff0c\u5c06\u6a21\u578b\u8f93\u51fa\u7684\u8d85\u5206\u7ed3\u679c\u4e0e\u5b9e\u9645\u9ad8\u5206\u8fa8\u7387\u56fe\u7247\u505a\u5bf9\u6bd4\uff0c\u4f7f\u7528\u4e09\u4e2a\u6307\u6807 MSE(Mean-Square Error) \u3001PSNR(Peak Signal-to-Noise Ratio) \u3001SSIM(Structural SIMilarity) \u6765\u8bc4\u4ef7\u56fe\u7247\u76f8\u4f3c\u5ea6\u3002\u56e0\u6b64\u6ca1\u6709\u4f7f\u7528 PaddleScience \u4e2d\u7684\u5185\u7f6e\u8bc4\u4f30\u5668\uff0c\u4e5f\u6ca1\u6709 <code>Solver.eval()</code> \u8fc7\u7a0b\u3002</p> <pre><code>def evaluate(cfg: DictConfig):\n    if cfg.EVAL.save_outs:\n        from matplotlib import image as Img\n\n        os.makedirs(osp.join(cfg.output_dir, \"eval_outs\"), exist_ok=True)\n\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    gen_funcs = func_module.GenFuncs(cfg.WEIGHT_GEN, None)\n\n    # load dataset\n    dataset_valid = hdf5storage.loadmat(cfg.DATASET_PATH_VALID)\n\n    # define Generator model\n    model_gen = ppsci.arch.Generator(**cfg.MODEL.gen_net)\n    model_gen.register_input_transform(gen_funcs.transform_in)\n\n    # define model_list\n    model_list = ppsci.arch.ModelList((model_gen,))\n\n    # load pretrained model\n    save_load.load_pretrain(model_list, cfg.EVAL.pretrained_model_path)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\n                \"density_low\": dataset_valid[\"density_low\"],\n            },\n            \"label\": {\"density_high\": dataset_valid[\"density_high\"]},\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": 1,\n    }\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"density_high\": lambda out: out[\"output_gen\"]},\n        metric={\"metric\": ppsci.metric.L2Rel()},\n        name=\"sup_validator_gen\",\n    )\n\n    # customized evalution\n    def scale(data):\n        smax = np.max(data)\n        smin = np.min(data)\n        return (data - smin) / (smax - smin)\n\n    eval_mse_list = []\n    eval_psnr_list = []\n    eval_ssim_list = []\n    for i, (input, label, _) in enumerate(sup_validator.data_loader):\n        output_dict = model_list({\"density_low\": input[\"density_low\"]})\n        output_arr = scale(np.squeeze(output_dict[\"output_gen\"].numpy()))\n        target_arr = scale(np.squeeze(label[\"density_high\"].numpy()))\n\n        eval_mse, eval_psnr, eval_ssim = func_module.evaluate_img(\n            target_arr, output_arr\n        )\n        eval_mse_list.append(eval_mse)\n        eval_psnr_list.append(eval_psnr)\n        eval_ssim_list.append(eval_ssim)\n\n        if cfg.EVAL.save_outs:\n            Img.imsave(\n                osp.join(cfg.output_dir, \"eval_outs\", f\"out_{i}.png\"),\n                output_arr,\n                vmin=0.0,\n                vmax=1.0,\n                cmap=\"gray\",\n            )\n    logger.message(\n        f\"MSE: {np.mean(eval_mse_list)}, PSNR: {np.mean(eval_psnr_list)}, SSIM: {np.mean(eval_ssim_list)}\"\n    )\n</code></pre> <p>\u53e6\u5916\uff0c\u5176\u4e2d\uff1a</p> <pre><code>if cfg.EVAL.save_outs:\n    Img.imsave(\n        osp.join(cfg.output_dir, \"eval_outs\", f\"out_{i}.png\"),\n        output_arr,\n        vmin=0.0,\n        vmax=1.0,\n        cmap=\"gray\",\n    )\n</code></pre> <p>\u63d0\u4f9b\u4e86\u4fdd\u5b58\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u9009\u62e9\uff0c\u4ee5\u4fbf\u66f4\u76f4\u89c2\u7684\u770b\u51fa\u8d85\u5206\u540e\u7684\u7ed3\u679c\uff0c\u662f\u5426\u5f00\u542f\u7531\u914d\u7f6e\u6587\u4ef6 <code>EVAL</code> \u4e2d\u7684 <code>save_outs</code> \u6307\u5b9a\uff1a</p> <pre><code>  checkpoint_path: null\n\n# evaluation settings\nEVAL:\n</code></pre>"},{"location":"zh/examples/tempoGAN/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"<p>\u5b8c\u6574\u4ee3\u7801\u5305\u542b PaddleScience \u5177\u4f53\u8bad\u7ec3\u6d41\u7a0b\u4ee3\u7801 tempoGAN.py \u548c\u6240\u6709\u81ea\u5b9a\u4e49\u51fd\u6570\u4ee3\u7801 functions.py\uff0c\u53e6\u5916\u8fd8\u5411 <code>ppsci.arch</code> \u6dfb\u52a0\u4e86\u7f51\u7edc\u7ed3\u6784\u4ee3\u7801 gan.py\uff0c\u4e00\u5e76\u663e\u793a\u5728\u4e0b\u9762\uff0c\u5982\u679c\u9700\u8981\u81ea\u5b9a\u4e49\u7f51\u7edc\u7ed3\u6784\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u53c2\u8003\u3002</p> tempoGAN.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom os import path as osp\n\nimport functions as func_module\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.utils import checker\nfrom ppsci.utils import logger\nfrom ppsci.utils import save_load\n\nif not checker.dynamic_import_to_globals(\"hdf5storage\"):\n    raise ImportError(\n        \"Could not import hdf5storage python package. \"\n        \"Please install it with `pip install hdf5storage`.\"\n    )\nimport hdf5storage\n\n\ndef train(cfg: DictConfig):\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    gen_funcs = func_module.GenFuncs(\n        cfg.WEIGHT_GEN, (cfg.WEIGHT_GEN_LAYER if cfg.USE_SPATIALDISC else None)\n    )\n    disc_funcs = func_module.DiscFuncs(cfg.WEIGHT_DISC)\n    data_funcs = func_module.DataFuncs(cfg.TILE_RATIO)\n\n    # load dataset\n    logger.message(\n        \"Attention! Start loading datasets, this will take tens of seconds to several minutes, please wait patiently.\"\n    )\n    dataset_train = hdf5storage.loadmat(cfg.DATASET_PATH)\n    logger.message(\"Finish loading training dataset.\")\n    dataset_valid = hdf5storage.loadmat(cfg.DATASET_PATH_VALID)\n    logger.message(\"Finish loading validation dataset.\")\n\n    # define Generator model\n    model_gen = ppsci.arch.Generator(**cfg.MODEL.gen_net)\n    model_gen.register_input_transform(gen_funcs.transform_in)\n    disc_funcs.model_gen = model_gen\n\n    model_tuple = (model_gen,)\n    # define Discriminators\n    if cfg.USE_SPATIALDISC:\n        model_disc = ppsci.arch.Discriminator(**cfg.MODEL.disc_net)\n        model_disc.register_input_transform(disc_funcs.transform_in)\n        model_tuple += (model_disc,)\n\n    # define temporal Discriminators\n    if cfg.USE_TEMPODISC:\n        model_disc_tempo = ppsci.arch.Discriminator(**cfg.MODEL.tempo_net)\n        model_disc_tempo.register_input_transform(disc_funcs.transform_in_tempo)\n        model_tuple += (model_disc_tempo,)\n\n    # define model_list\n    model_list = ppsci.arch.ModelList(model_tuple)\n\n    # initialize Adam optimizer\n    lr_scheduler_gen = ppsci.optimizer.lr_scheduler.Step(\n        step_size=cfg.TRAIN.epochs // 2, **cfg.TRAIN.lr_scheduler\n    )()\n    optimizer_gen = ppsci.optimizer.Adam(lr_scheduler_gen)(model_gen)\n    if cfg.USE_SPATIALDISC:\n        lr_scheduler_disc = ppsci.optimizer.lr_scheduler.Step(\n            step_size=cfg.TRAIN.epochs // 2, **cfg.TRAIN.lr_scheduler\n        )()\n        optimizer_disc = ppsci.optimizer.Adam(lr_scheduler_disc)(model_disc)\n    if cfg.USE_TEMPODISC:\n        lr_scheduler_disc_tempo = ppsci.optimizer.lr_scheduler.Step(\n            step_size=cfg.TRAIN.epochs // 2, **cfg.TRAIN.lr_scheduler\n        )()\n        optimizer_disc_tempo = ppsci.optimizer.Adam(lr_scheduler_disc_tempo)(\n            (model_disc_tempo,)\n        )\n\n    # Generator\n    # manually build constraint(s)\n    sup_constraint_gen = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": {\n                    \"density_low\": dataset_train[\"density_low\"],\n                    \"density_high\": dataset_train[\"density_high\"],\n                },\n                \"transforms\": (\n                    {\n                        \"FunctionalTransform\": {\n                            \"transform_func\": data_funcs.transform,\n                        },\n                    },\n                ),\n            },\n            \"batch_size\": cfg.TRAIN.batch_size.sup_constraint,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": False,\n            },\n        },\n        ppsci.loss.FunctionalLoss(gen_funcs.loss_func_gen),\n        {\n            \"output_gen\": lambda out: out[\"output_gen\"],\n            \"density_high\": lambda out: out[\"density_high\"],\n        },\n        name=\"sup_constraint_gen\",\n    )\n    constraint_gen = {sup_constraint_gen.name: sup_constraint_gen}\n    if cfg.USE_TEMPODISC:\n        sup_constraint_gen_tempo = ppsci.constraint.SupervisedConstraint(\n            {\n                \"dataset\": {\n                    \"name\": \"NamedArrayDataset\",\n                    \"input\": {\n                        \"density_low\": dataset_train[\"density_low_tempo\"],\n                        \"density_high\": dataset_train[\"density_high_tempo\"],\n                    },\n                    \"transforms\": (\n                        {\n                            \"FunctionalTransform\": {\n                                \"transform_func\": data_funcs.transform,\n                            },\n                        },\n                    ),\n                },\n                \"batch_size\": int(cfg.TRAIN.batch_size.sup_constraint // 3),\n                \"sampler\": {\n                    \"name\": \"BatchSampler\",\n                    \"drop_last\": False,\n                    \"shuffle\": False,\n                },\n            },\n            ppsci.loss.FunctionalLoss(gen_funcs.loss_func_gen_tempo),\n            {\n                \"output_gen\": lambda out: out[\"output_gen\"],\n                \"density_high\": lambda out: out[\"density_high\"],\n            },\n            name=\"sup_constraint_gen_tempo\",\n        )\n        constraint_gen[sup_constraint_gen_tempo.name] = sup_constraint_gen_tempo\n\n    # Discriminators\n    # manually build constraint(s)\n    if cfg.USE_SPATIALDISC:\n        sup_constraint_disc = ppsci.constraint.SupervisedConstraint(\n            {\n                \"dataset\": {\n                    \"name\": \"NamedArrayDataset\",\n                    \"input\": {\n                        \"density_low\": dataset_train[\"density_low\"],\n                        \"density_high\": dataset_train[\"density_high\"],\n                    },\n                    \"label\": {\n                        \"out_disc_from_target\": np.ones(\n                            (np.shape(dataset_train[\"density_high\"])[0], 1),\n                            dtype=paddle.get_default_dtype(),\n                        ),\n                        \"out_disc_from_gen\": np.ones(\n                            (np.shape(dataset_train[\"density_high\"])[0], 1),\n                            dtype=paddle.get_default_dtype(),\n                        ),\n                    },\n                    \"transforms\": (\n                        {\n                            \"FunctionalTransform\": {\n                                \"transform_func\": data_funcs.transform,\n                            },\n                        },\n                    ),\n                },\n                \"batch_size\": cfg.TRAIN.batch_size.sup_constraint,\n                \"sampler\": {\n                    \"name\": \"BatchSampler\",\n                    \"drop_last\": False,\n                    \"shuffle\": False,\n                },\n            },\n            ppsci.loss.FunctionalLoss(disc_funcs.loss_func),\n            name=\"sup_constraint_disc\",\n        )\n        constraint_disc = {sup_constraint_disc.name: sup_constraint_disc}\n\n    # temporal Discriminators\n    # manually build constraint(s)\n    if cfg.USE_TEMPODISC:\n        sup_constraint_disc_tempo = ppsci.constraint.SupervisedConstraint(\n            {\n                \"dataset\": {\n                    \"name\": \"NamedArrayDataset\",\n                    \"input\": {\n                        \"density_low\": dataset_train[\"density_low_tempo\"],\n                        \"density_high\": dataset_train[\"density_high_tempo\"],\n                    },\n                    \"label\": {\n                        \"out_disc_tempo_from_target\": np.ones(\n                            (np.shape(dataset_train[\"density_high_tempo\"])[0], 1),\n                            dtype=paddle.get_default_dtype(),\n                        ),\n                        \"out_disc_tempo_from_gen\": np.ones(\n                            (np.shape(dataset_train[\"density_high_tempo\"])[0], 1),\n                            dtype=paddle.get_default_dtype(),\n                        ),\n                    },\n                    \"transforms\": (\n                        {\n                            \"FunctionalTransform\": {\n                                \"transform_func\": data_funcs.transform,\n                            },\n                        },\n                    ),\n                },\n                \"batch_size\": int(cfg.TRAIN.batch_size.sup_constraint // 3),\n                \"sampler\": {\n                    \"name\": \"BatchSampler\",\n                    \"drop_last\": False,\n                    \"shuffle\": False,\n                },\n            },\n            ppsci.loss.FunctionalLoss(disc_funcs.loss_func_tempo),\n            name=\"sup_constraint_disc_tempo\",\n        )\n        constraint_disc_tempo = {\n            sup_constraint_disc_tempo.name: sup_constraint_disc_tempo\n        }\n\n    # initialize solver\n    solver_gen = ppsci.solver.Solver(\n        model_list,\n        constraint_gen,\n        cfg.output_dir,\n        optimizer_gen,\n        lr_scheduler_gen,\n        cfg.TRAIN.epochs_gen,\n        cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        use_amp=cfg.USE_AMP,\n        amp_level=cfg.TRAIN.amp_level,\n    )\n    if cfg.USE_SPATIALDISC:\n        solver_disc = ppsci.solver.Solver(\n            model_list,\n            constraint_disc,\n            cfg.output_dir,\n            optimizer_disc,\n            lr_scheduler_disc,\n            cfg.TRAIN.epochs_disc,\n            cfg.TRAIN.iters_per_epoch,\n            eval_during_train=cfg.TRAIN.eval_during_train,\n            use_amp=cfg.USE_AMP,\n            amp_level=cfg.TRAIN.amp_level,\n        )\n    if cfg.USE_TEMPODISC:\n        solver_disc_tempo = ppsci.solver.Solver(\n            model_list,\n            constraint_disc_tempo,\n            cfg.output_dir,\n            optimizer_disc_tempo,\n            lr_scheduler_disc_tempo,\n            cfg.TRAIN.epochs_disc_tempo,\n            cfg.TRAIN.iters_per_epoch,\n            eval_during_train=cfg.TRAIN.eval_during_train,\n            use_amp=cfg.USE_AMP,\n            amp_level=cfg.TRAIN.amp_level,\n        )\n\n    PRED_INTERVAL = 200\n    for i in range(1, cfg.TRAIN.epochs + 1):\n        logger.message(f\"\\nEpoch: {i}\\n\")\n        # plotting during training\n        if i == 1 or i % PRED_INTERVAL == 0 or i == cfg.TRAIN.epochs:\n            func_module.predict_and_save_plot(\n                cfg.output_dir, i, solver_gen, dataset_valid, cfg.TILE_RATIO\n            )\n\n        disc_funcs.model_gen = model_gen\n        # train disc, input: (x,y,G(x))\n        if cfg.USE_SPATIALDISC:\n            solver_disc.train()\n\n        # train disc tempo, input: (y_3,G(x)_3)\n        if cfg.USE_TEMPODISC:\n            solver_disc_tempo.train()\n\n        # train gen, input: (x,)\n        solver_gen.train()\n\n    ############### evaluation for training ###############\n    img_target = (\n        func_module.get_image_array(\n            os.path.join(cfg.output_dir, \"predict\", \"target.png\")\n        )\n        / 255.0\n    )\n    img_pred = (\n        func_module.get_image_array(\n            os.path.join(\n                cfg.output_dir, \"predict\", f\"pred_epoch_{cfg.TRAIN.epochs}.png\"\n            )\n        )\n        / 255.0\n    )\n    eval_mse, eval_psnr, eval_ssim = func_module.evaluate_img(img_target, img_pred)\n    logger.message(f\"MSE: {eval_mse}, PSNR: {eval_psnr}, SSIM: {eval_ssim}\")\n\n\ndef evaluate(cfg: DictConfig):\n    if cfg.EVAL.save_outs:\n        from matplotlib import image as Img\n\n        os.makedirs(osp.join(cfg.output_dir, \"eval_outs\"), exist_ok=True)\n\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    gen_funcs = func_module.GenFuncs(cfg.WEIGHT_GEN, None)\n\n    # load dataset\n    dataset_valid = hdf5storage.loadmat(cfg.DATASET_PATH_VALID)\n\n    # define Generator model\n    model_gen = ppsci.arch.Generator(**cfg.MODEL.gen_net)\n    model_gen.register_input_transform(gen_funcs.transform_in)\n\n    # define model_list\n    model_list = ppsci.arch.ModelList((model_gen,))\n\n    # load pretrained model\n    save_load.load_pretrain(model_list, cfg.EVAL.pretrained_model_path)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\n                \"density_low\": dataset_valid[\"density_low\"],\n            },\n            \"label\": {\"density_high\": dataset_valid[\"density_high\"]},\n        },\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": False,\n        },\n        \"batch_size\": 1,\n    }\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"density_high\": lambda out: out[\"output_gen\"]},\n        metric={\"metric\": ppsci.metric.L2Rel()},\n        name=\"sup_validator_gen\",\n    )\n\n    # customized evalution\n    def scale(data):\n        smax = np.max(data)\n        smin = np.min(data)\n        return (data - smin) / (smax - smin)\n\n    eval_mse_list = []\n    eval_psnr_list = []\n    eval_ssim_list = []\n    for i, (input, label, _) in enumerate(sup_validator.data_loader):\n        output_dict = model_list({\"density_low\": input[\"density_low\"]})\n        output_arr = scale(np.squeeze(output_dict[\"output_gen\"].numpy()))\n        target_arr = scale(np.squeeze(label[\"density_high\"].numpy()))\n\n        eval_mse, eval_psnr, eval_ssim = func_module.evaluate_img(\n            target_arr, output_arr\n        )\n        eval_mse_list.append(eval_mse)\n        eval_psnr_list.append(eval_psnr)\n        eval_ssim_list.append(eval_ssim)\n\n        if cfg.EVAL.save_outs:\n            Img.imsave(\n                osp.join(cfg.output_dir, \"eval_outs\", f\"out_{i}.png\"),\n                output_arr,\n                vmin=0.0,\n                vmax=1.0,\n                cmap=\"gray\",\n            )\n    logger.message(\n        f\"MSE: {np.mean(eval_mse_list)}, PSNR: {np.mean(eval_psnr_list)}, SSIM: {np.mean(eval_ssim_list)}\"\n    )\n\n\ndef export(cfg: DictConfig):\n    from paddle.static import InputSpec\n\n    # set models\n    gen_funcs = func_module.GenFuncs(cfg.WEIGHT_GEN, None)\n    model_gen = ppsci.arch.Generator(**cfg.MODEL.gen_net)\n    model_gen.register_input_transform(gen_funcs.transform_in)\n\n    # define model_list\n    model_list = ppsci.arch.ModelList((model_gen,))\n\n    # load pretrained model\n    solver = ppsci.solver.Solver(\n        model=model_list, pretrained_model_path=cfg.INFER.pretrained_model_path\n    )\n\n    # export models\n    input_spec = [\n        {\"density_low\": InputSpec([None, 1, 128, 128], \"float32\", name=\"density_low\")},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path, skip_prune_program=True)\n\n\ndef inference(cfg: DictConfig):\n    from matplotlib import image as Img\n\n    from deploy.python_infer import pinn_predictor\n\n    # set model predictor\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # load dataset\n    dataset_infer = {\n        \"density_low\": hdf5storage.loadmat(cfg.DATASET_PATH_VALID)[\"density_low\"]\n    }\n\n    output_dict = predictor.predict(dataset_infer, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output = [output_dict[key] for key in output_dict]\n\n    def scale(data):\n        smax = np.max(data)\n        smin = np.min(data)\n        return (data - smin) / (smax - smin)\n\n    for i, img in enumerate(output[0]):\n        img = scale(np.squeeze(img))\n        Img.imsave(\n            osp.join(cfg.output_dir, f\"out_{i}.png\"),\n            img,\n            vmin=0.0,\n            vmax=1.0,\n            cmap=\"gray\",\n        )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"tempogan.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> functions.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport numpy as np\nimport paddle\nimport paddle.nn.functional as F\nfrom matplotlib import image as Img\nfrom PIL import Image\nfrom skimage.metrics import mean_squared_error\nfrom skimage.metrics import peak_signal_noise_ratio\nfrom skimage.metrics import structural_similarity\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\n# train\ndef interpolate(\n    data: paddle.Tensor, ratio: int, mode: str = \"nearest\"\n) -&gt; paddle.Tensor:\n    \"\"\"Interpolate twice.\n\n    Args:\n        data (paddle.Tensor): The data to be interpolated.\n        ratio (int): Ratio of one interpolation.\n        mode (str, optional): Interpolation method. Defaults to \"nearest\".\n\n    Returns:\n        paddle.Tensor: Data interpolated.\n    \"\"\"\n    for _ in range(2):\n        data = F.interpolate(\n            data,\n            [data.shape[-2] * ratio, data.shape[-1] * ratio],\n            mode=mode,\n        )\n    return data\n\n\ndef reshape_input(input_dict: Dict[str, paddle.Tensor]) -&gt; Dict[str, paddle.Tensor]:\n    \"\"\"Reshape input data for temporally Discriminator. Reshape data from N, C, W, H to N * C, 1, H, W.\n        Which will merge N dimension and C dimension to 1 dimension but still keep 4 dimensions\n        to ensure the data can be used for training.\n\n    Args:\n        input_dict (Dict[str, paddle.Tensor]): input data dict.\n\n    Returns:\n        Dict[str, paddle.Tensor]: reshaped data dict.\n    \"\"\"\n    for key in input_dict:\n        input = input_dict[key]\n        N, C, H, W = input.shape\n        input_dict[key] = paddle.reshape(input, [N * C, 1, H, W])\n    return input_dict\n\n\ndef dereshape_input(\n    input_dict: Dict[str, paddle.Tensor], C: int\n) -&gt; Dict[str, paddle.Tensor]:\n    \"\"\"Dereshape input data for temporally Discriminator. Deeshape data from N * C, 1, H, W to N, C, W, H.\n\n    Args:\n        input_dict (Dict[str, paddle.Tensor]): input data dict.\n        C (int): Channel of dereshape.\n\n    Returns:\n        Dict[str, paddle.Tensor]: dereshaped data dict.\n    \"\"\"\n    for key in input_dict:\n        input = input_dict[key]\n        N, _, H, W = input.shape\n        if N &lt; C:\n            logger.warning(\n                f\"batch_size is smaller than {C}! Tempo needs at least {C} frames, input will be copied.\"\n            )\n            input_dict[key] = paddle.concat([input[:1]] * C, axis=1)\n        else:\n            N_new = int(N // C)\n            input_dict[key] = paddle.reshape(input[: N_new * C], [-1, C, H, W])\n    return input_dict\n\n\n# predict\ndef split_data(data: np.ndarray, tile_ratio: int) -&gt; np.ndarray:\n    \"\"\"Split a numpy image to tiles equally.\n\n    Args:\n        data (np.ndarray): The image to be Split.\n        tile_ratio (int): How many tiles of one dim.\n            Number of result tiles is tile_ratio * tile_ratio for a 2d image.\n\n    Returns:\n        np.ndarray: Tiles in [N,C,H,W] shape.\n    \"\"\"\n    _, _, h, w = data.shape\n    tile_h, tile_w = h // tile_ratio, w // tile_ratio\n    tiles = []\n    for i in range(tile_ratio):\n        for j in range(tile_ratio):\n            tiles.append(\n                data[\n                    :1,\n                    :,\n                    i * tile_h : i * tile_h + tile_h,\n                    j * tile_w : j * tile_w + tile_w,\n                ],\n            )\n    return np.concatenate(tiles, axis=0)\n\n\ndef concat_data(data: np.ndarray, tile_ratio: int) -&gt; np.ndarray:\n    \"\"\"Concat numpy tiles to a image equally.\n\n    Args:\n        data (np.ndarray): The tiles to be upsplited.\n        tile_ratio (int): How many tiles of one dim.\n            Number of input tiles is tile_ratio * tile_ratio for 2d result.\n\n    Returns:\n        np.ndarray: Image in [H,W] shape.\n    \"\"\"\n    _, _, tile_h, tile_w = data.shape\n    h, w = tile_h * tile_ratio, tile_w * tile_ratio\n    data_whole = np.ones([h, w], dtype=paddle.get_default_dtype())\n    tile_idx = 0\n    for i in range(tile_ratio):\n        for j in range(tile_ratio):\n            data_whole[\n                i * tile_h : i * tile_h + tile_h,\n                j * tile_w : j * tile_w + tile_w,\n            ] = data[tile_idx][0]\n            tile_idx += 1\n    return data_whole\n\n\ndef predict_and_save_plot(\n    output_dir: str,\n    epoch_id: int,\n    solver_gen: ppsci.solver.Solver,\n    dataset_valid: np.ndarray,\n    tile_ratio: int = 1,\n):\n    \"\"\"Predicting and plotting.\n\n    Args:\n        output_dir (str): Output dir path.\n        epoch_id (int): Which epoch it is.\n        solver_gen (ppsci.solver.Solver): Solver for predicting.\n        dataset_valid (np.ndarray): Valid dataset.\n        tile_ratio (int, optional): How many tiles of one dim. Defaults to 1.\n    \"\"\"\n    dir_pred = \"predict/\"\n    os.makedirs(os.path.join(output_dir, dir_pred), exist_ok=True)\n\n    start_idx = 190\n    density_low = dataset_valid[\"density_low\"][start_idx : start_idx + 3]\n    density_high = dataset_valid[\"density_high\"][start_idx : start_idx + 3]\n\n    # tile\n    density_low = (\n        split_data(density_low, tile_ratio) if tile_ratio != 1 else density_low\n    )\n    density_high = (\n        split_data(density_high, tile_ratio) if tile_ratio != 1 else density_high\n    )\n\n    pred_dict = solver_gen.predict(\n        {\n            \"density_low\": density_low,\n            \"density_high\": density_high,\n        },\n        {\"density_high\": lambda out: out[\"output_gen\"]},\n        batch_size=tile_ratio * tile_ratio if tile_ratio != 1 else 3,\n        no_grad=False,\n    )\n    if epoch_id == 1:\n        # plot interpolated input image\n        input_img = np.expand_dims(dataset_valid[\"density_low\"][start_idx], axis=0)\n        input_img = paddle.to_tensor(input_img, dtype=paddle.get_default_dtype())\n        input_img = F.interpolate(\n            input_img,\n            [input_img.shape[-2] * 4, input_img.shape[-1] * 4],\n            mode=\"nearest\",\n        ).numpy()\n        Img.imsave(\n            os.path.join(output_dir, dir_pred, \"input.png\"),\n            np.squeeze(input_img),\n            vmin=0.0,\n            vmax=1.0,\n            cmap=\"gray\",\n        )\n        # plot target image\n        Img.imsave(\n            os.path.join(output_dir, dir_pred, \"target.png\"),\n            np.squeeze(dataset_valid[\"density_high\"][start_idx]),\n            vmin=0.0,\n            vmax=1.0,\n            cmap=\"gray\",\n        )\n    # plot pred image\n    pred_img = (\n        concat_data(pred_dict[\"density_high\"].numpy(), tile_ratio)\n        if tile_ratio != 1\n        else np.squeeze(pred_dict[\"density_high\"][0].numpy())\n    )\n    Img.imsave(\n        os.path.join(output_dir, dir_pred, f\"pred_epoch_{str(epoch_id)}.png\"),\n        pred_img,\n        vmin=0.0,\n        vmax=1.0,\n        cmap=\"gray\",\n    )\n\n\n# evaluation\ndef evaluate_img(\n    img_target: np.ndarray, img_pred: np.ndarray\n) -&gt; Tuple[float, float, float]:\n    \"\"\"Evaluate two images.\n\n    Args:\n        img_target (np.ndarray): Target image.\n        img_pred (np.ndarray): Image generated by prediction.\n\n    Returns:\n        Tuple[float, float, float]: MSE, PSNR, SSIM.\n    \"\"\"\n    eval_mse = mean_squared_error(img_target, img_pred)\n    eval_psnr = peak_signal_noise_ratio(img_target, img_pred)\n    eval_ssim = structural_similarity(img_target, img_pred, data_range=1.0)\n    return eval_mse, eval_psnr, eval_ssim\n\n\ndef get_image_array(img_path):\n    return np.array(Image.open(img_path).convert(\"L\"))\n\n\nclass GenFuncs:\n    \"\"\"All functions used for Generator, including functions of transform and loss.\n\n    Args:\n        weight_gen (List[float]): Weights of L1 loss.\n        weight_gen_layer (List[float], optional): Weights of layers loss. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self, weight_gen: List[float], weight_gen_layer: List[float] = None\n    ) -&gt; None:\n        self.weight_gen = weight_gen\n        self.weight_gen_layer = weight_gen_layer\n\n    def transform_in(self, _in):\n        ratio = 2\n        input_dict = reshape_input(_in)\n        density_low = input_dict[\"density_low\"]\n        density_low_inp = interpolate(density_low, ratio, \"nearest\")\n        return {\"input_gen\": density_low_inp}\n\n    def loss_func_gen(self, output_dict: Dict, *args) -&gt; paddle.Tensor:\n        \"\"\"Calculate loss of generator when use spatial discriminator.\n            The loss consists of l1 loss, l2 loss and layer loss when use spatial discriminator.\n            Notice that all item of loss is optional because weight of them might be 0.\n\n        Args:\n            output_dict (Dict): output dict of model.\n\n        Returns:\n            paddle.Tensor: Loss of generator.\n        \"\"\"\n        # l1 loss\n        loss_l1 = F.l1_loss(\n            output_dict[\"output_gen\"], output_dict[\"density_high\"], \"mean\"\n        )\n        losses = loss_l1 * self.weight_gen[0]\n\n        # l2 loss\n        loss_l2 = F.mse_loss(\n            output_dict[\"output_gen\"], output_dict[\"density_high\"], \"mean\"\n        )\n        losses += loss_l2 * self.weight_gen[1]\n\n        if self.weight_gen_layer is not None:\n            # disc(generator_out) loss\n            out_disc_from_gen = output_dict[\"out_disc_from_gen\"][-1]\n            label_ones = paddle.ones_like(out_disc_from_gen)\n            loss_gen = F.binary_cross_entropy_with_logits(\n                out_disc_from_gen, label_ones, reduction=\"mean\"\n            )\n            losses += loss_gen\n\n            # layer loss\n            key_list = list(output_dict.keys())\n            # [\"out0_layer0\",\"out0_layer1\",\"out0_layer2\",\"out0_layer3\",\"out_disc_from_target\",\n            # \"out1_layer0\",\"out1_layer1\",\"out1_layer2\",\"out1_layer3\",\"out_disc_from_gen\"]\n            loss_layer = 0\n            for i in range(1, len(self.weight_gen_layer)):\n                # i = 0,1,2,3\n                loss_layer += (\n                    self.weight_gen_layer[i]\n                    * F.mse_loss(\n                        output_dict[key_list[i]],\n                        output_dict[key_list[5 + i]],\n                        reduction=\"sum\",\n                    )\n                    / 2\n                )\n            losses += loss_layer * self.weight_gen_layer[0]\n\n        return {\"output_gen\": losses}\n\n    def loss_func_gen_tempo(self, output_dict: Dict, *args) -&gt; paddle.Tensor:\n        \"\"\"Calculate loss of generator when use temporal discriminator.\n            The loss is cross entropy loss when use temporal discriminator.\n\n        Args:\n            output_dict (Dict): output dict of model.\n\n        Returns:\n            paddle.Tensor: Loss of generator.\n        \"\"\"\n        out_disc_tempo_from_gen = output_dict[\"out_disc_tempo_from_gen\"][-1]\n        label_t_ones = paddle.ones_like(out_disc_tempo_from_gen)\n\n        loss_gen_t = F.binary_cross_entropy_with_logits(\n            out_disc_tempo_from_gen, label_t_ones, reduction=\"mean\"\n        )\n        losses = loss_gen_t * self.weight_gen[2]\n        return {\"out_disc_tempo_from_gen\": losses}\n\n\nclass DiscFuncs:\n    \"\"\"All functions used for Discriminator and temporally Discriminator, including functions of transform and loss.\n\n    Args:\n        weight_disc (float): Weight of loss generated by the discriminator to judge the true target.\n    \"\"\"\n\n    def __init__(self, weight_disc: float) -&gt; None:\n        self.weight_disc = weight_disc\n        self.model_gen = None\n\n    def transform_in(self, _in):\n        ratio = 2\n        input_dict = reshape_input(_in)\n        density_low = input_dict[\"density_low\"]\n        density_high_from_target = input_dict[\"density_high\"]\n\n        density_low_inp = interpolate(density_low, ratio, \"nearest\")\n\n        density_high_from_gen = self.model_gen(input_dict)[\"output_gen\"]\n        density_high_from_gen.stop_gradient = True\n\n        density_input_from_target = paddle.concat(\n            [density_low_inp, density_high_from_target], axis=1\n        )\n        density_input_from_gen = paddle.concat(\n            [density_low_inp, density_high_from_gen], axis=1\n        )\n        return {\n            \"input_disc_from_target\": density_input_from_target,\n            \"input_disc_from_gen\": density_input_from_gen,\n        }\n\n    def transform_in_tempo(self, _in):\n        density_high_from_target = _in[\"density_high\"]\n\n        input_dict = reshape_input(_in)\n        density_high_from_gen = self.model_gen(input_dict)[\"output_gen\"]\n        density_high_from_gen.stop_gradient = True\n\n        input_trans = {\n            \"input_tempo_disc_from_target\": density_high_from_target,\n            \"input_tempo_disc_from_gen\": density_high_from_gen,\n        }\n\n        return dereshape_input(input_trans, 3)\n\n    def loss_func(self, output_dict, *args):\n        out_disc_from_target = output_dict[\"out_disc_from_target\"]\n        out_disc_from_gen = output_dict[\"out_disc_from_gen\"]\n\n        label_ones = paddle.ones_like(out_disc_from_target)\n        label_zeros = paddle.zeros_like(out_disc_from_gen)\n\n        loss_disc_from_target = F.binary_cross_entropy_with_logits(\n            out_disc_from_target, label_ones, reduction=\"mean\"\n        )\n        loss_disc_from_gen = F.binary_cross_entropy_with_logits(\n            out_disc_from_gen, label_zeros, reduction=\"mean\"\n        )\n        losses = loss_disc_from_target * self.weight_disc + loss_disc_from_gen\n        return losses\n\n    def loss_func_tempo(self, output_dict, *args):\n        out_disc_tempo_from_target = output_dict[\"out_disc_tempo_from_target\"]\n        out_disc_tempo_from_gen = output_dict[\"out_disc_tempo_from_gen\"]\n\n        label_ones = paddle.ones_like(out_disc_tempo_from_target)\n        label_zeros = paddle.zeros_like(out_disc_tempo_from_gen)\n\n        loss_disc_tempo_from_target = F.binary_cross_entropy_with_logits(\n            out_disc_tempo_from_target, label_ones, reduction=\"mean\"\n        )\n        loss_disc_tempo_from_gen = F.binary_cross_entropy_with_logits(\n            out_disc_tempo_from_gen, label_zeros, reduction=\"mean\"\n        )\n        losses = (\n            loss_disc_tempo_from_target * self.weight_disc + loss_disc_tempo_from_gen\n        )\n        return losses\n\n\nclass DataFuncs:\n    \"\"\"All functions used for data transform.\n\n    Args:\n        tile_ratio (int, optional): How many tiles of one dim. Defaults to 1.\n        density_min (float, optional): Minimize density of one tile. Defaults to 0.02.\n        max_turn (int, optional): Maximize turn of taking a tile from one image. Defaults to 20.\n    \"\"\"\n\n    def __init__(\n        self, tile_ratio: int = 1, density_min: float = 0.02, max_turn: int = 20\n    ) -&gt; None:\n        self.tile_ratio = tile_ratio\n        self.density_min = density_min\n        self.max_turn = max_turn\n\n    def transform(\n        self,\n        input_item: Dict[str, np.ndarray],\n        label_item: Dict[str, np.ndarray],\n        weight_item: Dict[str, np.ndarray],\n    ) -&gt; Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n        if self.tile_ratio == 1:\n            return input_item, label_item, weight_item\n        for _ in range(self.max_turn):\n            rand_ratio = np.random.rand()\n            density_low = self.cut_data(input_item[\"density_low\"], rand_ratio)\n            density_high = self.cut_data(input_item[\"density_high\"], rand_ratio)\n            if self.is_valid_tile(density_low):\n                break\n\n        input_item[\"density_low\"] = density_low\n        input_item[\"density_high\"] = density_high\n        return input_item, label_item, weight_item\n\n    def cut_data(self, data: np.ndarray, rand_ratio: float) -&gt; paddle.Tensor:\n        # data: C,H,W\n        _, H, W = data.shape\n        if H % self.tile_ratio != 0 or W % self.tile_ratio != 0:\n            exit(\n                f\"ERROR: input images cannot be divided into {self.tile_ratio} parts evenly!\"\n            )\n        tile_shape = [H // self.tile_ratio, W // self.tile_ratio]\n        rand_shape = np.floor(rand_ratio * (np.array([H, W]) - np.array(tile_shape)))\n        start = [int(rand_shape[0]), int(rand_shape[1])]\n        end = [int(rand_shape[0] + tile_shape[0]), int(rand_shape[1] + tile_shape[1])]\n        data = paddle.slice(\n            paddle.to_tensor(data), axes=[-2, -1], starts=start, ends=end\n        )\n\n        return data\n\n    def is_valid_tile(self, tile: paddle.Tensor):\n        img_density = tile[0].sum()\n        return img_density &gt;= (\n            self.density_min * tile.shape[0] * tile.shape[1] * tile.shape[2]\n        )\n</code></pre> gan.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport paddle\nimport paddle.nn as nn\n\nfrom ppsci.arch import activation as act_mod\nfrom ppsci.arch import base\n\n\nclass Conv2DBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channel,\n        out_channel,\n        kernel_size,\n        stride,\n        use_bn,\n        act,\n        mean,\n        std,\n        value,\n    ):\n        super().__init__()\n        weight_attr = paddle.ParamAttr(\n            initializer=nn.initializer.Normal(mean=mean, std=std)\n        )\n        bias_attr = paddle.ParamAttr(initializer=nn.initializer.Constant(value=value))\n        self.conv_2d = nn.Conv2D(\n            in_channel,\n            out_channel,\n            kernel_size,\n            stride,\n            padding=\"SAME\",\n            weight_attr=weight_attr,\n            bias_attr=bias_attr,\n        )\n        self.bn = nn.BatchNorm2D(out_channel) if use_bn else None\n        self.act = act_mod.get_activation(act) if act else None\n\n    def forward(self, x):\n        y = x\n        y = self.conv_2d(y)\n        if self.bn:\n            y = self.bn(y)\n        if self.act:\n            y = self.act(y)\n        return y\n\n\nclass VariantResBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channel,\n        out_channels,\n        kernel_sizes,\n        strides,\n        use_bns,\n        acts,\n        mean,\n        std,\n        value,\n    ):\n        super().__init__()\n        self.conv_2d_0 = Conv2DBlock(\n            in_channel=in_channel,\n            out_channel=out_channels[0],\n            kernel_size=kernel_sizes[0],\n            stride=strides[0],\n            use_bn=use_bns[0],\n            act=acts[0],\n            mean=mean,\n            std=std,\n            value=value,\n        )\n        self.conv_2d_1 = Conv2DBlock(\n            in_channel=out_channels[0],\n            out_channel=out_channels[1],\n            kernel_size=kernel_sizes[1],\n            stride=strides[1],\n            use_bn=use_bns[1],\n            act=acts[1],\n            mean=mean,\n            std=std,\n            value=value,\n        )\n\n        self.conv_2d_2 = Conv2DBlock(\n            in_channel=in_channel,\n            out_channel=out_channels[2],\n            kernel_size=kernel_sizes[2],\n            stride=strides[2],\n            use_bn=use_bns[2],\n            act=acts[2],\n            mean=mean,\n            std=std,\n            value=value,\n        )\n\n        self.act = act_mod.get_activation(\"relu\")\n\n    def forward(self, x):\n        y = x\n        y = self.conv_2d_0(y)\n        y = self.conv_2d_1(y)\n        short = self.conv_2d_2(x)\n        y = paddle.add(y, short)\n        y = self.act(y)\n        return y\n\n\nclass FCBlock(nn.Layer):\n    def __init__(self, in_channel, act, mean, std, value):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        weight_attr = paddle.ParamAttr(\n            initializer=nn.initializer.Normal(mean=mean, std=std)\n        )\n        bias_attr = paddle.ParamAttr(initializer=nn.initializer.Constant(value=value))\n        self.linear = nn.Linear(\n            in_channel,\n            1,\n            weight_attr=weight_attr,\n            bias_attr=bias_attr,\n        )\n        self.act = act_mod.get_activation(act) if act else None\n\n    def forward(self, x):\n        y = x\n        y = self.flatten(y)\n        y = self.linear(y)\n        if self.act:\n            y = self.act(y)\n        return y\n\n\nclass Generator(base.Arch):\n    \"\"\"Generator Net of GAN. Attention, the net using a kind of variant of ResBlock which is\n        unique to \"tempoGAN\" example but not an open source network.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input1\", \"input2\").\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output1\", \"output2\").\n        in_channel (int): Number of input channels of the first conv layer.\n        out_channels_tuple (Tuple[Tuple[int, ...], ...]): Number of output channels of all conv layers,\n            such as [[out_res0_conv0, out_res0_conv1], [out_res1_conv0, out_res1_conv1]]\n        kernel_sizes_tuple (Tuple[Tuple[int, ...], ...]): Number of kernel_size of all conv layers,\n            such as [[kernel_size_res0_conv0, kernel_size_res0_conv1], [kernel_size_res1_conv0, kernel_size_res1_conv1]]\n        strides_tuple (Tuple[Tuple[int, ...], ...]): Number of stride of all conv layers,\n            such as [[stride_res0_conv0, stride_res0_conv1], [stride_res1_conv0, stride_res1_conv1]]\n        use_bns_tuple (Tuple[Tuple[bool, ...], ...]): Whether to use the batch_norm layer after each conv layer.\n        acts_tuple (Tuple[Tuple[str, ...], ...]): Whether to use the activation layer after each conv layer. If so, witch activation to use,\n            such as [[act_res0_conv0, act_res0_conv1], [act_res1_conv0, act_res1_conv1]]\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; in_channel = 1\n        &gt;&gt;&gt; rb_channel0 = (2, 8, 8)\n        &gt;&gt;&gt; rb_channel1 = (128, 128, 128)\n        &gt;&gt;&gt; rb_channel2 = (32, 8, 8)\n        &gt;&gt;&gt; rb_channel3 = (2, 1, 1)\n        &gt;&gt;&gt; out_channels_tuple = (rb_channel0, rb_channel1, rb_channel2, rb_channel3)\n        &gt;&gt;&gt; kernel_sizes_tuple = (((5, 5), ) * 2 + ((1, 1), ), ) * 4\n        &gt;&gt;&gt; strides_tuple = ((1, 1, 1), ) * 4\n        &gt;&gt;&gt; use_bns_tuple = ((True, True, True), ) * 3 + ((False, False, False), )\n        &gt;&gt;&gt; acts_tuple = ((\"relu\", None, None), ) * 4\n        &gt;&gt;&gt; model = ppsci.arch.Generator((\"in\",), (\"out\",), in_channel, out_channels_tuple, kernel_sizes_tuple, strides_tuple, use_bns_tuple, acts_tuple)\n        &gt;&gt;&gt; batch_size = 4\n        &gt;&gt;&gt; height = 64\n        &gt;&gt;&gt; width = 64\n        &gt;&gt;&gt; input_data = paddle.randn([batch_size, in_channel, height, width])\n        &gt;&gt;&gt; input_dict = {'in': input_data}\n        &gt;&gt;&gt; output_data = model(input_dict)\n        &gt;&gt;&gt; print(output_data['out'].shape)\n        [4, 1, 64, 64]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        in_channel: int,\n        out_channels_tuple: Tuple[Tuple[int, ...], ...],\n        kernel_sizes_tuple: Tuple[Tuple[int, ...], ...],\n        strides_tuple: Tuple[Tuple[int, ...], ...],\n        use_bns_tuple: Tuple[Tuple[bool, ...], ...],\n        acts_tuple: Tuple[Tuple[str, ...], ...],\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.in_channel = in_channel\n        self.out_channels_tuple = out_channels_tuple\n        self.kernel_sizes_tuple = kernel_sizes_tuple\n        self.strides_tuple = strides_tuple\n        self.use_bns_tuple = use_bns_tuple\n        self.acts_tuple = acts_tuple\n\n        self.init_blocks()\n\n    def init_blocks(self):\n        blocks_list = []\n        for i in range(len(self.out_channels_tuple)):\n            in_channel = (\n                self.in_channel if i == 0 else self.out_channels_tuple[i - 1][-1]\n            )\n            blocks_list.append(\n                VariantResBlock(\n                    in_channel=in_channel,\n                    out_channels=self.out_channels_tuple[i],\n                    kernel_sizes=self.kernel_sizes_tuple[i],\n                    strides=self.strides_tuple[i],\n                    use_bns=self.use_bns_tuple[i],\n                    acts=self.acts_tuple[i],\n                    mean=0.0,\n                    std=0.04,\n                    value=0.1,\n                )\n            )\n        self.blocks = nn.LayerList(blocks_list)\n\n    def forward_tensor(self, x):\n        y = x\n        for block in self.blocks:\n            y = block(y)\n        return y\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        y = self.concat_to_tensor(x, self.input_keys, axis=-1)\n        y = self.forward_tensor(y)\n        y = self.split_to_dict(y, self.output_keys, axis=-1)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n        return y\n\n\nclass Discriminator(base.Arch):\n    \"\"\"Discriminator Net of GAN.\n\n    Args:\n        input_keys (Tuple[str, ...]): Name of input keys, such as (\"input1\", \"input2\").\n        output_keys (Tuple[str, ...]): Name of output keys, such as (\"output1\", \"output2\").\n        in_channel (int):  Number of input channels of the first conv layer.\n        out_channels (Tuple[int, ...]): Number of output channels of all conv layers,\n            such as (out_conv0, out_conv1, out_conv2).\n        fc_channel (int):  Number of input features of linear layer. Number of output features of the layer\n            is set to 1 in this Net to construct a fully_connected layer.\n        kernel_sizes (Tuple[int, ...]): Number of kernel_size of all conv layers,\n            such as (kernel_size_conv0, kernel_size_conv1, kernel_size_conv2).\n        strides (Tuple[int, ...]): Number of stride of all conv layers,\n            such as (stride_conv0, stride_conv1, stride_conv2).\n        use_bns (Tuple[bool, ...]): Whether to use the batch_norm layer after each conv layer.\n        acts (Tuple[str, ...]): Whether to use the activation layer after each conv layer. If so, witch activation to use,\n            such as (act_conv0, act_conv1, act_conv2).\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; in_channel = 2\n        &gt;&gt;&gt; in_channel_tempo = 3\n        &gt;&gt;&gt; out_channels = (32, 64, 128, 256)\n        &gt;&gt;&gt; fc_channel = 65536\n        &gt;&gt;&gt; kernel_sizes = ((4, 4), (4, 4), (4, 4), (4, 4))\n        &gt;&gt;&gt; strides = (2, 2, 2, 1)\n        &gt;&gt;&gt; use_bns = (False, True, True, True)\n        &gt;&gt;&gt; acts = (\"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", None)\n        &gt;&gt;&gt; output_keys_disc = (\"out_1\", \"out_2\", \"out_3\", \"out_4\", \"out_5\", \"out_6\", \"out_7\", \"out_8\", \"out_9\", \"out_10\")\n        &gt;&gt;&gt; model = ppsci.arch.Discriminator((\"in_1\",\"in_2\"), output_keys_disc, in_channel, out_channels, fc_channel, kernel_sizes, strides, use_bns, acts)\n        &gt;&gt;&gt; input_data = [paddle.to_tensor(paddle.randn([1, in_channel, 128, 128])),paddle.to_tensor(paddle.randn([1, in_channel, 128, 128]))]\n        &gt;&gt;&gt; input_dict = {\"in_1\": input_data[0],\"in_2\": input_data[1]}\n        &gt;&gt;&gt; out_dict = model(input_dict)\n        &gt;&gt;&gt; for k, v in out_dict.items():\n        ...     print(k, v.shape)\n        out_1 [1, 32, 64, 64]\n        out_2 [1, 64, 32, 32]\n        out_3 [1, 128, 16, 16]\n        out_4 [1, 256, 16, 16]\n        out_5 [1, 1]\n        out_6 [1, 32, 64, 64]\n        out_7 [1, 64, 32, 32]\n        out_8 [1, 128, 16, 16]\n        out_9 [1, 256, 16, 16]\n        out_10 [1, 1]\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keys: Tuple[str, ...],\n        output_keys: Tuple[str, ...],\n        in_channel: int,\n        out_channels: Tuple[int, ...],\n        fc_channel: int,\n        kernel_sizes: Tuple[int, ...],\n        strides: Tuple[int, ...],\n        use_bns: Tuple[bool, ...],\n        acts: Tuple[str, ...],\n    ):\n        super().__init__()\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n        self.in_channel = in_channel\n        self.out_channels = out_channels\n        self.fc_channel = fc_channel\n        self.kernel_sizes = kernel_sizes\n        self.strides = strides\n        self.use_bns = use_bns\n        self.acts = acts\n\n        self.init_layers()\n\n    def init_layers(self):\n        layers_list = []\n        for i in range(len(self.out_channels)):\n            in_channel = self.in_channel if i == 0 else self.out_channels[i - 1]\n            layers_list.append(\n                Conv2DBlock(\n                    in_channel=in_channel,\n                    out_channel=self.out_channels[i],\n                    kernel_size=self.kernel_sizes[i],\n                    stride=self.strides[i],\n                    use_bn=self.use_bns[i],\n                    act=self.acts[i],\n                    mean=0.0,\n                    std=0.04,\n                    value=0.1,\n                )\n            )\n\n        layers_list.append(\n            FCBlock(self.fc_channel, self.acts[4], mean=0.0, std=0.04, value=0.1)\n        )\n        self.layers = nn.LayerList(layers_list)\n\n    def forward_tensor(self, x):\n        y = x\n        y_list = []\n        for layer in self.layers:\n            y = layer(y)\n            y_list.append(y)\n        return y_list  # y_conv1, y_conv2, y_conv3, y_conv4, y_fc(y_out)\n\n    def forward(self, x):\n        if self._input_transform is not None:\n            x = self._input_transform(x)\n\n        y_list = []\n        # y1_conv1, y1_conv2, y1_conv3, y1_conv4, y1_fc, y2_conv1, y2_conv2, y2_conv3, y2_conv4, y2_fc\n        for k in x:\n            y_list.extend(self.forward_tensor(x[k]))\n\n        y = self.split_to_dict(y_list, self.output_keys)\n\n        if self._output_transform is not None:\n            y = self._output_transform(x, y)\n\n        return y\n\n    @staticmethod\n    def split_to_dict(\n        data_list: List[paddle.Tensor], keys: Tuple[str, ...]\n    ) -&gt; Dict[str, paddle.Tensor]:\n        \"\"\"Overwrite of split_to_dict() method belongs to Class base.Arch.\n\n        Reason for overwriting is there is no concat_to_tensor() method called in \"tempoGAN\" example.\n        That is because input in \"tempoGAN\" example is not in a regular format, but a format like:\n        {\n            \"input1\": paddle.concat([in1, in2], axis=1),\n            \"input2\": paddle.concat([in1, in3], axis=1),\n        }\n\n        Args:\n            data_list (List[paddle.Tensor]): The data to be split. It should be a list of tensor(s), but not a paddle.Tensor.\n            keys (Tuple[str, ...]): Keys of outputs.\n\n        Returns:\n            Dict[str, paddle.Tensor]: Dict with split data.\n        \"\"\"\n        if len(keys) == 1:\n            return {keys[0]: data_list[0]}\n        return {key: data_list[i] for i, key in enumerate(keys)}\n</code></pre>"},{"location":"zh/examples/tempoGAN/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u540e\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u4e0e\u76ee\u6807\u4e4b\u95f4\u7684 MSE\u3001PSNR\u3001SSIM\uff0c\u8bc4\u4f30\u6307\u6807\u7684\u503c\u4e3a\uff1a</p> MSE PSNR SSIM 4.21e-5 47.19 0.9974 <p>\u4e00\u4e2a\u6d41\u4f53\u8d85\u5206\u6837\u4f8b\u7684\u8f93\u5165\u3001\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u3001\u6570\u636e\u96c6\u4ecb\u7ecd\u4e2d\u5f00\u6e90\u4ee3\u7801\u5305 mantaflow \u76f4\u63a5\u751f\u6210\u7684\u7ed3\u679c\u5982\u4e0b\uff0c\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e0e\u751f\u6210\u7684\u76ee\u6807\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p> <p> </p> \u8f93\u5165\u7684\u4f4e\u5bc6\u5ea6\u6d41\u4f53 <p> </p> \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u540e\u63a8\u7406\u5f97\u5230\u7684\u9ad8\u5bc6\u5ea6\u6d41\u4f53 <p> </p>  \u76ee\u6807\u9ad8\u5bc6\u5ea6\u6d41\u4f53"},{"location":"zh/examples/tempoGAN/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li> <p>tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow</p> </li> <li> <p>\u53c2\u8003\u4ee3\u7801</p> </li> </ul>"},{"location":"zh/examples/topopt/","title":"TopOpt","text":""},{"location":"zh/examples/topopt/#topopt","title":"TopOpt","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 --create-dirs -o ./datasets/top_dataset.h5\npython topopt.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 --create-dirs -o ./datasets/top_dataset.h5\npython topopt.py mode=eval 'EVAL.pretrained_model_path_dict={'Uniform': 'https://paddle-org.bj.bcebos.com/paddlescience/models/topopt/uniform_pretrained.pdparams', 'Poisson5': 'https://paddle-org.bj.bcebos.com/paddlescience/models/topopt/poisson5_pretrained.pdparams', 'Poisson10': 'https://paddle-org.bj.bcebos.com/paddlescience/models/topopt/poisson10_pretrained.pdparams', 'Poisson30': 'https://paddle-org.bj.bcebos.com/paddlescience/models/topopt/poisson30_pretrained.pdparams'}'\n</code></pre> <pre><code>python topopt.py mode=export INFER.pretrained_model_name=Uniform\n</code></pre> <pre><code>python topopt.py mode=export INFER.pretrained_model_name=Poisson5\n</code></pre> <pre><code>python topopt.py mode=export INFER.pretrained_model_name=Poisson10\n</code></pre> <pre><code>python topopt.py mode=export INFER.pretrained_model_name=Poisson30\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 --create-dirs -o ./datasets/top_dataset.h5\npython topopt.py mode=infer INFER.pretrained_model_name=Uniform INFER.img_num=3\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 --create-dirs -o ./datasets/top_dataset.h5\npython topopt.py mode=infer INFER.pretrained_model_name=Poisson5 INFER.img_num=3\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 --create-dirs -o ./datasets/top_dataset.h5\npython topopt.py mode=infer INFER.pretrained_model_name=Poisson10 INFER.img_num=3\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 -P ./datasets/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/topopt/top_dataset.h5 --create-dirs -o ./datasets/top_dataset.h5\npython topopt.py mode=infer INFER.pretrained_model_name=Poisson30 INFER.img_num=3\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 topopt_uniform_pretrained.pdparams loss(sup_validator): [0.14336, 0.10211, 0.07927, 0.06433, 0.04970, 0.04612, 0.04201, 0.03566, 0.03623, 0.03314, 0.02929, 0.02857, 0.02498, 0.02517, 0.02523, 0.02618]metric.Binary_Acc(sup_validator): [0.9410, 0.9673, 0.9718, 0.9727, 0.9818, 0.9824, 0.9826, 0.9845, 0.9856, 0.9892, 0.9892, 0.9907, 0.9890, 0.9916, 0.9914, 0.9922]metric.IoU(sup_validator): [0.8887, 0.9367, 0.9452, 0.9468, 0.9644, 0.9655, 0.9659, 0.9695, 0.9717, 0.9787, 0.9787, 0.9816, 0.9784, 0.9835, 0.9831, 0.9845] topopt_poisson5_pretrained.pdparams loss(sup_validator): [0.11926, 0.09162, 0.08014, 0.06390, 0.05839, 0.05264, 0.04921, 0.04737, 0.04872, 0.04564, 0.04226, 0.04267, 0.04407, 0.04172, 0.03939, 0.03927]metric.Binary_Acc(sup_validator): [0.9471, 0.9619, 0.9702, 0.9742, 0.9782, 0.9801, 0.9803, 0.9825, 0.9824, 0.9837, 0.9850, 0.9850, 0.9870, 0.9863, 0.9870, 0.9872]metric.IoU(sup_validator): [0.8995, 0.9267, 0.9421, 0.9497, 0.9574, 0.9610, 0.9614, 0.9657, 0.9655, 0.9679, 0.9704, 0.9704, 0.9743, 0.9730, 0.9744, 0.9747] topopt_poisson10_pretrained.pdparams loss(sup_validator): [0.12886, 0.07201, 0.05946, 0.04622, 0.05072, 0.04178, 0.03823, 0.03677, 0.03623, 0.03029, 0.03398, 0.02978, 0.02861, 0.02946, 0.02831, 0.02817]metric.Binary_Acc(sup_validator): [0.9457, 0.9703, 0.9745, 0.9798, 0.9827, 0.9845, 0.9859, 0.9870, 0.9882, 0.9880, 0.9893, 0.9899, 0.9882, 0.9899, 0.9905, 0.9904]metric.IoU(sup_validator): [0.8969, 0.9424, 0.9502, 0.9604, 0.9660, 0.9696, 0.9722, 0.9743, 0.9767, 0.9762, 0.9789, 0.9800, 0.9768, 0.9801, 0.9813, 0.9810] topopt_poisson30_pretrained.pdparams loss(sup_validator): [0.19111, 0.10081, 0.06930, 0.04631, 0.03821, 0.03441, 0.02738, 0.03040, 0.02787, 0.02385, 0.02037, 0.02065, 0.01840, 0.01896, 0.01970, 0.01676]metric.Binary_Acc(sup_validator): [0.9257, 0.9595, 0.9737, 0.9832, 0.9828, 0.9883, 0.9885, 0.9892, 0.9901, 0.9916, 0.9924, 0.9925, 0.9926, 0.9929, 0.9937, 0.9936]metric.IoU(sup_validator): [0.8617, 0.9221, 0.9488, 0.9670, 0.9662, 0.9769, 0.9773, 0.9786, 0.9803, 0.9833, 0.9850, 0.9853, 0.9855, 0.9860, 0.9875, 0.9873]"},{"location":"zh/examples/topopt/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u62d3\u6251\u4f18\u5316 (Topolgy Optimization) \u662f\u4e00\u79cd\u6570\u5b66\u65b9\u6cd5\uff0c\u9488\u5bf9\u7ed9\u5b9a\u7684\u4e00\u7ec4\u8d1f\u8f7d\u3001\u8fb9\u754c\u6761\u4ef6\u548c\u7ea6\u675f\uff0c\u5728\u7ed9\u5b9a\u7684\u8bbe\u8ba1\u533a\u57df\u5185\uff0c\u4ee5\u6700\u5927\u5316\u7cfb\u7edf\u6027\u80fd\u4e3a\u76ee\u6807\u4f18\u5316\u6750\u6599\u7684\u5206\u5e03\u3002\u8fd9\u4e2a\u95ee\u9898\u5f88\u6709\u6311\u6218\u6027\u56e0\u4e3a\u5b83\u8981\u6c42\u89e3\u51b3\u65b9\u6848\u662f\u4e8c\u5143\u7684\uff0c\u5373\u5e94\u8be5\u8bf4\u660e\u8bbe\u8ba1\u533a\u57df\u7684\u6bcf\u4e2a\u90e8\u5206\u662f\u5426\u5b58\u5728\u6750\u6599\u6216\u4e0d\u5b58\u5728\u3002\u8fd9\u79cd\u4f18\u5316\u7684\u4e00\u4e2a\u5e38\u89c1\u4f8b\u5b50\u662f\u5728\u7ed9\u5b9a\u603b\u91cd\u91cf\u548c\u8fb9\u754c\u6761\u4ef6\u4e0b\u6700\u5c0f\u5316\u7269\u4f53\u7684\u5f39\u6027\u5e94\u53d8\u80fd\u3002\u968f\u774020\u4e16\u7eaa\u6c7d\u8f66\u548c\u822a\u7a7a\u822a\u5929\u5de5\u4e1a\u7684\u53d1\u5c55\uff0c\u62d3\u6251\u4f18\u5316\u5df2\u7ecf\u5c06\u5e94\u7528\u6269\u5c55\u5230\u5f88\u591a\u5176\u4ed6\u5b66\u79d1\uff1a\u5982\u6d41\u4f53\u3001\u58f0\u5b66\u3001\u7535\u78c1\u5b66\u3001\u5149\u5b66\u53ca\u5176\u7ec4\u5408\u3002SIMP (Simplied Isotropic Material with Penalization) \u662f\u76ee\u524d\u5e7f\u6cdb\u4f20\u64ad\u7684\u4e00\u79cd\u7b80\u5355\u800c\u9ad8\u6548\u7684\u62d3\u6251\u4f18\u5316\u6c42\u89e3\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u5bf9\u6750\u6599\u5bc6\u5ea6\u7684\u4e2d\u95f4\u503c\u8fdb\u884c\u60e9\u7f5a\uff0c\u63d0\u9ad8\u4e86\u4e8c\u5143\u89e3\u7684\u6536\u655b\u6027\u3002</p>"},{"location":"zh/examples/topopt/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u62d3\u6251\u4f18\u5316\u95ee\u9898\uff1a</p> \\[ \\begin{aligned} &amp; \\underset{\\mathbf{x}}{\\text{min}} \\quad &amp;&amp; c(\\mathbf{u}(\\mathbf{x}), \\mathbf{x}) = \\sum_{j=1}^{N} E_{j}(x_{j})\\mathbf{u}_{j}^{\\intercal}\\mathbf{k}_{0}\\mathbf{u}_{j} \\\\ &amp; \\text{s.t.} \\quad &amp;&amp; V(\\mathbf{x})/V_{0} = f_{0} \\\\ &amp; \\quad &amp;&amp; \\mathbf{K}\\mathbf{U} = \\mathbf{F} \\\\ &amp; \\quad &amp;&amp; x_{j} \\in \\{0, 1\\}, \\quad j = 1,...,N \\end{aligned} \\] <p>\u5176\u4e2d\uff1a\\(x_{j}\\) \u662f\u6750\u6599\u5206\u5e03 (material distribution)\uff1b\\(c\\) \u6307\u53ef\u5851\u6027 (compliance)\uff1b\\(\\mathbf{u}_{j}\\) \u662f element displacement vector\uff1b\\(\\mathbf{k}_{0}\\) \u662f element stiffness matrix for an element with unit Youngs modulu\uff1b\\(\\mathbf{U}\\), \\(\\mathbf{F}\\) \u662f global displacement and force vectors\uff1b\\(\\mathbf{K}\\) \u662f global stiffness matrix\uff1b\\(V(\\mathbf{x})\\), \\(V_{0}\\) \u662f\u6750\u6599\u4f53\u79ef\u548c\u8bbe\u8ba1\u533a\u57df\u7684\u4f53\u79ef\uff1b\\(f_{0}\\) \u662f\u9884\u5148\u6307\u5b9a\u7684\u4f53\u79ef\u6bd4\u3002</p>"},{"location":"zh/examples/topopt/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u5b9e\u9645\u6c42\u89e3\u4e0a\u8ff0\u95ee\u9898\u65f6\u4e3a\u505a\u7b80\u5316\uff0c\u4f1a\u628a\u6700\u540e\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u6362\u6210\u8fde\u7eed\u7684\u5f62\u5f0f\uff1a\\(x_{j} \\in [0, 1], \\quad j = 1,...,N\\)\u3002 \u5e38\u89c1\u7684\u4f18\u5316\u7b97\u6cd5\u662f SIMP \u7b97\u6cd5\uff0c\u5b83\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u8fed\u4ee3\u6cd5\uff0c\u5e76\u5bf9\u975e\u4e8c\u5143\u89e3\u505a\u60e9\u7f5a\uff1a\\(E_{j}(x_{j}) = E_{\\text{min}} + x_{j}^{p}(E_{0} - E_{\\text{min}})\\)\uff0c\u8fd9\u91cc\u6211\u4eec\u4e0d\u5bf9 SIMP \u7b97\u6cd5\u505a\u8fc7\u591a\u5c55\u5f00\u3002\u7531\u4e8e\u5229\u7528 SIMP \u65b9\u6cd5, \u6c42\u89e3\u5668\u53ea\u9700\u8981\u8fdb\u884c\u521d\u59cb\u7684 \\(N_{0}\\) \u6b21\u8fed\u4ee3\u5c31\u53ef\u4ee5\u5f97\u5230\u4e0e\u7ed3\u679c\u7684\u6700\u7ec8\u7ed3\u679c\u975e\u5e38\u76f8\u8fd1\u7684\u57fa\u672c\u89c6\u56fe\uff0c\u672c\u6848\u4f8b\u5e0c\u671b\u901a\u8fc7\u5c06 SIMP \u7684\u7b2c \\(N_{0}\\) \u6b21\u521d\u59cb\u8fed\u4ee3\u7ed3\u679c\u4e0e\u5176\u5bf9\u5e94\u7684\u68af\u5ea6\u4fe1\u606f\u4f5c\u4e3a Unet \u7684\u8f93\u5165\uff0c\u9884\u6d4b SIMP \u7684100\u6b21\u8fed\u4ee3\u6b65\u9aa4\u540e\u7ed9\u51fa\u7684\u4f18\u5316\u89e3\u3002</p>"},{"location":"zh/examples/topopt/#31","title":"3.1 \u6570\u636e\u96c6\u51c6\u5907","text":"<p>\u4e0b\u8f7d\u7684\u6570\u636e\u96c6\u4e3a\u6574\u7406\u8fc7\u7684\u5408\u6210\u6570\u636e\uff0c\u6574\u7406\u540e\u7684\u683c\u5f0f\u4e3a <code>\"iters\": shape = (10000, 100, 40, 40)</code>\uff0c<code>\"target\": shape = (10000, 1, 40, 40)</code></p> <ul> <li> <p>10000 - \u968f\u673a\u751f\u6210\u95ee\u9898\u7684\u4e2a\u6570</p> </li> <li> <p>100 - SIMP \u8fed\u4ee3\u6b21\u6570</p> </li> <li> <p>40 - \u56fe\u50cf\u9ad8\u5ea6</p> </li> <li> <p>40 - \u56fe\u50cf\u5bbd\u5ea6</p> </li> </ul> <p>\u6570\u636e\u96c6\u5730\u5740\u8bf7\u5b58\u50a8\u4e8e <code>./datasets/top_dataset.h5</code></p> <p>\u751f\u6210\u8bad\u7ec3\u96c6\uff1a\u539f\u59cb\u4ee3\u7801\u5229\u7528\u6240\u6709\u768410000\u95ee\u9898\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002</p> <pre><code>def generate_train_test(\n    data_iters: np.ndarray,\n    data_targets: np.ndarray,\n    train_test_ratio: float,\n    n_sample: int,\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n]:\n    \"\"\"Generate training and testing set\n\n    Args:\n        data_iters (np.ndarray): data with 100 channels corresponding to the results of 100 steps of SIMP algorithm\n        data_targets (np.ndarray): final optimization solution given by SIMP algorithm\n        train_test_ratio (float): split ratio of training and testing sets, if `train_test_ratio` = 1 then only return training data\n        n_sample (int): number of total samples in training and testing sets to be sampled from the h5 dataset\n\n    Returns:\n        Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]: if `train_test_ratio` = 1, return (train_inputs, train_labels), else return (train_inputs, train_labels, test_inputs, test_labels)\n    \"\"\"\n    n_obj = len(data_iters)\n    idx = np.arange(n_obj)\n    np.random.shuffle(idx)\n    train_idx = idx[: int(train_test_ratio * n_sample)]\n    if train_test_ratio == 1.0:\n        return data_iters[train_idx], data_targets[train_idx]\n\n    test_idx = idx[int(train_test_ratio * n_sample) :]\n    train_iters = data_iters[train_idx]\n    train_targets = data_targets[train_idx]\n    test_iters = data_iters[test_idx]\n    test_targets = data_targets[test_idx]\n    return train_iters, train_targets, test_iters, test_targets\n</code></pre> <pre><code># read h5 data\nh5data = h5py.File(cfg.DATA_PATH, \"r\")\ndata_iters = np.array(h5data[\"iters\"])\ndata_targets = np.array(h5data[\"targets\"])\n\n# generate training dataset\ninputs_train, labels_train = func_module.generate_train_test(\n    data_iters, data_targets, cfg.train_test_ratio, cfg.n_samples\n</code></pre>"},{"location":"zh/examples/topopt/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u7ecf\u8fc7 SIMP \u7684 \\(N_{0}\\) \u6b21\u521d\u59cb\u8fed\u4ee3\u6b65\u9aa4\u5f97\u5230\u7684\u56fe\u50cf \\(I\\) \u53ef\u4ee5\u770b\u4f5c\u662f\u6a21\u7cca\u4e86\u7684\u6700\u7ec8\u7ed3\u6784\u3002\u7531\u4e8e\u6700\u7ec8\u7684\u4f18\u5316\u89e3\u7ed9\u51fa\u7684\u56fe\u50cf \\(I^*\\) \u5e76\u4e0d\u5305\u542b\u4e2d\u95f4\u8fc7\u7a0b\u7684\u4fe1\u606f\uff0c\u56e0\u6b64 \\(I^*\\) \u53ef\u4ee5\u88ab\u89e3\u91ca\u4e3a\u56fe\u50cf \\(I\\) \u7684\u63a9\u7801\u3002\u4e8e\u662f \\(I \\rightarrow I^*\\) \u8fd9\u4e00\u4f18\u5316\u8fc7\u7a0b\u53ef\u4ee5\u770b\u4f5c\u662f\u4e8c\u5206\u7c7b\u7684\u56fe\u50cf\u5206\u5272\u6216\u8005\u524d\u666f-\u80cc\u666f\u5206\u5272\u8fc7\u7a0b\uff0c\u56e0\u6b64\u6784\u5efa Unet \u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c\u5177\u4f53\u7f51\u7edc\u7ed3\u6784\u5982\u56fe\u6240\u793a\uff1a </p> <pre><code># set model\n</code></pre> <p>\u8be6\u7ec6\u7684\u6a21\u578b\u4ee3\u7801\u5728 <code>examples/topopt/topoptmodel.py</code> \u4e2d\u3002</p>"},{"location":"zh/examples/topopt/#33","title":"3.3 \u53c2\u6570\u8bbe\u5b9a","text":"<p>\u6839\u636e\u8bba\u6587\u4ee5\u53ca\u539f\u59cb\u4ee3\u7801\u7ed9\u51fa\u4ee5\u4e0b\u8bad\u7ec3\u53c2\u6570\uff1a</p> <pre><code># other parameters\nn_samples: 10000\ntrain_test_ratio: 1.0 # use 10000 original data with different channels for training\nvol_coeff: 1 # coefficient for volume fraction constraint in the loss - beta in equation (3) in paper\n\n# training settings\n</code></pre> <pre><code># 4 training cases parameters\nLEARNING_RATE = cfg.TRAIN.learning_rate / (1 + cfg.TRAIN.epochs // 15)\n</code></pre>"},{"location":"zh/examples/topopt/#34-data-transform","title":"3.4 data transform","text":"<p>\u6839\u636e\u8bba\u6587\u4ee5\u53ca\u539f\u59cb\u4ee3\u7801\u7ed9\u51fa\u4ee5\u4e0b\u81ea\u5b9a\u4e49\u7684 data transform \u4ee3\u7801\uff0c\u5305\u62ec\u968f\u673a\u6c34\u5e73\u6216\u5782\u76f4\u7ffb\u8f6c\u548c\u968f\u673a90\u5ea6\u65cb\u8f6c\uff0c\u5bf9 input \u548c label \u540c\u65f6 transform\uff1a</p> <pre><code>def augmentation(\n    input_dict: Dict[str, np.ndarray],\n    label_dict: Dict[str, np.ndarray],\n    weight_dict: Dict[str, np.ndarray] = None,\n) -&gt; Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n    \"\"\"Apply random transformation from D4 symmetry group\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): input dict of np.ndarray size `(batch_size, any, height, width)`\n        label_dict (Dict[str, np.ndarray]): label dict of np.ndarray size `(batch_size, 1, height, width)`\n        weight_dict (Dict[str, np.ndarray]): weight dict if any\n    \"\"\"\n    inputs = input_dict[\"input\"]\n    labels = label_dict[\"output\"]\n    assert len(inputs.shape) == 3\n    assert len(labels.shape) == 3\n\n    # random horizontal flip\n    if np.random.random() &gt; 0.5:\n        inputs = np.flip(inputs, axis=2)\n        labels = np.flip(labels, axis=2)\n    # random vertical flip\n    if np.random.random() &gt; 0.5:\n        inputs = np.flip(inputs, axis=1)\n        labels = np.flip(labels, axis=1)\n    # random 90* rotation\n    if np.random.random() &gt; 0.5:\n        new_perm = list(range(len(inputs.shape)))\n        new_perm[-2], new_perm[-1] = new_perm[-1], new_perm[-2]\n        inputs = np.transpose(inputs, new_perm)\n        labels = np.transpose(labels, new_perm)\n</code></pre>"},{"location":"zh/examples/topopt/#35","title":"3.5 \u7ea6\u675f\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u6240\u4ee5\u4f7f\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code>\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set constraints\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"input\": inputs_train},\n            \"label\": {\"output\": labels_train},\n            \"transforms\": (\n                {\n                    \"FunctionalTransform\": {\n                        \"transform_func\": func_module.augmentation,\n                    },\n                },\n            ),\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.FunctionalLoss(loss_wrapper(cfg)),\n    name=\"sup_constraint\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u914d\u7f6e\u4e2d <code>\"dataset\"</code> \u5b57\u6bb5\u8868\u793a\u4f7f\u7528\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4fe1\u606f\uff0c\u5176\u5404\u4e2a\u5b57\u6bb5\u5206\u522b\u8868\u793a\uff1a</p> <ol> <li><code>name</code>\uff1a \u6570\u636e\u96c6\u7c7b\u578b\uff0c\u6b64\u5904 <code>\"NamedArrayDataset\"</code> \u8868\u793a\u5206 batch \u987a\u5e8f\u8bfb\u53d6\u7684 <code>np.ndarray</code> \u7c7b\u578b\u7684\u6570\u636e\u96c6\uff1b</li> <li><code>input</code>\uff1a \u8f93\u5165\u53d8\u91cf\u5b57\u5178\uff1a<code>{\"input_name\": input_dataset}</code>\uff1b</li> <li><code>label</code>\uff1a \u6807\u7b7e\u53d8\u91cf\u5b57\u5178\uff1a<code>{\"label_name\": label_dataset}</code>\uff1b</li> <li><code>transforms</code>\uff1a \u6570\u636e\u96c6\u9884\u5904\u7406\u914d\uff0c\u5176\u4e2d <code>\"FunctionalTransform\"</code> \u4e3a\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u9884\u5904\u7406\u65b9\u5f0f\u3002</li> </ol> <p>\u8bfb\u53d6\u914d\u7f6e\u4e2d <code>\"batch_size\"</code> \u5b57\u6bb5\u8868\u793a\u8bad\u7ec3\u65f6\u6307\u5b9a\u7684\u6279\u5927\u5c0f\uff0c<code>\"sampler\"</code> \u5b57\u6bb5\u8868\u793a dataloader \u7684\u76f8\u5173\u91c7\u6837\u914d\u7f6e\u3002</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u91cc\u4f7f\u7528\u81ea\u5b9a\u4e49\u635f\u5931\uff0c\u901a\u8fc7 <code>cfg.vol_coeff</code> \u786e\u5b9a\u635f\u5931\u516c\u5f0f\u4e2d \\(\\beta\\) \u5bf9\u5e94\u7684\u503c\u3002</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u6b21\u547d\u540d\u4e3a <code>\"sup_constraint\"</code>\u3002</p> <p>\u5728\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p>"},{"location":"zh/examples/topopt/#36","title":"3.6 \u91c7\u6837\u5668\u6784\u5efa","text":"<p>\u539f\u59cb\u6570\u636e\u7b2c\u4e8c\u7ef4\u6709100\u4e2a\u901a\u9053\uff0c\u5bf9\u5e94\u7684\u662f SIMP \u7b97\u6cd5 100 \u6b21\u7684\u8fed\u4ee3\u7ed3\u679c\uff0c\u672c\u6848\u4f8b\u6a21\u578b\u76ee\u6807\u662f\u7528 SIMP \u4e2d\u95f4\u67d0\u4e00\u6b65\u7684\u8fed\u4ee3\u7ed3\u679c\u76f4\u63a5\u9884\u6d4b SIMP \u7b97\u6cd5100\u6b65\u8fed\u4ee3\u540e\u6700\u7ec8\u7684\u4f18\u5316\u6c42\u89e3\u7ed3\u679c\uff0c\u8fd9\u91cc\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u901a\u9053\u91c7\u6837\u5668\uff0c\u7528\u6765\u5c06\u8f93\u5165\u6a21\u578b\u6570\u636e\u7684\u7b2c\u4e8c\u7ef4\u6309\u4e00\u5b9a\u7684\u6982\u7387\u5206\u5e03\u968f\u673a\u62bd\u53d6\u67d0\u4e00\u901a\u9053\u6216\u76f4\u63a5\u6307\u5b9a\u67d0\u4e00\u901a\u9053\uff0c\u518d\u8f93\u5165\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\u6216\u63a8\u7406\u3002\u672c\u6848\u4f8b\u5c06\u91c7\u6837\u6b65\u9aa4\u653e\u5165\u6a21\u578b\u7684 forward \u65b9\u6cd5\u4e2d\u3002</p> <pre><code>def uniform_sampler() -&gt; Callable[[], int]:\n    \"\"\"Generate uniform sampling function from 1 to 99\n\n    Returns:\n        sampler (Callable[[], int]): uniform sampling from 1 to 99\n    \"\"\"\n    return lambda: np.random.randint(1, 99)\n\n\ndef poisson_sampler(lam: int) -&gt; Callable[[], int]:\n    \"\"\"Generate poisson sampling function with parameter lam with range 1 to 99\n\n    Args:\n        lam (int): poisson rate parameter\n\n    Returns:\n        sampler (Callable[[], int]): poisson sampling function with parameter lam with range 1 to 99\n    \"\"\"\n\n    def func():\n        iter_ = max(np.random.poisson(lam), 1)\n        iter_ = min(iter_, 99)\n        return iter_\n\n    return func\n\n\ndef generate_sampler(sampler_type: str = \"Fixed\", num: int = 0) -&gt; Callable[[], int]:\n    \"\"\"Generate sampler for the number of initial iteration steps\n\n    Args:\n        sampler_type (str): \"Poisson\" for poisson sampler; \"Uniform\" for uniform sampler; \"Fixed\" for choosing a fixed number of initial iteration steps.\n        num (int): If `sampler_type` == \"Poisson\", `num` specifies the poisson rate parameter; If `sampler_type` == \"Fixed\", `num` specifies the fixed number of initial iteration steps.\n\n    Returns:\n        sampler (Callable[[], int]): sampler for the number of initial iteration steps\n    \"\"\"\n    if sampler_type == \"Poisson\":\n        return poisson_sampler(num)\n    elif sampler_type == \"Uniform\":\n        return uniform_sampler()\n    else:\n        return lambda: num\n</code></pre> <pre><code># initialize SIMP iteration stop time sampler\n</code></pre>"},{"location":"zh/examples/topopt/#37","title":"3.7 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(learning_rate=LEARNING_RATE, epsilon=1.0e-7)(\n    model\n</code></pre>"},{"location":"zh/examples/topopt/#38-lossmetric","title":"3.8 loss\u548cmetric\u6784\u5efa","text":""},{"location":"zh/examples/topopt/#381-loss","title":"3.8.1 loss\u6784\u5efa","text":"<p>\u635f\u5931\u51fd\u6570\u4e3a confidence loss + beta * volume fraction constraints:</p> \\[ \\mathcal{L} = \\mathcal{L}_{\\text{conf}}(X_{\\text{true}}, X_{\\text{pred}}) + \\beta * \\mathcal{L}_{\\text{vol}}(X_{\\text{true}}, X_{\\text{pred}}) \\] <p>confidence loss \u662f binary cross-entropy:</p> \\[ \\mathcal{L}_{\\text{conf}}(X_{\\text{true}}, X_{\\text{pred}}) = -\\frac{1}{NM}\\sum_{i=1}^{N}\\sum_{j=1}^{M}\\left[X_{\\text{true}}^{ij}\\log(X_{\\text{pred}}^{ij}) +  (1 - X_{\\text{true}}^{ij})\\log(1 - X_{\\text{pred}}^{ij})\\right] \\] <p>volume fraction constraints:</p> \\[ \\mathcal{L}_{\\text{vol}}(X_{\\text{true}}, X_{\\text{pred}}) = (\\bar{X}_{\\text{pred}} - \\bar{X}_{\\text{true}})^2 \\] <p>loss \u6784\u5efa\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># define loss wrapper\ndef loss_wrapper(cfg: DictConfig):\n    def loss_expr(output_dict, label_dict, weight_dict=None):\n        label_true = label_dict[\"output\"].reshape((-1, 1))\n        label_pred = output_dict[\"output\"].reshape((-1, 1))\n        conf_loss = paddle.mean(\n            nn.functional.log_loss(label_pred, label_true, epsilon=1.0e-7)\n        )\n        vol_loss = paddle.square(paddle.mean(label_true - label_pred))\n        return {\"output\": conf_loss + cfg.vol_coeff * vol_loss}\n</code></pre>"},{"location":"zh/examples/topopt/#382-metric","title":"3.8.2 metric\u6784\u5efa","text":"<p>\u672c\u6848\u4f8b\u539f\u59cb\u4ee3\u7801\u9009\u62e9 Binary Accuracy \u548c IoU \u8fdb\u884c\u8bc4\u4f30:</p> \\[ \\text{Bin. Acc.} = \\frac{w_{00}+w_{11}}{n_{0}+n_{1}} \\] \\[ \\text{IoU} = \\frac{1}{2}\\left[\\frac{w_{00}}{n_{0}+w_{10}} + \\frac{w_{11}}{n_{1}+w_{01}}\\right] \\] <p>\u5176\u4e2d \\(n_{0} = w_{00} + w_{01}\\) \uff0c \\(n_{1} = w_{10} + w_{11}\\) \uff0c\\(w_{tp}\\) \u8868\u793a\u5b9e\u9645\u662f \\(t\\) \u7c7b\u4e14\u88ab\u9884\u6d4b\u4e3a \\(p\\) \u7c7b\u7684\u50cf\u7d20\u70b9\u7684\u6570\u91cf metric \u6784\u5efa\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># define metric\ndef val_metric(output_dict, label_dict, weight_dict=None):\n    label_pred = output_dict[\"output\"]\n    label_true = label_dict[\"output\"]\n    accurates = paddle.equal(paddle.round(label_true), paddle.round(label_pred))\n    acc = paddle.mean(paddle.cast(accurates, dtype=paddle.get_default_dtype()))\n    true_negative = paddle.sum(\n        paddle.multiply(\n            paddle.equal(paddle.round(label_pred), 0.0),\n            paddle.equal(paddle.round(label_true), 0.0),\n        ),\n        dtype=paddle.get_default_dtype(),\n    )\n    true_positive = paddle.sum(\n        paddle.multiply(\n            paddle.equal(paddle.round(label_pred), 1.0),\n            paddle.equal(paddle.round(label_true), 1.0),\n        ),\n        dtype=paddle.get_default_dtype(),\n    )\n    false_negative = paddle.sum(\n        paddle.multiply(\n            paddle.equal(paddle.round(label_pred), 1.0),\n            paddle.equal(paddle.round(label_true), 0.0),\n        ),\n        dtype=paddle.get_default_dtype(),\n    )\n    false_positive = paddle.sum(\n        paddle.multiply(\n            paddle.equal(paddle.round(label_pred), 0.0),\n            paddle.equal(paddle.round(label_true), 1.0),\n        ),\n        dtype=paddle.get_default_dtype(),\n    )\n    n_negative = paddle.add(false_negative, true_negative)\n    n_positive = paddle.add(true_positive, false_positive)\n    iou = 0.5 * paddle.add(\n        paddle.divide(true_negative, paddle.add(n_negative, false_positive)),\n        paddle.divide(true_positive, paddle.add(n_positive, false_negative)),\n    )\n</code></pre>"},{"location":"zh/examples/topopt/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u672c\u6848\u4f8b\u6839\u636e\u91c7\u6837\u5668\u7684\u4e0d\u540c\u9009\u62e9\u5171\u6709\u56db\u7ec4\u5b50\u6848\u4f8b\uff0c\u6848\u4f8b\u53c2\u6570\u5982\u4e0b\uff1a</p> <pre><code># general settings\nmode: train # running mode: train/eval\nseed: 42\n</code></pre> <p>\u8bad\u7ec3\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># train models for 4 cases\nfor sampler_key, num in cfg.CASE_PARAM:\n\n    # initialize SIMP iteration stop time sampler\n    SIMP_stop_point_sampler = func_module.generate_sampler(sampler_key, num)\n\n    # initialize logger for training\n    sampler_name = sampler_key + str(num) if num else sampler_key\n    OUTPUT_DIR = osp.join(\n        cfg.output_dir, f\"{sampler_name}_vol_coeff{cfg.vol_coeff}\"\n    )\n    logger.init_logger(\"ppsci\", osp.join(OUTPUT_DIR, \"train.log\"), \"info\")\n\n    # set model\n    model = TopOptNN(**cfg.MODEL, channel_sampler=SIMP_stop_point_sampler)\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(learning_rate=LEARNING_RATE, epsilon=1.0e-7)(\n        model\n    )\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        OUTPUT_DIR,\n        optimizer,\n        epochs=cfg.TRAIN.epochs,\n        iters_per_epoch=ITERS_PER_EPOCH,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        seed=cfg.seed,\n    )\n\n    # train model\n</code></pre>"},{"location":"zh/examples/topopt/#310","title":"3.10 \u8bc4\u4f30\u6a21\u578b","text":"<p>\u5bf9\u56db\u4e2a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u5206\u522b\u4f7f\u7528\u4e0d\u540c\u7684\u901a\u9053\u91c7\u6837\u5668 (\u539f\u59cb\u6570\u636e\u7684\u7b2c\u4e8c\u7ef4\u5bf9\u5e94\u8868\u793a\u7684\u662f SIMP \u7b97\u6cd5\u7684 100 \u6b65\u8f93\u51fa\u7ed3\u679c\uff0c\u7edf\u4e00\u53d6\u539f\u59cb\u6570\u636e\u7b2c\u4e8c\u7ef4\u7684\u7b2c 5\uff0c10\uff0c15\uff0c20\uff0c...\uff0c80 \u901a\u9053\u4ee5\u53ca\u5bf9\u5e94\u7684\u68af\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u65b0\u7684\u8f93\u5165\u6784\u5efa\u8bc4\u4f30\u6570\u636e\u96c6) \u8fdb\u884c\u8bc4\u4f30\uff0c\u6bcf\u6b21\u8bc4\u4f30\u65f6\u53ea\u53d6 <code>cfg.EVAL.num_val_step</code> \u4e2a bacth \u7684\u6570\u636e\uff0c\u8ba1\u7b97\u5b83\u4eec\u7684\u5e73\u5747 Binary Accuracy \u548c IoU \u6307\u6807\uff1b\u540c\u65f6\u8bc4\u4f30\u7ed3\u679c\u9700\u8981\u4e0e\u8f93\u5165\u6570\u636e\u672c\u8eab\u7684\u9608\u503c\u5224\u5b9a\u7ed3\u679c (0.5\u4f5c\u4e3a\u9608\u503c) \u4f5c\u6bd4\u8f83\u3002\u5177\u4f53\u4ee3\u7801\u8bf7\u53c2\u8003\u5b8c\u6574\u4ee3\u7801</p>"},{"location":"zh/examples/topopt/#3101","title":"3.10.1 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u4e3a\u5e94\u7528 PaddleScience API\uff0c\u6b64\u5904\u5728\u6bcf\u4e00\u6b21\u8bc4\u4f30\u65f6\u6784\u5efa\u4e00\u4e2a\u8bc4\u4f30\u5668 SupervisedValidator \u8fdb\u884c\u8bc4\u4f30\uff1a</p> <pre><code>sup_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"NamedArrayDataset\",\n            \"input\": {\"input\": inputs_eval},\n            \"label\": {\"output\": labels_eval},\n            \"transforms\": (\n                {\n                    \"FunctionalTransform\": {\n                        \"transform_func\": func_module.augmentation,\n                    },\n                },\n            ),\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n        \"num_workers\": 0,\n    },\n    ppsci.loss.FunctionalLoss(loss_wrapper(cfg)),\n    {\"output\": lambda out: out[\"output\"]},\n    {\"metric\": ppsci.metric.FunctionalMetric(val_metric)},\n    name=\"sup_validator\",\n)\n</code></pre> <p>\u8bc4\u4f30\u5668\u914d\u7f6e\u4e0e \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\uff0c\u8bfb\u53d6\u914d\u7f6e\u4e2d <code>\"num_workers\"\uff1a0</code> \u8868\u793a\u5355\u7ebf\u7a0b\u8bfb\u53d6\uff1b\u8bc4\u4ef7\u6307\u6807 <code>\"metric\"</code> \u4e3a\u81ea\u5b9a\u4e49\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u542b Binary Accuracy \u548c IoU\u3002</p>"},{"location":"zh/examples/topopt/#311","title":"3.11 \u8bc4\u4f30\u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u4f7f\u7528 <code>ppsci.utils.misc.plot_curve()</code> \u65b9\u6cd5\u76f4\u63a5\u7ed8\u5236 Binary Accuracy \u548c IoU \u7684\u7ed3\u679c\uff1a</p> <pre><code>ppsci.utils.misc.plot_curve(\n    acc_results_summary,\n    xlabel=\"iteration\",\n    ylabel=\"accuracy\",\n    output_dir=cfg.output_dir,\n)\nppsci.utils.misc.plot_curve(\n    iou_results_summary, xlabel=\"iteration\", ylabel=\"iou\", output_dir=cfg.output_dir\n</code></pre>"},{"location":"zh/examples/topopt/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"topopt.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom os import path as osp\nfrom typing import Dict\n\nimport functions as func_module\nimport h5py\nimport hydra\nimport numpy as np\nimport paddle\nfrom omegaconf import DictConfig\nfrom paddle import nn\nfrom topoptmodel import TopOptNN\n\nimport ppsci\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # 4 training cases parameters\n    LEARNING_RATE = cfg.TRAIN.learning_rate / (1 + cfg.TRAIN.epochs // 15)\n    ITERS_PER_EPOCH = int(cfg.n_samples * cfg.train_test_ratio / cfg.TRAIN.batch_size)\n\n    # read h5 data\n    h5data = h5py.File(cfg.DATA_PATH, \"r\")\n    data_iters = np.array(h5data[\"iters\"])\n    data_targets = np.array(h5data[\"targets\"])\n\n    # generate training dataset\n    inputs_train, labels_train = func_module.generate_train_test(\n        data_iters, data_targets, cfg.train_test_ratio, cfg.n_samples\n    )\n\n    # set constraints\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"NamedArrayDataset\",\n                \"input\": {\"input\": inputs_train},\n                \"label\": {\"output\": labels_train},\n                \"transforms\": (\n                    {\n                        \"FunctionalTransform\": {\n                            \"transform_func\": func_module.augmentation,\n                        },\n                    },\n                ),\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.FunctionalLoss(loss_wrapper(cfg)),\n        name=\"sup_constraint\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # train models for 4 cases\n    for sampler_key, num in cfg.CASE_PARAM:\n\n        # initialize SIMP iteration stop time sampler\n        SIMP_stop_point_sampler = func_module.generate_sampler(sampler_key, num)\n\n        # initialize logger for training\n        sampler_name = sampler_key + str(num) if num else sampler_key\n        OUTPUT_DIR = osp.join(\n            cfg.output_dir, f\"{sampler_name}_vol_coeff{cfg.vol_coeff}\"\n        )\n        logger.init_logger(\"ppsci\", osp.join(OUTPUT_DIR, \"train.log\"), \"info\")\n\n        # set model\n        model = TopOptNN(**cfg.MODEL, channel_sampler=SIMP_stop_point_sampler)\n\n        # set optimizer\n        optimizer = ppsci.optimizer.Adam(learning_rate=LEARNING_RATE, epsilon=1.0e-7)(\n            model\n        )\n\n        # initialize solver\n        solver = ppsci.solver.Solver(\n            model,\n            constraint,\n            OUTPUT_DIR,\n            optimizer,\n            epochs=cfg.TRAIN.epochs,\n            iters_per_epoch=ITERS_PER_EPOCH,\n            eval_during_train=cfg.TRAIN.eval_during_train,\n            seed=cfg.seed,\n        )\n\n        # train model\n        solver.train()\n\n\n# evaluate 4 models\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n    # initialize logger\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, f\"{cfg.mode}.log\"), \"info\")\n\n    # fixed iteration stop times for evaluation\n    iterations_stop_times = range(5, 85, 5)\n    model = TopOptNN(**cfg.MODEL)\n\n    # evaluation for 4 cases\n    acc_results_summary = {}\n    iou_results_summary = {}\n\n    # read h5 data\n    h5data = h5py.File(cfg.DATA_PATH, \"r\")\n    data_iters = np.array(h5data[\"iters\"])\n    data_targets = np.array(h5data[\"targets\"])\n\n    for case_name, model_path in cfg.EVAL.pretrained_model_path_dict.items():\n        acc_results, iou_results = evaluate_model(\n            cfg, model, model_path, data_iters, data_targets, iterations_stop_times\n        )\n\n        acc_results_summary[case_name] = acc_results\n        iou_results_summary[case_name] = iou_results\n\n    # calculate thresholding results\n    th_acc_results = []\n    th_iou_results = []\n    for stop_iter in iterations_stop_times:\n        SIMP_stop_point_sampler = func_module.generate_sampler(\"Fixed\", stop_iter)\n\n        current_acc_results = []\n        current_iou_results = []\n\n        # only calculate for NUM_VAL_STEP times of iteration\n        for _ in range(cfg.EVAL.num_val_step):\n            input_full_channel, label = func_module.generate_train_test(\n                data_iters, data_targets, 1.0, cfg.EVAL.batch_size\n            )\n            # thresholding\n            SIMP_initial_iter_time = SIMP_stop_point_sampler()  # channel k\n            input_channel_k = paddle.to_tensor(\n                input_full_channel, dtype=paddle.get_default_dtype()\n            )[:, SIMP_initial_iter_time, :, :]\n            input_channel_k_minus_1 = paddle.to_tensor(\n                input_full_channel, dtype=paddle.get_default_dtype()\n            )[:, SIMP_initial_iter_time - 1, :, :]\n            input = paddle.stack(\n                (input_channel_k, input_channel_k - input_channel_k_minus_1), axis=1\n            )\n            out = paddle.cast(\n                paddle.to_tensor(input)[:, 0:1, :, :] &gt; 0.5,\n                dtype=paddle.get_default_dtype(),\n            )\n            th_result = val_metric(\n                {\"output\": out},\n                {\"output\": paddle.to_tensor(label, dtype=paddle.get_default_dtype())},\n            )\n            acc_results, iou_results = th_result[\"Binary_Acc\"], th_result[\"IoU\"]\n            current_acc_results.append(acc_results)\n            current_iou_results.append(iou_results)\n\n        th_acc_results.append(np.mean(current_acc_results))\n        th_iou_results.append(np.mean(current_iou_results))\n\n    acc_results_summary[\"thresholding\"] = th_acc_results\n    iou_results_summary[\"thresholding\"] = th_iou_results\n\n    ppsci.utils.misc.plot_curve(\n        acc_results_summary,\n        xlabel=\"iteration\",\n        ylabel=\"accuracy\",\n        output_dir=cfg.output_dir,\n    )\n    ppsci.utils.misc.plot_curve(\n        iou_results_summary, xlabel=\"iteration\", ylabel=\"iou\", output_dir=cfg.output_dir\n    )\n\n\ndef evaluate_model(\n    cfg, model, pretrained_model_path, data_iters, data_targets, iterations_stop_times\n):\n    # load model parameters\n    solver = ppsci.solver.Solver(\n        model,\n        epochs=1,\n        iters_per_epoch=cfg.EVAL.num_val_step,\n        eval_with_no_grad=True,\n        pretrained_model_path=pretrained_model_path,\n    )\n\n    acc_results = []\n    iou_results = []\n\n    # evaluation for different fixed iteration stop times\n    for stop_iter in iterations_stop_times:\n        # only evaluate for NUM_VAL_STEP times of iteration\n        inputs_eval, labels_eval = func_module.generate_train_test(\n            data_iters, data_targets, 1.0, cfg.EVAL.batch_size * cfg.EVAL.num_val_step\n        )\n\n        sup_validator = ppsci.validate.SupervisedValidator(\n            {\n                \"dataset\": {\n                    \"name\": \"NamedArrayDataset\",\n                    \"input\": {\"input\": inputs_eval},\n                    \"label\": {\"output\": labels_eval},\n                    \"transforms\": (\n                        {\n                            \"FunctionalTransform\": {\n                                \"transform_func\": func_module.augmentation,\n                            },\n                        },\n                    ),\n                },\n                \"batch_size\": cfg.EVAL.batch_size,\n                \"sampler\": {\n                    \"name\": \"BatchSampler\",\n                    \"drop_last\": False,\n                    \"shuffle\": True,\n                },\n                \"num_workers\": 0,\n            },\n            ppsci.loss.FunctionalLoss(loss_wrapper(cfg)),\n            {\"output\": lambda out: out[\"output\"]},\n            {\"metric\": ppsci.metric.FunctionalMetric(val_metric)},\n            name=\"sup_validator\",\n        )\n        validator = {sup_validator.name: sup_validator}\n        solver.validator = validator\n\n        # modify the channel_sampler in model\n        SIMP_stop_point_sampler = func_module.generate_sampler(\"Fixed\", stop_iter)\n        solver.model.channel_sampler = SIMP_stop_point_sampler\n\n        _, eval_result = solver.eval()\n\n        current_acc_results = eval_result[\"metric\"][\"Binary_Acc\"]\n        current_iou_results = eval_result[\"metric\"][\"IoU\"]\n\n        acc_results.append(current_acc_results)\n        iou_results.append(current_iou_results)\n\n    return acc_results, iou_results\n\n\n# define loss wrapper\ndef loss_wrapper(cfg: DictConfig):\n    def loss_expr(output_dict, label_dict, weight_dict=None):\n        label_true = label_dict[\"output\"].reshape((-1, 1))\n        label_pred = output_dict[\"output\"].reshape((-1, 1))\n        conf_loss = paddle.mean(\n            nn.functional.log_loss(label_pred, label_true, epsilon=1.0e-7)\n        )\n        vol_loss = paddle.square(paddle.mean(label_true - label_pred))\n        return {\"output\": conf_loss + cfg.vol_coeff * vol_loss}\n\n    return loss_expr\n\n\n# define metric\ndef val_metric(output_dict, label_dict, weight_dict=None):\n    label_pred = output_dict[\"output\"]\n    label_true = label_dict[\"output\"]\n    accurates = paddle.equal(paddle.round(label_true), paddle.round(label_pred))\n    acc = paddle.mean(paddle.cast(accurates, dtype=paddle.get_default_dtype()))\n    true_negative = paddle.sum(\n        paddle.multiply(\n            paddle.equal(paddle.round(label_pred), 0.0),\n            paddle.equal(paddle.round(label_true), 0.0),\n        ),\n        dtype=paddle.get_default_dtype(),\n    )\n    true_positive = paddle.sum(\n        paddle.multiply(\n            paddle.equal(paddle.round(label_pred), 1.0),\n            paddle.equal(paddle.round(label_true), 1.0),\n        ),\n        dtype=paddle.get_default_dtype(),\n    )\n    false_negative = paddle.sum(\n        paddle.multiply(\n            paddle.equal(paddle.round(label_pred), 1.0),\n            paddle.equal(paddle.round(label_true), 0.0),\n        ),\n        dtype=paddle.get_default_dtype(),\n    )\n    false_positive = paddle.sum(\n        paddle.multiply(\n            paddle.equal(paddle.round(label_pred), 0.0),\n            paddle.equal(paddle.round(label_true), 1.0),\n        ),\n        dtype=paddle.get_default_dtype(),\n    )\n    n_negative = paddle.add(false_negative, true_negative)\n    n_positive = paddle.add(true_positive, false_positive)\n    iou = 0.5 * paddle.add(\n        paddle.divide(true_negative, paddle.add(n_negative, false_positive)),\n        paddle.divide(true_positive, paddle.add(n_positive, false_negative)),\n    )\n    return {\"Binary_Acc\": acc, \"IoU\": iou}\n\n\n# export model\ndef export(cfg: DictConfig):\n    # set model\n    model = TopOptNN(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        eval_with_no_grad=True,\n        pretrained_model_path=cfg.INFER.pretrained_model_path_dict[\n            cfg.INFER.pretrained_model_name\n        ],\n    )\n\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [{\"input\": InputSpec([None, 2, 40, 40], \"float32\", name=\"input\")}]\n\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    # read h5 data\n    h5data = h5py.File(cfg.DATA_PATH, \"r\")\n    data_iters = np.array(h5data[\"iters\"])\n    data_targets = np.array(h5data[\"targets\"])\n    idx = np.random.choice(len(data_iters), cfg.INFER.img_num, False)\n    data_iters = data_iters[idx]\n    data_targets = data_targets[idx]\n\n    sampler = func_module.generate_sampler(cfg.INFER.sampler_key, cfg.INFER.sampler_num)\n    data_iters = channel_sampling(sampler, data_iters)\n\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    input_dict = {\"input\": data_iters}\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to output_key\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip({\"output\"}, output_dict.keys())\n    }\n\n    save_topopt_img(\n        input_dict,\n        output_dict,\n        data_targets,\n        cfg.INFER.save_res_path,\n        cfg.INFER.res_img_figsize,\n        cfg.INFER.save_npy,\n    )\n\n\n# used for inference\ndef channel_sampling(sampler, input):\n    SIMP_initial_iter_time = sampler()\n    input_channel_k = input[:, SIMP_initial_iter_time, :, :]\n    input_channel_k_minus_1 = input[:, SIMP_initial_iter_time - 1, :, :]\n    input = np.stack(\n        (input_channel_k, input_channel_k - input_channel_k_minus_1), axis=1\n    )\n    return input\n\n\n# used for inference\ndef save_topopt_img(\n    input_dict: Dict[str, np.ndarray],\n    output_dict: Dict[str, np.ndarray],\n    ground_truth: np.ndarray,\n    save_dir: str,\n    figsize: tuple = None,\n    save_npy: bool = False,\n):\n\n    input = input_dict[\"input\"]\n    output = output_dict[\"output\"]\n    import os\n\n    import matplotlib.pyplot as plt\n\n    os.makedirs(save_dir, exist_ok=True)\n    for i in range(len(input)):\n        plt.figure(figsize=figsize)\n        plt.subplot(1, 4, 1)\n        plt.axis(\"off\")\n        plt.imshow(input[i][0], cmap=\"gray\")\n        plt.title(\"Input Image\")\n        plt.subplot(1, 4, 2)\n        plt.axis(\"off\")\n        plt.imshow(input[i][1], cmap=\"gray\")\n        plt.title(\"Input Gradient\")\n        plt.subplot(1, 4, 3)\n        plt.axis(\"off\")\n        plt.imshow(np.round(output[i][0]), cmap=\"gray\")\n        plt.title(\"Prediction\")\n        plt.subplot(1, 4, 4)\n        plt.axis(\"off\")\n        plt.imshow(np.round(ground_truth[i][0]), cmap=\"gray\")\n        plt.title(\"Ground Truth\")\n        plt.show()\n        plt.savefig(osp.join(save_dir, f\"Prediction_{i}.png\"))\n        plt.close()\n        if save_npy:\n            with open(osp(save_dir, f\"Prediction_{i}.npy\"), \"wb\") as f:\n                np.save(f, output[i])\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"topopt.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> functions.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\n\n\ndef uniform_sampler() -&gt; Callable[[], int]:\n    \"\"\"Generate uniform sampling function from 1 to 99\n\n    Returns:\n        sampler (Callable[[], int]): uniform sampling from 1 to 99\n    \"\"\"\n    return lambda: np.random.randint(1, 99)\n\n\ndef poisson_sampler(lam: int) -&gt; Callable[[], int]:\n    \"\"\"Generate poisson sampling function with parameter lam with range 1 to 99\n\n    Args:\n        lam (int): poisson rate parameter\n\n    Returns:\n        sampler (Callable[[], int]): poisson sampling function with parameter lam with range 1 to 99\n    \"\"\"\n\n    def func():\n        iter_ = max(np.random.poisson(lam), 1)\n        iter_ = min(iter_, 99)\n        return iter_\n\n    return func\n\n\ndef generate_sampler(sampler_type: str = \"Fixed\", num: int = 0) -&gt; Callable[[], int]:\n    \"\"\"Generate sampler for the number of initial iteration steps\n\n    Args:\n        sampler_type (str): \"Poisson\" for poisson sampler; \"Uniform\" for uniform sampler; \"Fixed\" for choosing a fixed number of initial iteration steps.\n        num (int): If `sampler_type` == \"Poisson\", `num` specifies the poisson rate parameter; If `sampler_type` == \"Fixed\", `num` specifies the fixed number of initial iteration steps.\n\n    Returns:\n        sampler (Callable[[], int]): sampler for the number of initial iteration steps\n    \"\"\"\n    if sampler_type == \"Poisson\":\n        return poisson_sampler(num)\n    elif sampler_type == \"Uniform\":\n        return uniform_sampler()\n    else:\n        return lambda: num\n\n\ndef generate_train_test(\n    data_iters: np.ndarray,\n    data_targets: np.ndarray,\n    train_test_ratio: float,\n    n_sample: int,\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n]:\n    \"\"\"Generate training and testing set\n\n    Args:\n        data_iters (np.ndarray): data with 100 channels corresponding to the results of 100 steps of SIMP algorithm\n        data_targets (np.ndarray): final optimization solution given by SIMP algorithm\n        train_test_ratio (float): split ratio of training and testing sets, if `train_test_ratio` = 1 then only return training data\n        n_sample (int): number of total samples in training and testing sets to be sampled from the h5 dataset\n\n    Returns:\n        Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]: if `train_test_ratio` = 1, return (train_inputs, train_labels), else return (train_inputs, train_labels, test_inputs, test_labels)\n    \"\"\"\n    n_obj = len(data_iters)\n    idx = np.arange(n_obj)\n    np.random.shuffle(idx)\n    train_idx = idx[: int(train_test_ratio * n_sample)]\n    if train_test_ratio == 1.0:\n        return data_iters[train_idx], data_targets[train_idx]\n\n    test_idx = idx[int(train_test_ratio * n_sample) :]\n    train_iters = data_iters[train_idx]\n    train_targets = data_targets[train_idx]\n    test_iters = data_iters[test_idx]\n    test_targets = data_targets[test_idx]\n    return train_iters, train_targets, test_iters, test_targets\n\n\ndef augmentation(\n    input_dict: Dict[str, np.ndarray],\n    label_dict: Dict[str, np.ndarray],\n    weight_dict: Dict[str, np.ndarray] = None,\n) -&gt; Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n    \"\"\"Apply random transformation from D4 symmetry group\n\n    Args:\n        input_dict (Dict[str, np.ndarray]): input dict of np.ndarray size `(batch_size, any, height, width)`\n        label_dict (Dict[str, np.ndarray]): label dict of np.ndarray size `(batch_size, 1, height, width)`\n        weight_dict (Dict[str, np.ndarray]): weight dict if any\n    \"\"\"\n    inputs = input_dict[\"input\"]\n    labels = label_dict[\"output\"]\n    assert len(inputs.shape) == 3\n    assert len(labels.shape) == 3\n\n    # random horizontal flip\n    if np.random.random() &gt; 0.5:\n        inputs = np.flip(inputs, axis=2)\n        labels = np.flip(labels, axis=2)\n    # random vertical flip\n    if np.random.random() &gt; 0.5:\n        inputs = np.flip(inputs, axis=1)\n        labels = np.flip(labels, axis=1)\n    # random 90* rotation\n    if np.random.random() &gt; 0.5:\n        new_perm = list(range(len(inputs.shape)))\n        new_perm[-2], new_perm[-1] = new_perm[-1], new_perm[-2]\n        inputs = np.transpose(inputs, new_perm)\n        labels = np.transpose(labels, new_perm)\n\n    return {\"input\": inputs}, {\"output\": labels}, weight_dict\n</code></pre> topoptmodel.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport paddle\nfrom paddle import nn\n\nimport ppsci\n\n\n# NCHW data format\nclass TopOptNN(ppsci.arch.UNetEx):\n    \"\"\"Neural network for Topology Optimization, inherited from `ppsci.arch.UNetEx`\n\n    [Sosnovik, I., &amp; Oseledets, I. (2019). Neural networks for topology optimization. Russian Journal of Numerical Analysis and Mathematical Modelling, 34(4), 215-223.](https://arxiv.org/pdf/1709.09578)\n\n    Args:\n        input_key (str): Name of function data for input.\n        output_key (str): Name of function data for output.\n        in_channel (int): Number of channels of input.\n        out_channel (int): Number of channels of output.\n        kernel_size (int, optional): Size of kernel of convolution layer. Defaults to 3.\n        filters (Tuple[int, ...], optional): Number of filters. Defaults to (16, 32, 64).\n        layers (int, optional): Number of encoders or decoders. Defaults to 3.\n        channel_sampler (callable, optional): The sampling function for the initial iteration time\n                (corresponding to the channel number of the input) of SIMP algorithm. The default value\n                is None, when it is None, input for the forward method should be sampled and prepared\n                with the shape of [batch, 2, height, width] before passing to forward method.\n        weight_norm (bool, optional): Whether use weight normalization layer. Defaults to True.\n        batch_norm (bool, optional): Whether add batch normalization layer. Defaults to True.\n        activation (Type[nn.Layer], optional): Name of activation function. Defaults to nn.ReLU.\n\n    Examples:\n        &gt;&gt;&gt; import ppsci\n        &gt;&gt;&gt; model = ppsci.arch.ppsci.arch.TopOptNN(\"input\", \"output\", 2, 1, 3, (16, 32, 64), 2, lambda: 1, Flase, False)\n    \"\"\"\n\n    def __init__(\n        self,\n        input_key=\"input\",\n        output_key=\"output\",\n        in_channel=2,\n        out_channel=1,\n        kernel_size=3,\n        filters=(16, 32, 64),\n        layers=2,\n        channel_sampler=None,\n        weight_norm=False,\n        batch_norm=False,\n        activation=nn.ReLU,\n    ):\n        super().__init__(\n            input_key=input_key,\n            output_key=output_key,\n            in_channel=in_channel,\n            out_channel=out_channel,\n            kernel_size=kernel_size,\n            filters=filters,\n            layers=layers,\n            weight_norm=weight_norm,\n            batch_norm=batch_norm,\n            activation=activation,\n        )\n        self.in_channel = in_channel\n        self.out_channel = out_channel\n        self.filters = filters\n        self.channel_sampler = channel_sampler\n        self.activation = activation\n\n        # Modify Layers\n        self.encoder[1] = nn.Sequential(\n            nn.MaxPool2D(self.in_channel, padding=\"SAME\"),\n            self.encoder[1][0],\n            nn.Dropout2D(0.1),\n            self.encoder[1][1],\n        )\n        self.encoder[2] = nn.Sequential(\n            nn.MaxPool2D(2, padding=\"SAME\"), self.encoder[2]\n        )\n        # Conv2D used in reference code in decoder\n        self.decoders[0] = nn.Sequential(\n            nn.Conv2D(\n                self.filters[-1], self.filters[-1], kernel_size=3, padding=\"SAME\"\n            ),\n            self.activation(),\n            nn.Conv2D(\n                self.filters[-1], self.filters[-1], kernel_size=3, padding=\"SAME\"\n            ),\n            self.activation(),\n        )\n        self.decoders[1] = nn.Sequential(\n            nn.Conv2D(\n                sum(self.filters[-2:]), self.filters[-2], kernel_size=3, padding=\"SAME\"\n            ),\n            self.activation(),\n            nn.Dropout2D(0.1),\n            nn.Conv2D(\n                self.filters[-2], self.filters[-2], kernel_size=3, padding=\"SAME\"\n            ),\n            self.activation(),\n        )\n        self.decoders[2] = nn.Sequential(\n            nn.Conv2D(\n                sum(self.filters[:-1]), self.filters[-3], kernel_size=3, padding=\"SAME\"\n            ),\n            self.activation(),\n            nn.Conv2D(\n                self.filters[-3], self.filters[-3], kernel_size=3, padding=\"SAME\"\n            ),\n            self.activation(),\n        )\n        self.output = nn.Sequential(\n            nn.Conv2D(\n                self.filters[-3], self.out_channel, kernel_size=3, padding=\"SAME\"\n            ),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        if self.channel_sampler is not None:\n            SIMP_initial_iter_time = self.channel_sampler()  # channel k\n            input_channel_k = x[self.input_keys[0]][:, SIMP_initial_iter_time, :, :]\n            input_channel_k_minus_1 = x[self.input_keys[0]][\n                :, SIMP_initial_iter_time - 1, :, :\n            ]\n            x = paddle.stack(\n                (input_channel_k, input_channel_k - input_channel_k_minus_1), axis=1\n            )\n        else:\n            x = x[self.input_keys[0]]\n        # encode\n        upsampling_size = []\n        skip_connection = []\n        n_encoder = len(self.encoder)\n        for i in range(n_encoder):\n            x = self.encoder[i](x)\n            if i is not (n_encoder - 1):\n                upsampling_size.append(x.shape[-2:])\n                skip_connection.append(x)\n\n        # decode\n        n_decoder = len(self.decoders)\n        for i in range(n_decoder):\n            x = self.decoders[i](x)\n            if i is not (n_decoder - 1):\n                up_size = upsampling_size.pop()\n                x = nn.UpsamplingNearest2D(up_size)(x)\n                skip_output = skip_connection.pop()\n                x = paddle.concat((skip_output, x), axis=1)\n\n        out = self.output(x)\n        return {self.output_keys[0]: out}\n</code></pre>"},{"location":"zh/examples/topopt/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u56fe\u5c55\u793a\u4e864\u4e2a\u6a21\u578b\u5206\u522b\u572816\u7ec4\u4e0d\u540c\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec Binary Accuracy \u4ee5\u53ca IoU \u8fd9\u4e24\u79cd\u6307\u6807\u3002\u5176\u4e2d\u6a2a\u5750\u6807\u4ee3\u8868\u4e0d\u540c\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u4f8b\u5982\uff1a\u6a2a\u5750\u6807 \\(i\\) \u8868\u793a\u7531\u539f\u59cb\u6570\u636e\u7b2c\u4e8c\u7ef4\u7684\u7b2c \\(5\\cdot(i+1)\\) \u4e2a\u901a\u9053\u53ca\u5176\u5bf9\u5e94\u68af\u5ea6\u4fe1\u606f\u6784\u5efa\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff1b\u7eb5\u5750\u6807\u4e3a\u8bc4\u4f30\u6307\u6807\u3002<code>thresholding</code> \u5bf9\u5e94\u7684\u6307\u6807\u53ef\u4ee5\u7406\u89e3\u4e3a benchmark\u3002</p> <p> </p> Binary Accuracy\u7ed3\u679c <p> </p> IoU\u7ed3\u679c <p>\u7528\u8868\u683c\u8868\u793a\u4e0a\u56fe\u6307\u6807\u4e3a\uff1a</p> bin_acc eval_dataset_ch_5 eval_dataset_ch_10 eval_dataset_ch_15 eval_dataset_ch_20 eval_dataset_ch_25 eval_dataset_ch_30 eval_dataset_ch_35 eval_dataset_ch_40 eval_dataset_ch_45 eval_dataset_ch_50 eval_dataset_ch_55 eval_dataset_ch_60 eval_dataset_ch_65 eval_dataset_ch_70 eval_dataset_ch_75 eval_dataset_ch_80 Poisson5 0.9471 0.9619 0.9702 0.9742 0.9782 0.9801 0.9803 0.9825 0.9824 0.9837 0.9850 0.9850 0.9870 0.9863 0.9870 0.9872 Poisson10 0.9457 0.9703 0.9745 0.9798 0.9827 0.9845 0.9859 0.9870 0.9882 0.9880 0.9893 0.9899 0.9882 0.9899 0.9905 0.9904 Poisson30 0.9257 0.9595 0.9737 0.9832 0.9828 0.9883 0.9885 0.9892 0.9901 0.9916 0.9924 0.9925 0.9926 0.9929 0.9937 0.9936 Uniform 0.9410 0.9673 0.9718 0.9727 0.9818 0.9824 0.9826 0.9845 0.9856 0.9892 0.9892 0.9907 0.9890 0.9916 0.9914 0.9922 iou eval_dataset_ch_5 eval_dataset_ch_10 eval_dataset_ch_15 eval_dataset_ch_20 eval_dataset_ch_25 eval_dataset_ch_30 eval_dataset_ch_35 eval_dataset_ch_40 eval_dataset_ch_45 eval_dataset_ch_50 eval_dataset_ch_55 eval_dataset_ch_60 eval_dataset_ch_65 eval_dataset_ch_70 eval_dataset_ch_75 eval_dataset_ch_80 Poisson5 0.8995 0.9267 0.9421 0.9497 0.9574 0.9610 0.9614 0.9657 0.9655 0.9679 0.9704 0.9704 0.9743 0.9730 0.9744 0.9747 Poisson10 0.8969 0.9424 0.9502 0.9604 0.9660 0.9696 0.9722 0.9743 0.9767 0.9762 0.9789 0.9800 0.9768 0.9801 0.9813 0.9810 Poisson30 0.8617 0.9221 0.9488 0.9670 0.9662 0.9769 0.9773 0.9786 0.9803 0.9833 0.9850 0.9853 0.9855 0.9860 0.9875 0.9873 Uniform 0.8887 0.9367 0.9452 0.9468 0.9644 0.9655 0.9659 0.9695 0.9717 0.9787 0.9787 0.9816 0.9784 0.9835 0.9831 0.9845"},{"location":"zh/examples/topopt/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li> <p>Sosnovik I, &amp; Oseledets I. Neural networks for topology optimization</p> </li> <li> <p>\u53c2\u8003\u4ee3\u7801</p> </li> </ul>"},{"location":"zh/examples/viv/","title":"ViV","text":"","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#vivvortex-induced-vibration","title":"VIV(vortex induced vibration)","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code>python viv.py\n</code></pre> <pre><code>python viv.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/viv/viv_pretrained.pdparams\n</code></pre> <pre><code>python viv.py mode=export\n</code></pre> <pre><code>python viv.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 viv_pretrained.pdparamsviv_pretrained.pdeqn eta_l2/MSE.eta: 0.00875eta_l2/MSE.f: 0.00921","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6da1\u6fc0\u632f\u52a8\uff08Vortex-Induced Vibration\uff0cVIV\uff09\u662f\u4e00\u79cd\u6d41\u56fa\u8026\u5408\u632f\u52a8\u73b0\u8c61\uff0c\u4e3b\u8981\u53d1\u751f\u5728\u6d41\u4f53\u7ed5\u8fc7\u67f1\u4f53\u6216\u7ba1\u4f53\u65f6\u3002\u5728\u6d77\u6d0b\u5de5\u7a0b\u548c\u98ce\u5de5\u7a0b\u4e2d\uff0c\u8fd9\u79cd\u632f\u52a8\u73b0\u8c61\u5177\u6709\u91cd\u8981\u5e94\u7528\u3002</p> <p>\u5728\u6d77\u6d0b\u5de5\u7a0b\u4e2d\uff0c\u6da1\u6fc0\u632f\u52a8\u95ee\u9898\u4e3b\u8981\u6d89\u53ca\u6d77\u6d0b\u5e73\u53f0\uff08\u5982\u6869\u57fa\u3001\u7acb\u7ba1\u7b49\uff09\u7684\u6da1\u6fc0\u632f\u52a8\u54cd\u5e94\u5206\u6790\u3002\u8fd9\u4e9b\u5e73\u53f0\u5728\u6d77\u6d41\u4e2d\u8fd0\u884c\uff0c\u4f1a\u53d7\u5230\u6da1\u6fc0\u632f\u52a8\u7684\u5f71\u54cd\u3002\u8fd9\u79cd\u632f\u52a8\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5e73\u53f0\u7ed3\u6784\u7684\u75b2\u52b3\u635f\u4f24\uff0c\u56e0\u6b64\u5728\u8fdb\u884c\u6d77\u6d0b\u5e73\u53f0\u8bbe\u8ba1\u65f6\uff0c\u9700\u8981\u8003\u8651\u8fd9\u4e00\u95ee\u9898\u3002</p> <p>\u5728\u98ce\u5de5\u7a0b\u4e2d\uff0c\u6da1\u6fc0\u632f\u52a8\u95ee\u9898\u4e3b\u8981\u6d89\u53ca\u98ce\u529b\u53d1\u7535\u673a\u7684\u6da1\u6fc0\u632f\u52a8\u54cd\u5e94\u5206\u6790\u3002\u98ce\u529b\u53d1\u7535\u673a\u53f6\u7247\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u53d7\u5230\u6c14\u6d41\u7684\u6da1\u6fc0\u632f\u52a8\uff0c\u8fd9\u79cd\u632f\u52a8\u53ef\u80fd\u4f1a\u5bfc\u81f4\u53f6\u7247\u7684\u75b2\u52b3\u635f\u4f24\u3002\u4e3a\u4e86\u786e\u4fdd\u98ce\u529b\u53d1\u7535\u673a\u7684\u5b89\u5168\u8fd0\u884c\uff0c\u9700\u8981\u5bf9\u8fd9\u4e00\u95ee\u9898\u8fdb\u884c\u6df1\u5165\u7684\u7814\u7a76\u3002</p> <p>\u603b\u4e4b\uff0c\u6da1\u6fc0\u632f\u52a8\u95ee\u9898\u7684\u5e94\u7528\u4e3b\u8981\u6d89\u53ca\u6d77\u6d0b\u5de5\u7a0b\u548c\u98ce\u5de5\u7a0b\u9886\u57df\uff0c\u5bf9\u4e8e\u8fd9\u4e9b\u9886\u57df\u7684\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002</p> <p>\u5f53\u6da1\u6d41\u8131\u843d\u9891\u7387\u63a5\u8fd1\u7ed3\u6784\u7684\u56fa\u6709\u9891\u7387\u65f6\uff0c\u5706\u67f1\u4f1a\u53d1\u751f\u6da1\u6fc0\u632f\u52a8\uff0cVIV\u7cfb\u7edf\u76f8\u5f53\u4e8e\u4e00\u4e2a\u5f39\u7c27-\u963b\u5c3c\u7cfb\u7edf\uff1a</p> <p></p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u672c\u95ee\u9898\u6d89\u53ca\u7684\u63a7\u5236\u65b9\u7a0b\u6d89\u53ca\u4e09\u4e2a\u7269\u7406\u91cf\uff1a\\(\u03bb_1\\)\u3001\\(\u03bb_2\\) \u548c \\(\u03c1\\)\uff0c\u5206\u522b\u8868\u793a\u81ea\u7136\u963b\u5c3c\u3001\u7ed3\u6784\u7279\u6027\u521a\u5ea6\u548c\u8d28\u91cf\uff0c\u63a7\u5236\u65b9\u7a0b\u5b9a\u4e49\u5982\u4e0b\u6240\u793a\uff1a</p> \\[ \\rho \\dfrac{\\partial^2 \\eta}{\\partial t^2} + \\lambda_1 \\dfrac{\\partial \\eta}{\\partial t} + \\lambda_2 \\eta = f \\] <p>\u8be5\u6a21\u578b\u57fa\u4e8e\u65e0\u91cf\u7eb2\u901f\u5ea6 \\(U_r=\\dfrac{u}{f_n*d}=8.5\\) \u5bf9\u5e94 \\(Re=500\\) \u7684\u5047\u8bbe\u3002\u6211\u4eec\u4f7f\u7528\u901a\u8fc7\u5706\u67f1\u7684\u6d41\u4f53\u5f15\u8d77\u7684\u5706\u67f1\u632f\u52a8\u7684\u6a2a\u5411\u632f\u5e45 \\(\\eta\\) \u548c\u76f8\u5e94\u7684\u5347\u529b \\(f\\) \u4f5c\u4e3a\u76d1\u7763\u6570\u636e\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728 VIV \u95ee\u9898\u4e2d\uff0c\u7ed9\u5b9a\u65f6\u95f4 \\(t\\)\uff0c\u4e0a\u8ff0\u7cfb\u7edf\u90fd\u6709\u6a2a\u5411\u632f\u5e45 \\(\\eta\\) \u548c\u5347\u529b \\(f\\) \u4f5c\u4e3a\u5f85\u6c42\u89e3\u7684\u672a\u77e5\u91cf\uff0c\u5e76\u4e14\u8be5\u7cfb\u7edf\u672c\u8eab\u8fd8\u5305\u542b\u4e24\u4e2a\u53c2\u6570 \\(\\lambda_1, \\lambda_2\\)\u3002\u56e0\u6b64\u6211\u4eec\u5728\u8fd9\u91cc\u4f7f\u7528\u6bd4\u8f83\u7b80\u5355\u7684 MLP(Multilayer Perceptron, \u591a\u5c42\u611f\u77e5\u673a) \u6765\u8868\u793a \\(t\\) \u5230 \\((\\eta, f)\\) \u7684\u6620\u5c04\u51fd\u6570 \\(g: \\mathbb{R}^1 \\to \\mathbb{R}^2\\) \uff0c\u5373\uff1a</p> \\[ \\eta, f = g(t) \\] <p>\u4e0a\u5f0f\u4e2d \\(g\\) \u5373\u4e3a MLP \u6a21\u578b\u672c\u8eab\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>(\"t_f\",)</code>\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>(\"eta\",)</code>\uff0c  <code>t_f</code> \u4ee3\u8868\u8f93\u5165\u65f6\u95f4 \\(t\\)\uff0c<code>eta</code> \u4ee3\u8868\u8f93\u51fa\u632f\u5e45 \\(\\eta\\) \u8fd9\u4e9b\u547d\u540d\u4e0e\u540e\u7eed\u4ee3\u7801\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u63a5\u7740\u901a\u8fc7\u6307\u5b9a MLP \u7684\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\u4ee5\u53ca\u6fc0\u6d3b\u51fd\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u4e00\u4e2a\u62e5\u6709 5 \u5c42\u9690\u85cf\u795e\u7ecf\u5143\uff0c\u6bcf\u5c42\u795e\u7ecf\u5143\u6570\u4e3a 50\uff0c\u4f7f\u7528 \"tanh\" \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#32","title":"3.2 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e VIV \u4f7f\u7528\u7684\u662f VIV \u65b9\u7a0b\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>VIV</code>\u3002</p> <pre><code># set equation\nequation = {\"VIV\": ppsci.equation.Vibration(2, -4, 0)}\n</code></pre> <p>\u6211\u4eec\u5728\u8be5\u65b9\u7a0b\u4e2d\u6dfb\u52a0\u4e86\u4e24\u4e2a\u53ef\u5b66\u4e60\u7684\u53c2\u6570 <code>k1</code> \u548c <code>k2</code> \u6765\u4f30\u8ba1 \\(\\lambda_1\\) \u548c \\(\\lambda_2\\)\uff0c\u4e14\u5b83\u4eec\u7684\u5173\u7cfb\u662f \\(\\lambda_1 = e^{k1}, \\lambda_2 = e^{k2}\\)</p> <p>\u56e0\u6b64\u6211\u4eec\u5728\u5b9e\u4f8b\u5316 <code>VIV</code> \u7c7b\u65f6\u9700\u6307\u5b9a\u5fc5\u8981\u7684\u53c2\u6570\uff1a\u8d28\u91cf <code>rho=2</code>\uff0c\u521d\u59cb\u5316\u503c<code>k1=-4</code>\uff0c<code>k2=0</code>\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#33","title":"3.3 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>\u672c\u6587\u4e2d VIV \u95ee\u9898\u4f5c\u7528\u5728 \\(t \\in [0.0625, 9.9375]\\) \u4e2d\u7684 100 \u4e2a\u79bb\u6563\u65f6\u95f4\u70b9\u4e0a\uff0c\u8fd9 100 \u4e2a\u65f6\u95f4\u70b9\u5df2\u7ecf\u4fdd\u5b58\u5728\u6587\u4ef6 <code>examples/fsi/VIV_Training_Neta100.mat</code> \u4f5c\u4e3a\u8f93\u5165\u6570\u636e\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u663e\u5f0f\u6784\u5efa\u8ba1\u7b97\u57df\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"<p>\u672c\u6587\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u5bf9\u6a21\u578b\u8f93\u51fa \\(\\eta\\) \u548c\u57fa\u4e8e \\(\\eta\\) \u8ba1\u7b97\u51fa\u7684\u5347\u529b \\(f\\)\uff0c\u8fd9\u4e24\u4e2a\u7269\u7406\u91cf\u8fdb\u884c\u7ea6\u675f\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#341","title":"3.4.1 \u76d1\u7763\u7ea6\u675f","text":"<p>\u7531\u4e8e\u6211\u4eec\u4ee5\u76d1\u7763\u5b66\u4e60\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u6b64\u5904\u91c7\u7528\u76d1\u7763\u7ea6\u675f <code>SupervisedConstraint</code>\uff1a</p> <pre><code># set constraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    {\n        \"dataset\": {\n            \"name\": \"MatDataset\",\n            \"file_path\": cfg.VIV_DATA_PATH,\n            \"input_keys\": (\"t_f\",),\n            \"label_keys\": (\"eta\", \"f\"),\n            \"weight_dict\": {\"eta\": 100},\n        },\n        \"batch_size\": cfg.TRAIN.batch_size,\n        \"sampler\": {\n            \"name\": \"BatchSampler\",\n            \"drop_last\": False,\n            \"shuffle\": True,\n        },\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n    name=\"Sup\",\n)\n</code></pre> <p><code>SupervisedConstraint</code> \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u76d1\u7763\u7ea6\u675f\u7684\u8bfb\u53d6\u914d\u7f6e\uff0c\u6b64\u5904\u586b\u5165\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>train_dataloader_cfg</code>\uff1b</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u635f\u5931\u51fd\u6570\uff0c\u6b64\u5904\u6211\u4eec\u9009\u7528\u5e38\u7528\u7684MSE\u51fd\u6570\uff0c\u4e14 <code>reduction</code> \u8bbe\u7f6e\u4e3a <code>\"mean\"</code>\uff0c\u5373\u6211\u4eec\u4f1a\u5c06\u53c2\u4e0e\u8ba1\u7b97\u7684\u6240\u6709\u6570\u636e\u70b9\u4ea7\u751f\u7684\u635f\u5931\u9879\u6c42\u548c\u53d6\u5e73\u5747\uff1b</p> <p>\u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u65b9\u7a0b\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5982\u4f55\u8ba1\u7b97\u7ea6\u675f\u76ee\u6807\uff0c\u6b64\u5904\u586b\u5165 <code>eta</code> \u7684\u8ba1\u7b97\u51fd\u6570\u548c\u5728 3.2 \u65b9\u7a0b\u6784\u5efa \u7ae0\u8282\u4e2d\u5b9e\u4f8b\u5316\u597d\u7684 <code>equation[\"VIV\"].equations</code>\uff1b</p> <p>\u7b2c\u56db\u4e2a\u53c2\u6570\u662f\u7ea6\u675f\u6761\u4ef6\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u7ed9\u6bcf\u4e00\u4e2a\u7ea6\u675f\u6761\u4ef6\u547d\u540d\uff0c\u65b9\u4fbf\u540e\u7eed\u5bf9\u5176\u7d22\u5f15\u3002\u6b64\u5904\u6211\u4eec\u547d\u540d\u4e3a \"Sup\" \u5373\u53ef\u3002</p> <p>\u5728\u76d1\u7763\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u4f7f\u7528 10000 \u8f6e\u8bad\u7ec3\u8f6e\u6570\uff0c\u5e76\u6bcf\u9694 10000 \u4e2aepochs\u8bc4\u4f30\u4e00\u6b21\u6a21\u578b\u7cbe\u5ea6\u3002</p> <pre><code>TRAIN:\n  epochs: 100000\n  iters_per_epoch: 1\n  save_freq: 10000\n  eval_during_train: true\n  eval_freq: 1000\n  batch_size: 100\n  lr_scheduler:\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u548c <code>Step</code> \u95f4\u9694\u8870\u51cf\u5b66\u4e60\u7387\u3002</p> <pre><code># set optimizer\nlr_scheduler = ppsci.optimizer.lr_scheduler.Step(**cfg.TRAIN.lr_scheduler)()\noptimizer = ppsci.optimizer.Adam(lr_scheduler)((model,) + tuple(equation.values()))\n</code></pre> \u8bf4\u660e <p>VIV \u65b9\u7a0b\u542b\u6709\u4e24\u4e2a \u53ef\u5b66\u4e60\u53c2\u6570 k1\u548ck2\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u65b9\u7a0b\u4e0e <code>model</code> \u4e00\u8d77\u4f20\u5165\u4f18\u5316\u5668\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\neta_l2_validator = ppsci.validate.SupervisedValidator(\n    {\n        \"dataset\": {\n            \"name\": \"MatDataset\",\n            \"file_path\": cfg.VIV_DATA_PATH,\n            \"input_keys\": (\"t_f\",),\n            \"label_keys\": (\"eta\", \"f\"),\n        },\n        \"batch_size\": cfg.EVAL.batch_size,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n    metric={\"MSE\": ppsci.metric.L2Rel()},\n    name=\"eta_l2\",\n)\nvalidator = {eta_l2_validator.name: eta_l2_validator}\n</code></pre> <p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u9009\u62e9 <code>ppsci.metric.MSE</code> \u5373\u53ef\uff1b</p> <p>\u5176\u4f59\u914d\u7f6e\u4e0e 3.4.1 \u76d1\u7763\u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#38","title":"3.8 \u53ef\u89c6\u5316\u5668\u6784\u5efa","text":"<p>\u5728\u6a21\u578b\u8bc4\u4f30\u65f6\uff0c\u5982\u679c\u8bc4\u4f30\u7ed3\u679c\u662f\u53ef\u4ee5\u53ef\u89c6\u5316\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684\u53ef\u89c6\u5316\u5668\u6765\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\u3002</p> <p>\u672c\u6587\u9700\u8981\u53ef\u89c6\u5316\u7684\u6570\u636e\u662f \\(t-\\eta\\) \u548c \\(t-f\\) \u4e24\u7ec4\u5173\u7cfb\u56fe\uff0c\u5047\u8bbe\u6bcf\u4e2a\u65f6\u523b \\(t\\) \u7684\u5750\u6807\u662f \\(t_i\\)\uff0c\u5219\u5bf9\u5e94\u7f51\u7edc\u8f93\u51fa\u4e3a \\(\\eta_i\\)\uff0c\u5347\u529b\u4e3a \\(f_i\\)\uff0c\u56e0\u6b64\u6211\u4eec\u53ea\u9700\u8981\u5c06\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u6240\u6709 \\((t_i, \\eta_i, f_i)\\) \u4fdd\u5b58\u6210\u56fe\u7247\u5373\u53ef\u3002\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code># set visualizer(optional)\nvisu_mat = ppsci.utils.reader.load_mat_file(\n    cfg.VIV_DATA_PATH,\n    (\"t_f\", \"eta_gt\", \"f_gt\"),\n    alias_dict={\"eta_gt\": \"eta\", \"f_gt\": \"f\"},\n)\nvisualizer = {\n    \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n        visu_mat,\n        (\"t_f\",),\n        {\n            r\"$\\eta$\": lambda d: d[\"eta\"],  # plot with latex title\n            r\"$\\eta_{gt}$\": lambda d: d[\"eta_gt\"],  # plot with latex title\n            r\"$f$\": equation[\"VIV\"].equations[\"f\"],  # plot with latex title\n            r\"$f_{gt}$\": lambda d: d[\"f_gt\"],  # plot with latex title\n        },\n        num_timestamps=1,\n        prefix=\"viv_pred\",\n    )\n}\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#39","title":"3.9 \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u53ef\u89c6\u5316","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    optimizer=optimizer,\n    equation=equation,\n    validator=validator,\n    visualizer=visualizer,\n    cfg=cfg,\n)\n\n# train model\nsolver.train()\n# evaluate after finished training\nsolver.eval()\n# visualize prediction after finished training\nsolver.visualize()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"viv.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hydra\nfrom omegaconf import DictConfig\n\nimport ppsci\n\n\ndef train(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"VIV\": ppsci.equation.Vibration(2, -4, 0)}\n\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        {\n            \"dataset\": {\n                \"name\": \"MatDataset\",\n                \"file_path\": cfg.VIV_DATA_PATH,\n                \"input_keys\": (\"t_f\",),\n                \"label_keys\": (\"eta\", \"f\"),\n                \"weight_dict\": {\"eta\": 100},\n            },\n            \"batch_size\": cfg.TRAIN.batch_size,\n            \"sampler\": {\n                \"name\": \"BatchSampler\",\n                \"drop_last\": False,\n                \"shuffle\": True,\n            },\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n        name=\"Sup\",\n    )\n    # wrap constraints together\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set optimizer\n    lr_scheduler = ppsci.optimizer.lr_scheduler.Step(**cfg.TRAIN.lr_scheduler)()\n    optimizer = ppsci.optimizer.Adam(lr_scheduler)((model,) + tuple(equation.values()))\n\n    # set validator\n    eta_l2_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"MatDataset\",\n                \"file_path\": cfg.VIV_DATA_PATH,\n                \"input_keys\": (\"t_f\",),\n                \"label_keys\": (\"eta\", \"f\"),\n            },\n            \"batch_size\": cfg.EVAL.batch_size,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n        metric={\"MSE\": ppsci.metric.L2Rel()},\n        name=\"eta_l2\",\n    )\n    validator = {eta_l2_validator.name: eta_l2_validator}\n\n    # set visualizer(optional)\n    visu_mat = ppsci.utils.reader.load_mat_file(\n        cfg.VIV_DATA_PATH,\n        (\"t_f\", \"eta_gt\", \"f_gt\"),\n        alias_dict={\"eta_gt\": \"eta\", \"f_gt\": \"f\"},\n    )\n    visualizer = {\n        \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n            visu_mat,\n            (\"t_f\",),\n            {\n                r\"$\\eta$\": lambda d: d[\"eta\"],  # plot with latex title\n                r\"$\\eta_{gt}$\": lambda d: d[\"eta_gt\"],  # plot with latex title\n                r\"$f$\": equation[\"VIV\"].equations[\"f\"],  # plot with latex title\n                r\"$f_{gt}$\": lambda d: d[\"f_gt\"],  # plot with latex title\n            },\n            num_timestamps=1,\n            prefix=\"viv_pred\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        optimizer=optimizer,\n        equation=equation,\n        validator=validator,\n        visualizer=visualizer,\n        cfg=cfg,\n    )\n\n    # train model\n    solver.train()\n    # evaluate after finished training\n    solver.eval()\n    # visualize prediction after finished training\n    solver.visualize()\n\n\ndef evaluate(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set equation\n    equation = {\"VIV\": ppsci.equation.Vibration(2, -4, 0)}\n\n    # set validator\n    eta_l2_validator = ppsci.validate.SupervisedValidator(\n        {\n            \"dataset\": {\n                \"name\": \"MatDataset\",\n                \"file_path\": cfg.VIV_DATA_PATH,\n                \"input_keys\": (\"t_f\",),\n                \"label_keys\": (\"eta\", \"f\"),\n            },\n            \"batch_size\": cfg.EVAL.batch_size,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        {\"eta\": lambda out: out[\"eta\"], **equation[\"VIV\"].equations},\n        metric={\"MSE\": ppsci.metric.L2Rel()},\n        name=\"eta_l2\",\n    )\n    validator = {eta_l2_validator.name: eta_l2_validator}\n\n    # set visualizer(optional)\n    visu_mat = ppsci.utils.reader.load_mat_file(\n        cfg.VIV_DATA_PATH,\n        (\"t_f\", \"eta_gt\", \"f_gt\"),\n        alias_dict={\"eta_gt\": \"eta\", \"f_gt\": \"f\"},\n    )\n\n    visualizer = {\n        \"visualize_u\": ppsci.visualize.VisualizerScatter1D(\n            visu_mat,\n            (\"t_f\",),\n            {\n                r\"$\\eta$\": lambda d: d[\"eta\"],  # plot with latex title\n                r\"$\\eta_{gt}$\": lambda d: d[\"eta_gt\"],  # plot with latex title\n                r\"$f$\": equation[\"VIV\"].equations[\"f\"],  # plot with latex title\n                r\"$f_{gt}$\": lambda d: d[\"f_gt\"],  # plot with latex title\n            },\n            num_timestamps=1,\n            prefix=\"viv_pred\",\n        )\n    }\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        equation=equation,\n        validator=validator,\n        visualizer=visualizer,\n        cfg=cfg,\n    )\n\n    # evaluate\n    solver.eval()\n    # visualize prediction\n    solver.visualize()\n\n\ndef export(cfg: DictConfig):\n    from paddle import nn\n    from paddle.static import InputSpec\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n    # initialize equation\n    equation = {\"VIV\": ppsci.equation.Vibration(2, -4, 0)}\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        equation=equation,\n        cfg=cfg,\n    )\n    # Convert equation to func\n    f_func = ppsci.lambdify(\n        solver.equation[\"VIV\"].equations[\"f\"],\n        solver.model,\n        list(solver.equation[\"VIV\"].learnable_parameters),\n    )\n\n    class Wrapped_Model(nn.Layer):\n        def __init__(self, model, func):\n            super().__init__()\n            self.model = model\n            self.func = func\n\n        def forward(self, x):\n            model_out = self.model(x)\n            func_out = self.func(x)\n            return {**model_out, \"f\": func_out}\n\n    solver.model = Wrapped_Model(model, f_func)\n    # export models\n    input_spec = [\n        {key: InputSpec([None, 1], \"float32\", name=key) for key in model.input_keys},\n    ]\n    solver.export(input_spec, cfg.INFER.export_path, skip_prune_program=True)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    # set model predictor\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    infer_mat = ppsci.utils.reader.load_mat_file(\n        cfg.VIV_DATA_PATH,\n        (\"t_f\", \"eta_gt\", \"f_gt\"),\n        alias_dict={\"eta_gt\": \"eta\", \"f_gt\": \"f\"},\n    )\n\n    input_dict = {key: infer_mat[key] for key in cfg.INFER.input_keys}\n\n    output_dict = predictor.predict(input_dict, cfg.INFER.batch_size)\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.INFER.output_keys, output_dict.keys())\n    }\n    infer_mat.update(output_dict)\n\n    ppsci.visualize.plot.save_plot_from_1d_dict(\n        \"./viv_pred\", infer_mat, (\"t_f\",), (\"eta\", \"eta_gt\", \"f\", \"f_gt\")\n    )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"viv.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/viv/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u5982\u4e0b\u6240\u793a\uff0c\u6a2a\u8f74\u4e3a\u65f6\u95f4\u81ea\u53d8\u91cf\\(t\\)\uff0c\\(\\eta_{gt}\\)\u4e3a\u53c2\u8003\u632f\u5e45\uff0c\\(\\eta\\)\u4e3a\u6a21\u578b\u9884\u6d4b\u632f\u5e45\uff0c\\(f_{gt}\\)\u4e3a\u53c2\u8003\u5347\u529b\uff0c\\(f\\)\u4e3a\u6a21\u578b\u9884\u6d4b\u5347\u529b\u3002</p> <p> </p>  \u632f\u5e45 eta \u4e0e\u5347\u529b f \u968f\u65f6\u95f4t\u53d8\u5316\u7684\u9884\u6d4b\u7ed3\u679c\u548c\u53c2\u8003\u7ed3\u679c <p>\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u5bf9\u5728\\([0,10]\\)\u65f6\u95f4\u8303\u56f4\u5185\uff0c\u5bf9\u632f\u5e45\u548c\u5347\u529b\u7684\u9884\u6d4b\u7ed3\u679c\u4e0e\u53c2\u8003\u7ed3\u679c\u57fa\u672c\u4e00\u81f4\u3002</p>","tags":["\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","Vibration\u65b9\u7a0b","PINN\u6c42\u89e3\u65b9\u6cd5","\u7ebf\u6027\u4ee3\u6570","\u9ad8\u9636\u5fae\u5206"]},{"location":"zh/examples/volterra_ide/","title":"Volterra_IDE","text":"","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#volterra-integral-equation","title":"Volterra integral equation","text":"<p>AI Studio\u5feb\u901f\u4f53\u9a8c</p> \u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4\u6a21\u578b\u5bfc\u51fa\u547d\u4ee4\u6a21\u578b\u63a8\u7406\u547d\u4ee4 <pre><code>python volterra_ide.py\n</code></pre> <pre><code>python volterra_ide.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/volterra_ide/volterra_ide_pretrained.pdparams\n</code></pre> <pre><code>python volterra_ide.py mode=export\n</code></pre> <pre><code>python volterra_ide.py mode=infer\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 volterra_ide_pretrained.pdparams loss(L2Rel_Validator): 0.00023  L2Rel.u(L2Rel_Validator): 0.00023","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>Volterra integral equation(\u6c83\u5c14\u6cf0\u62c9\u79ef\u5206\u65b9\u7a0b)\u662f\u4e00\u79cd\u79ef\u5206\u65b9\u7a0b\uff0c\u5373\u65b9\u7a0b\u4e2d\u542b\u6709\u5bf9\u5f85\u6c42\u89e3\u51fd\u6570\u7684\u79ef\u5206\u8fd0\u7b97\uff0c\u5176\u6709\u4e24\u79cd\u5f62\u5f0f\uff0c\u5982\u4e0b\u6240\u793a</p> \\[ \\begin{aligned}   f(t) &amp;= \\int_a^t K(t, s) x(s) d s \\\\   x(t) &amp;= f(t)+\\int_a^t K(t, s) x(s) d s \\end{aligned} \\] <p>\u5728\u6570\u5b66\u9886\u57df\uff0c\u6c83\u5c14\u6cf0\u62c9\u65b9\u7a0b\u53ef\u4ee5\u7528\u4e8e\u8868\u8fbe\u5404\u79cd\u591a\u53d8\u91cf\u6982\u7387\u5206\u5e03\uff0c\u662f\u8fdb\u884c\u591a\u53d8\u91cf\u7edf\u8ba1\u5206\u6790\u7684\u6709\u529b\u5de5\u5177\u3002\u8fd9\u4f7f\u5f97\u5b83\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u7ed3\u6784\u65f6\u975e\u5e38\u6709\u7528\uff0c\u4f8b\u5982\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u3002\u6c83\u5c14\u6cf0\u62c9\u65b9\u7a0b\u8fd8\u53ef\u4ee5\u7528\u4e8e\u8ba1\u7b97\u4e0d\u540c\u7ef4\u5ea6\u5c5e\u6027\u7684\u76f8\u5173\u6027\uff0c\u4ee5\u53ca\u6a21\u62df\u590d\u6742\u7684\u6570\u636e\u96c6\u7ed3\u6784\uff0c\u4ee5\u4fbf\u4e3a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u6709\u6548\u7684\u6570\u636e\u652f\u6301\u3002</p> <p>\u5728\u751f\u7269\u5b66\u9886\u57df\uff0c\u6c83\u5c14\u6cf0\u62c9\u65b9\u7a0b\u88ab\u7528\u4f5c\u6e14\u4e1a\u751f\u4ea7\u7684\u6307\u5bfc\uff0c\u5bf9\u751f\u6001\u5e73\u8861\u548c\u73af\u5883\u4fdd\u62a4\u6709\u91cd\u8981\u610f\u4e49\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u7a0b\u8fd8\u5728\u75be\u75c5\u9632\u6cbb\uff0c\u4eba\u53e3\u7edf\u8ba1\u7b49\u65b9\u9762\u6709\u5e94\u7528\u3002\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u6c83\u5c14\u6cf0\u62c9\u65b9\u7a0b\u7684\u5efa\u7acb\u662f\u6570\u5b66\u5728\u751f\u7269\u5b66\u9886\u57df\u5e94\u7528\u7684\u9996\u6b21\u6210\u529f\u5c1d\u8bd5\uff0c\u63a8\u52a8\u4e86\u751f\u7269\u6570\u5b66\u8fd9\u95e8\u79d1\u5b66\u7684\u4ea7\u751f\u548c\u53d1\u5c55\u3002</p> <p>\u672c\u6848\u4f8b\u4ee5\u7b2c\u4e8c\u79cd\u65b9\u7a0b\u4e3a\u4f8b\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u5f0f\u8fdb\u884c\u6c42\u89e3\u3002</p>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u5047\u8bbe\u5b58\u5728\u5982\u4e0b IDE \u65b9\u7a0b\uff1a</p> \\[ u(t) = -\\dfrac{du}{dt} + \\int_{t_0}^t e^{t-s} u(s) d s \\] <p>\u5176\u4e2d \\(u(t)\\) \u5c31\u662f\u5f85\u6c42\u89e3\u7684\u51fd\u6570\uff0c\u800c \\(-\\dfrac{du}{dt}\\) \u5bf9\u5e94\u4e86 \\(f(t)\\)\uff0c\\(e^{t-s}\\) \u5bf9\u5e94\u4e86 \\(K(t,s)\\)\u3002 \u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4ee5 \\(t\\) \u4e3a\u8f93\u5165\uff0c\\(u(t)\\) \u4e3a\u8f93\u51fa\uff0c\u6839\u636e\u4e0a\u8ff0\u65b9\u7a0b\u6784\u5efa\u5fae\u5206\u7ea6\u675f\uff0c\u8fdb\u884c\u65e0\u76d1\u7763\u5b66\u4e60\u6700\u7ec8\u62df\u5408\u51fa\u5f85\u6c42\u89e3\u7684\u51fd\u6570 \\(u(t)\\)\u3002</p> <p>\u4e3a\u4e86\u65b9\u4fbf\u5728\u8ba1\u7b97\u673a\u4e2d\u8fdb\u884c\u6c42\u89e3\uff0c\u6211\u4eec\u5c06\u4e0a\u5f0f\u8fdb\u884c\u79fb\u9879\uff0c\u8ba9\u79ef\u5206\u9879\u4f5c\u4e3a\u5de6\u4fa7\uff0c\u975e\u79ef\u5206\u9879\u653e\u81f3\u53f3\u4fa7\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> \\[ \\int_{t_0}^t e^{t-s} u(s) d s = u(t) + \\dfrac{du}{dt} \\]","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#31","title":"3.1 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u4e0a\u8ff0\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u8f93\u5165\u4e3a \\(x\\)\uff0c\u8f93\u51fa\u4e3a \\(u(x)\\)\uff0c\u56e0\u6b64\u6211\u4eec\u4f7f\u7528\uff0c\u7528 PaddleScience \u4ee3\u7801\u8868\u793a\u5982\u4e0b\uff1a</p> <pre><code># set model\nmodel = ppsci.arch.MLP(**cfg.MODEL)\n</code></pre> <p>\u4e3a\u4e86\u5728\u8ba1\u7b97\u65f6\uff0c\u51c6\u786e\u5feb\u901f\u5730\u8bbf\u95ee\u5177\u4f53\u53d8\u91cf\u7684\u503c\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u7f51\u7edc\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u540d\u662f <code>\"x\"</code>(\u5373\u516c\u5f0f\u4e2d\u7684 \\(t\\))\uff0c\u8f93\u51fa\u53d8\u91cf\u540d\u662f <code>\"u\"</code>\uff0c\u63a5\u7740\u901a\u8fc7\u6307\u5b9a <code>MLP</code> \u7684\u9690\u85cf\u5c42\u5c42\u6570\u3001\u795e\u7ecf\u5143\u4e2a\u6570\uff0c\u6211\u4eec\u5c31\u5b9e\u4f8b\u5316\u51fa\u4e86\u795e\u7ecf\u7f51\u7edc\u6a21\u578b <code>model</code>\u3002</p>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#32","title":"3.2 \u8ba1\u7b97\u57df\u6784\u5efa","text":"<p>Volterra_IDE \u95ee\u9898\u7684\u79ef\u5206\u57df\u662f \\(a\\) ~ \\(t\\)\uff0c\u5176\u4e2d <code>a</code> \u4e3a\u56fa\u5b9a\u5e38\u6570 0\uff0c<code>t</code> \u7684\u8303\u56f4\u4e3a 0 ~ 5\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528PaddleScience \u5185\u7f6e\u7684\u4e00\u7ef4\u51e0\u4f55 <code>TimeDomain</code> \u4f5c\u4e3a\u8ba1\u7b97\u57df\u3002</p> <pre><code># set geometry\ngeom = {\"timedomain\": ppsci.geometry.TimeDomain(*cfg.BOUNDS)}\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#33","title":"3.3 \u65b9\u7a0b\u6784\u5efa","text":"<p>\u7531\u4e8e Volterra_IDE \u4f7f\u7528\u7684\u662f\u79ef\u5206\u65b9\u7a0b\uff0c\u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 PaddleScience \u5185\u7f6e\u7684 <code>ppsci.equation.Volterra</code>\uff0c\u5e76\u6307\u5b9a\u6240\u9700\u7684\u53c2\u6570\uff1a\u79ef\u5206\u4e0b\u9650 <code>a</code>\u3001<code>t</code> \u7684\u79bb\u6563\u53d6\u503c\u70b9\u6570 <code>num_points</code>\u3001\u4e00\u7ef4\u9ad8\u65af\u79ef\u5206\u70b9\u7684\u4e2a\u6570 <code>quad_deg</code>\u3001\\(K(t,s)\\) \u6838\u51fd\u6570 <code>kernel_func</code>\u3001\\(u(t) - f(t)\\) \u7b49\u5f0f\u53f3\u4fa7\u8868\u8fbe\u5f0f <code>func</code>\u3002</p> <pre><code># set equation\ndef kernel_func(x, s):\n    return np.exp(s - x)\n\ndef func(out):\n    x, u = out[\"x\"], out[\"u\"]\n    return jacobian(u, x) + u\n\nequation = {\n    \"volterra\": ppsci.equation.Volterra(\n        cfg.BOUNDS[0],\n        cfg.TRAIN.npoint_interior,\n        cfg.TRAIN.quad_deg,\n        kernel_func,\n        func,\n    )\n}\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#34","title":"3.4 \u7ea6\u675f\u6784\u5efa","text":"","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#341","title":"3.4.1 \u5185\u90e8\u70b9\u7ea6\u675f","text":"<p>\u672c\u6587\u91c7\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u5bf9\u79fb\u9879\u540e\u65b9\u7a0b\u7684\u5de6\u3001\u53f3\u4e24\u4fa7\u8fdb\u884c\u7ea6\u675f\uff0c\u8ba9\u5176\u5c3d\u91cf\u76f8\u7b49\u3002</p> <p>\u7531\u4e8e\u7b49\u5f0f\u5de6\u4fa7\u6d89\u53ca\u5230\u79ef\u5206\u8ba1\u7b97\uff08\u5b9e\u9645\u91c7\u7528\u9ad8\u65af\u79ef\u5206\u8fd1\u4f3c\u8ba1\u7b97\uff09\uff0c\u56e0\u6b64\u5728 0 ~ 5 \u533a\u95f4\u5185\u91c7\u6837\u51fa\u591a\u4e2a <code>t_i</code> \u70b9\u540e\uff0c\u8fd8\u9700\u8981\u8ba1\u7b97\u5176\u7528\u4e8e\u9ad8\u65af\u79ef\u5206\u7684\u70b9\u96c6\uff0c\u5373\u5bf9\u6bcf\u4e00\u4e2a <code>(0,t_i)</code> \u533a\u95f4\uff0c\u90fd\u8ba1\u7b97\u51fa\u4e00\u4e00\u5bf9\u5e94\u7684\u9ad8\u65af\u79ef\u5206\u70b9\u96c6 <code>quad_i</code> \u548c\u70b9\u6743 <code>weight_i</code>\u3002PaddleScience \u5c06\u8fd9\u4e00\u6b65\u4f5c\u4e3a\u8f93\u5165\u6570\u636e\u7684\u9884\u5904\u7406\uff0c\u52a0\u5165\u5230\u4ee3\u7801\u4e2d\uff0c\u5982\u4e0b\u6240\u793a</p> <pre><code># set constraint\n# set transform for input data\ndef input_data_quad_transform(\n    input: Dict[str, np.ndarray],\n    weight: Dict[str, np.ndarray],\n    label: Dict[str, np.ndarray],\n) -&gt; Tuple[\n    Dict[str, paddle.Tensor], Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]\n]:\n    \"\"\"Get sampling points for integral.\n\n    Args:\n        input (Dict[str, paddle.Tensor]): Raw input dict.\n        weight (Dict[str, paddle.Tensor]): Raw weight dict.\n        label (Dict[str, paddle.Tensor]): Raw label dict.\n\n    Returns:\n        Tuple[ Dict[str, paddle.Tensor], Dict[str, paddle.Tensor], Dict[str, paddle.Tensor] ]:\n            Input dict contained sampling points, weight dict and label dict.\n    \"\"\"\n    x = input[\"x\"]  # N points.\n    x_quad = equation[\"volterra\"].get_quad_points(x).reshape([-1, 1])  # NxQ\n    x_quad = paddle.concat((x, x_quad), axis=0)  # M+MxQ: [M|Q1|Q2,...,QM|]\n    return (\n        {\n            **input,\n            \"x\": x_quad,\n        },\n        weight,\n        label,\n    )\n\n# interior constraint\nide_constraint = ppsci.constraint.InteriorConstraint(\n    equation[\"volterra\"].equations,\n    {\"volterra\": 0},\n    geom[\"timedomain\"],\n    {\n        \"dataset\": {\n            \"name\": \"IterableNamedArrayDataset\",\n            \"transforms\": (\n                {\n                    \"FunctionalTransform\": {\n                        \"transform_func\": input_data_quad_transform,\n                    },\n                },\n            ),\n        },\n        \"batch_size\": cfg.TRAIN.npoint_interior,\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    evenly=True,\n    name=\"EQ\",\n)\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#342","title":"3.4.2 \u521d\u503c\u7ea6\u675f","text":"<p>\u5728 \\(t=0\\) \u65f6\uff0c\u6709\u4ee5\u4e0b\u521d\u503c\u6761\u4ef6\uff1a</p> \\[ u(0) = e^{-t} \\cosh(t)|_{t=0} = e^{0} \\cosh(0) = 1 \\] <p>\u56e0\u6b64\u53ef\u4ee5\u52a0\u5165 <code>t=0</code> \u65f6\u7684\u521d\u503c\u6761\u4ef6\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a</p> <pre><code># initial condition\ndef u_solution_func(in_):\n    if isinstance(in_[\"x\"], paddle.Tensor):\n        return paddle.exp(-in_[\"x\"]) * paddle.cosh(in_[\"x\"])\n    return np.exp(-in_[\"x\"]) * np.cosh(in_[\"x\"])\n\nic = ppsci.constraint.BoundaryConstraint(\n    {\"u\": lambda out: out[\"u\"]},\n    {\"u\": u_solution_func},\n    geom[\"timedomain\"],\n    {\n        \"dataset\": {\"name\": \"IterableNamedArrayDataset\"},\n        \"batch_size\": cfg.TRAIN.npoint_ic,\n        \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n    },\n    ppsci.loss.MSELoss(\"mean\"),\n    criteria=geom[\"timedomain\"].on_initial,\n    name=\"IC\",\n)\n</code></pre> <p>\u5728\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u3001\u521d\u503c\u7ea6\u675f\u6784\u5efa\u5b8c\u6bd5\u4e4b\u540e\uff0c\u4ee5\u6211\u4eec\u521a\u624d\u7684\u547d\u540d\u4e3a\u5173\u952e\u5b57\uff0c\u5c01\u88c5\u5230\u4e00\u4e2a\u5b57\u5178\u4e2d\uff0c\u65b9\u4fbf\u540e\u7eed\u8bbf\u95ee\u3002</p> <pre><code># wrap constraints together\nconstraint = {\n    ide_constraint.name: ide_constraint,\n    ic.name: ic,\n}\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#35","title":"3.5 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570\u548c\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u6211\u4eec\u6309\u5b9e\u9a8c\u7ecf\u9a8c\uff0c\u8ba9 <code>L-BFGS</code> \u4f18\u5316\u5668\u8fdb\u884c\u4e00\u8f6e\u4f18\u5316\u5373\u53ef\uff0c\u4f46\u4e00\u8f6e\u4f18\u5316\u5185\u7684 <code>max_iters</code> \u6570\u53ef\u4ee5\u8bbe\u7f6e\u4e3a\u4e00\u4e2a\u8f83\u5927\u7684\u4e00\u4e2a\u6570 <code>15000</code>\u3002</p> <pre><code># training settings\nTRAIN:\n  epochs: 1\n  iters_per_epoch: 1\n  save_freq: 1\n  eval_during_train: true\n  eval_freq: 1\n  optimizer:\n    learning_rate: 1\n    max_iter: 15000\n    max_eval: 1250\n    tolerance_grad: 1.0e-8\n    tolerance_change: 0\n    history_size: 100\n  quad_deg: 20\n  npoint_interior: 12\n  npoint_ic: 1\n  pretrained_model_path: null\n  checkpoint_path: null\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#36","title":"3.6 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>LBFGS</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.LBFGS(**cfg.TRAIN.optimizer)(model)\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#37","title":"3.7 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6\uff08\u6d4b\u8bd5\u96c6\uff09\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.GeometryValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code># set validator\nl2rel_validator = ppsci.validate.GeometryValidator(\n    {\"u\": lambda out: out[\"u\"]},\n    {\"u\": u_solution_func},\n    geom[\"timedomain\"],\n    {\n        \"dataset\": \"IterableNamedArrayDataset\",\n        \"total_size\": cfg.EVAL.npoint_eval,\n    },\n    ppsci.loss.L2RelLoss(),\n    evenly=True,\n    metric={\"L2Rel\": ppsci.metric.L2Rel()},\n    name=\"L2Rel_Validator\",\n)\nvalidator = {l2rel_validator.name: l2rel_validator}\n</code></pre> <p>\u8bc4\u4ef7\u6307\u6807 <code>metric</code> \u9009\u62e9 <code>ppsci.metric.L2Rel</code> \u5373\u53ef\u3002</p> <p>\u5176\u4f59\u914d\u7f6e\u4e0e 3.4 \u7ea6\u675f\u6784\u5efa \u7684\u8bbe\u7f6e\u7c7b\u4f3c\u3002</p>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#38","title":"3.8 \u6a21\u578b\u8bad\u7ec3","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    model,\n    constraint,\n    cfg.output_dir,\n    optimizer,\n    epochs=cfg.TRAIN.epochs,\n    iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n    eval_during_train=cfg.TRAIN.eval_during_train,\n    eval_freq=cfg.TRAIN.eval_freq,\n    equation=equation,\n    geom=geom,\n    validator=validator,\n    pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n    checkpoint_path=cfg.TRAIN.checkpoint_path,\n    eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n)\n# train model\nsolver.train()\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#39","title":"3.9 \u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u5728\u6a21\u578b\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u624b\u52a8\u6784\u9020 0 ~ 5 \u533a\u95f4\u5185\u5747\u5300 100 \u4e2a\u70b9\uff0c\u4f5c\u4e3a\u8bc4\u4f30\u7684\u79ef\u5206\u4e0a\u9650 <code>t</code> \u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u53ef\u89c6\u5316\u7ed3\u679c\u3002</p> <pre><code># visualize prediction after finished training\ninput_data = geom[\"timedomain\"].uniform_points(100)\nlabel_data = u_solution_func({\"x\": input_data})\noutput_data = solver.predict({\"x\": input_data}, return_numpy=True)[\"u\"]\n\nplt.plot(input_data, label_data, \"-\", label=r\"$u(t)$\")\nplt.plot(input_data, output_data, \"o\", label=r\"$\\hat{u}(t)$\", markersize=4.0)\nplt.legend()\nplt.xlabel(r\"$t$\")\nplt.ylabel(r\"$u$\")\nplt.title(r\"$u-t$\")\nplt.savefig(osp.join(cfg.output_dir, \"./Volterra_IDE.png\"), dpi=200)\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"volterra_ide.py<pre><code># Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Reference: https://github.com/lululxvi/deepxde/blob/master/examples/pinn_forward/Volterra_IDE.py\n\nfrom os import path as osp\nfrom typing import Dict\nfrom typing import Tuple\n\nimport hydra\nimport numpy as np\nimport paddle\nfrom matplotlib import pyplot as plt\nfrom omegaconf import DictConfig\n\nimport ppsci\nfrom ppsci.autodiff import jacobian\nfrom ppsci.utils import logger\n\n\ndef train(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n\n    # set output directory\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set geometry\n    geom = {\"timedomain\": ppsci.geometry.TimeDomain(*cfg.BOUNDS)}\n\n    # set equation\n    def kernel_func(x, s):\n        return np.exp(s - x)\n\n    def func(out):\n        x, u = out[\"x\"], out[\"u\"]\n        return jacobian(u, x) + u\n\n    equation = {\n        \"volterra\": ppsci.equation.Volterra(\n            cfg.BOUNDS[0],\n            cfg.TRAIN.npoint_interior,\n            cfg.TRAIN.quad_deg,\n            kernel_func,\n            func,\n        )\n    }\n\n    # set constraint\n    # set transform for input data\n    def input_data_quad_transform(\n        input: Dict[str, np.ndarray],\n        weight: Dict[str, np.ndarray],\n        label: Dict[str, np.ndarray],\n    ) -&gt; Tuple[\n        Dict[str, paddle.Tensor], Dict[str, paddle.Tensor], Dict[str, paddle.Tensor]\n    ]:\n        \"\"\"Get sampling points for integral.\n\n        Args:\n            input (Dict[str, paddle.Tensor]): Raw input dict.\n            weight (Dict[str, paddle.Tensor]): Raw weight dict.\n            label (Dict[str, paddle.Tensor]): Raw label dict.\n\n        Returns:\n            Tuple[ Dict[str, paddle.Tensor], Dict[str, paddle.Tensor], Dict[str, paddle.Tensor] ]:\n                Input dict contained sampling points, weight dict and label dict.\n        \"\"\"\n        x = input[\"x\"]  # N points.\n        x_quad = equation[\"volterra\"].get_quad_points(x).reshape([-1, 1])  # NxQ\n        x_quad = paddle.concat((x, x_quad), axis=0)  # M+MxQ: [M|Q1|Q2,...,QM|]\n        return (\n            {\n                **input,\n                \"x\": x_quad,\n            },\n            weight,\n            label,\n        )\n\n    # interior constraint\n    ide_constraint = ppsci.constraint.InteriorConstraint(\n        equation[\"volterra\"].equations,\n        {\"volterra\": 0},\n        geom[\"timedomain\"],\n        {\n            \"dataset\": {\n                \"name\": \"IterableNamedArrayDataset\",\n                \"transforms\": (\n                    {\n                        \"FunctionalTransform\": {\n                            \"transform_func\": input_data_quad_transform,\n                        },\n                    },\n                ),\n            },\n            \"batch_size\": cfg.TRAIN.npoint_interior,\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        evenly=True,\n        name=\"EQ\",\n    )\n\n    # initial condition\n    def u_solution_func(in_):\n        if isinstance(in_[\"x\"], paddle.Tensor):\n            return paddle.exp(-in_[\"x\"]) * paddle.cosh(in_[\"x\"])\n        return np.exp(-in_[\"x\"]) * np.cosh(in_[\"x\"])\n\n    ic = ppsci.constraint.BoundaryConstraint(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": u_solution_func},\n        geom[\"timedomain\"],\n        {\n            \"dataset\": {\"name\": \"IterableNamedArrayDataset\"},\n            \"batch_size\": cfg.TRAIN.npoint_ic,\n            \"iters_per_epoch\": cfg.TRAIN.iters_per_epoch,\n        },\n        ppsci.loss.MSELoss(\"mean\"),\n        criteria=geom[\"timedomain\"].on_initial,\n        name=\"IC\",\n    )\n    # wrap constraints together\n    constraint = {\n        ide_constraint.name: ide_constraint,\n        ic.name: ic,\n    }\n\n    # set optimizer\n    optimizer = ppsci.optimizer.LBFGS(**cfg.TRAIN.optimizer)(model)\n\n    # set validator\n    l2rel_validator = ppsci.validate.GeometryValidator(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": u_solution_func},\n        geom[\"timedomain\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"total_size\": cfg.EVAL.npoint_eval,\n        },\n        ppsci.loss.L2RelLoss(),\n        evenly=True,\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"L2Rel_Validator\",\n    )\n    validator = {l2rel_validator.name: l2rel_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        constraint,\n        cfg.output_dir,\n        optimizer,\n        epochs=cfg.TRAIN.epochs,\n        iters_per_epoch=cfg.TRAIN.iters_per_epoch,\n        eval_during_train=cfg.TRAIN.eval_during_train,\n        eval_freq=cfg.TRAIN.eval_freq,\n        equation=equation,\n        geom=geom,\n        validator=validator,\n        pretrained_model_path=cfg.TRAIN.pretrained_model_path,\n        checkpoint_path=cfg.TRAIN.checkpoint_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # train model\n    solver.train()\n\n    # visualize prediction after finished training\n    input_data = geom[\"timedomain\"].uniform_points(100)\n    label_data = u_solution_func({\"x\": input_data})\n    output_data = solver.predict({\"x\": input_data}, return_numpy=True)[\"u\"]\n\n    plt.plot(input_data, label_data, \"-\", label=r\"$u(t)$\")\n    plt.plot(input_data, output_data, \"o\", label=r\"$\\hat{u}(t)$\", markersize=4.0)\n    plt.legend()\n    plt.xlabel(r\"$t$\")\n    plt.ylabel(r\"$u$\")\n    plt.title(r\"$u-t$\")\n    plt.savefig(osp.join(cfg.output_dir, \"./Volterra_IDE.png\"), dpi=200)\n\n\ndef evaluate(cfg: DictConfig):\n    # set random seed for reproducibility\n    ppsci.utils.misc.set_random_seed(cfg.seed)\n\n    # set output directory\n    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"eval.log\"), \"info\")\n\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # set geometry\n    geom = {\"timedomain\": ppsci.geometry.TimeDomain(*cfg.BOUNDS)}\n    # set validator\n\n    def u_solution_func(in_) -&gt; np.ndarray:\n        if isinstance(in_[\"x\"], paddle.Tensor):\n            return paddle.exp(-in_[\"x\"]) * paddle.cosh(in_[\"x\"])\n        return np.exp(-in_[\"x\"]) * np.cosh(in_[\"x\"])\n\n    l2rel_validator = ppsci.validate.GeometryValidator(\n        {\"u\": lambda out: out[\"u\"]},\n        {\"u\": u_solution_func},\n        geom[\"timedomain\"],\n        {\n            \"dataset\": \"IterableNamedArrayDataset\",\n            \"total_size\": cfg.EVAL.npoint_eval,\n        },\n        ppsci.loss.L2RelLoss(),\n        evenly=True,\n        metric={\"L2Rel\": ppsci.metric.L2Rel()},\n        name=\"L2Rel_Validator\",\n    )\n    validator = {l2rel_validator.name: l2rel_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        output_dir=cfg.output_dir,\n        geom=geom,\n        validator=validator,\n        pretrained_model_path=cfg.EVAL.pretrained_model_path,\n        eval_with_no_grad=cfg.EVAL.eval_with_no_grad,\n    )\n    # evaluate model\n    solver.eval()\n\n    # visualize prediction\n    input_data = geom[\"timedomain\"].uniform_points(cfg.EVAL.npoint_eval)\n    label_data = u_solution_func({\"x\": input_data})\n    output_data = solver.predict({\"x\": input_data}, return_numpy=True)[\"u\"]\n\n    plt.plot(input_data, label_data, \"-\", label=r\"$u(t)$\")\n    plt.plot(input_data, output_data, \"o\", label=r\"$\\hat{u}(t)$\", markersize=4.0)\n    plt.legend()\n    plt.xlabel(r\"$t$\")\n    plt.ylabel(r\"$u$\")\n    plt.title(r\"$u-t$\")\n    plt.savefig(osp.join(cfg.output_dir, \"./Volterra_IDE.png\"), dpi=200)\n\n\ndef export(cfg: DictConfig):\n    # set model\n    model = ppsci.arch.MLP(**cfg.MODEL)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        model,\n        pretrained_model_path=cfg.INFER.pretrained_model_path,\n    )\n    # export model\n    from paddle.static import InputSpec\n\n    input_spec = [\n        {\n            key: InputSpec([None, 1], \"float32\", name=key)\n            for key in cfg.MODEL.input_keys\n        },\n    ]\n    solver.export(input_spec, cfg.INFER.export_path)\n\n\ndef inference(cfg: DictConfig):\n    from deploy.python_infer import pinn_predictor\n\n    predictor = pinn_predictor.PINNPredictor(cfg)\n\n    # set geometry\n    geom = {\"timedomain\": ppsci.geometry.TimeDomain(*cfg.BOUNDS)}\n\n    input_data = geom[\"timedomain\"].uniform_points(cfg.EVAL.npoint_eval)\n    input_dict = {\"x\": input_data}\n\n    output_dict = predictor.predict(\n        {key: input_dict[key] for key in cfg.MODEL.input_keys}, cfg.INFER.batch_size\n    )\n\n    # mapping data to cfg.INFER.output_keys\n    output_dict = {\n        store_key: output_dict[infer_key]\n        for store_key, infer_key in zip(cfg.MODEL.output_keys, output_dict.keys())\n    }\n\n    def u_solution_func(in_) -&gt; np.ndarray:\n        if isinstance(in_[\"x\"], paddle.Tensor):\n            return paddle.exp(-in_[\"x\"]) * paddle.cosh(in_[\"x\"])\n        return np.exp(-in_[\"x\"]) * np.cosh(in_[\"x\"])\n\n    label_data = u_solution_func({\"x\": input_data})\n    output_data = output_dict[\"u\"]\n\n    # save result\n    plt.plot(input_data, label_data, \"-\", label=r\"$u(t)$\")\n    plt.plot(input_data, output_data, \"o\", label=r\"$\\hat{u}(t)$\", markersize=4.0)\n    plt.legend()\n    plt.xlabel(r\"$t$\")\n    plt.ylabel(r\"$u$\")\n    plt.title(r\"$u-t$\")\n    plt.savefig(\"./Volterra_IDE_pred.png\", dpi=200)\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"volterra_ide.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    elif cfg.mode == \"export\":\n        export(cfg)\n    elif cfg.mode == \"infer\":\n        inference(cfg)\n    else:\n        raise ValueError(\n            f\"cfg.mode should in ['train', 'eval', 'export', 'infer'], but got '{cfg.mode}'\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u5982\u4e0b\u6240\u793a\uff0c\\(t\\)\u4e3a\u81ea\u53d8\u91cf\uff0c\\(u(t)\\)\u4e3a\u79ef\u5206\u65b9\u7a0b\u6807\u51c6\u89e3\u51fd\u6570\uff0c\\(\\hat{u}(t)\\)\u4e3a\u6a21\u578b\u9884\u6d4b\u7684\u79ef\u5206\u65b9\u7a0b\u89e3\u51fd\u6570</p> <p> </p> \u6a21\u578b\u6c42\u89e3\u7ed3\u679c(\u6a59\u8272\u6563\u70b9)\u548c\u53c2\u8003\u7ed3\u679c(\u84dd\u8272\u66f2\u7ebf) <p>\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u5bf9\u79ef\u5206\u65b9\u7a0b\u5728\\([0,5]\\)\u533a\u95f4\u5185\u7684\u9884\u6d4b\u7ed3\u679c\\(\\hat{u}(t)\\)\u548c\u6807\u51c6\u89e3\u7ed3\u679c\\(u(t)\\)\u57fa\u672c\u4e00\u81f4\u3002</p>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/volterra_ide/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li>DeepXDE - Antiderivative operator from an unaligned dataset</li> <li>Gaussian quadrature</li> <li>Volterra integral equation</li> </ul>","tags":["\u6570\u636e\u5904\u7406","\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","\u79ef\u5206\u65b9\u7a0b","\u7ebf\u6027\u4ee3\u6570"]},{"location":"zh/examples/xpinns/","title":"XPINN","text":""},{"location":"zh/examples/xpinns/#extended-physics-informed-neural-networks-xpinns","title":"Extended Physics-Informed Neural Networks (XPINNs)","text":"\u6a21\u578b\u8bad\u7ec3\u547d\u4ee4\u6a21\u578b\u8bc4\u4f30\u547d\u4ee4 <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/XPINN/XPINN_2D_PoissonEqn.mat -P ./data/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/XPINN/XPINN_2D_PoissonEqn.mat --create-dirs -o ./data/XPINN_2D_PoissonEqn.mat\npython xpinn.py\n</code></pre> <pre><code># linux\nwget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/XPINN/XPINN_2D_PoissonEqn.mat -P ./data/\n# windows\n# curl https://paddle-org.bj.bcebos.com/paddlescience/datasets/XPINN/XPINN_2D_PoissonEqn.mat --create-dirs -o ./data/XPINN_2D_PoissonEqn.mat\npython xpinn.py mode=eval EVAL.pretrained_model_path=https://paddle-org.bj.bcebos.com/paddlescience/models/XPINN/xpinn_pretrained.pdparams\n</code></pre> \u9884\u8bad\u7ec3\u6a21\u578b \u6307\u6807 xpinn_pretrained.pdparams L2Rel.l2_error: 0.04226"},{"location":"zh/examples/xpinns/#1","title":"1. \u80cc\u666f\u7b80\u4ecb","text":"<p>\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u662f\u4e00\u7c7b\u57fa\u7840\u7684\u7269\u7406\u95ee\u9898\uff0c\u968f\u7740\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u9ad8\u901f\u53d1\u5c55\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u6210\u4e3a\u65b0\u7684\u7814\u7a76\u8d8b\u52bf\u3002XPINNs\uff08Extended Physics-Informed Neural Networks\uff09\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7684\u5e7f\u4e49\u65f6\u7a7a\u57df\u5206\u89e3\u65b9\u6cd5\uff0c\u4ee5\u6c42\u89e3\u4efb\u610f\u590d\u6742\u51e0\u4f55\u57df\u4e0a\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u3002</p> <p>XPINNs \u901a\u8fc7\u5e7f\u4e49\u65f6\u7a7a\u533a\u57df\u5206\u89e3\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5e76\u884c\u80fd\u529b\uff0c\u5e76\u4e14\u652f\u6301\u9ad8\u5ea6\u4e0d\u89c4\u5219\u7684\u3001\u51f8/\u975e\u51f8\u7684\u65f6\u7a7a\u57df\u5206\u89e3\uff0c\u754c\u9762\u6761\u4ef6\u662f\u7b80\u5355\u7684\u3002XPINNs \u53ef\u6269\u5c55\u5230\u4efb\u610f\u7c7b\u578b\u7684\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u800c\u4e0d\u8bba\u65b9\u7a0b\u662f\u4f55\u79cd\u7269\u7406\u6027\u8d28\u3002</p> <p>\u7cbe\u786e\u6c42\u89e3\u9ad8\u7ef4\u590d\u6742\u7684\u65b9\u7a0b\u5df2\u7ecf\u6210\u4e3a\u79d1\u5b66\u8ba1\u7b97\u7684\u6700\u5927\u6311\u6218\u4e4b\u4e00\uff0cXPINNs \u7684\u4f18\u70b9\u4f7f\u5176\u6210\u4e3a\u6a21\u62df\u590d\u6742\u65b9\u7a0b\u7684\u9002\u7528\u65b9\u6cd5\u3002</p>"},{"location":"zh/examples/xpinns/#2","title":"2. \u95ee\u9898\u5b9a\u4e49","text":"<p>\u4e8c\u7ef4\u6cca\u677e\u65b9\u7a0b\uff1a</p> \\[ \\Delta u = f(x, y),  x,y \\in \\Omega \\subset R^2\\]"},{"location":"zh/examples/xpinns/#3","title":"3. \u95ee\u9898\u6c42\u89e3","text":"<p>\u63a5\u4e0b\u6765\u5f00\u59cb\u8bb2\u89e3\u5982\u4f55\u5c06\u95ee\u9898\u4e00\u6b65\u4e00\u6b65\u5730\u8f6c\u5316\u4e3a PaddleScience \u4ee3\u7801\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6c42\u89e3\u8be5\u95ee\u9898\u3002 \u4e3a\u4e86\u5feb\u901f\u7406\u89e3 PaddleScience\uff0c\u63a5\u4e0b\u6765\u4ec5\u5bf9\u6a21\u578b\u6784\u5efa\u3001\u65b9\u7a0b\u6784\u5efa\u3001\u8ba1\u7b97\u57df\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u9610\u8ff0\uff0c\u800c\u5176\u4f59\u7ec6\u8282\u8bf7\u53c2\u8003 API\u6587\u6863\u3002</p>"},{"location":"zh/examples/xpinns/#31","title":"3.1 \u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6570\u636e\u96c6\u5305\u542b\u8ba1\u7b97\u57df\u7684\u4e09\u4e2a\u5b50\u533a\u57df\u7684\u6570\u636e\uff1a\u7ea2\u8272\u533a\u57df\u7684\u8fb9\u754c\u548c\u6b8b\u5dee\u70b9\uff1b\u9ec4\u8272\u533a\u57df\u7684\u754c\u9762\uff1b\u4ee5\u53ca\u7eff\u8272\u533a\u57df\u7684\u754c\u9762\u3002</p> <p> </p> \u4e8c\u7ef4\u6cca\u677e\u65b9\u7a0b\u7684\u4e09\u4e2a\u5b50\u533a\u57df <p>\u8ba1\u7b97\u57df\u7684\u8fb9\u754c\u8868\u8fbe\u5f0f\u5982\u4e0b\u3002</p> \\[ \\gamma =1.5+0.14 sin(4\u03b8)+0.12 cos(6\u03b8)+0.09 cos(5\u03b8), \u03b8 \\in [0,2\u03c0) \\] <p>\u7ea2\u8272\u533a\u57df\u548c\u9ec4\u8272\u533a\u57df\u7684\u754c\u9762\u7684\u8868\u8fbe\u5f0f\u5982\u4e0b\u3002</p> \\[ \\gamma_1 =0.5+0.18 sin(3\u03b8)+0.08 cos(2\u03b8)+0.2 cos(5\u03b8), \u03b8 \\in [0,2\u03c0)\\] \\[ \\gamma_2 =0.34+0.04 sin(5\u03b8)+0.18 cos(3\u03b8)+0.1 cos(6\u03b8), \u03b8 \\in [0,2\u03c0) \\] <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7d\u5e76\u89e3\u538b\u6570\u636e\u96c6\u3002</p> <pre><code>wget -nc https://paddle-org.bj.bcebos.com/paddlescience/datasets/XPINN/XPINN_2D_PoissonEqn.mat -P ./data/\n</code></pre>"},{"location":"zh/examples/xpinns/#32","title":"3.2 \u6a21\u578b\u6784\u5efa","text":"<p>\u5728\u672c\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc <code>MLP</code> \u4f5c\u4e3a\u6a21\u578b\uff0c\u5728\u6a21\u578b\u4ee3\u7801\u4e2d\u5b9a\u4e49\u4e09\u4e2a <code>MLP</code> \uff0c\u5206\u522b\u4f5c\u4e3a\u4e09\u4e2a\u5b50\u533a\u57df\u7684\u6a21\u578b\u3002</p> <pre><code># set model\ncustom_model = model.Model(layer_list)\n</code></pre> <p>\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 XPINN \u65b9\u6cd5\u5206\u522b\u8ba1\u7b97\u6bcf\u4e2a\u5b50\u533a\u57df\u7684\u6a21\u578b\u635f\u5931\u3002</p> <p> </p> XPINN\u5b50\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b"},{"location":"zh/examples/xpinns/#33","title":"3.3 \u7ea6\u675f\u6784\u5efa","text":"<p>\u5728\u672c\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u76d1\u7763\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u76d1\u7763\u7ea6\u675f\u3002</p> <p>\u5728\u5b9a\u4e49\u7ea6\u675f\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u6307\u5b9a\u6570\u636e\u96c6\u7684\u8def\u5f84\u7b49\u76f8\u5173\u914d\u7f6e\uff0c\u5c06\u8fd9\u4e9b\u4fe1\u606f\u5b58\u653e\u5230\u5bf9\u5e94\u7684 YAML \u6587\u4ef6\u4e2d\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code># set training data file\nDATA_FILE: \"./data/XPINN_2D_PoissonEqn.mat\"\n</code></pre> <p>\u63a5\u7740\u5b9a\u4e49\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u8c03\u7528 XPINN \u65b9\u6cd5\u8ba1\u7b97\u635f\u5931\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>def loss_fun(\n    output_dict: Dict[str, paddle.Tensor],\n    label_dict: Dict[str, paddle.Tensor],\n    *args,\n) -&gt; float:\n    def residual_func(output_der: paddle.Tensor, input: paddle.Tensor) -&gt; paddle.Tensor:\n        return paddle.add_n(output_der) - paddle.add_n(\n            [paddle.exp(_in) for _in in input]\n        )\n\n    # subdomain 1\n    loss1 = _xpinn_loss(\n        training_pres=[output_dict[\"boundary_u\"]],\n        training_exacts=[label_dict[\"boundary_u_exact\"]],\n        training_weight=20,\n        residual_inputs=[[output_dict[\"residual1_x\"], output_dict[\"residual1_y\"]]],\n        residual_pres=[output_dict[\"residual1_u\"]],\n        residual_weight=1,\n        interface_inputs=[\n            [output_dict[\"interface1_x\"], output_dict[\"interface1_y\"]],\n            [output_dict[\"interface2_x\"], output_dict[\"interface2_y\"]],\n        ],\n        interface_pres=[\n            output_dict[\"interface1_u_sub1\"],\n            output_dict[\"interface2_u_sub1\"],\n        ],\n        interface_weight=20,\n        interface_neigh_pres=[\n            [output_dict[\"interface1_u_sub2\"]],\n            [output_dict[\"interface2_u_sub3\"]],\n        ],\n        interface_neigh_weight=1,\n        residual_func=residual_func,\n    )\n\n    # subdomain 2\n    loss2 = _xpinn_loss(\n        residual_inputs=[[output_dict[\"residual2_x\"], output_dict[\"residual2_y\"]]],\n        residual_pres=[output_dict[\"residual2_u\"]],\n        residual_weight=1,\n        interface_inputs=[[output_dict[\"interface1_x\"], output_dict[\"interface1_y\"]]],\n        interface_pres=[output_dict[\"interface1_u_sub1\"]],\n        interface_weight=20,\n        interface_neigh_pres=[[output_dict[\"interface1_u_sub2\"]]],\n        interface_neigh_weight=1,\n        residual_func=residual_func,\n    )\n\n    # subdomain 3\n    loss3 = _xpinn_loss(\n        residual_inputs=[[output_dict[\"residual3_x\"], output_dict[\"residual3_y\"]]],\n        residual_pres=[output_dict[\"residual3_u\"]],\n        residual_weight=1,\n        interface_inputs=[[output_dict[\"interface2_x\"], output_dict[\"interface2_y\"]]],\n        interface_pres=[output_dict[\"interface2_u_sub1\"]],\n        interface_weight=20,\n        interface_neigh_pres=[[output_dict[\"interface2_u_sub3\"]]],\n        interface_neigh_weight=1,\n        residual_func=residual_func,\n    )\n\n    return {\"residuals\": loss1 + loss2 + loss3}\n</code></pre> <p>\u6700\u540e\u6784\u5efa\u76d1\u7763\u7ea6\u675f\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code># set constraint\nsup_constraint = ppsci.constraint.SupervisedConstraint(\n    train_dataloader_cfg,\n    ppsci.loss.FunctionalLoss(loss_fun),\n    {\"residual1_u\": lambda out: out[\"residual1_u\"]},\n    name=\"sup_constraint\",\n)\nconstraint = {sup_constraint.name: sup_constraint}\n</code></pre>"},{"location":"zh/examples/xpinns/#34","title":"3.4 \u8d85\u53c2\u6570\u8bbe\u5b9a","text":"<p>\u8bbe\u7f6e\u8bad\u7ec3\u8f6e\u6570\u7b49\u53c2\u6570\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>epochs: 501\niters_per_epoch: 1\nsave_freq: 50\neval_during_train: true\neval_freq: 50\nlearning_rate: 0.0008\n</code></pre>"},{"location":"zh/examples/xpinns/#35","title":"3.5 \u4f18\u5316\u5668\u6784\u5efa","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u4f1a\u8c03\u7528\u4f18\u5316\u5668\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff0c\u6b64\u5904\u9009\u62e9\u8f83\u4e3a\u5e38\u7528\u7684 <code>Adam</code> \u4f18\u5316\u5668\u3002</p> <pre><code># set optimizer\noptimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(custom_model)\n</code></pre>"},{"location":"zh/examples/xpinns/#36","title":"3.6 \u8bc4\u4f30\u5668\u6784\u5efa","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4f1a\u6309\u4e00\u5b9a\u8f6e\u6570\u95f4\u9694\uff0c\u7528\u9a8c\u8bc1\u96c6(\u6d4b\u8bd5\u96c6)\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8bad\u7ec3\u60c5\u51b5\uff0c\u56e0\u6b64\u4f7f\u7528 <code>ppsci.validate.SupervisedValidator</code> \u6784\u5efa\u8bc4\u4f30\u5668\u3002</p> <pre><code>sup_validator = ppsci.validate.SupervisedValidator(\n    eval_dataloader_cfg,\n    loss=ppsci.loss.FunctionalLoss(loss_fun),\n    output_expr={\n        \"residual1_u\": lambda out: out[\"residual1_u\"],\n        \"residual2_u\": lambda out: out[\"residual2_u\"],\n        \"residual3_u\": lambda out: out[\"residual3_u\"],\n    },\n    metric={\"L2Rel\": ppsci.metric.FunctionalMetric(eval_l2_rel_func)},\n    name=\"sup_validator\",\n)\nvalidator = {sup_validator.name: sup_validator}\n</code></pre> <p>\u8bc4\u4f30\u6307\u6807\u4e3a\u9884\u6d4b\u7ed3\u679c\u548c\u771f\u5b9e\u7ed3\u679c\u7684 L2 \u76f8\u5bf9\u8bef\u5dee\u503c\uff0c\u8fd9\u91cc\u9700\u81ea\u5b9a\u4e49\u6307\u6807\u8ba1\u7b97\u51fd\u6570\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code>def eval_l2_rel_func(\n    output_dict: Dict[str, paddle.Tensor],\n    label_dict: Dict[str, paddle.Tensor],\n    *args,\n) -&gt; Dict[str, paddle.Tensor]:\n    u_pred = paddle.concat(\n        [\n            output_dict[\"residual1_u\"],\n            output_dict[\"residual2_u\"],\n            output_dict[\"residual3_u\"],\n        ]\n    )\n\n    # the shape of label_dict[\"residual_u_exact\"] is [22387, 1], and be cut into [18211, 1] `_eval_by_dataset`(ppsci/solver/eval.py).\n    u_exact = paddle.concat(\n        [\n            label_dict[\"residual_u_exact\"],\n            label_dict[\"residual2_u_exact\"],\n            label_dict[\"residual3_u_exact\"],\n        ]\n    )\n\n    error_total = paddle.linalg.norm(\n        u_exact.flatten() - u_pred.flatten(), 2\n    ) / paddle.linalg.norm(u_exact.flatten(), 2)\n    return {\"l2_error\": error_total}\n</code></pre>"},{"location":"zh/examples/xpinns/#37","title":"3.7 \u6a21\u578b\u8bad\u7ec3\u8bc4\u4f30","text":"<p>\u5b8c\u6210\u4e0a\u8ff0\u8bbe\u7f6e\u4e4b\u540e\uff0c\u53ea\u9700\u8981\u5c06\u4e0a\u8ff0\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u6309\u987a\u5e8f\u4f20\u9012\u7ed9 <code>ppsci.solver.Solver</code>\uff0c\u7136\u540e\u542f\u52a8\u8bad\u7ec3\u3001\u8bc4\u4f30\u3002</p> <pre><code># initialize solver\nsolver = ppsci.solver.Solver(\n    custom_model,\n    constraint,\n    optimizer=optimizer,\n    validator=validator,\n    cfg=cfg,\n)\n\nsolver.train()\nsolver.eval()\n</code></pre>"},{"location":"zh/examples/xpinns/#38","title":"3.8 \u7ed3\u679c\u53ef\u89c6\u5316","text":"<p>\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\u7a0b\u5e8f\u4f1a\u5bf9\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u4ee5\u56fe\u7247\u7684\u5f62\u5f0f\u5bf9\u7ed3\u679c\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <pre><code># visualize prediction\nwith solver.no_grad_context_manager(True):\n    for index, (_input, _label, _) in enumerate(sup_validator.data_loader):\n        u_exact = _label[\"residual_u_exact\"]\n        output_ = custom_model(_input)\n        u_pred = paddle.concat(\n            [output_[\"residual1_u\"], output_[\"residual2_u\"], output_[\"residual3_u\"]]\n        )\n\n        plotting.log_image(\n            residual1_x=_input[\"residual1_x\"],\n            residual1_y=_input[\"residual1_y\"],\n            residual2_x=_input[\"residual2_x\"],\n            residual2_y=_input[\"residual2_y\"],\n            residual3_x=_input[\"residual3_x\"],\n            residual3_y=_input[\"residual3_y\"],\n            interface1_x=_input[\"interface1_x\"],\n            interface1_y=_input[\"interface1_y\"],\n            interface2_x=_input[\"interface2_x\"],\n            interface2_y=_input[\"interface2_y\"],\n            boundary_x=_input[\"boundary_x\"],\n            boundary_y=_input[\"boundary_y\"],\n            residual_u_pred=u_pred,\n            residual_u_exact=u_exact,\n        )\n</code></pre>"},{"location":"zh/examples/xpinns/#4","title":"4. \u5b8c\u6574\u4ee3\u7801","text":"xpinn.py<pre><code># Copyright (c) 2024 PaddlePaddle Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport hydra\nimport model\nimport numpy as np\nimport paddle\nimport plotting\nfrom omegaconf import DictConfig\n\nimport ppsci\n\n# For the use of the second derivative: paddle.cos\npaddle.framework.core.set_prim_eager_enabled(True)\n\n\ndef _xpinn_loss(\n    training_pres: List[List[paddle.Tensor]] = None,\n    training_exacts: List[paddle.Tensor] = None,\n    training_weight: float = 1,\n    residual_inputs: List[List[paddle.Tensor]] = None,\n    residual_pres: List[paddle.Tensor] = None,\n    residual_weight: float = 1,\n    interface_inputs: List[List[paddle.Tensor]] = None,\n    interface_pres: List[paddle.Tensor] = None,\n    interface_weight: float = 1,\n    interface_neigh_pres: List[List[paddle.Tensor]] = None,\n    interface_neigh_weight: float = 1,\n    residual_func: Callable = lambda x, y: x - y,\n) -&gt; float:\n    \"\"\"XPINNs loss function for subdomain\n\n        `loss = W_u_q * MSE_u_q + W_F_q * MSE_F_q + W_I_q * MSE_avg_q + W_I_F_q * MSE_R`\n\n        `W_u_q * MSE_u_q` is data mismatch item.\n        `W_F_q * MSE_F_q` is residual item.\n        `W_I_q * MSE_avg_q` is interface item.\n        `W_I_F_q * MSE_R` is interface residual item.\n\n    Args:\n        training_pres (List[List[paddle.Tensor]], optional): the prediction result for training points input. Defaults to None.\n        training_exacts (List[paddle.Tensor], optional): the exact result for training points input. Defaults to None.\n        training_weight (float, optional): the weight of data mismatch item. Defaults to 1.\n        residual_inputs (List[List[paddle.Tensor]], optional): residual points input. Defaults to None.\n        residual_pres (List[paddle.Tensor], optional): the prediction result for residual points input. Defaults to None.\n        residual_weight (float, optional): the weight of residual item. Defaults to 1.\n        interface_inputs (List[List[paddle.Tensor]], optional): the prediction result for interface points input. Defaults to None.\n        interface_pres (List[paddle.Tensor], optional): the prediction result for interface points input. Defaults to None.\n        interface_weight (float, optional): the weight of iinterface item. Defaults to 1.\n        interface_neigh_pres (List[List[paddle.Tensor]], optional): the prediction result of neighbouring subdomain model for interface points input. Defaults to None.\n        interface_neigh_weight (float, optional): the weight of interface residual term. Defaults to 1.\n        residual_func (Callable, optional): residual calculation  function. Defaults to lambda x,y : x - y.\n    \"\"\"\n\n    def _get_grad(outputs: paddle.Tensor, inputs: paddle.Tensor) -&gt; paddle.Tensor:\n        grad = paddle.grad(outputs, inputs, retain_graph=True, create_graph=True)\n        return grad[0]\n\n    def _get_second_derivatives(\n        outputs_list: List[paddle.Tensor],\n        inputs_list: List[List[paddle.Tensor]],\n    ) -&gt; Tuple[List[List[paddle.Tensor]], List[List[paddle.Tensor]]]:\n        d1_list = [\n            [_get_grad(_out, _in) for _in in _ins]\n            for _out, _ins in zip(outputs_list, inputs_list)\n        ]\n        d2_list = [\n            [_get_grad(_d1, _in) for _d1, _in in zip(d1s_, _ins)]\n            for d1s_, _ins in zip(d1_list, inputs_list)\n        ]\n        return d2_list\n\n    residual_u_d2_list = _get_second_derivatives(residual_pres, residual_inputs)\n    interface_u_d2_list = _get_second_derivatives(interface_pres, interface_inputs)\n    interface_neigh_u_d2_list = _get_second_derivatives(\n        interface_neigh_pres, interface_inputs\n    )\n\n    MSE_u_q = 0\n\n    if training_pres is not None:\n        for _pre, _exact in zip(training_pres, training_exacts):\n            MSE_u_q += training_weight * paddle.mean(paddle.square(_pre - _exact))\n\n    MSE_F_q = 0\n\n    if residual_inputs is not None:\n        for _ins, _d2 in zip(residual_inputs, residual_u_d2_list):\n            MSE_F_q += residual_weight * paddle.mean(\n                paddle.square(residual_func(_d2, _ins))\n            )\n\n    MSE_avg_q = 0\n    MSE_R = 0\n\n    if interface_inputs is not None:\n        for _ins, _pre, _n_pres in zip(\n            interface_inputs, interface_pres, interface_neigh_pres\n        ):\n            pre_list = [_pre] + _n_pres\n            pre_avg = paddle.add_n(pre_list) / len(pre_list)\n            MSE_avg_q += interface_weight * paddle.mean(paddle.square(_pre - pre_avg))\n\n        for _ins, _d2, _n_d2 in zip(\n            interface_inputs, interface_u_d2_list, interface_neigh_u_d2_list\n        ):\n            MSE_R += interface_neigh_weight * paddle.mean(\n                paddle.square(residual_func(_d2, _ins) - residual_func(_n_d2, _ins))\n            )\n\n    return MSE_u_q + MSE_F_q + MSE_avg_q + MSE_R\n\n\ndef loss_fun(\n    output_dict: Dict[str, paddle.Tensor],\n    label_dict: Dict[str, paddle.Tensor],\n    *args,\n) -&gt; float:\n    def residual_func(output_der: paddle.Tensor, input: paddle.Tensor) -&gt; paddle.Tensor:\n        return paddle.add_n(output_der) - paddle.add_n(\n            [paddle.exp(_in) for _in in input]\n        )\n\n    # subdomain 1\n    loss1 = _xpinn_loss(\n        training_pres=[output_dict[\"boundary_u\"]],\n        training_exacts=[label_dict[\"boundary_u_exact\"]],\n        training_weight=20,\n        residual_inputs=[[output_dict[\"residual1_x\"], output_dict[\"residual1_y\"]]],\n        residual_pres=[output_dict[\"residual1_u\"]],\n        residual_weight=1,\n        interface_inputs=[\n            [output_dict[\"interface1_x\"], output_dict[\"interface1_y\"]],\n            [output_dict[\"interface2_x\"], output_dict[\"interface2_y\"]],\n        ],\n        interface_pres=[\n            output_dict[\"interface1_u_sub1\"],\n            output_dict[\"interface2_u_sub1\"],\n        ],\n        interface_weight=20,\n        interface_neigh_pres=[\n            [output_dict[\"interface1_u_sub2\"]],\n            [output_dict[\"interface2_u_sub3\"]],\n        ],\n        interface_neigh_weight=1,\n        residual_func=residual_func,\n    )\n\n    # subdomain 2\n    loss2 = _xpinn_loss(\n        residual_inputs=[[output_dict[\"residual2_x\"], output_dict[\"residual2_y\"]]],\n        residual_pres=[output_dict[\"residual2_u\"]],\n        residual_weight=1,\n        interface_inputs=[[output_dict[\"interface1_x\"], output_dict[\"interface1_y\"]]],\n        interface_pres=[output_dict[\"interface1_u_sub1\"]],\n        interface_weight=20,\n        interface_neigh_pres=[[output_dict[\"interface1_u_sub2\"]]],\n        interface_neigh_weight=1,\n        residual_func=residual_func,\n    )\n\n    # subdomain 3\n    loss3 = _xpinn_loss(\n        residual_inputs=[[output_dict[\"residual3_x\"], output_dict[\"residual3_y\"]]],\n        residual_pres=[output_dict[\"residual3_u\"]],\n        residual_weight=1,\n        interface_inputs=[[output_dict[\"interface2_x\"], output_dict[\"interface2_y\"]]],\n        interface_pres=[output_dict[\"interface2_u_sub1\"]],\n        interface_weight=20,\n        interface_neigh_pres=[[output_dict[\"interface2_u_sub3\"]]],\n        interface_neigh_weight=1,\n        residual_func=residual_func,\n    )\n\n    return {\"residuals\": loss1 + loss2 + loss3}\n\n\ndef eval_l2_rel_func(\n    output_dict: Dict[str, paddle.Tensor],\n    label_dict: Dict[str, paddle.Tensor],\n    *args,\n) -&gt; Dict[str, paddle.Tensor]:\n    u_pred = paddle.concat(\n        [\n            output_dict[\"residual1_u\"],\n            output_dict[\"residual2_u\"],\n            output_dict[\"residual3_u\"],\n        ]\n    )\n\n    # the shape of label_dict[\"residual_u_exact\"] is [22387, 1], and be cut into [18211, 1] `_eval_by_dataset`(ppsci/solver/eval.py).\n    u_exact = paddle.concat(\n        [\n            label_dict[\"residual_u_exact\"],\n            label_dict[\"residual2_u_exact\"],\n            label_dict[\"residual3_u_exact\"],\n        ]\n    )\n\n    error_total = paddle.linalg.norm(\n        u_exact.flatten() - u_pred.flatten(), 2\n    ) / paddle.linalg.norm(u_exact.flatten(), 2)\n    return {\"l2_error\": error_total}\n\n\ndef train(cfg: DictConfig):\n    # set training dataset transformation\n    def train_dataset_transform_func(\n        _input: Dict[str, np.ndarray],\n        _label: Dict[str, np.ndarray],\n        weight_: Dict[str, np.ndarray],\n    ) -&gt; Dict[str, np.ndarray]:\n        # Randomly select the residual points from sub-domains\n        id_x1 = np.random.choice(\n            _input[\"residual1_x\"].shape[0],\n            cfg.MODEL.num_residual1_points,\n            replace=False,\n        )\n        _input[\"residual1_x\"] = _input[\"residual1_x\"][id_x1, :]\n        _input[\"residual1_y\"] = _input[\"residual1_y\"][id_x1, :]\n\n        id_x2 = np.random.choice(\n            _input[\"residual2_x\"].shape[0],\n            cfg.MODEL.num_residual2_points,\n            replace=False,\n        )\n        _input[\"residual2_x\"] = _input[\"residual2_x\"][id_x2, :]\n        _input[\"residual2_y\"] = _input[\"residual2_y\"][id_x2, :]\n\n        id_x3 = np.random.choice(\n            _input[\"residual3_x\"].shape[0],\n            cfg.MODEL.num_residual3_points,\n            replace=False,\n        )\n        _input[\"residual3_x\"] = _input[\"residual3_x\"][id_x3, :]\n        _input[\"residual3_y\"] = _input[\"residual3_y\"][id_x3, :]\n\n        # Randomly select boundary points\n        id_x4 = np.random.choice(\n            _input[\"boundary_x\"].shape[0], cfg.MODEL.num_boundary_points, replace=False\n        )\n        _input[\"boundary_x\"] = _input[\"boundary_x\"][id_x4, :]\n        _input[\"boundary_y\"] = _input[\"boundary_y\"][id_x4, :]\n        _label[\"boundary_u_exact\"] = _label[\"boundary_u_exact\"][id_x4, :]\n\n        # Randomly select the interface points along two interfaces\n        id_xi1 = np.random.choice(\n            _input[\"interface1_x\"].shape[0], cfg.MODEL.num_interface1, replace=False\n        )\n        _input[\"interface1_x\"] = _input[\"interface1_x\"][id_xi1, :]\n        _input[\"interface1_y\"] = _input[\"interface1_y\"][id_xi1, :]\n\n        id_xi2 = np.random.choice(\n            _input[\"interface2_x\"].shape[0], cfg.MODEL.num_interface2, replace=False\n        )\n        _input[\"interface2_x\"] = _input[\"interface2_x\"][id_xi2, :]\n        _input[\"interface2_y\"] = _input[\"interface2_y\"][id_xi2, :]\n\n        return _input, _label, weight_\n\n    # set dataloader config\n    train_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATA_FILE,\n            \"input_keys\": cfg.TRAIN.input_keys,\n            \"label_keys\": cfg.TRAIN.label_keys,\n            \"alias_dict\": cfg.TRAIN.alias_dict,\n            \"transforms\": (\n                {\n                    \"FunctionalTransform\": {\n                        \"transform_func\": train_dataset_transform_func,\n                    },\n                },\n            ),\n        }\n    }\n\n    layer_list = (\n        cfg.MODEL.layers1,\n        cfg.MODEL.layers2,\n        cfg.MODEL.layers3,\n    )\n\n    # set model\n    custom_model = model.Model(layer_list)\n\n    # set constraint\n    sup_constraint = ppsci.constraint.SupervisedConstraint(\n        train_dataloader_cfg,\n        ppsci.loss.FunctionalLoss(loss_fun),\n        {\"residual1_u\": lambda out: out[\"residual1_u\"]},\n        name=\"sup_constraint\",\n    )\n    constraint = {sup_constraint.name: sup_constraint}\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATA_FILE,\n            \"input_keys\": cfg.TRAIN.input_keys,\n            \"label_keys\": cfg.EVAL.label_keys,\n            \"alias_dict\": cfg.EVAL.alias_dict,\n        }\n    }\n\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(loss_fun),\n        output_expr={\n            \"residual1_u\": lambda out: out[\"residual1_u\"],\n            \"residual2_u\": lambda out: out[\"residual2_u\"],\n            \"residual3_u\": lambda out: out[\"residual3_u\"],\n        },\n        metric={\"L2Rel\": ppsci.metric.FunctionalMetric(eval_l2_rel_func)},\n        name=\"sup_validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # set optimizer\n    optimizer = ppsci.optimizer.Adam(cfg.TRAIN.learning_rate)(custom_model)\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        custom_model,\n        constraint,\n        optimizer=optimizer,\n        validator=validator,\n        cfg=cfg,\n    )\n\n    solver.train()\n    solver.eval()\n\n    # visualize prediction\n    with solver.no_grad_context_manager(True):\n        for index, (_input, _label, _) in enumerate(sup_validator.data_loader):\n            u_exact = _label[\"residual_u_exact\"]\n            output_ = custom_model(_input)\n            u_pred = paddle.concat(\n                [output_[\"residual1_u\"], output_[\"residual2_u\"], output_[\"residual3_u\"]]\n            )\n\n            plotting.log_image(\n                residual1_x=_input[\"residual1_x\"],\n                residual1_y=_input[\"residual1_y\"],\n                residual2_x=_input[\"residual2_x\"],\n                residual2_y=_input[\"residual2_y\"],\n                residual3_x=_input[\"residual3_x\"],\n                residual3_y=_input[\"residual3_y\"],\n                interface1_x=_input[\"interface1_x\"],\n                interface1_y=_input[\"interface1_y\"],\n                interface2_x=_input[\"interface2_x\"],\n                interface2_y=_input[\"interface2_y\"],\n                boundary_x=_input[\"boundary_x\"],\n                boundary_y=_input[\"boundary_y\"],\n                residual_u_pred=u_pred,\n                residual_u_exact=u_exact,\n            )\n\n\ndef evaluate(cfg: DictConfig):\n    layer_list = (\n        cfg.MODEL.layers1,\n        cfg.MODEL.layers2,\n        cfg.MODEL.layers3,\n    )\n\n    custom_model = model.Model(layer_list)\n\n    # set validator\n    eval_dataloader_cfg = {\n        \"dataset\": {\n            \"name\": \"IterableMatDataset\",\n            \"file_path\": cfg.DATA_FILE,\n            \"input_keys\": cfg.TRAIN.input_keys,\n            \"label_keys\": cfg.EVAL.label_keys,\n            \"alias_dict\": cfg.EVAL.alias_dict,\n        }\n    }\n\n    sup_validator = ppsci.validate.SupervisedValidator(\n        eval_dataloader_cfg,\n        loss=ppsci.loss.FunctionalLoss(loss_fun),\n        output_expr={\n            \"residual1_u\": lambda out: out[\"residual1_u\"],\n            \"residual2_u\": lambda out: out[\"residual2_u\"],\n            \"residual3_u\": lambda out: out[\"residual3_u\"],\n        },\n        metric={\"L2Rel\": ppsci.metric.FunctionalMetric(eval_l2_rel_func)},\n        name=\"sup_validator\",\n    )\n    validator = {sup_validator.name: sup_validator}\n\n    # initialize solver\n    solver = ppsci.solver.Solver(\n        custom_model,\n        validator=validator,\n        cfg=cfg,\n    )\n\n    solver.eval()\n\n    # visualize prediction\n    with solver.no_grad_context_manager(True):\n        for index, (_input, _label, _) in enumerate(sup_validator.data_loader):\n            u_exact = _label[\"residual_u_exact\"]\n            _output = custom_model(_input)\n            u_pred = paddle.concat(\n                [_output[\"residual1_u\"], _output[\"residual2_u\"], _output[\"residual3_u\"]]\n            )\n\n            plotting.log_image(\n                residual1_x=_input[\"residual1_x\"],\n                residual1_y=_input[\"residual1_y\"],\n                residual2_x=_input[\"residual2_x\"],\n                residual2_y=_input[\"residual2_y\"],\n                residual3_x=_input[\"residual3_x\"],\n                residual3_y=_input[\"residual3_y\"],\n                interface1_x=_input[\"interface1_x\"],\n                interface1_y=_input[\"interface1_y\"],\n                interface2_x=_input[\"interface2_x\"],\n                interface2_y=_input[\"interface2_y\"],\n                boundary_x=_input[\"boundary_x\"],\n                boundary_y=_input[\"boundary_y\"],\n                residual_u_pred=u_pred,\n                residual_u_exact=u_exact,\n            )\n\n\n@hydra.main(version_base=None, config_path=\"./conf\", config_name=\"xpinn.yaml\")\ndef main(cfg: DictConfig):\n    if cfg.mode == \"train\":\n        train(cfg)\n    elif cfg.mode == \"eval\":\n        evaluate(cfg)\n    else:\n        raise ValueError(f\"cfg.mode should in ['train', 'eval'], but got '{cfg.mode}'\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"zh/examples/xpinns/#5","title":"5. \u7ed3\u679c\u5c55\u793a","text":"<p>\u4e0b\u65b9\u5c55\u793a\u4e86\u5bf9\u8ba1\u7b97\u57df\u4e2d\u6bcf\u4e2a\u70b9\u7684\u9884\u6d4b\u503c\u7ed3\u679c\u3001\u53c2\u8003\u7ed3\u679c\u548c\u76f8\u5bf9\u8bef\u5dee\u3002</p> <p> </p> \u9884\u6d4b\u7ed3\u679c\u548c\u53c2\u8003\u7ed3\u679c\u7684\u5bf9\u6bd4 <p>\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u76f8\u8fd1\uff0c\u82e5\u589e\u5927\u8bad\u7ec3\u8f6e\u6570\uff0c\u6a21\u578b\u7cbe\u5ea6\u4f1a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u3002</p>"},{"location":"zh/examples/xpinns/#6","title":"6. \u53c2\u8003\u6587\u732e","text":"<ul> <li>Extended Physics-Informed Neural Networks (XPINNs): A Generalized Space-Time Domain Decomposition Based Deep Learning Framework for Nonlinear Partial Differential Equations</li> </ul>"},{"location":"zh/tags/","title":"Tags","text":""},{"location":"zh/tags/#tags","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"zh/tags/#allen-cahn","title":"Allen-Cahn\u65b9\u7a0b","text":"<ul> <li>AllenCahn</li> </ul>"},{"location":"zh/tags/#biharmonic","title":"Biharmonic\u65b9\u7a0b","text":"<ul> <li>Biharmonic2D</li> </ul>"},{"location":"zh/tags/#deeponet","title":"DeepONet\u6c42\u89e3\u65b9\u6cd5","text":"<ul> <li>DeepONet</li> </ul>"},{"location":"zh/tags/#eulerbernoulli","title":"Euler\u2013Bernoulli\u65b9\u7a0b","text":"<ul> <li>Euler_Beam</li> </ul>"},{"location":"zh/tags/#fno","title":"FNO\u6c42\u89e3\u65b9\u6cd5","text":"<ul> <li>FourCastNet</li> </ul>"},{"location":"zh/tags/#heatexchanger","title":"HeatExchanger\u65b9\u7a0b","text":"<ul> <li>Heat_Exchanger</li> </ul>"},{"location":"zh/tags/#laplace","title":"Laplace\u65b9\u7a0b","text":"<ul> <li>Laplace2D</li> </ul>"},{"location":"zh/tags/#linearelasticity","title":"LinearElasticity\u65b9\u7a0b","text":"<ul> <li>Bracket</li> </ul>"},{"location":"zh/tags/#nlsmb","title":"NLSMB\u65b9\u7a0b","text":"<ul> <li>NLSMB</li> </ul>"},{"location":"zh/tags/#navier-stokes","title":"Navier-Stokes\u65b9\u7a0b","text":"<ul> <li>LDC2D_steady</li> </ul>"},{"location":"zh/tags/#pinn","title":"PINN\u6c42\u89e3\u65b9\u6cd5","text":"<ul> <li>AllenCahn</li> <li>Biharmonic2D</li> <li>Bracket</li> <li>Darcy2D</li> <li>Euler_Beam</li> <li>Laplace2D</li> <li>LDC2D_steady</li> <li>NLSMB</li> <li>ViV</li> </ul>"},{"location":"zh/tags/#poisson","title":"Poisson\u65b9\u7a0b","text":"<ul> <li>Darcy2D</li> </ul>"},{"location":"zh/tags/#vibration","title":"Vibration\u65b9\u7a0b","text":"<ul> <li>ViV</li> </ul>"},{"location":"zh/tags/#_1","title":"\u4e8c\u9636\u4f18\u5316","text":"<ul> <li>NLSMB</li> </ul>"},{"location":"zh/tags/#_2","title":"\u5085\u91cc\u53f6\u53d8\u6362","text":"<ul> <li>FourCastNet</li> </ul>"},{"location":"zh/tags/#_3","title":"\u51e0\u4f55\u5f62\u72b6\u5b9a\u4e49","text":"<ul> <li>Bracket</li> <li>Darcy2D</li> <li>Heat_Exchanger</li> <li>Laplace2D</li> <li>LDC2D_steady</li> <li>Volterra_IDE</li> </ul>"},{"location":"zh/tags/#_4","title":"\u590d\u6570","text":"<ul> <li>FourCastNet</li> <li>NLSMB</li> </ul>"},{"location":"zh/tags/#_5","title":"\u6570\u636e\u5904\u7406","text":"<ul> <li>FourCastNet</li> <li>Volterra_IDE</li> </ul>"},{"location":"zh/tags/#_6","title":"\u65b9\u7a0b\u7b26\u53f7\u5316\u5b9a\u4e49","text":"<ul> <li>AllenCahn</li> <li>Biharmonic2D</li> <li>Bracket</li> <li>Darcy2D</li> <li>Euler_Beam</li> <li>Heat_Exchanger</li> <li>Laplace2D</li> <li>LDC2D_steady</li> <li>NLSMB</li> <li>ViV</li> </ul>"},{"location":"zh/tags/#_7","title":"\u6982\u7387\u7edf\u8ba1","text":"<ul> <li>Biharmonic2D</li> </ul>"},{"location":"zh/tags/#_8","title":"\u79ef\u5206\u65b9\u7a0b","text":"<ul> <li>Volterra_IDE</li> </ul>"},{"location":"zh/tags/#_9","title":"\u7a00\u758f\u8ba1\u7b97","text":"<ul> <li>AMGNet</li> </ul>"},{"location":"zh/tags/#_10","title":"\u7ebf\u6027\u4ee3\u6570","text":"<ul> <li>AllenCahn</li> <li>Biharmonic2D</li> <li>Bracket</li> <li>Darcy2D</li> <li>Euler_Beam</li> <li>FourCastNet</li> <li>Laplace2D</li> <li>LDC2D_steady</li> <li>NLSMB</li> <li>ViV</li> <li>Volterra_IDE</li> </ul>"},{"location":"zh/tags/#_11","title":"\u9ad8\u9636\u5fae\u5206","text":"<ul> <li>AllenCahn</li> <li>Biharmonic2D</li> <li>Bracket</li> <li>Darcy2D</li> <li>Euler_Beam</li> <li>Laplace2D</li> <li>LDC2D_steady</li> <li>NLSMB</li> <li>ViV</li> </ul>"}]}